<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.CV](#cs.CV) [Total: 64]
- [cs.RO](#cs.RO) [Total: 16]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 12]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.SI](#cs.SI) [Total: 3]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [eess.SP](#eess.SP) [Total: 6]
- [cs.DC](#cs.DC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [math.RT](#math.RT) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for Safeguarding Small Language Models against Quantization-induced Risks and Vulnerabilities](https://arxiv.org/abs/2505.05619)
*Kalyan Nakka,Jimmy Dani,Ausmit Mondal,Nitesh Saxena*

Main category: cs.CR

TLDR: 论文提出LiteLMGuard（LLMG），一种用于量化小型语言模型（SLMs）的实时提示防护系统，旨在解决SLMs因压缩技术带来的公平性、伦理和隐私风险。LLMG通过深度学习分类任务过滤有害提示，准确率达97.75%，并能防御87%以上的有害提示。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的普及，小型语言模型（SLMs）因其轻量化特性被部署在边缘设备上，但压缩技术可能引入公平性、伦理和隐私风险，尤其是量化后的SLMs可能直接响应有害查询。

Method: 提出LLMG，一种模型无关的实时提示防护系统，通过深度学习将提示过滤任务形式化为可回答性分类问题，使用ELECTRA模型实现高精度分类。

Result: LLMG能防御87%以上的有害提示，包括直接指令和越狱攻击策略，过滤准确率达94%，平均延迟135毫秒。

Conclusion: LLMG为量化SLMs提供了一种高效、低延迟的安全防护方案，显著提升了模型的安全性和用户信任。

Abstract: The growing adoption of Large Language Models (LLMs) has influenced the
development of their lighter counterparts-Small Language Models (SLMs)-to
enable on-device deployment across smartphones and edge devices. These SLMs
offer enhanced privacy, reduced latency, server-free functionality, and
improved user experience. However, due to resource constraints of on-device
environment, SLMs undergo size optimization through compression techniques like
quantization, which can inadvertently introduce fairness, ethical and privacy
risks. Critically, quantized SLMs may respond to harmful queries directly,
without requiring adversarial manipulation, raising significant safety and
trust concerns.
  To address this, we propose LiteLMGuard (LLMG), an on-device prompt guard
that provides real-time, prompt-level defense for quantized SLMs. Additionally,
our prompt guard is designed to be model-agnostic such that it can be
seamlessly integrated with any SLM, operating independently of underlying
architectures. Our LLMG formalizes prompt filtering as a deep learning
(DL)-based prompt answerability classification task, leveraging semantic
understanding to determine whether a query should be answered by any SLM. Using
our curated dataset, Answerable-or-Not, we trained and fine-tuned several DL
models and selected ELECTRA as the candidate, with 97.75% answerability
classification accuracy.
  Our safety effectiveness evaluations demonstrate that LLMG defends against
over 87% of harmful prompts, including both direct instruction and jailbreak
attack strategies. We further showcase its ability to mitigate the Open
Knowledge Attacks, where compromised SLMs provide unsafe responses without
adversarial prompting. In terms of prompt filtering effectiveness, LLMG
achieves near state-of-the-art filtering accuracy of 94%, with an average
latency of 135 ms, incurring negligible overhead for users.

</details>

### [2] [Invariant-Based Cryptography](https://arxiv.org/abs/2505.05653)
*Stanislav Semenov*

Main category: cs.CR

TLDR: 提出一种基于离散振荡函数隐藏参数的功能不变量的新型对称密码方案，通过四点代数恒等式编码秘密整数，安全性源于结构一致性而非代数反转。


<details>
  <summary>Details</summary>
Motivation: 设计一种紧凑、自验证的机制，适用于安全认证、参数交换和轻量级通信协议。

Method: 基于离散振荡函数的功能不变量，利用四点代数恒等式编码秘密整数，并通过结构一致性确保安全性。

Result: 开发了完整的分析和模块化框架，证明了精确恒等式，定义了索引恢复程序，并分析了安全性假设。

Conclusion: 该方案是一种紧凑、自验证的机制，适用于多种安全通信场景。

Abstract: We propose a new symmetric cryptographic scheme based on functional
invariants defined over discrete oscillatory functions with hidden parameters.
The scheme encodes a secret integer through a four-point algebraic identity
preserved under controlled parameterization. Security arises not from algebraic
inversion but from structural coherence: the transmitted values satisfy an
invariant that is computationally hard to forge or invert without knowledge of
the shared secret. We develop the full analytic and modular framework, prove
exact identities, define index-recovery procedures, and analyze security
assumptions, including oscillator construction, hash binding, and invertibility
conditions. The result is a compact, self-verifying mechanism suitable for
secure authentication, parameter exchange, and lightweight communication
protocols.

</details>

### [3] [Bringing Forensic Readiness to Modern Computer Firmware](https://arxiv.org/abs/2505.05697)
*Tobias Latzo,Florian Hantke,Lukas Kotschi,Felix Freiling*

Main category: cs.CR

TLDR: 论文介绍了UEberForensIcs，一种基于UEFI的应用，用于从固件中获取内存数据，类似于冷启动攻击，并展示了其在取证中的应用。


<details>
  <summary>Details</summary>
Motivation: UEFI取代了传统的PC-BIOS，但其核心任务（启动操作系统）未变。UEFI的网络栈等功能使其适用于其他应用，如取证。

Method: 开发了UEberForensIcs，利用UEFI运行时调用的代码，从固件中获取内存数据。

Result: 成功实现了类似冷启动攻击的内存获取，并展示了其在取证中的实用性。

Conclusion: UEFI的应用潜力超出传统启动任务，UEberForensIcs为取证提供了新工具。

Abstract: Today's computer systems come with a pre-installed tiny operating system,
which is also known as UEFI. UEFI has slowly displaced the former legacy
PC-BIOS while the main task has not changed: It is responsible for booting the
actual operating system. However, features like the network stack make it also
useful for other applications. This paper introduces UEberForensIcs, a UEFI
application that makes it easy to acquire memory from the firmware, similar to
the well-known cold boot attacks. There is even UEFI code called by the
operating system during runtime, and we demonstrate how to utilize this for
forensic purposes.

</details>

### [4] [LLM-Text Watermarking based on Lagrange Interpolation](https://arxiv.org/abs/2505.05712)
*Jarosław Janas,Paweł Morawiecki,Josef Pieprzyk*

Main category: cs.CR

TLDR: 提出了一种基于拉格朗日插值的LLM生成文本水印方案，用于追踪作者身份，即使文本被严重篡改也能恢复。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成文本的归属问题，对抗虚假信息、抄袭等挑战。

Method: 通过嵌入连续的直线点序列（x, f(x)），利用LFSR或NFSR生成x坐标，实现水印嵌入。

Result: 实验表明，即使仅剩三个点，也能有效恢复作者身份。

Conclusion: 该方法高效且抗篡改，适用于高安全需求场景。

Abstract: The rapid advancement of LLMs (Large Language Models) has established them as
a foundational technology for many AI and ML powered human computer
interactions. A critical challenge in this context is the attribution of
LLM-generated text, either to the specific language model used or to the
individual user who generated it. This is essential for combating
misinformation, fake news, misinterpretation, and plagiarism. One of the key
techniques for addressing this issue is watermarking.
  This work presents a watermarking scheme for LLM-generated text based on
Lagrange interpolation, which enables the recovery of a secret author identity
even when the text has been heavily redacted by an adversary. The core idea is
to embed a continuous sequence of points (x, f(x)) that lie on a single
straight line. The x-coordinates are generated pseudorandomly using either an
LFSR (when security is not a priority) or a cryptographically secure NFSR for
high-security applications. The scheme efficiency and resilience to adversarial
modifications are analysed. Experimental results show that the proposed method
is highly effective, allowing the recovery of the author identity when as few
as three points survive adversarial manipulation.

</details>

### [5] [Efficient Full-Stack Private Federated Deep Learning with Post-Quantum Security](https://arxiv.org/abs/2505.05751)
*Yiwei Zhang,Rouzbeh Behnia,Attila A. Yavuz,Reza Ebrahimi,Elisa Bertino*

Main category: cs.CR

TLDR: Beskar是一个新颖的联邦学习框架，结合了后量子安全聚合和差分隐私，解决了隐私保护和计算效率的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护用户数据隐私的同时仍面临隐私攻击的威胁，现有方法如安全聚合和差分隐私各有局限，缺乏协同作用。

Method: Beskar框架引入后量子安全聚合，优化计算开销，并结合差分隐私于不同训练阶段，定义全面的威胁模型。

Result: 框架分析了安全、性能和模型准确性之间的权衡，首次全面研究了后量子安全聚合与多种差分隐私方法的结合。

Conclusion: Beskar旨在解决联邦学习的隐私和安全问题，确保量子安全性和鲁棒性能。

Abstract: Federated learning (FL) enables collaborative model training while preserving
user data privacy by keeping data local. Despite these advantages, FL remains
vulnerable to privacy attacks on user updates and model parameters during
training and deployment. Secure aggregation protocols have been proposed to
protect user updates by encrypting them, but these methods often incur high
computational costs and are not resistant to quantum computers. Additionally,
differential privacy (DP) has been used to mitigate privacy leakages, but
existing methods focus on secure aggregation or DP, neglecting their potential
synergies. To address these gaps, we introduce Beskar, a novel framework that
provides post-quantum secure aggregation, optimizes computational overhead for
FL settings, and defines a comprehensive threat model that accounts for a wide
spectrum of adversaries. We also integrate DP into different stages of FL
training to enhance privacy protection in diverse scenarios. Our framework
provides a detailed analysis of the trade-offs between security, performance,
and model accuracy, representing the first thorough examination of secure
aggregation protocols combined with various DP approaches for post-quantum
secure FL. Beskar aims to address the pressing privacy and security issues FL
while ensuring quantum-safety and robust performance.

</details>

### [6] [Intrusion Detection System Using Deep Learning for Network Security](https://arxiv.org/abs/2505.05810)
*Soham Chatterjee,Satvik Chaudhary,Aswani Kumar Cherukuri*

Main category: cs.CR

TLDR: 论文提出了一种基于深度学习技术的入侵检测系统（IDS）模型实验评估，通过分析多种架构（如CNN、ANN和LSTM）在真实网络流量环境中的表现，最佳模型准确率达96%。


<details>
  <summary>Details</summary>
Motivation: 随着网络攻击数量和复杂性的增加，传统入侵检测方法难以应对，需要更高效的IDS解决方案。

Method: 实验评估了多种深度学习模型（CNN、ANN、LSTM），在模拟的真实网络流量环境中进行分类测试。

Result: 最佳模型准确率达到96%，证明了深度学习在提升IDS效率和响应速度方面的潜力。

Conclusion: 研究展示了不同架构的有效性及其权衡，为开发自适应IDS解决方案和提升网络安全提供了框架。

Abstract: As the number of cyberattacks and their particualr nature escalate, the need
for effective intrusion detection systems (IDS) has become indispensable for
ensuring the security of contemporary networks. Adaptive and more sophisticated
threats are often beyond the reach of traditional approaches to intrusion
detection and access control. This paper proposes an experimental evaluation of
IDS models based on deep learning techniques, focusing on the classification of
network traffic into malicious and benign categories. We analyze and retrain an
assortment of architectures, such as Convolutional Neural Networks (CNN),
Artificial Neural Networks (ANN), and LSTM models. Each model was tested based
on a real dataset simulated in a multi-faceted and everchanging network traffic
environment. Among the tested models, the best achieved an accuracy of 96
percent, underscoring the potential of deep learning models in improving
efficiency and rapid response in IDS systems. The goal of the research is to
demonstrate the effectiveness of distinct architectures and their corresponding
trade-offs to enhance framework development for adaptive IDS solutions and
improve overall network security.

</details>

### [7] [Enhancing Noisy Functional Encryption for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2505.05843)
*Linda Scheu-Hachtel,Jasmin Zalonis*

Main category: cs.CR

TLDR: 论文扩展了噪声多输入功能加密（NMIFE）为动态噪声多客户端功能加密（DyNMCFE），提出DyNo方案，更高效且安全，并应用于隐私保护机器学习（PPML）。


<details>
  <summary>Details</summary>
Motivation: 功能加密（FE）在隐私保护机器学习（PPML）中具有独特优势，但现有方案在灵活性和安全性上存在不足。

Method: 提出DyNMCFE定义，并设计DyNo方案，支持动态多客户端和标签访问控制，提升效率和安全性。

Result: DyNo方案在空间和运行时效率上显著提升，支持客户端腐败，并成功应用于PPML中的逻辑回归训练。

Conclusion: DyNMCFE和DyNo为PPML提供了更灵活、高效和安全的解决方案。

Abstract: Functional encryption (FE) has recently attracted interest in
privacy-preserving machine learning (PPML) for its unique ability to compute
specific functions on encrypted data. A related line of work focuses on noisy
FE, which ensures differential privacy in the output while keeping the data
encrypted. We extend the notion of noisy multi-input functional encryption
(NMIFE) to (dynamic) noisy multi-client functional encryption ((Dy)NMCFE),
which allows for more flexibility in the number of data holders and analyses,
while protecting the privacy of the data holder with fine-grained access
through the usage of labels. Following our new definition of DyNMCFE, we
present DyNo, a concrete inner-product DyNMCFE scheme. Our scheme captures all
the functionalities previously introduced in noisy FE schemes, while being
significantly more efficient in terms of space and runtime and fulfilling a
stronger security notion by allowing the corruption of clients. To further
prove the applicability of DyNMCFE, we present a protocol for PPML based on
DyNo. According to this protocol, we train a privacy-preserving logistic
regression.

</details>

### [8] [AgentXploit: End-to-End Redteaming of Black-Box AI Agents](https://arxiv.org/abs/2505.05849)
*Zhun Wang,Vincent Siu,Zhe Ye,Tianneng Shi,Yuzhou Nie,Xuandong Zhao,Chenguang Wang,Wenbo Guo,Dawn Song*

Main category: cs.CR

TLDR: AgentXploit是一个黑盒模糊测试框架，用于自动发现和利用LLM代理中的间接提示注入漏洞，效果显著。


<details>
  <summary>Details</summary>
Motivation: LLM代理的强大功能引入了间接提示注入的安全风险，需要一种自动化的方法来发现和利用这些漏洞。

Method: 通过构建高质量种子语料库，结合MCTS算法迭代优化输入，以最大化发现漏洞的可能性。

Result: 在AgentDojo和VWA-adv基准测试中，成功率分别达到71%和70%，远超基线攻击。

Conclusion: AgentXploit不仅高效，还具有跨任务和防御的强迁移能力，成功应用于现实场景。

Abstract: The strong planning and reasoning capabilities of Large Language Models
(LLMs) have fostered the development of agent-based systems capable of
leveraging external tools and interacting with increasingly complex
environments. However, these powerful features also introduce a critical
security risk: indirect prompt injection, a sophisticated attack vector that
compromises the core of these agents, the LLM, by manipulating contextual
information rather than direct user prompts. In this work, we propose a generic
black-box fuzzing framework, AgentXploit, designed to automatically discover
and exploit indirect prompt injection vulnerabilities across diverse LLM
agents. Our approach starts by constructing a high-quality initial seed corpus,
then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)
to iteratively refine inputs, thereby maximizing the likelihood of uncovering
agent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo
and VWA-adv, where it achieves 71% and 70% success rates against agents based
on o3-mini and GPT-4o, respectively, nearly doubling the performance of
baseline attacks. Moreover, AgentXploit exhibits strong transferability across
unseen tasks and internal LLMs, as well as promising results against defenses.
Beyond benchmark evaluations, we apply our attacks in real-world environments,
successfully misleading agents to navigate to arbitrary URLs, including
malicious sites.

</details>

### [9] [A Taxonomy of Attacks and Defenses in Split Learning](https://arxiv.org/abs/2505.05872)
*Aqsa Shabbir,Halil İbrahim Kanpak,Alptekin Küpçü,Sinem Sav*

Main category: cs.CR

TLDR: 本文系统化研究了Split Learning（SL）中的攻击与防御，提出了分类框架，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: SL作为一种分布式深度学习范式，虽具潜力，但仍面临隐私与安全威胁，现有防御机制缺乏系统性理解。

Method: 通过分类攻击与防御的三个维度（策略、约束、有效性），构建了全面的分类体系。

Result: 提出了SL中攻击与防御的分类框架，并识别了关键挑战与研究空白。

Conclusion: 研究为SL的安全性提供了系统化视角，并指明了未来研究的潜在方向。

Abstract: Split Learning (SL) has emerged as a promising paradigm for distributed deep
learning, allowing resource-constrained clients to offload portions of their
model computation to servers while maintaining collaborative learning. However,
recent research has demonstrated that SL remains vulnerable to a range of
privacy and security threats, including information leakage, model inversion,
and adversarial attacks. While various defense mechanisms have been proposed, a
systematic understanding of the attack landscape and corresponding
countermeasures is still lacking. In this study, we present a comprehensive
taxonomy of attacks and defenses in SL, categorizing them along three key
dimensions: employed strategies, constraints, and effectiveness. Furthermore,
we identify key open challenges and research gaps in SL based on our
systematization, highlighting potential future directions.

</details>

### [10] [Exploring the Susceptibility to Fraud of Monetary Incentive Mechanisms for Strengthening FOSS Projects](https://arxiv.org/abs/2505.05897)
*Ben Swierzy,Timo Pohl,Marc Ohm,Michael Meier*

Main category: cs.CR

TLDR: 研究探讨了FOSS（自由开源软件）的外部资金激励需求，并比较了集中式（STF）与分散式（tea）激励机制的欺诈风险。


<details>
  <summary>Details</summary>
Motivation: 随着FOSS在IT系统中的普及，其可持续性和安全性问题日益突出，但现有研究未深入探讨外部激励的必要性和欺诈风险。

Method: 通过文献综述评估资金需求，并通过案例研究（STF和tea项目）比较集中式与分散式激励的欺诈风险。

Result: 非商业激励填补了项目长期可持续性的空白；STF抗欺诈能力强，而tea项目易受欺诈（如npm上的Sybil攻击）。

Conclusion: 使用定量仓库指标时需特别谨慎，无论是否存在欺诈风险。

Abstract: Free and open source software (FOSS) is ubiquitous on modern IT systems,
accelerating the speed of software engineering over the past decades. With its
increasing importance and historical reliance on uncompensated contributions,
questions have been raised regarding the continuous maintenance of FOSS and its
implications from a security perspective. In recent years, different funding
programs have emerged to provide external incentives to reinforce community
FOSS' sustainability. Past research primarily focused on analyses what type of
projects have been funded and for what reasons. However, it has neither been
considered whether there is a need for such external incentives, nor whether
the incentive mechanisms, especially with the development of decentralized
approaches, are susceptible to fraud. In this study, we explore the need for
funding through a literature review and compare the susceptibility to fraud of
centralized and decentralized incentive programs by performing case studies on
the Sovereign Tech Fund (STF) and the tea project. We find non-commercial
incentives to fill an important gap, ensuring longevity and sustainability of
projects. Furthermore, we find the STF to be able to achieve a high resilience
against fraud attempts, while tea is highly susceptible to fraud, as evidenced
by revelation of an associated sybil attack on npm. Our results imply that
special considerations must be taken into account when utilizing quantitative
repository metrics regardless whether spoofing is expected.

</details>

### [11] [Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward Secure Inference in FinTech Applications](https://arxiv.org/abs/2505.05920)
*Faneela,Baraq Ghaleb,Jawad Ahmad,William J. Buchanan,Sana Ullah Jan*

Main category: cs.CR

TLDR: PP-FinTech是一种基于CKKS的全同态加密方案，用于金融应用，通过混合核和自适应阈值机制实现隐私保护，性能接近明文模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习在金融云环境中的应用引发数据安全和隐私问题，全同态加密虽能解决但计算成本高。

Method: 采用CKKS加密的软间隔SVM，结合混合核和非线性模式建模，以及自适应阈值机制。

Result: 在信用卡审批数据集上表现接近明文模型，平衡了隐私和效率。

Conclusion: PP-FinTech为金融机器学习系统提供了一种隐私保护且高效的解决方案。

Abstract: The growing use of machine learning in cloud environments raises critical
concerns about data security and privacy, especially in finance. Fully
Homomorphic Encryption (FHE) offers a solution by enabling computations on
encrypted data, but its high computational cost limits practicality. In this
paper, we propose PP-FinTech, a privacy-preserving scheme for financial
applications that employs a CKKS-based encrypted soft-margin SVM, enhanced with
a hybrid kernel for modeling non-linear patterns and an adaptive thresholding
mechanism for robust encrypted classification. Experiments on the Credit Card
Approval dataset demonstrate comparable performance to the plaintext models,
highlighting PP-FinTech's ability to balance privacy, and efficiency in secure
financial ML systems.

</details>

### [12] [CAPE: Context-Aware Prompt Perturbation Mechanism with Differential Privacy](https://arxiv.org/abs/2505.05922)
*Haoqi Wu,Wei Dai,Li Wang,Qiang Yan*

Main category: cs.CR

TLDR: Cape是一种基于差分隐私的上下文感知提示扰动机制，旨在提高大型语言模型（LLMs）的隐私-效用平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在文本理解和生成方面表现出色，但其在推理服务中的广泛应用引发了用户敏感数据泄露的担忧。现有解决方案在效率、隐私和效用之间存在权衡。

Method: 提出Cape，结合差分隐私和上下文感知提示扰动，引入混合效用函数以更好地捕捉标记相似性，并提出桶化采样机制处理大采样空间。

Result: 在多个数据集上的实验表明，Cape在隐私-效用平衡上优于现有最优方法。

Conclusion: Cape通过改进的隐私保护机制，为LLMs的高效推理提供了更好的解决方案。

Abstract: Large Language Models (LLMs) have gained significant popularity due to their
remarkable capabilities in text understanding and generation. However, despite
their widespread deployment in inference services such as ChatGPT, concerns
about the potential leakage of sensitive user data have arisen. Existing
solutions primarily rely on privacy-enhancing technologies to mitigate such
risks, facing the trade-off among efficiency, privacy, and utility. To narrow
this gap, we propose Cape, a context-aware prompt perturbation mechanism based
on differential privacy, to enable efficient inference with an improved
privacy-utility trade-off. Concretely, we introduce a hybrid utility function
that better captures the token similarity. Additionally, we propose a
bucketized sampling mechanism to handle large sampling space, which might lead
to long-tail phenomenons. Extensive experiments across multiple datasets, along
with ablation studies, demonstrate that Cape achieves a better privacy-utility
trade-off compared to prior state-of-the-art works.

</details>

### [13] [Cryptanalysis of a Lattice-Based PIR Scheme for Arbitrary Database Sizes](https://arxiv.org/abs/2505.05934)
*Svenja Lage*

Main category: cs.CR

TLDR: 本文提出了一种针对Melchor和Gaborit的PIR方案的新型两阶段攻击方法，扩展了Liu和Bi的工作，适用于任意大小的数据库，并通过预处理技术显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: Melchor和Gaborit的PIR方案在通信开销和服务器计算成本之间取得了平衡，但Liu和Bi发现其在小型数据库中存在漏洞。然而，攻击的计算成本限制了其实际应用。本文旨在扩展这一攻击方法，使其适用于任意大小的数据库。

Method: 采用了一种类似二分查找的预处理技术，显著减少了需要考虑的格问题数量，从而降低了计算成本。

Result: 实验证明，使用普通笔记本电脑即可在几分钟内攻破该方案。

Conclusion: 本文提出的两阶段攻击方法不仅扩展了攻击范围，还大幅降低了计算成本，对PIR方案的安全性提出了新的挑战。

Abstract: Private Information Retrieval (PIR) schemes enable users to securely retrieve
files from a server without disclosing the content of their queries, thereby
preserving their privacy. In 2008, Melchor and Gaborit proposed a PIR scheme
that achieves a balance between communication overhead and server-side
computational cost. However, for particularly small databases, Liu and Bi
identified a vulnerability in the scheme using lattice-based methods.
Nevertheless, the rapid increase in computational cost associated with the
attack limited its practical applicability, leaving the scheme's overall
security largely intact. In this paper, we present a novel two-stage attack
that extends the work of Liu and Bi to databases of arbitrary sizes. To this
end, we employ a binary-search-like preprocessing technique, which enables a
significant reduction in the number of lattice problems that need to be
considered. Specifically, we demonstrate how to compromise the scheme in a
matter of minutes using an ordinary laptop. Our findings are substantiated
through both rigorous analytical proofs and comprehensive numerical
experiments.

</details>

### [14] [Towards Quantum Resilience: Data-Driven Migration Strategy Design](https://arxiv.org/abs/2505.05959)
*Nahid Aliyev,Ozan Cetin,Emil Huseynov*

Main category: cs.CR

TLDR: 论文提出了一种数据驱动的决策支持框架，帮助组织评估传统密码系统在量子计算威胁下的脆弱性，并推荐迁移到后量子密码的策略。


<details>
  <summary>Details</summary>
Motivation: 量子计算的进步威胁到传统密码系统（如RSA和ECC），需要动态解决方案来指导组织迁移到量子安全的密码学。

Method: 通过半合成数据集（包含密钥大小、网络复杂性和敏感度等特征），使用决策树和随机森林模型训练分类器，推荐迁移策略（如持续监控、计划迁移或立即混合实施）。

Result: 研究结果突出了影响策略决策的关键特征，并提供了基于系统背景的可操作建议。

Conclusion: 该框架为组织提供了结构化的迁移路线图，支持动态评估和量子安全的密码现代化。

Abstract: The advancements in quantum computing are a threat to classical cryptographic
systems. The traditional cryptographic methods that utilize factorization-based
or discrete-logarithm-based algorithms, such as RSA and ECC, are some of these.
This paper thoroughly investigates the vulnerabilities of traditional
cryptographic methods against quantum attacks and provides a decision-support
framework to help organizations in recommending mitigation plans and
determining appropriate transition strategies to post-quantum cryptography. A
semi-synthetic dataset, consisting of key features such as key size, network
complexity, and sensitivity levels, is crafted, with each configuration labeled
according to its recommended mitigation plan. Using decision tree and random
forest models, a classifier is trained to recommend appropriate
mitigation/transition plans such as continuous monitoring, scheduled
transitions, and immediate hybrid implementation. The proposed approach
introduces a data-driven and dynamic solution for organizations to assess the
scale of the migration, specifying a structured roadmap toward quantum
resilience. The results highlight important features that influence strategy
decisions and support actionable recommendations for cryptographic
modernization based on system context.

</details>

### [15] [HashKitty: Distributed Password Analysis](https://arxiv.org/abs/2505.06084)
*Pedro Antunes,Tomás Santos,Daniel Fuentes,Luís Frazão*

Main category: cs.CR

TLDR: HashKitty是一个基于hashcat的分布式密码分析平台，旨在提高攻防安全操作的效率。


<details>
  <summary>Details</summary>
Motivation: 利用和优化hashcat工具，开发一个连接不同计算节点的中心平台，支持多样化设备和操作系统，实现分布式密码分析。

Method: 采用可扩展的模块化分布式架构，包括计算节点、集成控制软件、Web平台和数据库服务器，使用WebSocket实现实时通信。

Result: 平台成功实现了目标，展示了在不同类型节点上的高效任务分配和密码分析能力。

Conclusion: HashKitty是一个有效的分布式密码分析解决方案，适用于多样化设备和操作系统。

Abstract: This article documents the HashKitty platform, a distributed solution for
password analysis based on the hashcat tool, designed to improve efficiency in
both offensive and defensive security operations. The main objectives of this
work are to utilise and characterise the hashcat tool, to develop a central
platform that connects various computational nodes, to allow the use of nodes
with different equipment and manufacturers, to distribute tasks among the nodes
through a web platform, and to perform distributed password analysis. The
results show that the presented solution achieves the proposed objectives,
demonstrating effectiveness in workload distribution and password analysis
using different types of nodes based on various operating systems and
architectures. The architecture of HashKitty is based on a scalable and modular
distributed architecture, composed of several components such as computational
nodes, integration and control software, a web platform that implements our
API, and database servers. In order to achieve a fast and organised development
process for our application we used multiple frameworks, runtimes and
libraries. For the communication between the computational nodes and the other
software we made use of websockets so that we have real-time updates between
them.

</details>

### [16] [Self-Supervised Federated GNSS Spoofing Detection with Opportunistic Data](https://arxiv.org/abs/2505.06171)
*Wenjie Liu,Panos Papadimitratos*

Main category: cs.CR

TLDR: 论文提出了一种基于自监督联邦学习的GNSS欺骗检测框架，解决了传统深度学习方法需要大量标注数据、计算资源消耗大及隐私问题。


<details>
  <summary>Details</summary>
Motivation: GNSS易受欺骗攻击，传统深度学习方法存在数据标注、计算资源和隐私问题，因此需要一种更高效且隐私保护的解决方案。

Method: 采用自监督联邦学习框架，结合LSTM网络和本地生成的标签，通过FedAvg算法聚合模型参数。

Result: 该框架在检测欺骗攻击方面优于基于位置和深度学习的方法，同时保护数据隐私。

Conclusion: 自监督联邦学习框架为GNSS欺骗检测提供了一种高效且隐私保护的解决方案。

Abstract: Global navigation satellite systems (GNSS) are vulnerable to spoofing
attacks, with adversarial signals manipulating the location or time information
of receivers, potentially causing severe disruptions. The task of discerning
the spoofing signals from benign ones is naturally relevant for machine
learning, thus recent interest in applying it for detection. While deep
learning-based methods are promising, they require extensive labeled datasets,
consume significant computational resources, and raise privacy concerns due to
the sensitive nature of position data. This is why this paper proposes a
self-supervised federated learning framework for GNSS spoofing detection. It
consists of a cloud server and local mobile platforms. Each mobile platform
employs a self-supervised anomaly detector using long short-term memory (LSTM)
networks. Labels for training are generated locally through a
spoofing-deviation prediction algorithm, ensuring privacy. Local models are
trained independently, and only their parameters are uploaded to the cloud
server, which aggregates them into a global model using FedAvg. The updated
global model is then distributed back to the mobile platforms and trained
iteratively. The evaluation shows that our self-supervised federated learning
framework outperforms position-based and deep learning-based methods in
detecting spoofing attacks while preserving data privacy.

</details>

### [17] [Leakage-resilient Algebraic Manipulation Detection Codes with Optimal Parameters](https://arxiv.org/abs/2505.06174)
*Divesh Aggarwal,Tomasz Kazana,Maciej Obremski*

Main category: cs.CR

TLDR: 本文研究了泄漏弹性代数操作检测（AMD）码的泄漏率r和码率k的界限，证明了2r + k < 1和r + k < 1的界限，并提出了接近最优的构造。


<details>
  <summary>Details</summary>
Motivation: 传统AMD码假设敌手无法看到码字，但在某些应用中敌手可能拥有部分信息。因此需要研究泄漏弹性AMD码的安全性和构造。

Method: 通过理论分析证明了泄漏率和码率的界限，并构造了接近最优的泄漏弹性AMD码。

Result: 证明了2r + k < 1和r + k < 1的界限，并展示了接近最优的构造。

Conclusion: 泄漏弹性AMD码的界限和构造是接近最优的，但在计算受限的泄漏函数下可以突破这些界限。

Abstract: Algebraic Manipulation Detection (AMD) codes is a cryptographic primitive
that was introduced by Cramer, Dodis, Fehr, Padro and Wichs. They are keyless
message authentication codes that protect messages against additive tampering
by the adversary assuming that the adversary cannot "see" the codeword. For
certain applications, it is unreasonable to assume that the adversary computes
the added offset without any knowledge of the codeword c. Recently, Ahmadi and
Safavi-Naini, and then Lin, Safavi-Naini, and Wang gave a construction of
leakage-resilient AMD codes where the adversary has some partial information
about the codeword before choosing added offset, and the scheme is secure even
conditioned on this partial information. In this paper we establish bounds on
the leakage rate r and the code rate k for leakage-resilient AMD codes. In
particular we prove that 2r + k < 1 and for the weak case (security is averaged
over a uniformly random message) r + k < 1. These bounds hold even if adversary
is polynomial-time bounded, as long as we allow leakage function to be
arbitrary. We present constructions of AMD codes that (asymptotically) fulfill
the above bounds for almost full range of parameters r and k. This shows that
the above bounds and constructions are in-fact optimal. In the last section we
show that if a leakage function is computationally bounded (we use the Ideal
Cipher Model) then it is possible to break these bounds.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Continuous Thought Machines](https://arxiv.org/abs/2505.05522)
*Luke Darlow,Ciaran Regan,Sebastian Risi,Jeffrey Seely,Llion Jones*

Main category: cs.LG

TLDR: 论文提出了一种名为连续思维机器（CTM）的模型，通过引入神经元级时间处理和神经同步作为核心表示，挑战了传统深度学习忽略时间动态的范式。


<details>
  <summary>Details</summary>
Motivation: 生物大脑的神经活动具有复杂的时间动态，而传统深度学习模型往往简化了这一点。作者希望通过CTM模型重新引入时间动态作为基础元素。

Method: CTM模型包含两个核心创新：神经元级时间处理（每个神经元使用独特权重处理历史信号）和神经同步作为潜在表示。

Result: CTM在多个任务（如图像分类、迷宫求解、问答等）中表现出色，能够适应不同计算需求，并展示丰富的内部表示。

Conclusion: CTM是迈向更生物合理且强大的人工智能系统的重要一步，尽管其目标并非追求最新性能，而是分享创新方法。

Abstract: Biological brains demonstrate complex neural activity, where the timing and
interplay between neurons is critical to how brains process information. Most
deep learning architectures simplify neural activity by abstracting away
temporal dynamics. In this paper we challenge that paradigm. By incorporating
neuron-level processing and synchronization, we can effectively reintroduce
neural timing as a foundational element. We present the Continuous Thought
Machine (CTM), a model designed to leverage neural dynamics as its core
representation. The CTM has two core innovations: (1) neuron-level temporal
processing, where each neuron uses unique weight parameters to process a
history of incoming signals; and (2) neural synchronization employed as a
latent representation. The CTM aims to strike a balance between oversimplified
neuron abstractions that improve computational efficiency, and biological
realism. It operates at a level of abstraction that effectively captures
essential temporal dynamics while remaining computationally tractable for deep
learning. We demonstrate the CTM's strong performance and versatility across a
range of challenging tasks, including ImageNet-1K classification, solving 2D
mazes, sorting, parity computation, question-answering, and RL tasks. Beyond
displaying rich internal representations and offering a natural avenue for
interpretation owing to its internal process, the CTM is able to perform tasks
that require complex sequential reasoning. The CTM can also leverage adaptive
compute, where it can stop earlier for simpler tasks, or keep computing when
faced with more challenging instances. The goal of this work is to share the
CTM and its associated innovations, rather than pushing for new
state-of-the-art results. To that end, we believe the CTM represents a
significant step toward developing more biologically plausible and powerful
artificial intelligence systems.

</details>

### [19] [A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows](https://arxiv.org/abs/2505.05525)
*Selim Mecanna,Aurore Loisy,Christophe Eloy*

Main category: cs.LG

TLDR: 论文评估了强化学习方法在部分可观测流场导航中的表现，发现PPO算法优于常用方法（如Q-Learning和A2C），并匹配理论最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决自主导航在流体中的挑战，评估强化学习算法的实际效果。

Method: 引入定向导航问题，测试Q-Learning、A2C和PPO算法在多种流场中的表现。

Result: PPO表现最佳，匹配理论最优性能，而其他算法表现较差。

Conclusion: 算法选择、实现细节和调优对复杂流场中的智能导航至关重要。

Abstract: Navigating in a fluid flow while being carried by it, using only information
accessible from on-board sensors, is a problem commonly faced by small
planktonic organisms. It is also directly relevant to autonomous robots
deployed in the oceans. In the last ten years, the fluid mechanics community
has widely adopted reinforcement learning, often in the form of its simplest
implementations, to address this challenge. But it is unclear how good are the
strategies learned by these algorithms. In this paper, we perform a
quantitative assessment of reinforcement learning methods applied to navigation
in partially observable flows. We first introduce a well-posed problem of
directional navigation for which a quasi-optimal policy is known analytically.
We then report on the poor performance and robustness of commonly used
algorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered
in the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and
two-dimensional turbulence. We show that they are vastly surpassed by PPO
(Proximal Policy Optimization), a more advanced algorithm that has established
dominance across a wide range of benchmarks in the reinforcement learning
community. In particular, our custom implementation of PPO matches the
theoretical quasi-optimal performance in turbulent flow and does so in a robust
manner. Reaching this result required the use of several additional techniques,
such as vectorized environments and generalized advantage estimation, as well
as hyperparameter optimization. This study demonstrates the importance of
algorithm selection, implementation details, and fine-tuning for discovering
truly smart autonomous navigation strategies in complex flows.

</details>

### [20] [ADMM-Based Training for Spiking Neural Networks](https://arxiv.org/abs/2505.05527)
*Giovanni Perin,Cesare Bidini,Riccardo Mazzieri,Michele Rossi*

Main category: cs.LG

TLDR: 提出了一种基于ADMM的新型SNN训练方法，解决了传统反向传播算法在SNN中的局限性。


<details>
  <summary>Details</summary>
Motivation: SNN在时间序列处理中潜力巨大但缺乏高效训练算法，传统方法存在可扩展性和数值精度问题。

Method: 采用ADMM方法解决SNN阶跃函数的不可微性问题，并推导闭式更新。

Result: 实验证明该方法具有收敛性、潜力大，并提出了改进方向。

Conclusion: ADMM为SNN训练提供了新思路，未来可进一步优化。

Abstract: In recent years, spiking neural networks (SNNs) have gained momentum due to
their high potential in time-series processing combined with minimal energy
consumption. However, they still lack a dedicated and efficient training
algorithm. The popular backpropagation with surrogate gradients, adapted from
stochastic gradient descent (SGD)-derived algorithms, has several drawbacks
when used as an optimizer for SNNs. Specifically, it suffers from low
scalability and numerical imprecision. In this paper, we propose a novel SNN
training method based on the alternating direction method of multipliers
(ADMM). Our ADMM-based training aims to solve the problem of the SNN step
function's non-differentiability. We formulate the problem, derive closed-form
updates, and empirically show the optimizer's convergence properties, great
potential, and possible new research directions to improve the method in a
simulated proof-of-concept.

</details>

### [21] [Low-bit Model Quantization for Deep Neural Networks: A Survey](https://arxiv.org/abs/2505.05530)
*Kai Liu,Qian Zheng,Kaiwen Tao,Zhiteng Li,Haotong Qin,Wenbo Li,Yong Guo,Xianglong Liu,Linghe Kong,Guihai Chen,Yulun Zhang,Xiaokang Yang*

Main category: cs.LG

TLDR: 该论文综述了深度神经网络（DNNs）低比特量化的五年进展，分类并比较了前沿量化方法，探讨了潜在研究方向。


<details>
  <summary>Details</summary>
Motivation: DNNs的高计算成本和模型大小限制了实际部署，量化技术成为关键解决方案，但精度损失问题亟待研究。

Method: 将量化方法分为8大类24子类，基于核心技术进行分类和比较。

Result: 提供了量化方法的系统分类和比较，并指出了未来研究方向。

Conclusion: 量化技术是DNN部署的关键，未来研究需关注精度损失的补偿方法。

Abstract: With unprecedented rapid development, deep neural networks (DNNs) have deeply
influenced almost all fields. However, their heavy computation costs and model
sizes are usually unacceptable in real-world deployment. Model quantization, an
effective weight-lighting technique, has become an indispensable procedure in
the whole deployment pipeline. The essence of quantization acceleration is the
conversion from continuous floating-point numbers to discrete integer ones,
which significantly speeds up the memory I/O and calculation, i.e., addition
and multiplication. However, performance degradation also comes with the
conversion because of the loss of precision. Therefore, it has become
increasingly popular and critical to investigate how to perform the conversion
and how to compensate for the information loss. This article surveys the recent
five-year progress towards low-bit quantization on DNNs. We discuss and compare
the state-of-the-art quantization methods and classify them into 8 main
categories and 24 sub-categories according to their core techniques.
Furthermore, we shed light on the potential research opportunities in the field
of model quantization. A curated list of model quantization is provided at
https://github.com/Kai-Liu001/Awesome-Model-Quantization.

</details>

### [22] [Rethinking Graph Contrastive Learning through Relative Similarity Preservation](https://arxiv.org/abs/2505.05533)
*Zhiyuan Ning,Pengfei Wang,Ziyue Qiao,Pengyang Wang,Yuanchun Zhou*

Main category: cs.LG

TLDR: 论文提出了一种新的图对比学习框架RELGCL，通过保留图中自然存在的相对相似性模式，解决了传统方法在非欧几里得图数据中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统图对比学习（GCL）方法在计算机视觉中基于绝对相似性的范式在图数据中面临挑战，因为图的离散和非欧几里得特性导致视图生成和相似性验证不可靠。

Method: 通过分析11个真实世界图数据，发现标签一致性随结构距离增加而衰减的普遍模式，并基于随机游走理论提供理论保证。提出RELGCL框架，通过成对和列表式实现保留这些模式。

Result: 实验表明，RELGCL在20种现有方法中表现最优，验证了利用自然相对相似性优于人工绝对相似性的有效性。

Conclusion: RELGCL通过捕捉图中自然的相对相似性模式，显著提升了图对比学习的性能，适用于同质性和异质性图。

Abstract: Graph contrastive learning (GCL) has achieved remarkable success by following
the computer vision paradigm of preserving absolute similarity between
augmented views. However, this approach faces fundamental challenges in graphs
due to their discrete, non-Euclidean nature -- view generation often breaks
semantic validity and similarity verification becomes unreliable. Through
analyzing 11 real-world graphs, we discover a universal pattern transcending
the homophily-heterophily dichotomy: label consistency systematically
diminishes as structural distance increases, manifesting as smooth decay in
homophily graphs and oscillatory decay in heterophily graphs. We establish
theoretical guarantees for this pattern through random walk theory, proving
label distribution convergence and characterizing the mechanisms behind
different decay behaviors. This discovery reveals that graphs naturally encode
relative similarity patterns, where structurally closer nodes exhibit
collectively stronger semantic relationships. Leveraging this insight, we
propose RELGCL, a novel GCL framework with complementary pairwise and listwise
implementations that preserve these inherent patterns through collective
similarity objectives. Extensive experiments demonstrate that our method
consistently outperforms 20 existing approaches across both homophily and
heterophily graphs, validating the effectiveness of leveraging natural relative
similarity over artificial absolute similarity.

</details>

### [23] [Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching and ResNet](https://arxiv.org/abs/2505.05538)
*Md Kamrujjaman Mobin,Md Saiful Islam,Sadik Al Barid,Md Masum*

Main category: cs.LG

TLDR: Cardioformer是一种新型多粒度混合模型，通过结合跨通道分块、分层残差学习和两阶段自注意力机制，显著提升心电图分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有心电图分类方法难以同时捕捉局部形态细节和长程时间依赖关系，限制了自动化心脏疾病诊断的准确性。

Method: Cardioformer采用多尺度令牌嵌入捕获细粒度局部特征和全局上下文信息，并通过粒内和粒间自注意力选择性融合这些表征。

Result: 在三个基准ECG数据集上，Cardioformer的AUROC分别为96.34±0.11（MIMIC-IV）、89.99±0.12（PTB-XL）和95.59±1.66（PTB），优于现有基线模型。

Conclusion: Cardioformer展示了在自动化ECG分析中的潜力，为更准确和稳健的心血管疾病诊断铺平了道路。

Abstract: Electrocardiogram (ECG) classification is crucial for automated cardiac
disease diagnosis, yet existing methods often struggle to capture local
morphological details and long-range temporal dependencies simultaneously. To
address these challenges, we propose Cardioformer, a novel multi-granularity
hybrid model that integrates cross-channel patching, hierarchical residual
learning, and a two-stage self-attention mechanism. Cardioformer first encodes
multi-scale token embeddings to capture fine-grained local features and global
contextual information and then selectively fuses these representations through
intra- and inter-granularity self-attention. Extensive evaluations on three
benchmark ECG datasets under subject-independent settings demonstrate that
model consistently outperforms four state-of-the-art baselines. Our
Cardioformer model achieves the AUROC of 96.34$\pm$0.11, 89.99$\pm$0.12, and
95.59$\pm$1.66 in MIMIC-IV, PTB-XL and PTB dataset respectively outperforming
PatchTST, Reformer, Transformer, and Medformer models. It also demonstrates
strong cross-dataset generalization, achieving 49.18% AUROC on PTB and 68.41%
on PTB-XL when trained on MIMIC-IV. These findings underscore the potential of
Cardioformer to advance automated ECG analysis, paving the way for more
accurate and robust cardiovascular disease diagnosis. We release the source
code at https://github.com/KMobin555/Cardioformer.

</details>

### [24] [Griffin: Towards a Graph-Centric Relational Database Foundation Model](https://arxiv.org/abs/2505.05568)
*Yanbo Wang,Xiyuan Wang,Quan Gan,Minjie Wang,Qibin Yang,David Wipf,Muhan Zhang*

Main category: cs.LG

TLDR: Griffin是首个专为关系数据库（RDB）设计的基础模型，通过统一数据编码器和任务解码器处理多样化任务，并引入跨注意力模块和新聚合器。


<details>
  <summary>Details</summary>
Motivation: 解决现有小模型仅针对单一RDB任务的局限性，提出统一框架以提升多任务处理能力。

Method: 结合单表和RDB数据集预训练，使用高级编码器处理分类、数值和元数据特征，并引入跨注意力模块和增强MPNN。

Result: 在大规模异构和时序图上表现优于或媲美单独训练的模型，尤其在低数据场景和跨数据集任务中表现出色。

Conclusion: Griffin展示了作为通用RDB基础模型的潜力，具备强迁移性和适应性。

Abstract: We introduce Griffin, the first foundation model attemptation designed
specifically for Relational Databases (RDBs). Unlike previous smaller models
focused on single RDB tasks, Griffin unifies the data encoder and task decoder
to handle diverse tasks. Additionally, we enhance the architecture by
incorporating a cross-attention module and a novel aggregator. Griffin utilizes
pretraining on both single-table and RDB datasets, employing advanced encoders
for categorical, numerical, and metadata features, along with innovative
components such as cross-attention modules and enhanced message-passing neural
networks (MPNNs) to capture the complexities of relational data. Evaluated on
large-scale, heterogeneous, and temporal graphs extracted from RDBs across
various domains (spanning over 150 million nodes), Griffin demonstrates
superior or comparable performance to individually trained models, excels in
low-data scenarios, and shows strong transferability with similarity and
diversity in pretraining across new datasets and tasks, highlighting its
potential as a universally applicable foundation model for RDBs. Code available
at https://github.com/yanxwb/Griffin.

</details>

### [25] [PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models](https://arxiv.org/abs/2505.05577)
*Alejandro Velez-Arce,Marinka Zitnik*

Main category: cs.LG

TLDR: PyTDC是一个开源机器学习平台，整合多模态生物数据和多种机器学习任务，提供端到端的训练、评估和推理基础设施。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学基准缺乏整合多模态数据和多样化机器学习任务的端到端基础设施。

Method: PyTDC统一分布式、异构和持续更新的数据源及模型权重，标准化基准测试和推理端点。

Result: 在单细胞药物靶点提名任务中，现有图表示学习和图论方法表现不佳，但一种上下文感知的几何深度学习方法优于基线方法。

Conclusion: PyTDC为开发多模态、上下文感知的生物医学AI基础模型提供了研究平台。

Abstract: Existing biomedical benchmarks do not provide end-to-end infrastructure for
training, evaluation, and inference of models that integrate multimodal
biological data and a broad range of machine learning tasks in therapeutics. We
present PyTDC, an open-source machine-learning platform providing streamlined
training, evaluation, and inference software for multimodal biological AI
models. PyTDC unifies distributed, heterogeneous, continuously updated data
sources and model weights and standardizes benchmarking and inference
endpoints. This paper discusses the components of PyTDC's architecture and, to
our knowledge, the first-of-its-kind case study on the introduced single-cell
drug-target nomination ML task. We find state-of-the-art methods in graph
representation learning and domain-specific methods from graph theory perform
poorly on this task. Though we find a context-aware geometric deep learning
method that outperforms the evaluated SoTA and domain-specific baseline
methods, the model is unable to generalize to unseen cell types or incorporate
additional modalities, highlighting PyTDC's capacity to facilitate an exciting
avenue of research developing multimodal, context-aware, foundation models for
open problems in biomedical AI.

</details>

### [26] [Anticipating Gaming to Incentivize Improvement: Guiding Agents in (Fair) Strategic Classification](https://arxiv.org/abs/2505.05594)
*Sura Alhanouti,Parinaz Naghizadeh*

Main category: cs.LG

TLDR: 研究探讨人类在机器学习算法决策下的策略行为，分析个体选择提升资质或欺骗算法的动机，并通过博弈论模型揭示最优分类器设计及其公平性影响。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习算法在关键决策中的作用增强，理解人类如何策略性应对这些系统变得至关重要。

Method: 采用Stackelberg博弈模型，分析个体在提升资质与欺骗算法之间的选择，并研究算法设计者如何通过公平分类器引导这些行为。

Result: 揭示了不同类型的代理人响应模式，并据此优化分类器设计，表明公平策略不仅能防止欺骗，还能激励个体提升资质。

Conclusion: 研究强调了算法设计者预见策略行为的重要性，公平策略在特定情况下能有效引导积极行为。

Abstract: As machine learning algorithms increasingly influence critical decision
making in different application areas, understanding human strategic behavior
in response to these systems becomes vital. We explore individuals' choice
between genuinely improving their qualifications (``improvement'') vs.
attempting to deceive the algorithm by manipulating their features
(``manipulation'') in response to an algorithmic decision system. We further
investigate an algorithm designer's ability to shape these strategic responses,
and its fairness implications. Specifically, we formulate these interactions as
a Stackelberg game, where a firm deploys a (fair) classifier, and individuals
strategically respond. Our model incorporates both different costs and
stochastic efficacy for manipulation and improvement. The analysis reveals
different potential classes of agent responses, and characterizes optimal
classifiers accordingly. Based on these, we highlight the impact of the firm's
anticipation of strategic behavior, identifying when and why a (fair) strategic
policy can not only prevent manipulation, but also incentivize agents to opt
for improvement.

</details>

### [27] [This part looks alike this: identifying important parts of explained instances and prototypes](https://arxiv.org/abs/2505.05597)
*Jacek Karolczak,Jerzy Stefanowski*

Main category: cs.LG

TLDR: 提出了一种新方法，通过识别原型中最具信息量的特征（称为相似部分）来改进原型解释，提升用户理解。


<details>
  <summary>Details</summary>
Motivation: 原型解释虽易于理解，但常未能突出最相关特征，需改进以增强用户理解。

Method: 利用特征重要性评分识别原型与实例重叠的最相关特征，并将其纳入原型选择算法的目标函数以提升全局原型多样性。

Result: 在六个基准数据集上的实验表明，该方法提升了用户理解，同时保持或提高了预测准确性。

Conclusion: 该方法通过强调原型中最相关特征，有效改进了原型解释的实用性。

Abstract: Although prototype-based explanations provide a human-understandable way of
representing model predictions they often fail to direct user attention to the
most relevant features. We propose a novel approach to identify the most
informative features within prototypes, termed alike parts. Using feature
importance scores derived from an agnostic explanation method, it emphasizes
the most relevant overlapping features between an instance and its nearest
prototype. Furthermore, the feature importance score is incorporated into the
objective function of the prototype selection algorithms to promote global
prototypes diversity. Through experiments on six benchmark datasets, we
demonstrate that the proposed approach improves user comprehension while
maintaining or even increasing predictive accuracy.

</details>

### [28] [The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion](https://arxiv.org/abs/2505.05605)
*Andrew Qiu,Shubham Barhate,Hin Wai Lui,Runze Su,Rafael Rios Müller,Kungang Li,Ling Leng,Han Sun,Shayan Ehsani,Zhifang Liu*

Main category: cs.LG

TLDR: 论文探讨了深度学习在广告转化预测中的应用，重点解决了嵌入表训练中的梯度稀疏性和多轮过拟合问题，并提出了一种频率自适应学习率方法。


<details>
  <summary>Details</summary>
Motivation: 在线广告中，深度学习模型需要处理高基数分类特征和多目标预测，但嵌入表训练面临梯度稀疏性和多轮过拟合的挑战。

Method: 提出了一种频率自适应学习率方法，并与嵌入表重新初始化方法进行比较，使用工业级数据集进行离线评估。

Result: 稀疏优化器加速了收敛，频率自适应学习率有效缓解了多轮过拟合问题。

Conclusion: 频率自适应学习率是一种有效的解决方案，适用于多任务模型中不同目标的过拟合问题。

Abstract: Deep learning for conversion prediction has found widespread applications in
online advertising. These models have become more complex as they are trained
to jointly predict multiple objectives such as click, add-to-cart, checkout and
other conversion types. Additionally, the capacity and performance of these
models can often be increased with the use of embedding tables that encode high
cardinality categorical features such as advertiser, user, campaign, and
product identifiers (IDs). These embedding tables can be pre-trained, but also
learned end-to-end jointly with the model to directly optimize the model
objectives. Training these large tables is challenging due to: gradient
sparsity, the high cardinality of the categorical features, the non-uniform
distribution of IDs and the very high label sparsity. These issues make
training prone to both slow convergence and overfitting after the first epoch.
Previous works addressed the multi-epoch overfitting issue by using: stronger
feature hashing to reduce cardinality, filtering of low frequency IDs,
regularization of the embedding tables, re-initialization of the embedding
tables after each epoch, etc. Some of these techniques reduce overfitting at
the expense of reduced model performance if used too aggressively. In this
paper, we share key learnings from the development of embedding table
optimization and multi-epoch training in Pinterest Ads Conversion models. We
showcase how our Sparse Optimizer speeds up convergence, and how multi-epoch
overfitting varies in severity between different objectives in a multi-task
model depending on label sparsity. We propose a new approach to deal with
multi-epoch overfitting: the use of a frequency-adaptive learning rate on the
embedding tables and compare it to embedding re-initialization. We evaluate
both methods offline using an industrial large-scale production dataset.

</details>

### [29] [On Corruption-Robustness in Performative Reinforcement Learning](https://arxiv.org/abs/2505.05609)
*Vasilis Pollatos,Debmalya Mandal,Goran Radanovic*

Main category: cs.LG

TLDR: 本文提出了一种在数据被污染的情况下进行强化学习的方法，通过凸凹优化和鲁棒梯度估计，实现了对近似稳定策略的收敛。


<details>
  <summary>Details</summary>
Motivation: 研究在数据被污染的情况下，如何扩展现有的重复训练方法，以保持强化学习的稳定性。

Method: 采用凸凹优化和一种新的鲁棒梯度估计方法，处理Huber的ε-污染模型。

Result: 证明了方法能够收敛到近似稳定策略，误差与√ε线性相关。

Conclusion: 实验表明，考虑数据污染对强化学习至关重要，提出的方法有效。

Abstract: In performative Reinforcement Learning (RL), an agent faces a
policy-dependent environment: the reward and transition functions depend on the
agent's policy. Prior work on performative RL has studied the convergence of
repeated retraining approaches to a performatively stable policy. In the finite
sample regime, these approaches repeatedly solve for a saddle point of a
convex-concave objective, which estimates the Lagrangian of a regularized
version of the reinforcement learning problem. In this paper, we aim to extend
such repeated retraining approaches, enabling them to operate under corrupted
data. More specifically, we consider Huber's $\epsilon$-contamination model,
where an $\epsilon$ fraction of data points is corrupted by arbitrary
adversarial noise. We propose a repeated retraining approach based on
convex-concave optimization under corrupted gradients and a novel
problem-specific robust mean estimator for the gradients. We prove that our
approach exhibits last-iterate convergence to an approximately stable policy,
with the approximation error linear in $\sqrt{\epsilon}$. We experimentally
demonstrate the importance of accounting for corruption in performative RL.

</details>

### [30] [SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation](https://arxiv.org/abs/2505.05625)
*Wenqing Peng,Zhi-Song Liu,Michael Boy*

Main category: cs.LG

TLDR: 提出了一种名为SPIN-ODE的框架，用于解决复杂化学反应中速率常数估计的挑战，通过三阶段优化过程实现稳定训练和收敛。


<details>
  <summary>Details</summary>
Motivation: 复杂化学反应中的速率常数估计对详细化学研究至关重要，但现实大气化学系统的刚性导致训练不稳定和收敛困难。

Method: 采用三阶段优化：1) 潜在神经ODE学习化学浓度与时间导数的连续轨迹；2) 显式化学反应神经网络提取速率系数；3) 使用神经ODE求解器微调速率系数估计。

Result: 在合成和真实数据集上的实验验证了方法的有效性和鲁棒性。

Conclusion: 作为首个针对刚性神经ODE的化学速率系数发现研究，为神经网络与详细化学的结合开辟了新方向。

Abstract: Estimating rate constants from complex chemical reactions is essential for
advancing detailed chemistry. However, the stiffness inherent in real-world
atmospheric chemistry systems poses severe challenges, leading to training
instability and poor convergence that hinder effective rate constant estimation
using learning-based approaches. To address this, we propose a Stiff
Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction
modelling. Our method introduces a three-stage optimisation process: first, a
latent neural ODE learns the continuous and differentiable trajectory between
chemical concentrations and their time derivatives; second, an explicit
Chemical Reaction Neural Network (CRNN) extracts the underlying rate
coefficients based on the learned dynamics; and third, fine-tune CRNN using a
neural ODE solver to further improve rate coefficient estimation. Extensive
experiments on both synthetic and newly proposed real-world datasets validate
the effectiveness and robustness of our approach. As the first work on stiff
Neural ODEs for chemical rate coefficient discovery, our study opens promising
directions for integrating neural networks with detailed chemistry.

</details>

### [31] [EquiHGNN: Scalable Rotationally Equivariant Hypergraph Neural Networks](https://arxiv.org/abs/2505.05650)
*Tien Dang,Truong-Son Hy*

Main category: cs.LG

TLDR: EquiHGNN是一个基于超图的神经网络框架，通过引入对称性感知表示来改进分子建模，特别适用于捕捉高阶分子相互作用。


<details>
  <summary>Details</summary>
Motivation: 传统基于图的模型仅能捕捉成对相互作用，无法充分描述分子系统中的高阶关系。超图模型能更好地建模这些复杂相互作用。

Method: 提出EquiHGNN框架，通过对称性约束（如几何和拓扑性质的保持）增强超图神经网络的表达能力。

Result: 实验表明，高阶相互作用对大型分子性能提升显著，而几何特征的加入进一步提高了模型表现。

Conclusion: EquiHGNN通过结合高阶相互作用和对称性约束，为分子建模提供了更鲁棒且物理意义明确的表示。

Abstract: Molecular interactions often involve high-order relationships that cannot be
fully captured by traditional graph-based models limited to pairwise
connections. Hypergraphs naturally extend graphs by enabling multi-way
interactions, making them well-suited for modeling complex molecular systems.
In this work, we introduce EquiHGNN, an Equivariant HyperGraph Neural Network
framework that integrates symmetry-aware representations to improve molecular
modeling. By enforcing the equivariance under relevant transformation groups,
our approach preserves geometric and topological properties, leading to more
robust and physically meaningful representations. We examine a range of
equivariant architectures and demonstrate that integrating symmetry constraints
leads to notable performance gains on large-scale molecular datasets.
Experiments on both small and large molecules show that high-order interactions
offer limited benefits for small molecules but consistently outperform 2D
graphs on larger ones. Adding geometric features to these high-order structures
further improves the performance, emphasizing the value of spatial information
in molecular learning. Our source code is available at
https://github.com/HySonLab/EquiHGNN/

</details>

### [32] [Conditional Front-door Adjustment for Heterogeneous Treatment Assignment Effect Estimation Under Non-adherence](https://arxiv.org/abs/2505.05677)
*Winston Chen,Trenton Chang,Jenna Wiens*

Main category: cs.LG

TLDR: 论文比较了标准后门调整（SBD）和条件前门调整（CFD）在非依从性情况下的治疗效果估计方差，发现CFD在真实效应较小时方差更低，并提出LobsterNet模型以减少估计误差。


<details>
  <summary>Details</summary>
Motivation: 在非依从性情况下，现有方法（如SBD和CFD）的估计方差差异未被充分研究，需要探索更优的估计方法。

Method: 理论分析和实证研究表明CFD在真实效应较小时方差更低；提出LobsterNet，一种多任务神经网络，联合建模CFD的多个干扰参数。

Result: CFD在真实效应较小时方差低于SBD；LobsterNet在半合成和真实数据集上减少了估计误差。

Conclusion: CFD结合共享干扰参数建模（如LobsterNet）可改善非依从性下的治疗效果估计。

Abstract: Estimates of heterogeneous treatment assignment effects can inform treatment
decisions. Under the presence of non-adherence (e.g., patients do not adhere to
their assigned treatment), both the standard backdoor adjustment (SBD) and the
conditional front-door adjustment (CFD) can recover unbiased estimates of the
treatment assignment effects. However, the estimation variance of these
approaches may vary widely across settings, which remains underexplored in the
literature. In this work, we demonstrate theoretically and empirically that CFD
yields lower-variance estimates than SBD when the true effect of treatment
assignment is small (i.e., assigning an intervention leads to small changes in
patients' future outcome). Additionally, since CFD requires estimating multiple
nuisance parameters, we introduce LobsterNet, a multi-task neural network that
implements CFD with joint modeling of the nuisance parameters. Empirically,
LobsterNet reduces estimation error across several semi-synthetic and
real-world datasets compared to baselines. Our findings suggest CFD with shared
nuisance parameter modeling can improve treatment assignment effect estimation
under non-adherence.

</details>

### [33] [Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A Dash-Based Approach with SHAP, LIME, and Comorbidity Insights](https://arxiv.org/abs/2505.05683)
*Udaya Allani*

Main category: cs.LG

TLDR: 基于机器学习的糖尿病风险预测工具，使用多种模型和采样策略，LightGBM表现最佳，结合SHAP和LIME解释预测，提供用户友好的交互界面。


<details>
  <summary>Details</summary>
Motivation: 开发一款基于机器学习的交互式健康风险预测工具，帮助用户评估糖尿病风险并提升健康意识。

Method: 使用2015年CDC BRFSS数据集，评估Logistic回归、随机森林、XGBoost、LightGBM、KNN和神经网络模型，采用原始数据、SMOTE和欠采样策略。

Result: LightGBM结合欠采样策略在召回率上表现最佳，适合风险检测。工具整合SHAP和LIME解释预测，并通过Pearson分析突出共病相关性。

Conclusion: 该工具通过Dash界面提供用户友好的交互功能，支持数据驱动的健康意识提升，LightGBM是糖尿病风险预测的理想选择。

Abstract: This study presents a web-based interactive health risk prediction tool
designed to assess diabetes risk using machine learning models. Built on the
2015 CDC BRFSS dataset, the study evaluates models including Logistic
Regression, Random Forest, XGBoost, LightGBM, KNN, and Neural Networks under
original, SMOTE, and undersampling strategies. LightGBM with undersampling
achieved the best recall, making it ideal for risk detection. The tool
integrates SHAP and LIME to explain predictions and highlights comorbidity
correlations using Pearson analysis. A Dash-based UI enables user-friendly
interaction with model predictions, personalized suggestions, and feature
insights, supporting data-driven health awareness.

</details>

### [34] [Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning](https://arxiv.org/abs/2505.05702)
*Seongjin Choi,Gahee Kim,Yong-Geun Oh*

Main category: cs.LG

TLDR: 通过对称单纯集解决超图中邻接关系和方向系统的缺失问题，提出Hypergraph Neural Sheaf Diffusion（HNSD）方法，并在实验中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 超图中缺乏固有的邻接关系和方向系统，导致构建任意度的层拉普拉斯算子存在挑战。

Method: 利用对称单纯集编码超边内的所有可能方向子关系，定义邻接关系并保留超边来源，提出HNSD方法。

Result: HNSD在实验中表现出竞争力，验证了方法的有效性。

Conclusion: 该方法解决了超图学习中的方向模糊和邻接稀疏问题，为超图神经网络提供了理论基础。

Abstract: The absence of intrinsic adjacency relations and orientation systems in
hypergraphs creates fundamental challenges for constructing sheaf Laplacians of
arbitrary degrees. We resolve these limitations through symmetric simplicial
sets derived directly from hypergraphs, which encode all possible oriented
subrelations within each hyperedge as ordered tuples. This construction
canonically defines adjacency via facet maps while inherently preserving
hyperedge provenance. We establish that the normalized degree zero sheaf
Laplacian on our induced symmetric simplicial set reduces exactly to the
traditional graph normalized sheaf Laplacian when restricted to graphs,
validating its mathematical consistency with prior graph-based sheaf theory.
Furthermore, the induced structure preserves all structural information from
the original hypergraph, ensuring that every multi-way relational detail is
faithfully retained. Leveraging this framework, we introduce Hypergraph Neural
Sheaf Diffusion (HNSD), the first principled extension of Neural Sheaf
Diffusion (NSD) to hypergraphs. HNSD operates via normalized degree zero sheaf
Laplacians over symmetric simplicial sets, resolving orientation ambiguity and
adjacency sparsity inherent to hypergraph learning. Experimental evaluations
demonstrate HNSD's competitive performance across established benchmarks.

</details>

### [35] [Crowding Out The Noise: Algorithmic Collective Action Under Differential Privacy](https://arxiv.org/abs/2505.05707)
*Rushabh Solanki,Meghana Bhange,Ulrich Aïvodji,Elliot Creager*

Main category: cs.LG

TLDR: 论文探讨了AI的负责任部署，特别是通过算法集体行动（Algorithmic Collective Action）影响AI学习过程的可能性，并研究了差分隐私（DPSGD）对集体行动效果的影响。


<details>
  <summary>Details</summary>
Motivation: AI的广泛应用引发了对其潜在危害和社会不平等的担忧，研究如何通过集体行动和隐私保护技术（如差分隐私）平衡AI的可信度与用户影响力。

Method: 通过理论分析和实验模拟，研究差分隐私（DPSGD）对算法集体行动效果的影响，包括集体规模和隐私参数的作用。

Result: 差分隐私保护了个人数据，但削弱了集体行动的有效性；研究量化了集体行动成功的下限与隐私参数的关系。

Conclusion: 差分隐私与集体行动之间存在权衡，需在隐私保护和用户影响力之间找到平衡。

Abstract: The integration of AI into daily life has generated considerable attention
and excitement, while also raising concerns about automating algorithmic harms
and re-entrenching existing social inequities. While the responsible deployment
of trustworthy AI systems is a worthy goal, there are many possible ways to
realize it, from policy and regulation to improved algorithm design and
evaluation. In fact, since AI trains on social data, there is even a
possibility for everyday users, citizens, or workers to directly steer its
behavior through Algorithmic Collective Action, by deliberately modifying the
data they share with a platform to drive its learning process in their favor.
This paper considers how these grassroots efforts to influence AI interact with
methods already used by AI firms and governments to improve model
trustworthiness. In particular, we focus on the setting where the AI firm
deploys a differentially private model, motivated by the growing regulatory
focus on privacy and data protection. We investigate how the use of
Differentially Private Stochastic Gradient Descent (DPSGD) affects the
collective's ability to influence the learning process. Our findings show that
while differential privacy contributes to the protection of individual data, it
introduces challenges for effective algorithmic collective action. We
characterize lower bounds on the success of algorithmic collective action under
differential privacy as a function of the collective's size and the firm's
privacy parameters, and verify these trends experimentally by simulating
collective action during the training of deep neural network classifiers across
several datasets.

</details>

### [36] [Automated Learning of Semantic Embedding Representations for Diffusion Models](https://arxiv.org/abs/2505.05732)
*Limai Jiang,Yunpeng Cai*

Main category: cs.LG

TLDR: 该论文提出了一种多级去噪自编码器框架，扩展了去噪扩散模型的表示能力，通过自条件扩散学习生成语义丰富的嵌入表示，实验表明其性能优于现有自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 去噪扩散模型（DDMs）在生成任务中表现出色，但其表示学习能力尚未充分开发。本文旨在通过改进框架，提升DDMs在表示学习中的表现。

Method: 采用多级去噪自编码器框架，引入时序一致的扩散变换器和时间步依赖编码器，通过自条件扩散学习在去噪马尔可夫链上获取嵌入表示。

Result: 实验表明，该方法生成的嵌入表示在语义上优于现有自监督学习方法，展示了DDMs在通用深度学习任务中的潜力。

Conclusion: DDMs不仅适用于生成任务，在通用深度学习应用中也具有潜在优势。

Abstract: Generative models capture the true distribution of data, yielding
semantically rich representations. Denoising diffusion models (DDMs) exhibit
superior generative capabilities, though efficient representation learning for
them are lacking. In this work, we employ a multi-level denoising autoencoder
framework to expand the representation capacity of DDMs, which introduces
sequentially consistent Diffusion Transformers and an additional
timestep-dependent encoder to acquire embedding representations on the
denoising Markov chain through self-conditional diffusion learning.
Intuitively, the encoder, conditioned on the entire diffusion process,
compresses high-dimensional data into directional vectors in latent under
different noise levels, facilitating the learning of image embeddings across
all timesteps. To verify the semantic adequacy of embeddings generated through
this approach, extensive experiments are conducted on various datasets,
demonstrating that optimally learned embeddings by DDMs surpass
state-of-the-art self-supervised representation learning methods in most cases,
achieving remarkable discriminative semantic representation quality. Our work
justifies that DDMs are not only suitable for generative tasks, but also
potentially advantageous for general-purpose deep learning applications.

</details>

### [37] [Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering](https://arxiv.org/abs/2505.05738)
*Yiming Niu,Jinliang Deng,Lulu Zhang,Zimu Zhou,Yongxin Tong*

Main category: cs.LG

TLDR: FOCUS是一种新型多变量时间序列预测方法，通过离线聚类提取原型，简化长程依赖建模，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如基于Transformer的模型）计算复杂度高，难以高效捕捉长程依赖和实体间交互。

Method: FOCUS通过离线聚类提取原型，在线阶段动态适配输入并捕捉依赖关系，将计算复杂度降至线性。

Result: 实验表明FOCUS在保持高精度的同时显著降低计算成本。

Conclusion: FOCUS为多变量时间序列预测提供了一种高效且准确的解决方案。

Abstract: Accurate and efficient multivariate time series (MTS) forecasting is
essential for applications such as traffic management and weather prediction,
which depend on capturing long-range temporal dependencies and interactions
between entities. Existing methods, particularly those based on Transformer
architectures, compute pairwise dependencies across all time steps, leading to
a computational complexity that scales quadratically with the length of the
input. To overcome these challenges, we introduce the Forecaster with Offline
Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that
simplifies long-range dependency modeling through the use of prototypes
extracted via offline clustering. These prototypes encapsulate high-level
events in the real-world system underlying the data, summarizing the key
characteristics of similar time segments. In the online phase, FOCUS
dynamically adapts these patterns to the current input and captures
dependencies between the input segment and high-level events, enabling both
accurate and efficient forecasting. By identifying prototypes during the
offline clustering phase, FOCUS reduces the computational complexity of
modeling long-range dependencies in the online phase to linear scaling.
Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves
state-of-the-art accuracy while significantly reducing computational costs.

</details>

### [38] [Deep-ICE: The first globally optimal algorithm for empirical risk minimization of two-layer maxout and ReLU networks](https://arxiv.org/abs/2505.05740)
*Xi He,Yi Miao,Max A. Little*

Main category: cs.LG

TLDR: 本文提出了一种针对两层Maxout和ReLU网络经验风险最小化问题的全局最优算法，旨在最小化分类错误数。算法的最坏时间复杂度为$O(N^{DK+1})$，并可通过核心集选择方法扩展到大规模数据集。实验表明，该算法在小规模数据集上提供精确解，并在大规模数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决两层Maxout和ReLU网络经验风险最小化问题的全局最优解需求，特别是在小规模数据集上的精确解和大规模数据集上的高效处理。

Method: 提出一种全局最优算法，时间复杂度为$O(N^{DK+1})$，并通过核心集选择方法扩展到大规模数据集。

Result: 在小规模数据集上提供精确解；在大规模数据集上，分类错误数减少20-30%，优于梯度下降训练的神经网络和支持向量机。

Conclusion: 该算法为两层Maxout和ReLU网络提供了全局最优解，并通过核心集选择方法实现了高效的大规模数据处理，显著提升了性能。

Abstract: This paper introduces the first globally optimal algorithm for the empirical
risk minimization problem of two-layer maxout and ReLU networks, i.e.,
minimizing the number of misclassifications. The algorithm has a worst-case
time complexity of $O\left(N^{DK+1}\right)$, where $K$ denotes the number of
hidden neurons and $D$ represents the number of features. It can be can be
generalized to accommodate arbitrary computable loss functions without
affecting its computational complexity. Our experiments demonstrate that the
proposed algorithm provides provably exact solutions for small-scale datasets.
To handle larger datasets, we introduce a novel coreset selection method that
reduces the data size to a manageable scale, making it feasible for our
algorithm. This extension enables efficient processing of large-scale datasets
and achieves significantly improved performance, with a 20-30\% reduction in
misclassifications for both training and prediction, compared to
state-of-the-art approaches (neural networks trained using gradient descent and
support vector machines), when applied to the same models (two-layer networks
with fixed hidden nodes and linear models).

</details>

### [39] [Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification](https://arxiv.org/abs/2505.05744)
*Ruxue Shi,Hengrui Gu,Xu Shen,Xin Wang*

Main category: cs.LG

TLDR: 提出了一种基于LLM解释的上下文学习框架，通过指导小型SLM进行表格预测，解决了现有方法资源消耗高、演示选择不佳和可解释性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在表格学习中存在资源需求高、演示选择不优和可解释性不足的问题，限制了其实际应用。

Method: 框架分为三个阶段：LLM生成解释、解释指导演示选择、解释指导SLM预测。

Result: 实验表明，该框架在多个表格数据集上平均准确率提升5.31%。

Conclusion: 该框架显著提升了表格预测的性能和可解释性，具有实际应用潜力。

Abstract: Large Language Models (LLMs) have shown remarkable ability in solving complex
tasks, making them a promising tool for enhancing tabular learning. However,
existing LLM-based methods suffer from high resource requirements, suboptimal
demonstration selection, and limited interpretability, which largely hinder
their prediction performance and application in the real world. To overcome
these problems, we propose a novel in-context learning framework for tabular
prediction. The core idea is to leverage the explanations generated by LLMs to
guide a smaller, locally deployable Surrogate Language Model (SLM) to make
interpretable tabular predictions. Specifically, our framework mainly involves
three stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to
generate explanations for question-answer pairs in candidate demonstrations,
providing insights into the reasoning behind the answer. (ii) Post Hoc
Explanation-Guided Demonstrations Selection, which utilizes explanations
generated by LLMs to guide the process of demonstration selection from
candidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM
Prediction, which utilizes the demonstrations obtained in step (ii) as
in-context and merges corresponding explanations as rationales to improve the
performance of SLM and guide the model to generate interpretable outputs.
Experimental results highlight the framework's effectiveness, with an average
accuracy improvement of 5.31% across various tabular datasets in diverse
domains.

</details>

### [40] [BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection](https://arxiv.org/abs/2505.05763)
*Yize Zhou,Jie Zhang,Meijie Wang,Lun Yu*

Main category: cs.LG

TLDR: BMMDetect是一种多模态深度学习框架，用于检测生物医学研究中的学术不端行为，通过整合期刊元数据、语义嵌入和GPT-4o挖掘的文本属性，显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测生物医学研究中的学术不端行为时存在算法局限性和分析流程碎片化的问题。

Method: BMMDetect整合了期刊元数据（如SJR）、语义嵌入（PubMedBERT）和GPT-4o挖掘的文本属性，通过多模态融合减少检测偏差。

Result: BMMDetect的AUC达到74.33%，比单模态基线高出8.6%，并在生物医学子领域表现出可迁移性。

Conclusion: 该研究为保护研究诚信提供了可扩展且可解释的工具。

Abstract: Academic misconduct detection in biomedical research remains challenging due
to algorithmic narrowness in existing methods and fragmented analytical
pipelines. We present BMMDetect, a multimodal deep learning framework that
integrates journal metadata (SJR, institutional data), semantic embeddings
(PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics,
data anomalies) for holistic manuscript evaluation. Key innovations include:
(1) multimodal fusion of domain-specific features to reduce detection bias; (2)
quantitative evaluation of feature importance, identifying journal authority
metrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as
dominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with
13,160 retracted articles and 53,411 controls. BMMDetect achieves 74.33% AUC,
outperforming single-modality baselines by 8.6%, and demonstrates
transferability across biomedical subfields. This work advances scalable,
interpretable tools for safeguarding research integrity.

</details>

### [41] [Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective](https://arxiv.org/abs/2505.05785)
*Henan Sun,Xunkai Li,Lei Zhu,Junyi Han,Guang Zeng,Ronghua Li,Guoren Wang*

Main category: cs.LG

TLDR: 论文提出了一种基于可学习随机游走（LRW）的方法LRW-OOD，用于提升图神经网络在分布偏移下的泛化能力，通过参数化转移矩阵和KDE-based MI损失优化随机游走序列，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在分布偏移下性能下降的问题，现有方法对不变知识的解释（如图拓扑或谱）可能与现实场景不符。

Method: 提出LRW-OOD，参数化转移矩阵（LRW-sampler和路径编码器），并使用KDE-based MI损失生成符合OOD原则的随机游走序列。

Result: 实验表明，LRW-OOD能有效提升图OOD泛化能力，在多种分布偏移下显著优于现有方法，准确率提升3.87%。

Conclusion: LRW-OOD通过可学习随机游走视角实现了更优的图OOD泛化，为现实场景中的分布偏移问题提供了新思路。

Abstract: Out-Of-Distribution (OOD) generalization has gained increasing attentions for
machine learning on graphs, as graph neural networks (GNNs) often exhibit
performance degradation under distribution shifts. Existing graph OOD methods
tend to follow the basic ideas of invariant risk minimization and structural
causal models, interpreting the invariant knowledge across datasets under
various distribution shifts as graph topology or graph spectrum. However, these
interpretations may be inconsistent with real-world scenarios, as neither
invariant topology nor spectrum is assured. In this paper, we advocate the
learnable random walk (LRW) perspective as the instantiation of invariant
knowledge, and propose LRW-OOD to realize graph OOD generalization learning.
Instead of employing fixed probability transition matrix (i.e.,
degree-normalized adjacency matrix), we parameterize the transition matrix with
an LRW-sampler and a path encoder. Furthermore, we propose the kernel density
estimation (KDE)-based mutual information (MI) loss to generate random walk
sequences that adhere to OOD principles. Extensive experiment demonstrates that
our model can effectively enhance graph OOD generalization under various types
of distribution shifts and yield a significant accuracy improvement of 3.87%
over state-of-the-art graph OOD generalization baselines.

</details>

### [42] [Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes](https://arxiv.org/abs/2505.05798)
*Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TLDR: 将ECOC集成到KAN框架中，通过多二进制任务提升多分类性能，在医疗图像分类中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升KAN在多分类任务中的鲁棒性和准确性，特别是在医疗AI应用中的表现。

Method: 将ECOC与KAN结合，通过Hamming距离解码将多分类任务转化为多个二进制任务。

Result: 在血液细胞分类数据集上表现优于原始KAN，且在不同超参数下均取得更高准确率。

Conclusion: ECOC显著提升了KAN在医疗图像多分类任务中的泛化能力，是首次将ECOC与KAN结合的创新方法。

Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using
univariate spline compositions without nonlinear activations. In this work, we
integrate Error-Correcting Output Codes (ECOC) into the KAN framework to
transform multi-class classification into multiple binary tasks, improving
robustness via Hamming-distance decoding. Our proposed KAN with ECOC method
outperforms vanilla KAN on a challenging blood cell classification dataset,
achieving higher accuracy under diverse hyperparameter settings. Ablation
studies further confirm that ECOC consistently enhances performance across
FastKAN and FasterKAN variants. These results demonstrate that ECOC integration
significantly boosts KAN generalizability in critical healthcare AI
applications. To the best of our knowledge, this is the first integration of
ECOC with KAN for enhancing multi-class medical image classification
performance.

</details>

### [43] [MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design](https://arxiv.org/abs/2505.05799)
*Haojie Duanmu,Xiuhong Li,Zhihang Yuan,Size Zheng,Jiangfei Duan,Xingcheng Zhang,Dahua Lin*

Main category: cs.LG

TLDR: MxMoE是一个混合精度优化框架，针对MoE模型的部署挑战，通过考虑参数敏感性和专家激活动态，实现了高效的混合精度配置和优化的并行计算。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型因参数多和计算需求大而面临的部署难题。

Method: 提出MxMoE框架，结合算法和系统视角，优化混合精度配置，并自动生成优化的GroupGEMM内核。

Result: MxMoE在Wikitext-2上比GPTQ低2.4困惑度，速度比全精度快3.4倍，比均匀量化快29.4%。

Conclusion: MxMoE有效提升了MoE模型的部署效率和性能。

Abstract: Mixture-of-Experts (MoE) models face deployment challenges due to their large
parameter counts and computational demands. We explore quantization for MoE
models and highlight two key insights: 1) linear blocks exhibit varying
quantization sensitivity, and 2) divergent expert activation frequencies create
heterogeneous computational characteristics. Based on these observations, we
introduce MxMoE, a mixed-precision optimization framework for MoE models that
considers both algorithmic and system perspectives. MxMoE navigates the design
space defined by parameter sensitivity, expert activation dynamics, and
hardware resources to derive efficient mixed-precision configurations.
Additionally, MxMoE automatically generates optimized mixed-precision GroupGEMM
kernels, enabling parallel execution of GEMMs with different precisions.
Evaluations show that MxMoE outperforms existing methods, achieving 2.4 lower
Wikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup
over full precision, as well as up to 29.4% speedup over uniform quantization
at equivalent accuracy with 5-bit weight-activation quantization. Our code is
available at https://github.com/cat538/MxMoE.

</details>

### [44] [A novel Neural-ODE model for the state of health estimation of lithium-ion battery using charging curve](https://arxiv.org/abs/2505.05803)
*Yiming Li,Man He,Jiapeng Liu*

Main category: cs.LG

TLDR: 本文提出了一种名为ACLA的混合模型，结合注意力机制、CNN和LSTM，用于提高锂离子电池健康状态（SOH）估计的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有SOH估计方法泛化能力有限，影响电动汽车的安全可靠运行。

Method: 构建ACLA模型，整合注意力机制、CNN和LSTM于ANODE框架中，利用恒流充电阶段的电压对应充电时间作为输入。

Result: 在TJU和HUST数据集上，ACLA的SOH估计RMSE低至1.01%和2.24%，优于基准模型。

Conclusion: ACLA模型显著提升了SOH估计的准确性和泛化能力。

Abstract: The state of health (SOH) of lithium-ion batteries (LIBs) is crucial for
ensuring the safe and reliable operation of electric vehicles. Nevertheless,
the prevailing SOH estimation methods often have limited generalizability. This
paper introduces a data-driven approach for estimating the SOH of LIBs, which
is designed to improve generalization. We construct a hybrid model named ACLA,
which integrates the attention mechanism, convolutional neural network (CNN),
and long short-term memory network (LSTM) into the augmented neural ordinary
differential equation (ANODE) framework. This model employs normalized charging
time corresponding to specific voltages in the constant current charging phase
as input and outputs the SOH as well as remaining useful of life. The model is
trained on NASA and Oxford datasets and validated on the TJU and HUST datasets.
Compared to the benchmark models NODE and ANODE, ACLA exhibits higher accuracy
with root mean square errors (RMSE) for SOH estimation as low as 1.01% and
2.24% on the TJU and HUST datasets, respectively.

</details>

### [45] [BCE vs. CE in Deep Feature Learning](https://arxiv.org/abs/2505.05813)
*Qiufu Li,Huibin Xiao,Linlin Shen*

Main category: cs.LG

TLDR: 论文比较了二元交叉熵（BCE）和多类交叉熵（CE）在深度特征学习中的表现，证明BCE也能最大化类内紧凑性和类间区分性，并指出其通过调整决策分数的绝对值和分类器偏置来增强特征属性。


<details>
  <summary>Details</summary>
Motivation: 研究BCE在多类任务中的表现，探索其与CE在特征学习中的差异及其对特征紧凑性和区分性的影响。

Method: 理论分析BCE和CE的优化目标，证明BCE在最小化时也能达到神经崩溃（NC），并通过实验验证其效果。

Result: 实验结果表明，BCE能提升分类性能，并增强特征的紧凑性和区分性。

Conclusion: BCE在多类任务中表现优异，通过调整决策分数和分类器偏置，显式增强特征属性，优于CE。

Abstract: When training classification models, it expects that the learned features are
compact within classes, and can well separate different classes. As the
dominant loss function for training classification models, minimizing
cross-entropy (CE) loss maximizes the compactness and distinctiveness, i.e.,
reaching neural collapse (NC). The recent works show that binary CE (BCE)
performs also well in multi-class tasks. In this paper, we compare BCE and CE
in deep feature learning. For the first time, we prove that BCE can also
maximize the intra-class compactness and inter-class distinctiveness when
reaching its minimum, i.e., leading to NC. We point out that CE measures the
relative values of decision scores in the model training, implicitly enhancing
the feature properties by classifying samples one-by-one. In contrast, BCE
measures the absolute values of decision scores and adjust the
positive/negative decision scores across all samples to uniformly high/low
levels. Meanwhile, the classifier biases in BCE present a substantial
constraint on the decision scores to explicitly enhance the feature properties
in the training. The experimental results are aligned with above analysis, and
show that BCE could improve the classification and leads to better compactness
and distinctiveness among sample features. The codes will be released.

</details>

### [46] [New Statistical and Computational Results for Learning Junta Distributions](https://arxiv.org/abs/2505.05819)
*Lorenzo Beretta*

Main category: cs.LG

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning junta distributions on $\{0, 1\}^n$, where a
distribution is a $k$-junta if its probability mass function depends on a
subset of at most $k$ variables. We make two main contributions:
  - We show that learning $k$-junta distributions is \emph{computationally}
equivalent to learning $k$-parity functions with noise (LPN), a landmark
problem in computational learning theory.
  - We design an algorithm for learning junta distributions whose statistical
complexity is optimal, up to polylogarithmic factors. Computationally, our
algorithm matches the complexity of previous (non-sample-optimal) algorithms.
  Combined, our two contributions imply that our algorithm cannot be
significantly improved, statistically or computationally, barring a
breakthrough for LPN.

</details>

### [47] [Mixed-Integer Optimization for Responsible Machine Learning](https://arxiv.org/abs/2505.05857)
*Nathan Justin,Qingshi Sun,Andrés Gómez,Phebe Vayanos*

Main category: cs.LG

TLDR: 机器学习在多个领域取得成功，但部署中的公平性、透明性等问题日益突出。混合整数优化（MIO）为负责任机器学习提供了框架，本教程论文介绍了其理论和实践应用。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在敏感领域的广泛应用，公平性、透明性等问题成为关键挑战，需要负责任的方法。

Method: 论文通过混合整数优化（MIO）框架，将负责任机器学习的原则直接嵌入学习过程，并结合数学公式和实例说明。

Result: MIO能够构建透明且符合公平性等约束的模型，论文提供了解决相关问题的实用策略和工具。

Conclusion: 论文总结了MIO在负责任机器学习中的潜力，并指出当前局限性和未来研究方向。

Abstract: In the last few decades, Machine Learning (ML) has achieved significant
success across domains ranging from healthcare, sustainability, and the social
sciences, to criminal justice and finance. But its deployment in increasingly
sophisticated, critical, and sensitive areas affecting individuals, the groups
they belong to, and society as a whole raises critical concerns around
fairness, transparency, robustness, and privacy, among others. As the
complexity and scale of ML systems and of the settings in which they are
deployed grow, so does the need for responsible ML methods that address these
challenges while providing guaranteed performance in deployment.
  Mixed-integer optimization (MIO) offers a powerful framework for embedding
responsible ML considerations directly into the learning process while
maintaining performance. For example, it enables learning of inherently
transparent models that can conveniently incorporate fairness or other domain
specific constraints. This tutorial paper provides an accessible and
comprehensive introduction to this topic discussing both theoretical and
practical aspects. It outlines some of the core principles of responsible ML,
their importance in applications, and the practical utility of MIO for building
ML models that align with these principles. Through examples and mathematical
formulations, it illustrates practical strategies and available tools for
efficiently solving MIO problems for responsible ML. It concludes with a
discussion on current limitations and open research questions, providing
suggestions for future work.

</details>

### [48] [Open Set Label Shift with Test Time Out-of-Distribution Reference](https://arxiv.org/abs/2505.05868)
*Changkun Ye,Russell Tsuchida,Lars Petersson,Nick Barnes*

Main category: cs.LG

TLDR: 该论文提出了一种估计开放集标签偏移（OSLS）的方法，通过三个阶段估计源和目标标签分布，并量化采样误差，无需重新训练即可调整分类器。


<details>
  <summary>Details</summary>
Motivation: 开放集标签偏移（OSLS）导致目标分布中新增了分布外（OOD）类别，需要有效估计源和目标标签分布以调整分类器。

Method: 使用源域内分布（ID）分类器和ID/OOD分类器，分三个阶段估计标签分布：1）估计OOD类的源标签分布，2）EM算法估计目标标签分布，3）在放宽假设下估计OOD类的目标标签分布。

Result: 实验表明该方法在多种开放集标签偏移场景中有效，且无需重新训练即可调整分类器。

Conclusion: 提出的方法能有效估计开放集标签偏移，并通过量化误差优化分类器性能。

Abstract: Open set label shift (OSLS) occurs when label distributions change from a
source to a target distribution, and the target distribution has an additional
out-of-distribution (OOD) class. In this work, we build estimators for both
source and target open set label distributions using a source domain
in-distribution (ID) classifier and an ID/OOD classifier. With reasonable
assumptions on the ID/OOD classifier, the estimators are assembled into a
sequence of three stages: 1) an estimate of the source label distribution of
the OOD class, 2) an EM algorithm for Maximum Likelihood estimates (MLE) of the
target label distribution, and 3) an estimate of the target label distribution
of OOD class under relaxed assumptions on the OOD classifier. The sampling
errors of estimates in 1) and 3) are quantified with a concentration
inequality. The estimation result allows us to correct the ID classifier
trained on the source distribution to the target distribution without
retraining. Experiments on a variety of open set label shift settings
demonstrate the effectiveness of our model. Our code is available at
https://github.com/ChangkunYe/OpenSetLabelShift.

</details>

### [49] [Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification](https://arxiv.org/abs/2505.06032)
*Leon Eshuijs,Shihan Wang,Antske Fokkens*

Main category: cs.LG

TLDR: 论文研究了语言模型中基于虚假相关性（捷径）的决策机制，提出了一种检测和缓解捷径的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型如何依赖虚假相关性（捷径）进行决策，并揭示其内部处理机制。

Method: 使用电影评论中的演员名称作为可控捷径，通过机制解释方法识别特定关注头，并提出基于头的标记归因（HTA）方法。

Result: 发现特定关注头会提前做出决策，HTA能有效检测捷径并选择性关闭相关头以缓解问题。

Conclusion: HTA为理解和缓解语言模型中的捷径依赖提供了有效工具。

Abstract: Reliance on spurious correlations (shortcuts) has been shown to underlie many
of the successes of language models. Previous work focused on identifying the
input elements that impact prediction. We investigate how shortcuts are
actually processed within the model's decision-making mechanism. We use actor
names in movie reviews as controllable shortcuts with known impact on the
outcome. We use mechanistic interpretability methods and identify specific
attention heads that focus on shortcuts. These heads gear the model towards a
label before processing the complete input, effectively making premature
decisions that bypass contextual analysis. Based on these findings, we
introduce Head-based Token Attribution (HTA), which traces intermediate
decisions back to input tokens. We show that HTA is effective in detecting
shortcuts in LLMs and enables targeted mitigation by selectively deactivating
shortcut-related attention heads.

</details>

### [50] [Generative Discovery of Partial Differential Equations by Learning from Math Handbooks](https://arxiv.org/abs/2505.05869)
*Hao Xu,Yuntian Chen,Rui Cao,Tianning Tang,Mengge Du,Jian Li,Adrian H. Callaghan,Dongxiao Zhang*

Main category: cs.LG

TLDR: 该论文提出了一种知识引导的方法，通过结合数学手册中的已知偏微分方程（PDEs）来辅助数据驱动的PDE发现。


<details>
  <summary>Details</summary>
Motivation: 纯粹的数据驱动方法在搜索空间与优化效率之间存在矛盾，因此需要一种更高效的方法来发现复杂系统的PDEs。

Method: 将已知PDEs编码为类似句子的结构，训练生成模型EqGPT，并通过生成-评估-优化的循环自主识别最合适的PDE。

Result: 实验表明，该方法能高效准确地恢复多种PDE形式，包括复杂时间导数或空间项，并适用于不规则空间域和高维场景。

Conclusion: 该方法不仅具有实际应用潜力，还成功发现了一种新的非线性表面重力波PDE，展示了其在科学发现中的价值。

Abstract: Data driven discovery of partial differential equations (PDEs) is a promising
approach for uncovering the underlying laws governing complex systems. However,
purely data driven techniques face the dilemma of balancing search space with
optimization efficiency. This study introduces a knowledge guided approach that
incorporates existing PDEs documented in a mathematical handbook to facilitate
the discovery process. These PDEs are encoded as sentence like structures
composed of operators and basic terms, and used to train a generative model,
called EqGPT, which enables the generation of free form PDEs. A loop of
generation evaluation optimization is constructed to autonomously identify the
most suitable PDE. Experimental results demonstrate that this framework can
recover a variety of PDE forms with high accuracy and computational efficiency,
particularly in cases involving complex temporal derivatives or intricate
spatial terms, which are often beyond the reach of conventional methods. The
approach also exhibits generalizability to irregular spatial domains and higher
dimensional settings. Notably, it succeeds in discovering a previously
unreported PDE governing strongly nonlinear surface gravity waves propagating
toward breaking, based on real world experimental data, highlighting its
applicability to practical scenarios and its potential to support scientific
discovery.

</details>

### [51] [A 3D pocket-aware and evolutionary conserved interaction guided diffusion model for molecular optimization](https://arxiv.org/abs/2505.05874)
*Anjie Qiao,Hao Zhang,Qianmu Yuan,Qirui Deng,Jingtian Su,Weifeng Huang,Huihao Zhou,Guo-Bo Li,Zhen Wang,Jinping Lei*

Main category: cs.LG

TLDR: DiffDecip是一种新型3D目标感知扩散模型，通过结合蛋白质-配体结合相互作用和蛋白质残基的进化保守信息，优化分子生成，提高亲和力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成的分子可能未与高度保守的残基形成相互作用，而这些残基对蛋白质功能和配体生物活性至关重要。

Method: 开发DiffDecip模型，在扩散和采样过程中显式结合蛋白质-配体结合相互作用和残基保守信息，用于分子优化。

Result: DiffDecip在分子优化中表现优于基线模型DiffDec，能形成更多与非保守残基的非共价相互作用。

Conclusion: DiffDecip通过结合保守信息，显著提升了分子生成的亲和力和功能性。

Abstract: Generating molecules that bind to specific protein targets via diffusion
models has shown good promise for structure-based drug design and molecule
optimization. Especially, the diffusion models with binding interaction
guidance enables molecule generation with high affinity through forming
favorable interaction within protein pocket. However, the generated molecules
may not form interactions with the highly conserved residues, which are
important for protein functions and bioactivities of the ligands. Herein, we
developed a new 3D target-aware diffusion model DiffDecip, which explicitly
incorporates the protein-ligand binding interactions and evolutionary
conservation information of protein residues into both diffusion and sampling
process, for molecule optimization through scaffold decoration. The model
performance revealed that DiffDecip outperforms baseline model DiffDec on
molecule optimization towards higher affinity through forming more non-covalent
interactions with highly conserved residues in the protein pocket.

</details>

### [52] [Multi-Modal Molecular Representation Learning via Structure Awareness](https://arxiv.org/abs/2505.05877)
*Rong Yin,Ruyue Liu,Xiaoshuai Hao,Xingrui Zhou,Yong Liu,Can Ma,Weiping Wang*

Main category: cs.LG

TLDR: 提出了一种基于结构感知的多模态自监督分子表示预训练框架（MMSA），通过利用分子间不变知识增强分子图表示，显著提升了分子表示的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法直接融合不同模态信息，忽略了模态间交互潜力，未能充分捕捉分子间复杂高阶关系和不变特征。

Method: MMSA框架包含多模态分子表示学习模块和结构感知模块，前者处理多模态信息生成统一嵌入，后者通过超图结构建模分子间高阶关系并引入记忆机制存储典型分子表示。

Result: 在MoleculeNet基准测试中，MMSA表现优异，ROC-AUC平均提升1.8%至9.6%。

Conclusion: MMSA通过结构感知和多模态协同学习，显著提升了分子表示的性能和泛化能力。

Abstract: Accurate extraction of molecular representations is a critical step in the
drug discovery process. In recent years, significant progress has been made in
molecular representation learning methods, among which multi-modal molecular
representation methods based on images, and 2D/3D topologies have become
increasingly mainstream. However, existing these multi-modal approaches often
directly fuse information from different modalities, overlooking the potential
of intermodal interactions and failing to adequately capture the complex
higher-order relationships and invariant features between molecules. To
overcome these challenges, we propose a structure-awareness-based multi-modal
self-supervised molecular representation pre-training framework (MMSA) designed
to enhance molecular graph representations by leveraging invariant knowledge
between molecules. The framework consists of two main modules: the multi-modal
molecular representation learning module and the structure-awareness module.
The multi-modal molecular representation learning module collaboratively
processes information from different modalities of the same molecule to
overcome intermodal differences and generate a unified molecular embedding.
Subsequently, the structure-awareness module enhances the molecular
representation by constructing a hypergraph structure to model higher-order
correlations between molecules. This module also introduces a memory mechanism
for storing typical molecular representations, aligning them with memory
anchors in the memory bank to integrate invariant knowledge, thereby improving
the model generalization ability. Extensive experiments have demonstrated the
effectiveness of MMSA, which achieves state-of-the-art performance on the
MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to
9.6% over baseline methods.

</details>

### [53] [IRNN: Innovation-driven Recurrent Neural Network for Time-Series Data Modeling and Prediction](https://arxiv.org/abs/2505.05916)
*Yifan Zhou,Yibo Wang,Chao Shang*

Main category: cs.LG

TLDR: 提出了一种基于创新驱动的RNN（IRNN），通过引入卡尔曼滤波器中的“创新”概念，利用历史预测误差更新隐藏状态，显著提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据包含丰富的动态信息，现有RNN虽有效但未充分利用预测误差信息，因此提出IRNN以改进预测性能。

Method: IRNN将卡尔曼滤波器的“创新”概念引入RNN，利用历史预测误差更新隐藏状态，并提出IU-BPTT训练算法交替更新创新数据和网络参数。

Result: 实验表明，IRNN在多种RNN变体上显著提升了预测精度，且训练成本未显著增加。

Conclusion: IRNN通过创新驱动机制有效提升了时间序列预测性能，为动态建模提供了新思路。

Abstract: Many real-world datasets are time series that are sequentially collected and
contain rich temporal information. Thus, a common interest in practice is to
capture dynamics of time series and predict their future evolutions. To this
end, the recurrent neural network (RNN) has been a prevalent and effective
machine learning option, which admits a nonlinear state-space model
representation. Motivated by the resemblance between RNN and Kalman filter (KF)
for linear state-space models, we propose in this paper Innovation-driven RNN
(IRNN), a novel RNN architecture tailored to time-series data modeling and
prediction tasks. By adapting the concept of "innovation" from KF to RNN, past
prediction errors are adopted as additional input signals to update hidden
states of RNN and boost prediction performance. Since innovation data depend on
network parameters, existing training algorithms for RNN do not apply to IRNN
straightforwardly. Thus, a tailored training algorithm dubbed input
updating-based back-propagation through time (IU-BPTT) is further proposed,
which alternates between updating innovations and optimizing network parameters
via gradient descent. Experiments on real-world benchmark datasets show that
the integration of innovations into various forms of RNN leads to remarkably
improved prediction accuracy of IRNN without increasing the training cost
substantially.

</details>

### [54] [Autoencoder-Based Hybrid Replay for Class-Incremental Learning](https://arxiv.org/abs/2505.05926)
*Milad Khademi Nori,Il-Min Kim,Guanghui Wang*

Main category: cs.LG

TLDR: 提出了一种基于自动编码器的混合重放策略（AHR），通过新型混合自动编码器（HAE）降低内存需求，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决类增量学习（CIL）中的任务混淆和灾难性遗忘问题，减少内存和计算复杂度。

Method: 使用HAE作为压缩器，结合能量最小化方程和排斥力算法，实现分类和重放功能。

Result: AHR在相同内存/计算预算下优于现有基线方法。

Conclusion: AHR是一种高效且性能优越的类增量学习策略。

Abstract: In class-incremental learning (CIL), effective incremental learning
strategies are essential to mitigate task confusion and catastrophic
forgetting, especially as the number of tasks $t$ increases. Current exemplar
replay strategies impose $\mathcal{O}(t)$ memory/compute complexities. We
propose an autoencoder-based hybrid replay (AHR) strategy that leverages our
new hybrid autoencoder (HAE) to function as a compressor to alleviate the
requirement for large memory, achieving $\mathcal{O}(0.1 t)$ at the worst case
with the computing complexity of $\mathcal{O}(t)$ while accomplishing
state-of-the-art performance. The decoder later recovers the exemplar data
stored in the latent space, rather than in raw format. Additionally, HAE is
designed for both discriminative and generative modeling, enabling
classification and replay capabilities, respectively. HAE adopts the charged
particle system energy minimization equations and repulsive force algorithm for
the incremental embedding and distribution of new class centroids in its latent
space. Our results demonstrate that AHR consistently outperforms recent
baselines across multiple benchmarks while operating with the same
memory/compute budgets. The source code is included in the supplementary
material and will be open-sourced upon publication.

</details>

### [55] [FloE: On-the-Fly MoE Inference](https://arxiv.org/abs/2505.05950)
*Yuxin Zhou,Zheng Li,Jun Zhang,Jue Wang,Yiping Wang,Zhongle Xie,Ke Chen,Lidan Shou*

Main category: cs.LG

TLDR: FloE是一种针对内存受限GPU的MoE推理系统，通过压缩专家内部参数矩阵和稀疏预测，显著减少数据移动负载，提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 随着MoE模型的广泛应用，内存受限设备上的高效推理需求增加，但现有方法因PCIe带宽限制无法满足低延迟场景。

Method: 利用专家内部参数矩阵的冗余性，采用多种压缩技术结合稀疏预测，减少数据移动负载。

Result: 在Mixtral-8x7B上实现每专家参数9.3倍压缩，内存占用减少8.5倍，推理速度提升48.7倍。

Conclusion: FloE有效解决了内存受限设备上MoE推理的效率和性能问题。

Abstract: With the widespread adoption of Mixture-of-Experts (MoE) models, there is a
growing demand for efficient inference on memory-constrained devices. While
offloading expert parameters to CPU memory and loading activated experts on
demand has emerged as a potential solution, the large size of activated experts
overburdens the limited PCIe bandwidth, hindering the effectiveness in
latency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly
MoE inference system on memory-constrained GPUs. FloE is built on the insight
that there exists substantial untapped redundancy within sparsely activated
experts. It employs various compression techniques on the expert's internal
parameter matrices to reduce the data movement load, combined with low-cost
sparse prediction, achieving perceptible inference acceleration in wall-clock
time on resource-constrained devices. Empirically, FloE achieves a 9.3x
compression of parameters per expert in Mixtral-8x7B; enables deployment on a
GPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and
delivers a 48.7x inference speedup compared to DeepSpeed-MII on a single
GeForce RTX 3090.

</details>

### [56] [Learning Power Control Protocol for In-Factory 6G Subnetworks](https://arxiv.org/abs/2505.05967)
*Uyoata E. Uyoata,Gilberto Berardinelli,Ramoni Adeogun*

Main category: cs.LG

TLDR: 本文提出了一种多智能体强化学习框架，用于在6G工厂子网络中自主学习和优化信令与功率控制协议，显著降低信令开销并保持接近理想性能。


<details>
  <summary>Details</summary>
Motivation: 现有功率控制方法主要关注数据平面，忽略了信令开销的影响，且假设中央控制器拥有完整且实时的信道状态信息（CSI），这在工厂子网络高密度场景下不切实际。

Method: 采用多智能体近端策略优化（MAPPO），将问题建模为部分可观测马尔可夫决策过程（POMDP），使接入点能自主学习和优化信令与功率控制协议。

Result: 仿真结果显示，该方法将信令开销降低了8倍，同时缓冲刷新率仅比理想“Genie”方法低5%。

Conclusion: 提出的MARL框架在工厂子网络中有效解决了信令开销与性能平衡问题，为6G短距离通信提供了实用解决方案。

Abstract: In-X Subnetworks are envisioned to meet the stringent demands of short-range
communication in diverse 6G use cases. In the context of In-Factory scenarios,
effective power control is critical to mitigating the impact of interference
resulting from potentially high subnetwork density. Existing approaches to
power control in this domain have predominantly emphasized the data plane,
often overlooking the impact of signaling overhead. Furthermore, prior work has
typically adopted a network-centric perspective, relying on the assumption of
complete and up-to-date channel state information (CSI) being readily available
at the central controller. This paper introduces a novel multi-agent
reinforcement learning (MARL) framework designed to enable access points to
autonomously learn both signaling and power control protocols in an In-Factory
Subnetwork environment. By formulating the problem as a partially observable
Markov decision process (POMDP) and leveraging multi-agent proximal policy
optimization (MAPPO), the proposed approach achieves significant advantages.
The simulation results demonstrate that the learning-based method reduces
signaling overhead by a factor of 8 while maintaining a buffer flush rate that
lags the ideal "Genie" approach by only 5%.

</details>

### [57] [Offline Multi-agent Reinforcement Learning via Score Decomposition](https://arxiv.org/abs/2505.05968)
*Dan Qiao,Wenhao Li,Shanchao Yang,Hongyuan Zha,Baoxiang Wang*

Main category: cs.LG

TLDR: 论文提出了一种两阶段框架，通过扩散生成模型和顺序评分函数分解机制，解决了离线多智能体强化学习中的分布偏移问题，并在实验中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 离线多智能体强化学习面临分布偏移和高维联合动作空间的挑战，传统方法易受OOD动作影响且性能不佳。论文旨在解决这些问题。

Method: 采用扩散生成模型捕捉复杂行为策略，并引入顺序评分函数分解机制，实现分散化执行。

Result: 在连续控制任务中，性能优于现有方法26.3%。

Conclusion: 该框架为离线协作和均衡选择提供了新思路。

Abstract: Offline multi-agent reinforcement learning (MARL) faces critical challenges
due to distributional shifts, further exacerbated by the high dimensionality of
joint action spaces and the diversity in coordination strategies and quality
among agents. Conventional approaches, including independent learning
frameworks and value decomposition methods based on pessimistic principles,
remain susceptible to out-of-distribution (OOD) joint actions and often yield
suboptimal performance. Through systematic analysis of prevalent offline MARL
benchmarks, we identify that this limitation primarily stems from the
inherently multimodal nature of joint collaborative policies induced by offline
data collection. To address these challenges, we propose a novel two-stage
framework: First, we employ a diffusion-based generative model to explicitly
capture the complex behavior policy, enabling accurate modeling of diverse
multi-agent coordination patterns. Second, we introduce a sequential score
function decomposition mechanism to regularize individual policies and enable
decentralized execution. Extensive experiments on continuous control tasks
demonstrate state-of-the-art performance across multiple standard offline MARL
benchmarks, outperforming existing methods by 26.3\% in normalized returns. Our
approach provides new insights into offline coordination and equilibrium
selection in cooperative multi-agent systems.

</details>

### [58] [Architectural Exploration of Hybrid Neural Decoders for Neuromorphic Implantable BMI](https://arxiv.org/abs/2505.05983)
*Vivek Mohan,Biyan Zhou,Zhou Wang,Anil Bharath,Emmanuel Drakakis,Arindam Basu*

Main category: cs.LG

TLDR: 提出了一种高效的神经形态植入式脑机接口解码流程，利用稀疏神经事件数据，显著减少计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 传统iBMI系统需要信号恢复、尖峰检测或排序，计算和内存需求高，不适合低功耗植入或可穿戴设备。

Method: 引入可调事件过滤器（EvFilter）和尖峰检测器（EvFilter-SPD），减少事件处理量；使用ANN和SNN解码器。

Result: 解码性能达R^2=0.73；SNN解码器计算和内存需求减少5-23倍。

Conclusion: 该流程显著降低资源需求，适合低功耗植入或可穿戴iBMI。

Abstract: This work presents an efficient decoding pipeline for neuromorphic
implantable brain-machine interfaces (Neu-iBMI), leveraging sparse neural event
data from an event-based neural sensing scheme. We introduce a tunable event
filter (EvFilter), which also functions as a spike detector (EvFilter-SPD),
significantly reducing the number of events processed for decoding by 192X and
554X, respectively. The proposed pipeline achieves high decoding performance,
up to R^2=0.73, with ANN- and SNN-based decoders, eliminating the need for
signal recovery, spike detection, or sorting, commonly performed in
conventional iBMI systems. The SNN-Decoder reduces computations and memory
required by 5-23X compared to NN-, and LSTM-Decoders, while the ST-NN-Decoder
delivers similar performance to an LSTM-Decoder requiring 2.5X fewer resources.
This streamlined approach significantly reduces computational and memory
demands, making it ideal for low-power, on-implant, or wearable iBMIs.

</details>

### [59] [Differentiable Fuzzy Neural Networks for Recommender Systems](https://arxiv.org/abs/2505.06000)
*Stephan Bartl,Kevin Innerebner,Elisabeth Lex*

Main category: cs.LG

TLDR: 论文提出了一种基于模糊神经网络（FNNs）的神经符号方法，用于构建透明且用户中心的推荐系统，通过逻辑规则和模糊逻辑表达式实现决策透明性。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统复杂性增加，透明性对用户信任、责任和法规遵从至关重要。神经符号方法结合符号推理和子符号学习，为透明系统提供了可能。

Method: 使用模糊神经网络（FNNs）作为神经符号方法，学习基于逻辑的规则，并通过模糊逻辑表达式实现透明决策。

Result: 在合成数据集和MovieLens 1M数据集上的实验表明，该方法在保持竞争力的同时提供了透明的决策过程。

Conclusion: 该方法不仅准确捕捉用户行为，还支持与其他神经模型的集成，为开发混合透明推荐系统提供了可能。

Abstract: As recommender systems become increasingly complex, transparency is essential
to increase user trust, accountability, and regulatory compliance.
Neuro-symbolic approaches that integrate symbolic reasoning with sub-symbolic
learning offer a promising approach toward transparent and user-centric
systems. In this work-in-progress, we investigate using fuzzy neural networks
(FNNs) as a neuro-symbolic approach for recommendations that learn logic-based
rules over predefined, human-readable atoms. Each rule corresponds to a fuzzy
logic expression, making the recommender's decision process inherently
transparent. In contrast to black-box machine learning methods, our approach
reveals the reasoning behind a recommendation while maintaining competitive
performance. We evaluate our method on a synthetic and MovieLens 1M datasets
and compare it to state-of-the-art recommendation algorithms. Our results
demonstrate that our approach accurately captures user behavior while providing
a transparent decision-making process. Finally, the differentiable nature of
this approach facilitates an integration with other neural models, enabling the
development of hybrid, transparent recommender systems.

</details>

### [60] [Fuzzy-UCS Revisited: Self-Adaptation of Rule Representations in Michigan-Style Learning Fuzzy-Classifier Systems](https://arxiv.org/abs/2505.06017)
*Hiroki Shiraishi,Yohei Hayamizu,Tomonori Hashiyama*

Main category: cs.LG

TLDR: 本文提出了一种自适应的规则表示机制（Adaptive-UCS），通过模糊指示器优化规则表示，提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统规则表示方法在处理未知数据特征时表现不佳，需要改进。

Method: 提出Adaptive-UCS，引入模糊指示器作为新规则参数，通过进化算子优化规则表示。

Result: 实验表明，Adaptive-UCS在分类准确率上优于传统方法，且对噪声和不确定性数据具有鲁棒性。

Conclusion: Adaptive-UCS通过自适应规则表示机制显著提升了分类性能，适用于复杂数据环境。

Abstract: This paper focuses on the impact of rule representation in Michigan-style
Learning Fuzzy-Classifier Systems (LFCSs) on its classification performance. A
well-representation of the rules in an LFCS is crucial for improving its
performance. However, conventional rule representations frequently need help
addressing problems with unknown data characteristics. To address this issue,
this paper proposes a supervised LFCS (i.e., Fuzzy-UCS) with a self-adaptive
rule representation mechanism, entitled Adaptive-UCS. Adaptive-UCS incorporates
a fuzzy indicator as a new rule parameter that sets the membership function of
a rule as either rectangular (i.e., crisp) or triangular (i.e., fuzzy) shapes.
The fuzzy indicator is optimized with evolutionary operators, allowing the
system to search for an optimal rule representation. Results from extensive
experiments conducted on continuous space problems demonstrate that
Adaptive-UCS outperforms other UCSs with conventional crisp-hyperrectangular
and fuzzy-hypertrapezoidal rule representations in classification accuracy.
Additionally, Adaptive-UCS exhibits robustness in the case of noisy inputs and
real-world problems with inherent uncertainty, such as missing values, leading
to stable classification performance.

</details>

### [61] [Universal Approximation Theorem for Deep Q-Learning via FBSDE System](https://arxiv.org/abs/2505.06023)
*Qian Qi*

Main category: cs.LG

TLDR: 该论文提出了一种针对特定深度Q网络（DQN）的通用逼近定理（UAT），其架构设计模拟了贝尔曼更新中的迭代优化过程，并揭示了网络深度与值函数迭代次数的直接对应关系。


<details>
  <summary>Details</summary>
Motivation: 现有通用逼近定理（UAT）未充分利用最优Q函数的结构特性，本文旨在填补这一空白，通过结合贝尔曼方程的结构特性，为DQN提供更精确的理论支持。

Method: 利用贝尔曼算子的迭代特性，设计深度残差网络层作为神经算子，模拟贝尔曼算子的作用，并通过动态规划原理分析值函数迭代的均匀正则性。

Result: 证明了网络深度与值函数迭代次数的直接关系，并展示了误差传播的可控性，为DQN的逼近能力提供了结构化的理论依据。

Conclusion: 该研究为DQN的逼近能力提供了与问题结构直接相关的理论框架，揭示了网络在值函数空间中的动态系统视角。

Abstract: The approximation capabilities of Deep Q-Networks (DQNs) are commonly
justified by general Universal Approximation Theorems (UATs) that do not
leverage the intrinsic structural properties of the optimal Q-function, the
solution to a Bellman equation. This paper establishes a UAT for a class of
DQNs whose architecture is designed to emulate the iterative refinement process
inherent in Bellman updates. A central element of our analysis is the
propagation of regularity: while the transformation induced by a single Bellman
operator application exhibits regularity, for which Backward Stochastic
Differential Equations (BSDEs) theory provides analytical tools, the uniform
regularity of the entire sequence of value iteration iterates--specifically,
their uniform Lipschitz continuity on compact domains under standard Lipschitz
assumptions on the problem data--is derived from finite-horizon dynamic
programming principles. We demonstrate that layers of a deep residual network,
conceived as neural operators acting on function spaces, can approximate the
action of the Bellman operator. The resulting approximation theorem is thus
intrinsically linked to the control problem's structure, offering a proof
technique wherein network depth directly corresponds to iterations of value
function refinement, accompanied by controlled error propagation. This
perspective reveals a dynamic systems view of the network's operation on a
space of value functions.

</details>

### [62] [PYRREGULAR: A Unified Framework for Irregular Time Series, with Classification Benchmarks](https://arxiv.org/abs/2505.06047)
*Francesco Spinnato,Cristiano Landi*

Main category: cs.LG

TLDR: 提出统一框架和标准化数据集库，用于不规则时间序列分类，并评估12种分类器模型。


<details>
  <summary>Details</summary>
Motivation: 解决不规则时间数据（如记录频率不一、观测时长不同、缺失值）在多领域中的挑战，避免研究碎片化。

Method: 构建基于通用数组格式的统一框架和标准化数据集库，包含34个数据集，并评估12种分类器模型。

Result: 建立首个标准化数据集库，为不规则时间序列分类提供统一评估平台。

Conclusion: 该工作旨在集中研究资源，提升不规则时间序列分析方法的鲁棒性评估。

Abstract: Irregular temporal data, characterized by varying recording frequencies,
differing observation durations, and missing values, presents significant
challenges across fields like mobility, healthcare, and environmental science.
Existing research communities often overlook or address these challenges in
isolation, leading to fragmented tools and methods. To bridge this gap, we
introduce a unified framework, and the first standardized dataset repository
for irregular time series classification, built on a common array format to
enhance interoperability. This repository comprises 34 datasets on which we
benchmark 12 classifier models from diverse domains and communities. This work
aims to centralize research efforts and enable a more robust evaluation of
irregular temporal data analysis methods.

</details>

### [63] [Safe-EF: Error Feedback for Nonsmooth Constrained Optimization](https://arxiv.org/abs/2505.06053)
*Rustem Islamov,Yarden As,Ilyas Fatkhullin*

Main category: cs.LG

TLDR: 论文提出Safe-EF算法，解决联邦学习中非光滑目标和安全约束下的通信压缩问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中高维模型更新导致通信瓶颈，现有方法（如Top-K压缩）在非光滑目标和安全约束下性能受限。

Method: 提出Safe-EF算法，结合误差反馈和压缩技术，支持非光滑目标和安全约束。

Result: 实验验证Safe-EF在分布式人形机器人训练中有效减少通信复杂度并确保安全性。

Conclusion: Safe-EF填补了理论与实践的差距，为联邦学习中的非光滑问题和安全约束提供了高效解决方案。

Abstract: Federated learning faces severe communication bottlenecks due to the high
dimensionality of model updates. Communication compression with contractive
compressors (e.g., Top-K) is often preferable in practice but can degrade
performance without proper handling. Error feedback (EF) mitigates such issues
but has been largely restricted for smooth, unconstrained problems, limiting
its real-world applicability where non-smooth objectives and safety constraints
are critical. We advance our understanding of EF in the canonical non-smooth
convex setting by establishing new lower complexity bounds for first-order
algorithms with contractive compression. Next, we propose Safe-EF, a novel
algorithm that matches our lower bound (up to a constant) while enforcing
safety constraints essential for practical applications. Extending our approach
to the stochastic setting, we bridge the gap between theory and practical
implementation. Extensive experiments in a reinforcement learning setup,
simulating distributed humanoid robot training, validate the effectiveness of
Safe-EF in ensuring safety and reducing communication complexity.

</details>

### [64] [Fault Diagnosis of 3D-Printed Scaled Wind Turbine Blades](https://arxiv.org/abs/2505.06080)
*Luis Miguel Esquivel-Sancho,Maryam Ghandchi Tehrani,Mauricio Muñoz-Arias,Mahmoud Askari*

Main category: cs.LG

TLDR: 提出了一种结合3D打印模型、有限元模拟、实验模态分析和机器学习的风力涡轮机叶片故障检测方法，分类准确率超过94%。


<details>
  <summary>Details</summary>
Motivation: 开发一种综合方法，用于风力涡轮机叶片的故障检测，结合数值模拟和实验验证，为结构健康监测系统提供支持。

Method: 使用3D打印的NREL 5MW叶片缩比模型，引入裂纹损伤，通过有限元分析预测损伤对固有频率的影响，并通过实验模态分析验证。振动数据提取时域和频域特征，利用ANOVA识别关键变量，采用SVM和KNN分类器进行分类。

Result: 机器学习分类器准确率超过94%，振动模式3、4和6对结构异常敏感。

Conclusion: 该方法验证了数值模拟与实验验证结合的可行性，为风能应用中的结构健康监测系统奠定了基础。

Abstract: This study presents an integrated methodology for fault detection in wind
turbine blades using 3D-printed scaled models, finite element simulations,
experimental modal analysis, and machine learning techniques. A scaled model of
the NREL 5MW blade was fabricated using 3D printing, and crack-type damages
were introduced at critical locations. Finite Element Analysis was employed to
predict the impact of these damages on the natural frequencies, with the
results validated through controlled hammer impact tests. Vibration data was
processed to extract both time-domain and frequency-domain features, and key
discriminative variables were identified using statistical analyses (ANOVA).
Machine learning classifiers, including Support Vector Machine and K-Nearest
Neighbors, achieved classification accuracies exceeding 94%. The results
revealed that vibration modes 3, 4, and 6 are particularly sensitive to
structural anomalies for this blade. This integrated approach confirms the
feasibility of combining numerical simulations with experimental validations
and paves the way for structural health monitoring systems in wind energy
applications.

</details>

### [65] [Deep Diffusion Maps](https://arxiv.org/abs/2505.06087)
*Sergio García-Heredia,Ángela Fernández,Carlos M. Alaíz*

Main category: cs.LG

TLDR: 提出了一种基于深度学习的扩散映射嵌入方法，解决了传统方法无法应用于新数据、计算复杂和高内存消耗的问题。


<details>
  <summary>Details</summary>
Motivation: 传统非线性降维方法（如扩散映射）存在无法应用于新数据、计算复杂和高内存消耗的缺点，需要改进。

Method: 将扩散映射嵌入重新表述为一个无约束最小化问题，并设计了一个神经网络的损失函数，无需谱分解即可计算嵌入。

Result: 在真实和合成数据集上验证了该方法的有效性，并与扩散映射和Nystrom方法进行了比较。

Conclusion: 基于深度学习的方法能够有效解决传统扩散映射的局限性，适用于更广泛的数据集。

Abstract: One of the fundamental problems within the field of machine learning is
dimensionality reduction. Dimensionality reduction methods make it possible to
combat the so-called curse of dimensionality, visualize high-dimensional data
and, in general, improve the efficiency of storing and processing large data
sets. One of the best-known nonlinear dimensionality reduction methods is
Diffusion Maps. However, despite their virtues, both Diffusion Maps and many
other manifold learning methods based on the spectral decomposition of kernel
matrices have drawbacks such as the inability to apply them to data outside the
initial set, their computational complexity, and high memory costs for large
data sets. In this work, we propose to alleviate these problems by resorting to
deep learning. Specifically, a new formulation of Diffusion Maps embedding is
offered as a solution to a certain unconstrained minimization problem and,
based on it, a cost function to train a neural network which computes Diffusion
Maps embedding -- both inside and outside the training sample -- without the
need to perform any spectral decomposition. The capabilities of this approach
are compared on different data sets, both real and synthetic, with those of
Diffusion Maps and the Nystrom method.

</details>

### [66] [UniSymNet: A Unified Symbolic Network Guided by Transformer](https://arxiv.org/abs/2505.06091)
*Xinxin Li,Juan Zhang,Da Li,Xingyu Liu,Jin Xu,Junping Yin*

Main category: cs.LG

TLDR: 提出了一种统一符号网络（UniSymNet），通过将非线性二元运算符统一为嵌套一元运算符，并采用预训练的Transformer模型指导结构选择，提高了符号回归的性能。


<details>
  <summary>Details</summary>
Motivation: 主流符号回归算法在复杂树结构下性能受限，现有符号网络在多变量运算符扩展和固定架构训练中存在挑战。

Method: 将二元运算符统一为嵌套一元运算符，预训练Transformer模型指导结构选择，并采用目标优化策略学习参数。

Result: UniSymNet在高拟合精度、优秀符号解率和低表达式复杂度方面表现优异，在低维和高维基准测试中竞争力强。

Conclusion: UniSymNet为符号回归提供了一种高效且性能优越的新方法。

Abstract: Symbolic Regression (SR) is a powerful technique for automatically
discovering mathematical expressions from input data. Mainstream SR algorithms
search for the optimal symbolic tree in a vast function space, but the
increasing complexity of the tree structure limits their performance. Inspired
by neural networks, symbolic networks have emerged as a promising new paradigm.
However, most existing symbolic networks still face certain challenges: binary
nonlinear operators $\{\times, \div\}$ cannot be naturally extended to
multivariate operators, and training with fixed architecture often leads to
higher complexity and overfitting. In this work, we propose a Unified Symbolic
Network that unifies nonlinear binary operators into nested unary operators and
define the conditions under which UniSymNet can reduce complexity. Moreover, we
pre-train a Transformer model with a novel label encoding method to guide
structural selection, and adopt objective-specific optimization strategies to
learn the parameters of the symbolic network. UniSymNet shows high fitting
accuracy, excellent symbolic solution rate, and relatively low expression
complexity, achieving competitive performance on low-dimensional Standard
Benchmarks and high-dimensional SRBench.

</details>

### [67] [LLMs Outperform Experts on Challenging Biology Benchmarks](https://arxiv.org/abs/2505.06108)
*Lennart Justen*

Main category: cs.LG

TLDR: 该研究系统评估了27个前沿大语言模型在八个多样化生物学基准上的表现，发现模型性能显著提升，部分模型甚至超越专家水平。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在生物学领域的性能，以了解其进展和局限性。

Method: 对2022年11月至2025年4月间发布的模型进行十次独立运行测试，涵盖分子生物学、遗传学、克隆、病毒学和生物安全等领域。

Result: 模型性能显著提升，部分任务表现超越专家水平，但某些基准出现性能瓶颈。

Conclusion: 需要更复杂的评估方法以应对AI系统的快速发展。

Abstract: This study systematically evaluates 27 frontier Large Language Models on
eight diverse biology benchmarks spanning molecular biology, genetics, cloning,
virology, and biosecurity. Models from major AI developers released between
November 2022 and April 2025 were assessed through ten independent runs per
benchmark. The findings reveal dramatic improvements in biological
capabilities. Top model performance increased more than 4-fold on the
challenging text-only subset of the Virology Capabilities Test over the study
period, with the top model now performing twice as well as expert virologists.
Several models now match or exceed expert-level performance on other
challenging benchmarks, including LAB-Bench CloningScenarios and the biology
subsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not
substantially improve performance over zero-shot evaluation, while extended
reasoning features in o3-mini and Claude 3.7 Sonnet typically improved
performance as predicted by inference scaling. Benchmarks such as PubMedQA and
the MMLU and WMDP biology subsets exhibited performance plateaus well below
100%, suggesting benchmark saturation and errors in the underlying benchmark
data. The analysis highlights the need for more sophisticated evaluation
methodologies as AI systems continue to advance.

</details>

### [68] [FIC-TSC: Learning Time Series Classification with Fisher Information Constraint](https://arxiv.org/abs/2505.06114)
*Xiwen Chen,Wenhui Zhu,Peijie Qiu,Hao Wang,Huayu Li,Zihan Li,Yalin Wang,Aristeidis Sotiras,Abolfazl Razi*

Main category: cs.LG

TLDR: 提出了一种基于Fisher信息约束的时间序列分类框架FIC-TSC，显著提升了模型在域偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在多个领域至关重要，但域偏移问题严重影响分类性能。现有方法（如实例归一化）在分类任务中表现不佳。

Method: 提出FIC-TSC框架，利用Fisher信息约束引导模型收敛至平坦极小值，增强对分布偏移的泛化能力。

Result: 在30个UEA多变量和85个UCR单变量数据集上验证，FIC-TSC优于14种最新方法。

Conclusion: FIC-TSC是一种高效且有效的时间序列分类解决方案，显著提升了模型在域偏移下的性能。

Abstract: Analyzing time series data is crucial to a wide spectrum of applications,
including economics, online marketplaces, and human healthcare. In particular,
time series classification plays an indispensable role in segmenting different
phases in stock markets, predicting customer behavior, and classifying worker
actions and engagement levels. These aspects contribute significantly to the
advancement of automated decision-making and system optimization in real-world
applications. However, there is a large consensus that time series data often
suffers from domain shifts between training and test sets, which dramatically
degrades the classification performance. Despite the success of (reversible)
instance normalization in handling the domain shifts for time series regression
tasks, its performance in classification is unsatisfactory. In this paper, we
propose \textit{FIC-TSC}, a training framework for time series classification
that leverages Fisher information as the constraint. We theoretically and
empirically show this is an efficient and effective solution to guide the model
converge toward flatter minima, which enhances its generalizability to
distribution shifts. We rigorously evaluate our method on 30 UEA multivariate
and 85 UCR univariate datasets. Our empirical results demonstrate the
superiority of the proposed method over 14 recent state-of-the-art methods.

</details>

### [69] [Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena](https://arxiv.org/abs/2505.06123)
*Philip Naumann,Jacob Kauffmann,Grégoire Montavon*

Main category: cs.LG

TLDR: 提出了一种基于可解释AI的新方法，用于高效准确地归因Wasserstein距离到数据的不同组成部分。


<details>
  <summary>Details</summary>
Motivation: 单纯计算Wasserstein距离或分析传输图可能不足以理解其高低的原因。

Method: 基于可解释AI的解决方案，归因Wasserstein距离到数据子组、输入特征或可解释子空间。

Result: 方法在多样数据集和Wasserstein距离规格下均表现高准确性，并在两个用例中验证了实用性。

Conclusion: 该方法为理解Wasserstein距离的贡献因素提供了有效工具。

Abstract: Wasserstein distances provide a powerful framework for comparing data
distributions. They can be used to analyze processes over time or to detect
inhomogeneities within data. However, simply calculating the Wasserstein
distance or analyzing the corresponding transport map (or coupling) may not be
sufficient for understanding what factors contribute to a high or low
Wasserstein distance. In this work, we propose a novel solution based on
Explainable AI that allows us to efficiently and accurately attribute
Wasserstein distances to various data components, including data subgroups,
input features, or interpretable subspaces. Our method achieves high accuracy
across diverse datasets and Wasserstein distance specifications, and its
practical utility is demonstrated in two use cases.

</details>

### [70] [Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation](https://arxiv.org/abs/2505.06134)
*Julian F. Schumann,Jeroen Hagenus,Frederik Baymler Mathiesen,Arkady Zgonnikov*

Main category: cs.LG

TLDR: 论文提出了一种更全面的对抗性测试方法，通过扰动过去和未来状态来评估轨迹预测模型的鲁棒性，揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性攻击方法仅扰动过去位置，可能导致不现实的场景评估，忽略了关键漏洞，需要更全面的测试方法。

Method: 提出了一种新方法，扰动过去和未来状态，结合动态约束和战术行为，设计更有效的对抗性攻击。

Result: 测试显示预测误差和碰撞率显著增加，攻击暴露了模型无法检测潜在碰撞等关键弱点。

Conclusion: 需要更全面的对抗性测试以提升轨迹预测模型的可靠性。

Abstract: Trajectory prediction is a key element of autonomous vehicle systems,
enabling them to anticipate and react to the movements of other road users.
Evaluating the robustness of prediction models against adversarial attacks is
essential to ensure their reliability in real-world traffic. However, current
approaches tend to focus on perturbing the past positions of surrounding
agents, which can generate unrealistic scenarios and overlook critical
vulnerabilities. This limitation may result in overly optimistic assessments of
model performance in real-world conditions.
  In this work, we demonstrate that perturbing not just past but also future
states of adversarial agents can uncover previously undetected weaknesses and
thereby provide a more rigorous evaluation of model robustness. Our novel
approach incorporates dynamic constraints and preserves tactical behaviors,
enabling more effective and realistic adversarial attacks. We introduce new
performance measures to assess the realism and impact of these adversarial
trajectories. Testing our method on a state-of-the-art prediction model
revealed significant increases in prediction errors and collision rates under
adversarial conditions. Qualitative analysis further showed that our attacks
can expose critical weaknesses, such as the inability of the model to detect
potential collisions in what appear to be safe predictions. These results
underscore the need for more comprehensive adversarial testing to better
evaluate and improve the reliability of trajectory prediction models for
autonomous vehicles.

</details>

### [71] [On the Depth of Monotone ReLU Neural Networks and ICNNs](https://arxiv.org/abs/2505.06169)
*Egor Bakaev,Florestan Brunck,Christoph Hertrich,Daniel Reichman,Amir Yehudayoff*

Main category: cs.LG

TLDR: 论文研究了ReLU神经网络的两种模型（单调网络和输入凸神经网络），重点分析了它们在深度上的表达能力，并证明了相关下界。


<details>
  <summary>Details</summary>
Motivation: 探讨ReLU神经网络的表达能力，特别是在深度方面的限制，以理解不同模型的计算能力差异。

Method: 通过分析单调网络和输入凸神经网络对MAX_n函数的计算能力，结合多面体几何和三角剖分的等周性质，证明深度下界。

Result: 单调网络无法计算或近似MAX_n函数；输入凸神经网络对MAX_n的深度复杂度有严格下界n；ReLU网络与输入凸神经网络在深度上存在分离。

Conclusion: 不同神经网络模型在深度和表达能力上存在显著差异，多面体几何和等周性质为分析提供了有力工具。

Abstract: We study two models of ReLU neural networks: monotone networks (ReLU$^+$) and
input convex neural networks (ICNN). Our focus is on expressivity, mostly in
terms of depth, and we prove the following lower bounds. For the maximum
function MAX$_n$ computing the maximum of $n$ real numbers, we show that
ReLU$^+$ networks cannot compute MAX$_n$, or even approximate it. We prove a
sharp $n$ lower bound on the ICNN depth complexity of MAX$_n$. We also prove
depth separations between ReLU networks and ICNNs; for every $k$, there is a
depth-2 ReLU network of size $O(k^2)$ that cannot be simulated by a depth-$k$
ICNN. The proofs are based on deep connections between neural networks and
polyhedral geometry, and also use isoperimetric properties of triangulations.

</details>

### [72] [A Large Language Model-Enhanced Q-learning for Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2505.06178)
*Linjiang Cao,Maonan Wang,Xi Xiong*

Main category: cs.LG

TLDR: 本文提出了一种基于LLM增强的Q学习框架，用于解决带实时紧急约束的CVRPTW问题，通过自适应两阶段训练和三重自校正机制，实现了比传统Q学习更低的成本和更快的收敛。


<details>
  <summary>Details</summary>
Motivation: CVRPTW是一个经典的NP难组合优化问题，传统方法难以应对车辆容量和时间窗口的约束。LLM的进展为解决这一问题提供了新思路。

Method: 采用LLM增强的Q学习框架，包括自适应两阶段训练（LLM引导探索和自主优化）和三重自校正机制（语法验证、语义验证和物理约束强制执行）。

Result: 实验结果表明，该框架平均成本降低7.3%，收敛所需的训练步骤更少。

Conclusion: LLM增强的Q学习框架在解决CVRPTW问题上表现出色，为复杂优化问题提供了新方法。

Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
classic NP-hard combinatorial optimization problem widely applied in logistics
distribution and transportation management. Its complexity stems from the
constraints of vehicle capacity and time windows, which pose significant
challenges to traditional approaches. Advances in Large Language Models (LLMs)
provide new possibilities for finding approximate solutions to CVRPTW. This
paper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW
with real-time emergency constraints. Our solution introduces an adaptive
two-phase training mechanism that transitions from the LLM-guided exploration
phase to the autonomous optimization phase of Q-network. To ensure reliability,
we design a three-tier self-correction mechanism based on the Chain-of-Thought
(CoT) for LLMs: syntactic validation, semantic verification, and physical
constraint enforcement. In addition, we also prioritized replay of the
experience generated by LLMs to amplify the regulatory role of LLMs in the
architecture. Experimental results demonstrate that our framework achieves a
7.3\% average reduction in cost compared to traditional Q-learning, with fewer
training steps required for convergence.

</details>

### [73] [Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet](https://arxiv.org/abs/2505.06185)
*Kodai Hirata,Tsuyoshi Okita*

Main category: cs.LG

TLDR: MTL-Swin-Unet方法通过多任务学习结合分类和语义分割，利用图像重建和语义分割增强表示，在无协变量偏移和有协变量偏移情况下分别优于其他分类器。


<details>
  <summary>Details</summary>
Motivation: 解决虚假相关性问题，通过多任务学习提升图像表示能力。

Method: 使用MTL-Swin-Unet，结合分类和语义分割任务，并利用图像重建和语义分割增强表示。

Result: 在无协变量偏移下F值更优，有协变量偏移下AUC更优。

Conclusion: MTL-Swin-Unet在多任务学习和表示增强方面表现优异。

Abstract: This paper proposes a method MTL-Swin-Unet which is multi-task learning using
transformers for classification and semantic segmentation. For
spurious-correlation problems, this method allows us to enhance the image
representation with two other image representations: representation obtained by
semantic segmentation and representation obtained by image reconstruction. In
our experiments, the proposed method outperformed in F-value measure than other
classifiers when the test data included slices from the same patient (no
covariate shift). Similarly, when the test data did not include slices from the
same patient (covariate shift setting), the proposed method outperformed in AUC
measure.

</details>

### [74] [Auto Tensor Singular Value Thresholding: A Non-Iterative and Rank-Free Framework for Tensor Denoising](https://arxiv.org/abs/2505.06203)
*Hiroki Hasegawa,Yukihiko Okada*

Main category: cs.LG

TLDR: 提出了一种新的低秩张量近似方法，通过统计基础的奇异值阈值处理，自动提取重要成分，无需预设秩或迭代优化，显著提升了高维噪声数据的处理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代数据驱动任务中，传统矩阵方法难以处理高维数据的噪声和结构保留问题，张量方法虽能捕捉多维关系，但经典方法计算成本高且需预设秩，因此需要更高效的方法。

Method: 采用基于统计的奇异值阈值处理，对张量的模式矩阵化进行低秩近似，自动提取重要成分，避免了预设秩和迭代优化的需求。

Result: 在合成和真实张量数据上的实验表明，该方法在估计精度和计算效率上优于现有技术，尤其在高维噪声环境中表现突出。

Conclusion: 该方法为高维噪声数据提供了一种高效、自动化的低秩张量近似解决方案，显著提升了数据处理的准确性和实用性。

Abstract: In modern data-driven tasks such as classification, optimization, and
forecasting, mitigating the effects of intrinsic noise is crucial for improving
predictive accuracy. While numerous denoising techniques have been developed,
the rising dimensionality of real-world datasets limits conventional
matrix-based methods in preserving data structure and accuracy. This challenge
has led to increasing interest in tensor-based approaches, which naturally
capture multi-way data relationships. However, classical tensor decomposition
methods (e.g., HOSVD, HOOI) typically require pre-specified ranks and iterative
optimization, making them computationally expensive and less practical. In this
work, we propose a novel low-rank approximation method for tensor data that
avoids these limitations. Our approach applies statistically grounded singular
value thresholding to mode-wise matricizations, enabling automatic extraction
of significant components without requiring prior rank specification or
iterative refinement. Experiments on synthetic and real-world tensors show that
our method consistently outperforms existing techniques in terms of estimation
accuracy and computational efficiency, especially in noisy high-dimensional
settings.

</details>

### [75] [Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks](https://arxiv.org/abs/2505.06224)
*Christos Plachouras,Julien Guinot,George Fazekas,Elio Quinton,Emmanouil Benetos,Johan Pauwels*

Main category: cs.LG

TLDR: 论文提出了一种超越下游探测的表示评估方法，引入标准化协议量化模型表示的信息性、等变性、不变性和解缠性。


<details>
  <summary>Details</summary>
Motivation: 下游探测主要评估任务相关信息，忽略了表示的其他重要属性（如等变性、不变性、解缠性），这些属性对实际应用中的可解释性和适应性至关重要。目前缺乏统一的评估框架。

Method: 引入标准化协议，量化表示的信息性、等变性、不变性和解缠性，并在图像和语音领域的不同模型上进行评估。

Result: 发现下游性能相似的模型在这些属性上表现差异显著，提示其性能机制可能不同。

Conclusion: 强调表示评估的重要性，为理解和改进表示提供了新的研究方向。

Abstract: Downstream probing has been the dominant method for evaluating model
representations, an important process given the increasing prominence of
self-supervised learning and foundation models. However, downstream probing
primarily assesses the availability of task-relevant information in the model's
latent space, overlooking attributes such as equivariance, invariance, and
disentanglement, which contribute to the interpretability, adaptability, and
utility of representations in real-world applications. While some attempts have
been made to measure these qualities in representations, no unified evaluation
framework with modular, generalizable, and interpretable metrics exists.
  In this paper, we argue for the importance of representation evaluation
beyond downstream probing. We introduce a standardized protocol to quantify
informativeness, equivariance, invariance, and disentanglement of factors of
variation in model representations. We use it to evaluate representations from
a variety of models in the image and speech domains using different
architectures and pretraining approaches on identified controllable factors of
variation. We find that representations from models with similar downstream
performance can behave substantially differently with regard to these
attributes. This hints that the respective mechanisms underlying their
downstream performance are functionally different, prompting new research
directions to understand and improve representations.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [76] [KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification](https://arxiv.org/abs/2505.05583)
*Qianbo Zang,Christophe Zgrzendek,Igor Tchappi,Afshin Khadangi,Johannes Sedlmeir*

Main category: cs.CL

TLDR: KG-HTC结合知识图谱和大型语言模型，通过RAG方法解决层次文本分类中的数据不足和标签分布问题，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中标注数据不足，层次文本分类面临大标签空间和长尾分布问题。

Method: 利用知识图谱和LLMs，通过RAG方法检索相关子图，增强标签语义理解。

Result: 在三个开源数据集上显著优于基线方法，尤其在深层层次表现突出。

Conclusion: 结合结构化知识能有效解决层次文本分类的挑战。

Abstract: Hierarchical Text Classification (HTC) involves assigning documents to labels
organized within a taxonomy. Most previous research on HTC has focused on
supervised methods. However, in real-world scenarios, employing supervised HTC
can be challenging due to a lack of annotated data. Moreover, HTC often faces
issues with large label spaces and long-tail distributions. In this work, we
present Knowledge Graphs for zero-shot Hierarchical Text Classification
(KG-HTC), which aims to address these challenges of HTC in applications by
integrating knowledge graphs with Large Language Models (LLMs) to provide
structured semantic context during classification. Our method retrieves
relevant subgraphs from knowledge graphs related to the input text using a
Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to
understand label semantics at various hierarchy levels. We evaluate KG-HTC on
three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental
results show that KG-HTC significantly outperforms three baselines in the
strict zero-shot setting, particularly achieving substantial improvements at
deeper levels of the hierarchy. This evaluation demonstrates the effectiveness
of incorporating structured knowledge into LLMs to address HTC's challenges in
large label spaces and long-tailed label distributions. Our code is available
at: https://github.com/QianboZang/KG-HTC.

</details>

### [77] [Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation](https://arxiv.org/abs/2505.05648)
*Abdelrahman Abouelenin,Mohamed Abdelrehim,Raffy Fahim,Amr Hendy,Mohamed Afify*

Main category: cs.CL

TLDR: 论文训练了一个基于差分隐私（DP）的Transformer模型用于SwiftKey中的语言建模，通过实验平衡模型大小、运行速度和准确性，取得了比生产GRU模型更好的效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保护用户隐私（通过DP）的同时，提升语言模型的性能和效率。

Method: 采用两阶段训练：先在大规模通用数据上训练种子模型，再用DP微调用户输入数据；同时缩小GPT2架构以适应需求。

Result: 模型在下一词预测和准确性上取得小幅但一致的提升，同时内存和速度增加可控。

Conclusion: 通过DP和两阶段训练，成功实现了高效且隐私保护的语言模型，适用于SwiftKey应用。

Abstract: In this paper we train a transformer using differential privacy (DP) for
language modeling in SwiftKey. We run multiple experiments to balance the
trade-off between the model size, run-time speed and accuracy. We show that we
get small and consistent gains in the next-word-prediction and accuracy with
graceful increase in memory and speed compared to the production GRU. This is
obtained by scaling down a GPT2 architecture to fit the required size and a two
stage training process that builds a seed model on general data and DP
finetunes it on typing data. The transformer is integrated using ONNX offering
both flexibility and efficiency.

</details>

### [78] [Exploration of COVID-19 Discourse on Twitter: American Politician Edition](https://arxiv.org/abs/2505.05687)
*Cindy Kim,Daniela Puchall,Jiangyi Liang,Jiwon Kim*

Main category: cs.CL

TLDR: 论文研究了美国两党（共和党和民主党）在COVID-19疫情中的推特言论差异，使用语言模型分析关键词和情感，发现民主党更关注疫情伤亡和医疗建议，而共和党更注重政治责任和媒体通报。


<details>
  <summary>Details</summary>
Motivation: 探讨COVID-19疫情如何加剧政治极化，并分析两党在应对疫情时的不同态度和策略。

Method: 收集美国政治人物的推特数据，采用词袋模型、双词模型和TF-IDF模型分析关键词、主题和情感。

Result: 民主党更关注疫情伤亡和医疗建议，共和党更注重政治责任和媒体通报。

Conclusion: 提出了一种基于语言模型的分类算法，用于预测推文的政治立场（左倾或右倾）。

Abstract: The advent of the COVID-19 pandemic has undoubtedly affected the political
scene worldwide and the introduction of new terminology and public opinions
regarding the virus has further polarized partisan stances. Using a collection
of tweets gathered from leading American political figures online (Republican
and Democratic), we explored the partisan differences in approach, response,
and attitude towards handling the international crisis. Implementation of the
bag-of-words, bigram, and TF-IDF models was used to identify and analyze
keywords, topics, and overall sentiments from each party. Results suggest that
Democrats are more concerned with the casualties of the pandemic, and give more
medical precautions and recommendations to the public whereas Republicans are
more invested in political responsibilities such as keeping the public updated
through media and carefully watching the progress of the virus. We propose a
systematic approach to predict and distinguish a tweet's political stance (left
or right leaning) based on its COVID-19 related terms using different
classification algorithms on different language models.

</details>

### [79] [Assessing Robustness to Spurious Correlations in Post-Training Language Models](https://arxiv.org/abs/2505.05704)
*Julia Shuieh,Prasann Singhal,Apaar Shanker,John Heyer,George Pu,Samuel Denton*

Main category: cs.CL

TLDR: 本文系统评估了三种后训练算法（SFT、DPO、KTO）在不同任务和虚假相关性条件下的表现，发现不同方法在不同任务中各有优劣。


<details>
  <summary>Details</summary>
Motivation: 研究真实训练数据中的虚假相关性如何影响大语言模型的性能和泛化能力。

Method: 在数学推理、指令遵循和文档问答任务中，比较SFT、DPO和KTO在不同虚假相关性条件下的表现。

Result: 偏好方法（DPO/KTO）在数学推理中表现稳健，而SFT在复杂任务中更优。

Conclusion: 后训练策略的选择需根据任务类型和虚假相关性特征决定，无单一最优方法。

Abstract: Supervised and preference-based fine-tuning techniques have become popular
for aligning large language models (LLMs) with user intent and correctness
criteria. However, real-world training data often exhibits spurious
correlations -- arising from biases, dataset artifacts, or other "shortcut"
features -- that can compromise a model's performance or generalization. In
this paper, we systematically evaluate three post-training algorithms --
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO
(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and
spuriousness conditions. Our tasks span mathematical reasoning, constrained
instruction-following, and document-grounded question answering. We vary the
degree of spurious correlation (10% vs. 90%) and investigate two forms of
artifacts: "Feature Ambiguity" and "Distributional Narrowness." Our results
show that the models often but not always degrade under higher spuriousness.
The preference-based methods (DPO/KTO) can demonstrate relative robustness in
mathematical reasoning tasks. By contrast, SFT maintains stronger performance
in complex, context-intensive tasks. These findings highlight that no single
post-training strategy universally outperforms in all scenarios; the best
choice depends on the type of target task and the nature of spurious
correlations.

</details>

### [80] [TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries](https://arxiv.org/abs/2505.05714)
*Jinze Lv,Jian Chen,Zi Long,Xianghua Fu,Yin Chen*

Main category: cs.CL

TLDR: 论文提出了TopicVD数据集，用于视频支持的多模态机器翻译（MMT）研究，并提出了基于跨模态双向注意力模块的MMT模型。实验表明视觉信息和全局上下文能提升翻译性能，但跨领域性能下降显著。


<details>
  <summary>Details</summary>
Motivation: 现有MMT数据集多为静态图像或短视频，无法满足真实任务（如纪录片翻译）需求。因此，构建TopicVD数据集以推动视频支持的MMT研究。

Method: 收集纪录片视频-字幕对，按主题分类，提出基于跨模态双向注意力模块的MMT模型。

Result: 视觉信息提升翻译性能，但跨领域性能下降；全局上下文能有效改善翻译效果。

Conclusion: TopicVD数据集和提出的模型为视频支持的MMT研究提供了新方向，但需进一步研究领域适应方法。

Abstract: Most existing multimodal machine translation (MMT) datasets are predominantly
composed of static images or short video clips, lacking extensive video data
across diverse domains and topics. As a result, they fail to meet the demands
of real-world MMT tasks, such as documentary translation. In this study, we
developed TopicVD, a topic-based dataset for video-supported multimodal machine
translation of documentaries, aiming to advance research in this field. We
collected video-subtitle pairs from documentaries and categorized them into
eight topics, such as economy and nature, to facilitate research on domain
adaptation in video-guided MMT. Additionally, we preserved their contextual
information to support research on leveraging the global context of
documentaries in video-guided MMT. To better capture the shared semantics
between text and video, we propose an MMT model based on a cross-modal
bidirectional attention module. Extensive experiments on the TopicVD dataset
demonstrate that visual information consistently improves the performance of
the NMT model in documentary translation. However, the MMT model's performance
significantly declines in out-of-domain scenarios, highlighting the need for
effective domain adaptation methods. Additionally, experiments demonstrate that
global context can effectively improve translation performance. % Dataset and
our implementations are available at https://github.com/JinzeLv/TopicVD

</details>

### [81] [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/abs/2505.05755)
*Dhruvesh Patel,Aishwarya Sahoo,Avinash Amballa,Tahira Naseem,Tim G. J. Rudner,Andrew McCallum*

Main category: cs.CL

TLDR: 插入语言模型（ILMs）通过任意位置插入令牌解决了自回归模型（ARMs）和掩码扩散模型（MDMs）在序列生成中的限制，表现优于两者。


<details>
  <summary>Details</summary>
Motivation: 自回归模型和掩码扩散模型在处理复杂约束或非顺序依赖的序列时存在不足，需要更灵活的生成方法。

Method: 提出插入语言模型（ILMs），通过联合选择插入位置和词汇元素，并采用定制网络参数化和去噪目标进行训练。

Result: ILMs在规划任务中优于ARMs和MDMs，在无条件文本生成任务中与ARMs相当，同时在任意长度文本填充中比MDMs更灵活。

Conclusion: ILMs提供了一种更灵活的序列生成方法，能够更好地处理复杂约束和非顺序依赖的序列。

Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one
``from left to right,'' have achieved significant success across a wide range
of sequence generation tasks. However, they struggle to accurately represent
sequences that require satisfying sophisticated constraints or whose sequential
dependencies are better addressed by out-of-order generation. Masked Diffusion
Models (MDMs) address some of these limitations, but the process of unmasking
multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs
cannot handle arbitrary infilling constraints when the number of tokens to be
filled in is not known in advance. In this work, we introduce Insertion
Language Models (ILMs), which learn to insert tokens at arbitrary positions in
a sequence -- that is, they select jointly both the position and the vocabulary
element to be inserted. By inserting tokens one at a time, ILMs can represent
strong dependencies between tokens, and their ability to generate sequences in
arbitrary order allows them to accurately model sequences where token
dependencies do not follow a left-to-right sequential structure. To train ILMs,
we propose a tailored network parameterization and use a simple denoising
objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs
and MDMs on common planning tasks. Furthermore, we show that ILMs outperform
MDMs and perform on par with ARMs in an unconditional text generation task
while offering greater flexibility than MDMs in arbitrary-length text
infilling.

</details>

### [82] [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](https://arxiv.org/abs/2505.05772)
*Zehao Fan,Garrett Gagnon,Zhenyu Liu,Liu Liu*

Main category: cs.CL

TLDR: STARC是一种针对PIM架构优化的稀疏数据映射方案，通过语义相似性聚类KV对并映射到连续内存区域，显著降低LLM解码的延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的自回归解码对内存系统造成压力，尤其是KV缓存的动态和稀疏访问模式，导致PIM架构的工作负载不平衡。

Method: STARC通过聚类KV对并按语义相似性映射到连续内存区域，利用预计算中心点实现选择性注意和并行处理。

Result: 实验显示STARC降低注意力层延迟19%--31%，能耗19%--27%，在KV缓存预算下延迟和能耗进一步显著降低。

Conclusion: STARC在保持模型精度的同时，显著提升了PIM架构上长上下文LLM推理的效率。

Abstract: Transformer-based models are the foundation of modern machine learning, but
their execution, particularly during autoregressive decoding in large language
models (LLMs), places significant pressure on memory systems due to frequent
memory accesses and growing key-value (KV) caches. This creates a bottleneck in
memory bandwidth, especially as context lengths increase. Processing-in-memory
(PIM) architectures are a promising solution, offering high internal bandwidth
and compute parallelism near memory. However, current PIM designs are primarily
optimized for dense attention and struggle with the dynamic, irregular access
patterns introduced by modern KV cache sparsity techniques. Consequently, they
suffer from workload imbalance, reducing throughput and resource utilization.
In this work, we propose STARC, a novel sparsity-optimized data mapping scheme
tailored specifically for efficient LLM decoding on PIM architectures. STARC
clusters KV pairs by semantic similarity and maps them to contiguous memory
regions aligned with PIM bank structures. During decoding, queries retrieve
relevant tokens at cluster granularity by matching against precomputed
centroids, enabling selective attention and parallel processing without
frequent reclustering or data movement overhead. Experiments on the HBM-PIM
system show that, compared to common token-wise sparsity methods, STARC reduces
attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a
KV cache budget of 1024, it achieves up to 54%--74% latency reduction and
45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC
maintains model accuracy comparable to state-of-the-art sparse attention
methods, demonstrating its effectiveness in enabling efficient and
hardware-friendly long-context LLM inference on PIM architectures.

</details>

### [83] [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/abs/2505.05815)
*Machi Shimmei,Masaki Uto,Yuichiroh Matsubayashi,Kentaro Inui,Aditi Mallavarapu,Noboru Matsuda*

Main category: cs.CL

TLDR: AnaQuest是一种创新的提示技术，用于生成选择题（MCQ），结合了形成性和总结性评估，其生成的题目在难度和区分度上更接近人工编写的题目。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够生成高质量选择题的技术，以支持教育评估，同时利用预训练语言模型的潜力。

Method: AnaQuest通过分析学生对开放问题的回答生成正确和错误的断言，形成选择题，并使用IRT比较其与ChatGPT生成题目及人工编写题目的特性。

Result: 专家评估认为AnaQuest和ChatGPT生成的题目与人工编写的题目同样有效，但IRT分析显示AnaQuest生成的题目（尤其是错误选项）更接近人工编写的题目。

Conclusion: AnaQuest在生成高质量选择题方面表现优异，尤其是在错误选项的设计上，优于ChatGPT，接近人工编写水平。

Abstract: The primary goal of this study is to develop and evaluate an innovative
prompting technique, AnaQuest, for generating multiple-choice questions (MCQs)
using a pre-trained large language model. In AnaQuest, the choice items are
sentence-level assertions about complex concepts. The technique integrates
formative and summative assessments. In the formative phase, students answer
open-ended questions for target concepts in free text. For summative
assessment, AnaQuest analyzes these responses to generate both correct and
incorrect assertions. To evaluate the validity of the generated MCQs, Item
Response Theory (IRT) was applied to compare item characteristics between MCQs
generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An
empirical study found that expert instructors rated MCQs generated by both AI
models to be as valid as those created by human instructors. However, IRT-based
analysis revealed that AnaQuest-generated questions - particularly those with
incorrect assertions (foils) - more closely resembled human-crafted items in
terms of difficulty and discrimination than those produced by ChatGPT.

</details>

### [84] [Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI](https://arxiv.org/abs/2505.05864)
*Junhyeong Lee,Jong Min Yuk,Chan-Woo Lee*

Main category: cs.CL

TLDR: 提出了一种混合文本挖掘框架，结合多步和直接方法的优势，将非结构化科学文本转化为结构化数据，并通过实体标记技术显著提升了实体识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（多步和直接方法）在独立应用时存在局限性，需要一种结合两者优势的新方法以提高数据提取的效率和准确性。

Method: 提出了一种混合框架，先将原始文本转化为实体识别文本，再转化为结构化数据，并引入实体标记技术以增强实体识别。

Result: 在三个基准数据集上，该方法在实体级和关系级F1分数上分别提升了58%和83%，显著优于直接方法。

Conclusion: 混合框架和实体标记技术的结合有效提升了科学文本的结构化数据提取性能。

Abstract: The construction of experimental datasets is essential for expanding the
scope of data-driven scientific discovery. Recent advances in natural language
processing (NLP) have facilitated automatic extraction of structured data from
unstructured scientific literature. While existing approaches-multi-step and
direct methods-offer valuable capabilities, they also come with limitations
when applied independently. Here, we propose a novel hybrid text-mining
framework that integrates the advantages of both methods to convert
unstructured scientific text into structured data. Our approach first
transforms raw text into entity-recognized text, and subsequently into
structured form. Furthermore, beyond the overall data structuring framework, we
also enhance entity recognition performance by introducing an entity marker-a
simple yet effective technique that uses symbolic annotations to highlight
target entities. Specifically, our entity marker-based hybrid approach not only
consistently outperforms previous entity recognition approaches across three
benchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the
quality of final structured data-yielding up to a 58% improvement in
entity-level F1 score and up to 83% improvement in relation-level F1 score
compared to direct approach.

</details>

### [85] [Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2](https://arxiv.org/abs/2505.05946)
*Vytenis Šliogeris,Povilas Daniušis,Artūras Nakvosas*

Main category: cs.CL

TLDR: 实验研究了在Gemma2 20亿参数大语言模型上使用弹性权重巩固（EWC）进行自回归预训练，探讨了其对多语言任务（包括立陶宛语）的持续学习效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过EWC减轻大语言模型在持续学习中的灾难性遗忘问题，并探索其对学习新任务的潜在益处。

Method: 在Gemma2模型上应用EWC，评估其在多语言理解基准测试（如Arc、Belebele等）和困惑度测试中的表现。

Result: 实证表明，EWC不仅能缓解灾难性遗忘，还可能对新任务学习有益。

Conclusion: EWC是持续学习中的有效方法，尤其适用于大语言模型的多语言任务。

Abstract: This technical report describes an experiment on autoregressive pre-training
of Gemma2 2 billion parameter large language model (LLM) with 10\% on the
Lithuanian language component of CulturaX from the point of view of continual
learning. We apply elastic weight consolidation (EWC) to the full set of the
model's parameters and investigate language understanding benchmarks,
consisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande
sets (both in English and Lithuanian versions), and perplexity benchmarks. We
empirically demonstrate that EWC regularisation allows us not only to mitigate
catastrophic forgetting effects but also that it is potentially beneficial for
learning of the new task with LLMs.

</details>

### [86] [Summarisation of German Judgments in conjunction with a Class-based Evaluation](https://arxiv.org/abs/2505.05947)
*Bianca Steffes,Nils Torben Wiedemann,Alexander Gratz,Pamela Hochreither,Jana Elina Meyer,Katharina Luise Schilke*

Main category: cs.CL

TLDR: 本文通过微调解码器大语言模型自动生成德国判决书的摘要，并引入法律实体信息提升效果，但生成摘要的质量尚不足以实际应用。


<details>
  <summary>Details</summary>
Motivation: 为法律专家提供自动化工具以简化长法律文件的摘要生成。

Method: 微调解码器大语言模型，并在训练前为判决书添加法律实体信息。

Result: 使用法律实体信息有助于模型找到相关内容，但生成的摘要质量尚未达到实际应用标准。

Conclusion: 尽管模型在生成摘要方面取得进展，仍需进一步优化以提升质量。

Abstract: The automated summarisation of long legal documents can be a great aid for
legal experts in their daily work. We automatically create summaries (guiding
principles) of German judgments by fine-tuning a decoder-based large language
model. We enrich the judgments with information about legal entities before the
training. For the evaluation of the created summaries, we define a set of
evaluation classes which allows us to measure their language, pertinence,
completeness and correctness. Our results show that employing legal entities
helps the generative model to find the relevant content, but the quality of the
created summaries is not yet sufficient for a use in practice.

</details>

### [87] [NeoQA: Evidence-based Question Answering with Generated News Events](https://arxiv.org/abs/2505.05949)
*Max Glockner,Xiang Jiang,Leonardo F. R. Ribeiro,Iryna Gurevych,Markus Dreyer*

Main category: cs.CL

TLDR: NeoQA是一个新的基准测试，用于评估大型语言模型（LLMs）在检索增强生成（RAG）中的表现，通过虚构新闻事件和实体确保模型无法依赖预训练知识。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试容易过时，难以区分基于证据的推理和预训练知识的回忆。

Method: 构建虚构新闻事件的时间线和知识库，生成问答对，确保模型只能依赖检索到的证据回答问题。

Result: LLMs在问题和证据之间细微不匹配时表现不佳，且在关键信息缺失时容易走捷径推理。

Conclusion: NeoQA提供了一个可控的评估平台，揭示了LLMs在基于证据推理中的关键局限性。

Abstract: Evaluating Retrieval-Augmented Generation (RAG) in large language models
(LLMs) is challenging because benchmarks can quickly become stale. Questions
initially requiring retrieval may become answerable from pretraining knowledge
as newer models incorporate more recent information during pretraining, making
it difficult to distinguish evidence-based reasoning from recall. We introduce
NeoQA (News Events for Out-of-training Question Answering), a benchmark
designed to address this issue. To construct NeoQA, we generated timelines and
knowledge bases of fictional news events and entities along with news articles
and Q\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring
that no prior evidence exists in their training data. We propose our dataset as
a new platform for evaluating evidence-based question answering, as it requires
LLMs to generate responses exclusively from retrieved evidence and only when
sufficient evidence is available. NeoQA enables controlled evaluation across
various evidence scenarios, including cases with missing or misleading details.
Our findings indicate that LLMs struggle to distinguish subtle mismatches
between questions and evidence, and suffer from short-cut reasoning when key
information required to answer a question is missing from the evidence,
underscoring key limitations in evidence-based reasoning.

</details>

### [88] [Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models](https://arxiv.org/abs/2505.05970)
*Lennart Stöpler,Rufat Asadli,Mitja Nikolaus,Ryan Cotterell,Alex Warstadt*

Main category: cs.CL

TLDR: 提出了一种基于交互式语言学习的训练方法，模拟儿童语言习得过程，通过单轮对话中的奖励机制优化语言模型。


<details>
  <summary>Details</summary>
Motivation: 受儿童语言习得启发，探索在抽象问答任务中通过交互式学习提升语言模型的性能。

Method: 使用强化学习微调语言模型，通过问答任务中的奖励信号间接评估语法正确性。

Result: 实验表明，通信通道的认知约束能改变说话者行为，但尚未在语言评估中观察到显著提升。

Conclusion: 未来可通过改进任务设计和训练配置，进一步验证交互式学习对语言模型的益处。

Abstract: We propose a method for training language models in an interactive setting
inspired by child language acquisition. In our setting, a speaker attempts to
communicate some information to a listener in a single-turn dialogue and
receives a reward if communicative success is achieved. Unlike earlier related
work using image--caption data for interactive reference games, we
operationalize communicative success in a more abstract language-only
question--answering setting. First, we present a feasibility study
demonstrating that our reward provides an indirect signal about grammaticality.
Second, we conduct experiments using reinforcement learning to fine-tune
language models. We observe that cognitively plausible constraints on the
communication channel lead to interpretable changes in speaker behavior.
However, we do not yet see improvements on linguistic evaluations from our
training regime. We outline potential modifications to the task design and
training configuration that could better position future work to use our
methodology to observe the benefits of interaction on language learning in
computational cognitive models.

</details>

### [89] [An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition](https://arxiv.org/abs/2505.05973)
*M. Maziyah Mohamed,R. H. Baayen*

Main category: cs.CL

TLDR: 论文探讨了语义透明度的嵌入式测量方法及其对阅读的影响，通过聚类分析和线性判别分析验证了这些方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究语义透明度的计算操作化及其在词汇识别中的作用。

Method: 采用t-SNE聚类分析和线性判别分析，对马来语前缀词进行语义空间几何分析，并评估五种简单测量方法对词汇决策延迟的预测能力。

Result: 所有测量方法均能预测词汇决策延迟，其中词与中心词的相关性模型拟合最佳。

Conclusion: 嵌入式的语义透明度测量方法对词汇识别有显著影响，为语义透明度的计算提供了新思路。

Abstract: Studies of morphological processing have shown that semantic transparency is
crucial for word recognition. Its computational operationalization is still
under discussion. Our primary objectives are to explore embedding-based
measures of semantic transparency, and assess their impact on reading. First,
we explored the geometry of complex words in semantic space. To do so, we
conducted a t-distributed Stochastic Neighbor Embedding clustering analysis on
4,226 Malay prefixed words. Several clusters were observed for complex words
varied by their prefix class. Then, we derived five simple measures, and
investigated whether they were significant predictors of lexical decision
latencies. Two sets of Linear Discriminant Analyses were run in which the
prefix of a word is predicted from either word embeddings or shift vectors
(i.e., a vector subtraction of the base word from the derived word). The
accuracy with which the model predicts the prefix of a word indicates the
degree of transparency of the prefix. Three further measures were obtained by
comparing embeddings between each word and all other words containing the same
prefix (i.e., centroid), between each word and the shift from their base word,
and between each word and the predicted word of the Functional Representations
of Affixes in Compositional Semantic Space model. In a series of Generalized
Additive Mixed Models, all measures predicted decision latencies after
accounting for word frequency, word length, and morphological family size. The
model that included the correlation between each word and their centroid as a
predictor provided the best fit to the data.

</details>

### [90] [Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models](https://arxiv.org/abs/2505.06004)
*Dawid Wisniewski,Antoni Solarski,Artur Nowakowski*

Main category: cs.CL

TLDR: 研究分析了17个流行模型在英语、德语、意大利语和瑞典语的多语言语法纠错任务中的表现，推荐了6个表现优秀的模型，并指出Gemma 9B是目前最佳选择。


<details>
  <summary>Details</summary>
Motivation: 探索单一模型在多语言语法纠错任务中的表现，以提升文本的语法正确性。

Method: 分析17个模型在四种语言中的输出，重点关注减少语法错误并保持最小改动。

Result: 确定了6个在四种语言中均表现优秀的模型，Gemma 9B表现最佳。

Conclusion: 研究揭示了多语言语法纠错模型的常见问题，并推荐了适用于多语言任务的模型。

Abstract: Recent language models can successfully solve various language-related tasks,
and many understand inputs stated in different languages. In this paper, we
explore the performance of 17 popular models used to correct grammatical issues
in texts stated in English, German, Italian, and Swedish when using a single
model to correct texts in all those languages. We analyze the outputs generated
by these models, focusing on decreasing the number of grammatical errors while
keeping the changes small. The conclusions drawn help us understand what
problems occur among those models and which models can be recommended for
multilingual grammatical error correction tasks. We list six models that
improve grammatical correctness in all four languages and show that Gemma 9B is
currently the best performing one for the languages considered.

</details>

### [91] [Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective](https://arxiv.org/abs/2505.06010)
*Dawid Wisniewski,Mikolaj Pokrywka,Zofia Rostek*

Main category: cs.CL

TLDR: 分析当前机器翻译模型在保留特定实体（如URL、IBAN、电子邮件）时的表现，并提出一个新数据集以评估质量。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译模型在大多数场景下表现良好，但在保留某些实体（如URL、IBAN、电子邮件）时仍存在问题。

Method: 评估OPUS、Google Translate、MADLAD和EuroLLM等流行NMT模型在四种语言（英语、德语、波兰语、乌克兰语）中保留实体的能力。

Result: 分析显示，某些类别（如表情符号）对模型构成显著挑战，并讨论了错误原因。

Conclusion: 提出一个包含36,000句的多语言合成数据集，用于评估九类实体在四种语言中的传递质量。

Abstract: Current machine translation models provide us with high-quality outputs in
most scenarios. However, they still face some specific problems, such as
detecting which entities should not be changed during translation. In this
paper, we explore the abilities of popular NMT models, including models from
the OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities
such as URL addresses, IBAN numbers, or emails when producing translations
between four languages: English, German, Polish, and Ukrainian. We investigate
the quality of popular NMT models in terms of accuracy, discuss errors made by
the models, and examine the reasons for errors. Our analysis highlights
specific categories, such as emojis, that pose significant challenges for many
models considered. In addition to the analysis, we propose a new multilingual
synthetic dataset of 36,000 sentences that can help assess the quality of
entity transfer across nine categories and four aforementioned languages.

</details>

### [92] [Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation](https://arxiv.org/abs/2505.06027)
*Stefan Vasilev,Christian Herold,Baohao Liao,Seyyed Hadi Hashemi,Shahram Khadivi,Christof Monz*

Main category: cs.CL

TLDR: Unilogit是一种新型的自蒸馏方法，用于大型语言模型的机器遗忘，动态调整目标logits以实现均匀概率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在遵守GDPR等数据隐私法规时，选择性遗忘特定信息同时保持模型整体效用的挑战。

Method: 动态调整目标logits，利用当前模型输出生成更准确的自蒸馏目标，无需额外超参数。

Result: 在公开基准和内部数据集上表现优异，平衡遗忘与保留目标，优于NPO和UnDIAL。

Conclusion: Unilogit在多种场景下表现稳健，实用性强，能有效实现机器遗忘。

Abstract: This paper introduces Unilogit, a novel self-distillation method for machine
unlearning in Large Language Models. Unilogit addresses the challenge of
selectively forgetting specific information while maintaining overall model
utility, a critical task in compliance with data privacy regulations like GDPR.
Unlike prior methods that rely on static hyperparameters or starting model
outputs, Unilogit dynamically adjusts target logits to achieve a uniform
probability for the target token, leveraging the current model's outputs for
more accurate self-distillation targets. This approach not only eliminates the
need for additional hyperparameters but also enhances the model's ability to
approximate the golden targets. Extensive experiments on public benchmarks and
an in-house e-commerce dataset demonstrate Unilogit's superior performance in
balancing forget and retain objectives, outperforming state-of-the-art methods
such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness
across various scenarios, highlighting its practical applicability and
effectiveness in achieving efficacious machine unlearning.

</details>

### [93] [Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information](https://arxiv.org/abs/2505.06046)
*Joshua Harris,Fan Grayson,Felix Feldman,Timothy Laurence,Toby Nonnenmacher,Oliver Higgins,Leo Loman,Selina Patel,Thomas Finnie,Samuel Collins,Michael Borowitz*

Main category: cs.CL

TLDR: 论文提出了一个名为PubHealthBench的新基准，用于评估大型语言模型（LLMs）在公共健康领域的知识掌握情况，发现最新私有LLMs在多选题回答上表现优异，但在自由回答中表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及，了解其在特定领域（如公共健康）的知识准确性变得至关重要，尤其是在可能影响公众健康的情况下。

Method: 通过自动化流程创建了包含8000多个问题的PubHealthBench基准，评估了24个LLMs在多选题和自由回答中的表现。

Result: 最新私有LLMs在多选题回答中表现优异（>90%），优于人类；但在自由回答中表现较差（<75%）。

Conclusion: 尽管SOTA LLMs在公共健康信息提供上表现出潜力，但在自由回答中仍需额外保障措施。

Abstract: As Large Language Models (LLMs) become widely accessible, a detailed
understanding of their knowledge within specific domains becomes necessary for
successful real world use. This is particularly critical in public health,
where failure to retrieve relevant, accurate, and current information could
significantly impact UK residents. However, currently little is known about LLM
knowledge of UK Government public health information. To address this issue,
this paper introduces a new benchmark, PubHealthBench, with over 8000 questions
for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form
responses to public health queries, created via an automated pipeline. We also
release a new dataset of the extracted UK Government public health guidance
documents used as source text for PubHealthBench. Assessing 24 LLMs on
PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a
high degree of knowledge, achieving >90% in the MCQA setup, and outperform
humans with cursory search engine use. However, in the free form setup we see
lower performance with no model scoring >75%. Therefore, whilst there are
promising signs that state of the art (SOTA) LLMs are an increasingly accurate
source of public health information, additional safeguards or tools may still
be needed when providing free form responses on public health topics.

</details>

### [94] [Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax](https://arxiv.org/abs/2505.06062)
*Iuliia Zaitova,Vitalii Hirak,Badr M. Abdullah,Dietrich Klakow,Bernd Möbius,Tania Avgustinova*

Main category: cs.CL

TLDR: 研究分析了基于BERT架构的微调编码器模型对多词表达（MWEs）的注意力模式，比较了习语和微句法单元（MSUs）。结果显示微调任务类型显著影响模型对MWEs的注意力分配。


<details>
  <summary>Details</summary>
Motivation: 理解微调任务（语义或句法）如何影响BERT模型对MWEs（习语和MSUs）的注意力模式。

Method: 使用六种印欧语言的单语模型和数据集，分析预训练和微调BERT模型对MWEs的注意力分数。

Result: 语义任务微调模型对习语的注意力在各层更均匀；句法任务微调模型在低层对MSUs的注意力增加。

Conclusion: 微调任务类型显著影响模型对MWEs的注意力分配，语义和句法任务分别对应习语和MSUs的不同注意力模式。

Abstract: This study analyzes the attention patterns of fine-tuned encoder-only models
based on the BERT architecture (BERT-based models) towards two distinct types
of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms
present challenges in semantic non-compositionality, whereas MSUs demonstrate
unconventional syntactic behavior that does not conform to standard grammatical
categorizations. We aim to understand whether fine-tuning BERT-based models on
specific tasks influences their attention to MWEs, and how this attention
differs between semantic and syntactic tasks. We examine attention scores to
MWEs in both pre-trained and fine-tuned BERT-based models. We utilize
monolingual models and datasets in six Indo-European languages - English,
German, Dutch, Polish, Russian, and Ukrainian. Our results show that
fine-tuning significantly influences how models allocate attention to MWEs.
Specifically, models fine-tuned on semantic tasks tend to distribute attention
to idiomatic expressions more evenly across layers. Models fine-tuned on
syntactic tasks show an increase in attention to MSUs in the lower layers,
corresponding with syntactic processing requirements.

</details>

### [95] [Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models](https://arxiv.org/abs/2505.06110)
*Jugal Gajjar,Kaustik Ranaware*

Main category: cs.CL

TLDR: 使用基于Transformer的早期融合方法在CMU-MOSEI数据集上进行多模态情感分析，取得了高精度和F1分数。


<details>
  <summary>Details</summary>
Motivation: 探索多模态（文本、音频、视觉）情感分析中早期融合策略的有效性，并验证Transformer架构的优越性。

Method: 采用BERT编码器分别处理各模态数据，通过早期融合（拼接嵌入）进行分类，使用Adam优化器和早停策略。

Result: 模型在测试集上达到97.87%的7类准确率和0.9682的F1分数，MAE为0.1060，表现优异。

Conclusion: 早期融合和Transformer架构在多模态情感分析中效果显著，未来可比较融合策略或提升可解释性。

Abstract: This project performs multimodal sentiment analysis using the CMU-MOSEI
dataset, using transformer-based models with early fusion to integrate text,
audio, and visual modalities. We employ BERT-based encoders for each modality,
extracting embeddings that are concatenated before classification. The model
achieves strong performance, with 97.87\% 7-class accuracy and a 0.9682
F1-score on the test set, demonstrating the effectiveness of early fusion in
capturing cross-modal interactions. The training utilized Adam optimization
(lr=1e-4), dropout (0.3), and early stopping to ensure generalization and
robustness. Results highlight the superiority of transformer architectures in
modeling multimodal sentiment, with a low MAE (0.1060) indicating precise
sentiment intensity prediction. Future work may compare fusion strategies or
enhance interpretability. This approach utilizes multimodal learning by
effectively combining linguistic, acoustic, and visual cues for sentiment
analysis.

</details>

### [96] [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)
*Philippe Laban,Hiroaki Hayashi,Yingbo Zhou,Jennifer Neville*

Main category: cs.CL

TLDR: LLMs在单轮和多轮对话中的性能对比显示，多轮对话中性能平均下降39%，主要原因是模型在早期假设错误后难以恢复。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在多轮对话中的表现，因为现有评估主要关注单轮任务，而实际应用中用户常通过多轮对话逐步明确需求。

Method: 通过大规模模拟实验，比较LLMs在单轮和多轮对话中的性能，并分析性能下降的原因。

Result: 多轮对话中LLMs性能显著下降（平均39%），主要由于早期假设错误和过度依赖错误解决方案。

Conclusion: LLMs在多轮对话中表现不佳，需改进模型以更好地处理逐步明确需求的任务。

Abstract: Large Language Models (LLMs) are conversational interfaces. As such, LLMs
have the potential to assist their users not only when they can fully specify
the task at hand, but also to help them define, explore, and refine what they
need through multi-turn conversational exchange. Although analysis of LLM
conversation logs has confirmed that underspecification occurs frequently in
user instructions, LLM evaluation has predominantly focused on the single-turn,
fully-specified instruction setting. In this work, we perform large-scale
simulation experiments to compare LLM performance in single- and multi-turn
settings. Our experiments confirm that all the top open- and closed-weight LLMs
we test exhibit significantly lower performance in multi-turn conversations
than single-turn, with an average drop of 39% across six generation tasks.
Analysis of 200,000+ simulated conversations decomposes the performance
degradation into two components: a minor loss in aptitude and a significant
increase in unreliability. We find that LLMs often make assumptions in early
turns and prematurely attempt to generate final solutions, on which they overly
rely. In simpler terms, we discover that *when LLMs take a wrong turn in a
conversation, they get lost and do not recover*.

</details>

### [97] [Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies](https://arxiv.org/abs/2505.06145)
*Xu Han,Yumeng Sun,Weiqiang Huang,Hongye Zheng,Junliang Du*

Main category: cs.CL

TLDR: 该论文提出了一种结合自适应微调、对比学习和正则化优化的策略，以提升Transformer模型在少样本文本分类中的性能。实验表明，该方法在FewRel 2.0数据集上表现优异，尤其在5-shot任务中。


<details>
  <summary>Details</summary>
Motivation: 少样本文本分类在低资源环境中具有重要应用价值，但现有方法在处理模糊语义边界或复杂特征分布时表现不佳。

Method: 结合自适应微调、对比学习和正则化优化，改进Transformer模型的分类性能。

Result: T5-small、DeBERTa-v3和RoBERTa-base在FewRel 2.0数据集上表现良好，尤其在5-shot任务中。对比损失和正则化损失增强了模型的泛化能力。

Conclusion: 使用更强的自注意力机制（如Transformer或生成架构）可以提升少样本分类的稳定性和准确性。

Abstract: Few-shot text classification has important application value in low-resource
environments. This paper proposes a strategy that combines adaptive
fine-tuning, contrastive learning, and regularization optimization to improve
the classification performance of Transformer-based models. Experiments on the
FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform
well in few-shot tasks, especially in the 5-shot setting, which can more
effectively capture text features and improve classification accuracy. The
experiment also found that there are significant differences in the
classification difficulty of different relationship categories. Some categories
have fuzzy semantic boundaries or complex feature distributions, making it
difficult for the standard cross entropy loss to learn the discriminative
information required to distinguish categories. By introducing contrastive loss
and regularization loss, the generalization ability of the model is enhanced,
effectively alleviating the overfitting problem in few-shot environments. In
addition, the research results show that the use of Transformer models or
generative architectures with stronger self-attention mechanisms can help
improve the stability and accuracy of few-shot classification.

</details>

### [98] [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
*Faeze Ghorbanpour,Daryna Dementieva,Alexander Fraser*

Main category: cs.CL

TLDR: 该论文探讨了多语言大语言模型（如LLaMA、Aya等）在零样本和少样本提示下检测仇恨言论的效果，发现其虽不及微调编码器模型，但在泛化能力上表现更好，且提示设计对性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测方法忽视语言多样性，多语言大语言模型的能力尚未充分探索。

Method: 评估了八种非英语语言中LLM提示技术的效果，并与微调编码器模型对比。

Result: 零样本和少样本提示在真实数据集上表现较差，但在泛化测试中优于微调模型，且提示设计对性能影响显著。

Conclusion: 提示设计是关键，需针对不同语言定制提示技术以优化仇恨言论检测性能。

Abstract: Despite growing interest in automated hate speech detection, most existing
approaches overlook the linguistic diversity of online content. Multilingual
instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ
offer promising capabilities across languages, but their effectiveness in
identifying hate speech through zero-shot and few-shot prompting remains
underexplored. This work evaluates LLM prompting-based detection across eight
non-English languages, utilizing several prompting techniques and comparing
them to fine-tuned encoder models. We show that while zero-shot and few-shot
prompting lag behind fine-tuned encoder models on most of the real-world
evaluation sets, they achieve better generalization on functional tests for
hate speech detection. Our study also reveals that prompt design plays a
critical role, with each language often requiring customized prompting
techniques to maximize performance.

</details>

### [99] [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/abs/2505.06150)
*Ryan Lagasse,Aidan Kiernans,Avijit Ghosh,Shiri Dori-Hacohen*

Main category: cs.CL

TLDR: 论文提出了一种考虑数据组成的大语言模型（LLM）微调缩放定律，实验表明数据组成显著影响token效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅通过总token数量衡量训练数据，而数据组成（如样本数量和平均长度）对模型性能有决定性影响。

Method: 在固定计算预算下，提出了一种考虑数据组成的缩放定律，并通过BRICC和MMLU数据集实验验证。

Result: 实验显示数据组成显著影响token效率，为资源受限的LLM微调提供了优化方向。

Conclusion: 研究为实际LLM微调提供了更精细的缩放定律，适用于资源受限场景。

Abstract: We introduce a scaling law for fine-tuning large language models (LLMs) under
fixed compute budgets that explicitly accounts for data composition.
Conventional approaches measure training data solely by total tokens, yet the
number of examples and their average token length -- what we term \emph{dataset
volume} -- play a decisive role in model performance. Our formulation is tuned
following established procedures. Experiments on the BRICC dataset
\cite{salavati2024reducing} and subsets of the MMLU dataset
\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple
subsampling strategies, reveal that data composition significantly affects
token efficiency. These results motivate refined scaling laws for practical LLM
fine-tuning in resource-constrained settings.

</details>

### [100] [Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework](https://arxiv.org/abs/2505.06151)
*Alice Rueda,Argyrios Perivolaris,Niloy Roy,Dylan Weston,Sarmed Shaya,Zachary Cote,Martin Ivanov,Bazen G. Teferra,Yuqi Wu,Sirisha Rambhatla,Divya Sharma,Andrew Greenshaw,Rakesh Jetly,Yanbo Zhang,Bo Cao,Reza Samavi,Sridhar Krishnan,Venkat Bhat*

Main category: cs.CL

TLDR: 提出一种基于NLP的多维框架，用于客观分类心理咨询会话中的参与质量，通过特征提取和分类器优化，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 客户与治疗师之间的参与质量是治疗成功的关键因素，需要一种客观、可扩展的方法来评估。

Method: 使用253份心理访谈文本，提取42个特征（包括对话动态、语义相似性等），并优化多种分类器（如随机森林、SVM）。

Result: 随机森林在增强数据上达到88.9%的准确率和94.6%的AUC，语义相似性和对话动态是重要特征。

Conclusion: 该框架为评估治疗会话参与质量提供了可扩展的数据驱动方法，支持未来多模态扩展。

Abstract: Engagement between client and therapist is a critical determinant of
therapeutic success. We propose a multi-dimensional natural language processing
(NLP) framework that objectively classifies engagement quality in counseling
sessions based on textual transcripts. Using 253 motivational interviewing
transcripts (150 high-quality, 103 low-quality), we extracted 42 features
across four domains: conversational dynamics, semantic similarity as topic
alignment, sentiment classification, and question detection. Classifiers,
including Random Forest (RF), Cat-Boost, and Support Vector Machines (SVM),
were hyperparameter tuned and trained using a stratified 5-fold
cross-validation and evaluated on a holdout test set. On balanced
(non-augmented) data, RF achieved the highest classification accuracy (76.7%),
and SVM achieved the highest AUC (85.4%). After SMOTE-Tomek augmentation,
performance improved significantly: RF achieved up to 88.9% accuracy, 90.0%
F1-score, and 94.6% AUC, while SVM reached 81.1% accuracy, 83.1% F1-score, and
93.6% AUC. The augmented data results reflect the potential of the framework in
future larger-scale applications. Feature contribution revealed conversational
dynamics and semantic similarity between clients and therapists were among the
top contributors, led by words uttered by the client (mean and standard
deviation). The framework was robust across the original and augmented datasets
and demonstrated consistent improvements in F1 scores and recall. While
currently text-based, the framework supports future multimodal extensions
(e.g., vocal tone, facial affect) for more holistic assessments. This work
introduces a scalable, data-driven method for evaluating engagement quality of
the therapy session, offering clinicians real-time feedback to enhance the
quality of both virtual and in-person therapeutic interactions.

</details>

### [101] [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/abs/2505.06186)
*Massimiliano Pronesti,Joao Bettencourt-Silva,Paul Flanagan,Alessandra Pascale,Oisin Redmond,Anya Belz,Yufang Hou*

Main category: cs.CL

TLDR: 论文提出了一种名为URCA的检索增强生成框架，用于从Cochrane系统评价中提取科学证据，并在新数据集CochraneForest上验证其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决临床研究中科学证据提取的挑战，特别是在证据冲突的情况下，需要高效的方法支持证据合成。

Method: 创建CochraneForest数据集，包含202个注释的森林图和相关研究问题，提出URCA框架进行证据提取。

Result: URCA在F1分数上比现有方法高出10.3%，但数据集复杂性表明仍需改进。

Conclusion: CochraneForest为自动化证据合成系统提供了具有挑战性的测试平台，URCA展现了潜力。

Abstract: Extracting scientific evidence from biomedical studies for clinical research
questions (e.g., Does stem cell transplantation improve quality of life in
patients with medically refractory Crohn's disease compared to placebo?) is a
crucial step in synthesising biomedical evidence. In this paper, we focus on
the task of document-level scientific evidence extraction for clinical
questions with conflicting evidence. To support this task, we create a dataset
called CochraneForest, leveraging forest plots from Cochrane systematic
reviews. It comprises 202 annotated forest plots, associated clinical research
questions, full texts of studies, and study-specific conclusions. Building on
CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a
retrieval-augmented generation framework designed to tackle the unique
challenges of evidence extraction. Our experiments show that URCA outperforms
the best existing methods by up to 10.3% in F1 score on this task. However, the
results also underscore the complexity of CochraneForest, establishing it as a
challenging testbed for advancing automated evidence synthesis systems.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods](https://arxiv.org/abs/2505.05541)
*Markov Grey,Charbel-Raphaël Segerie*

Main category: cs.AI

TLDR: 本文综述了AI安全评估的快速发展领域，提出了围绕三个维度的系统分类法：测量属性、测量方法及如何整合到框架中，旨在为AI系统的安全评估提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI系统向变革性能力发展，需要改进测量和评估方法以确保安全并指导治理。

Method: 通过行为技术（如脚手架、红队测试和监督微调）和内部技术（如表示分析和机制可解释性）测量能力、倾向和控制。

Result: 展示了评估方法如何超越基准测试，测量模型在极限下的能力、默认行为倾向以及安全措施的有效性。

Conclusion: 本文为理解AI安全评估提供了中心参考点，同时指出了挑战和未来研究方向。

Abstract: As frontier AI systems advance toward transformative capabilities, we need a
parallel transformation in how we measure and evaluate these systems to ensure
safety and inform governance. While benchmarks have been the primary method for
estimating model capabilities, they often fail to establish true upper bounds
or predict deployment behavior. This literature review consolidates the rapidly
evolving field of AI safety evaluations, proposing a systematic taxonomy around
three dimensions: what properties we measure, how we measure them, and how
these measurements integrate into frameworks. We show how evaluations go beyond
benchmarks by measuring what models can do when pushed to the limit
(capabilities), the behavioral tendencies exhibited by default (propensities),
and whether our safety measures remain effective even when faced with
subversive adversarial AI (control). These properties are measured through
behavioral techniques like scaffolding, red teaming and supervised fine-tuning,
alongside internal techniques such as representation analysis and mechanistic
interpretability. We provide deeper explanations of some safety-critical
capabilities like cybersecurity exploitation, deception, autonomous
replication, and situational awareness, alongside concerning propensities like
power-seeking and scheming. The review explores how these evaluation methods
integrate into governance frameworks to translate results into concrete
development decisions. We also highlight challenges to safety evaluations -
proving absence of capabilities, potential model sandbagging, and incentives
for "safetywashing" - while identifying promising research directions. By
synthesizing scattered resources, this literature review aims to provide a
central reference point for understanding AI safety evaluations.

</details>

### [103] [HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics](https://arxiv.org/abs/2505.05602)
*Lennart Luettgau,Harry Coppock,Magda Dubois,Christopher Summerfield,Cozmin Ududec*

Main category: cs.AI

TLDR: HiBayES是一个分层贝叶斯建模框架，用于在低数据场景下稳健评估AI系统的能力，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的发展，需要从随机输出中稳健估计其能力，并量化不确定性，同时高级AI评估具有复杂性和高成本。

Method: 基于广义线性模型（GLMs）、贝叶斯数据分析和正式模型比较，构建了HiBayES框架。

Result: HiBayES支持经典问答基准和高级代理评估，提供稳健的参数估计和不确定性量化。

Conclusion: HiBayES是一个通用且实用的工具，适用于多级贝叶斯GLMs的实现，并提供了软件包支持。

Abstract: As Large Language Models (LLMs) and other AI systems evolve, robustly
estimating their capabilities from inherently stochastic outputs while
systematically quantifying uncertainty in these estimates becomes increasingly
important. Further, advanced AI evaluations often have a nested hierarchical
structure, exhibit high levels of complexity, and come with high costs in
testing the most advanced AI systems. To address these challenges, we introduce
HiBayES, a generalizable Hierarchical Bayesian modeling framework for AI
Evaluation Statistics. HiBayES supports robust inferences in classical
question-answer benchmarks and advanced agentic evaluations, particularly in
low-data scenarios (e.g., < 20 data points per evaluation). Built on
Generalized Linear Models (GLMs), Bayesian data analysis, and formal model
comparison, HiBayES provides principled uncertainty quantification and robust
parameter estimation. This paper offers a comprehensive introduction to
HiBayES, including illustrative examples, comparisons to conventional
statistical methods, and practical guidance for implementing multilevel
Bayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta
version) for out-of-the-box implementation.

</details>

### [104] [scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction](https://arxiv.org/abs/2505.05612)
*Qing Wang,Yining Pan,Minghao Zhou,Zijia Tang,Yanfei Wang,Guangyu Wang,Qianqian Song*

Main category: cs.AI

TLDR: scDrugMap是一个用于单细胞数据药物反应预测的集成框架，评估了多种基础模型，并在大规模数据集上展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 药物抗性是癌症治疗的主要挑战，单细胞分析可揭示细胞异质性，但大规模基础模型在单细胞数据中的应用尚未充分探索。

Method: 开发了scDrugMap框架，评估了8种单细胞模型和2种大型语言模型，采用层冻结和LoRA微调策略，在36个数据集上进行基准测试。

Result: 在pooled-data场景中，scFoundation表现最佳（F1分数0.971）；在cross-data场景中，UCE微调后表现最佳（F1分数0.774），scGPT在零样本学习中领先（F1分数0.858）。

Conclusion: scDrugMap首次为单细胞数据药物反应预测提供了大规模基准测试，并作为用户友好平台推动药物发现和转化研究。

Abstract: Drug resistance presents a major challenge in cancer therapy. Single cell
profiling offers insights into cellular heterogeneity, yet the application of
large-scale foundation models for predicting drug response in single cell data
remains underexplored. To address this, we developed scDrugMap, an integrated
framework featuring both a Python command-line interface and a web server for
drug response prediction. scDrugMap evaluates a wide range of foundation
models, including eight single-cell models and two large language models, using
a curated dataset of over 326,000 cells in the primary collection and 18,800
cells in the validation set, spanning 36 datasets and diverse tissue and cancer
types. We benchmarked model performance under pooled-data and cross-data
evaluation settings, employing both layer freezing and Low-Rank Adaptation
(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation
achieved the best performance, with mean F1 scores of 0.971 (layer freezing)
and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.
In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),
while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap
provides the first large-scale benchmark of foundation models for drug response
prediction in single-cell data and serves as a user-friendly, flexible platform
for advancing drug discovery and translational research.

</details>

### [105] [Leveraging Large Language Models for enzymatic reaction prediction and characterization](https://arxiv.org/abs/2505.05616)
*Lorenzo Di Fruscia,Jana Marie Weber*

Main category: cs.AI

TLDR: 评估Llama-3.1家族LLM在酶反应预测任务中的表现，包括EC编号预测、正向合成和逆向合成，发现多任务学习能提升性能。


<details>
  <summary>Details</summary>
Motivation: 酶反应预测在生物催化、代谢工程和药物发现中至关重要，但复杂且资源密集。LLM在科学领域表现优异，因此探索其在生化任务中的应用。

Method: 使用Llama-3.1（8B和70B）模型，比较单任务和多任务学习策略，采用LoRA适配器进行参数高效微调，并评估不同数据量下的表现。

Result: 微调后的LLM能捕捉生化知识，多任务学习通过共享酶信息提升正向和逆向合成预测性能，但在EC分类层级上存在局限性。

Conclusion: LLM在生化建模中具有潜力，但需进一步改进以解决EC分类等挑战。

Abstract: Predicting enzymatic reactions is crucial for applications in biocatalysis,
metabolic engineering, and drug discovery, yet it remains a complex and
resource-intensive task. Large Language Models (LLMs) have recently
demonstrated remarkable success in various scientific domains, e.g., through
their ability to generalize knowledge, reason over complex structures, and
leverage in-context learning strategies. In this study, we systematically
evaluate the capability of LLMs, particularly the Llama-3.1 family (8B and
70B), across three core biochemical tasks: Enzyme Commission number prediction,
forward synthesis, and retrosynthesis. We compare single-task and multitask
learning strategies, employing parameter-efficient fine-tuning via LoRA
adapters. Additionally, we assess performance across different data regimes to
explore their adaptability in low-data settings. Our results demonstrate that
fine-tuned LLMs capture biochemical knowledge, with multitask learning
enhancing forward- and retrosynthesis predictions by leveraging shared
enzymatic information. We also identify key limitations, for example challenges
in hierarchical EC classification schemes, highlighting areas for further
improvement in LLM-driven biochemical modeling.

</details>

### [106] [Prompted Meta-Learning for Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2505.05684)
*Han Wu,Jie Yin*

Main category: cs.AI

TLDR: 提出了一种名为PromptMeta的新框架，通过结合元语义和关系信息来解决少样本知识图谱补全问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注关系信息，忽略了知识图谱中的丰富语义，PromptMeta旨在填补这一空白。

Method: PromptMeta包含两个关键创新：元语义提示池和可学习的融合提示，两者在元学习框架中优化。

Result: 在两个基准数据集上的实验证明了该方法的有效性。

Conclusion: PromptMeta通过整合元语义和关系信息，显著提升了少样本知识图谱补全的性能。

Abstract: Few-shot knowledge graph completion (KGC) has obtained significant attention
due to its practical applications in real-world scenarios, where new knowledge
often emerges with limited available data. While most existing methods for
few-shot KGC have predominantly focused on leveraging relational information,
rich semantics inherent in KGs have been largely overlooked. To address this
gap, we propose a novel prompted meta-learning (PromptMeta) framework that
seamlessly integrates meta-semantics with relational information for few-shot
KGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that
captures and consolidates high-level meta-semantics, enabling effective
knowledge transfer and adaptation to rare and newly emerging relations. (2) a
learnable fusion prompt that dynamically combines meta-semantic information
with task-specific relational information tailored to different few-shot tasks.
Both components are optimized together with model parameters within a
meta-learning framework. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our approach.

</details>

### [107] [Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning](https://arxiv.org/abs/2505.05701)
*Jongchan Park,Mingyu Park,Donghwan Lee*

Main category: cs.AI

TLDR: 提出一种简单有效的预训练方法，通过共享Q网络结构提升离线强化学习的数据效率。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习需要从静态数据集中学习策略，但数据收集成本高且受限，因此如何用最小数据集学习最优策略是关键问题。

Method: 提出共享Q网络结构，通过预测下一状态和Q值的监督回归任务预训练，并与多种离线RL方法结合。

Result: 在D4RL、Robomimic和V-D4RL基准测试中提升性能，仅用10%数据集即可超越标准算法的全数据集表现。

Conclusion: 该方法显著提升了离线RL的数据效率，适用于不同数据质量和分布。

Abstract: Offline reinforcement learning (RL) aims to learn a policy from a static
dataset without further interactions with the environment. Collecting
sufficiently large datasets for offline RL is exhausting since this data
collection requires colossus interactions with environments and becomes tricky
when the interaction with the environment is restricted. Hence, how an agent
learns the best policy with a minimal static dataset is a crucial issue in
offline RL, similar to the sample efficiency problem in online RL. In this
paper, we propose a simple yet effective plug-and-play pretraining method to
initialize a feature of a $Q$-network to enhance data efficiency in offline RL.
Specifically, we introduce a shared $Q$-network structure that outputs
predictions of the next state and $Q$-value. We pretrain the shared $Q$-network
through a supervised regression task that predicts a next state and trains the
shared $Q$-network using diverse offline RL methods. Through extensive
experiments, we empirically demonstrate that our method enhances the
performance of existing popular offline RL methods on the D4RL, Robomimic and
V-D4RL benchmarks. Furthermore, we show that our method significantly boosts
data-efficient offline RL across various data qualities and data distributions
trough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of
the dataset outperforms standard algorithms even with full datasets.

</details>

### [108] [APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning](https://arxiv.org/abs/2505.05758)
*Azim Ospanov,Roozbeh Yousefzadeh*

Main category: cs.AI

TLDR: APOLLO是一个结合Lean编译器与LLM的自动化证明修复系统，显著提升了定理证明的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生成完全正确的形式化证明时的高采样需求问题。

Method: APOLLO通过模块化流程，利用Lean分析错误、修复语法、隔离子引理，并低预算调用LLM。

Result: 在miniF2F基准上达到75.0%准确率，显著降低采样复杂度。

Conclusion: 编译器引导的LLM输出修复在效率和正确性上均有显著提升，为自动化定理证明提供了新范式。

Abstract: Formal reasoning and automated theorem proving constitute a challenging
subfield of machine learning, in which machines are tasked with proving
mathematical theorems using formal languages like Lean. A formal verification
system can check whether a formal proof is correct or not almost
instantaneously, but generating a completely correct formal proof with large
language models (LLMs) remains a formidable task. The usual approach in the
literature is to prompt the LLM many times (up to several thousands) until one
of the generated proofs passes the verification system. In this work, we
present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a
modular, model-agnostic pipeline that combines the strengths of the Lean
compiler with an LLM's reasoning abilities to achieve better proof-generation
results at a low sampling budget. Apollo directs a fully automated process in
which the LLM generates proofs for theorems, a set of agents analyze the
proofs, fix the syntax errors, identify the mistakes in the proofs using Lean,
isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on
each remaining goal with a low top-K budget. The repaired sub-proofs are
recombined and reverified, iterating up to a user-controlled maximum number of
attempts. On the miniF2F benchmark, we establish a new state-of-the-art
accuracy of 75.0% among 7B-parameter models while keeping the sampling budget
below one thousand. Moreover, Apollo raises the state-of-the-art accuracy for
Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few
hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%
accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM
outputs yields dramatic gains in both efficiency and correctness, suggesting a
general paradigm for scalable automated theorem proving.

</details>

### [109] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/abs/2505.05880)
*Bettina Fazzinga,Sergio Flesca,Filippo Furfaro,Luigi Pontieri,Francesco Scala*

Main category: cs.AI

TLDR: 论文提出了一种结合神经符号方法的高效数据/计算解决方案，用于解决事件到活动映射的不确定性问题，通过序列标记模型和AAF推理器的结合，减少了对大量标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现代企业和组织需要监控和分析过程痕迹，但在事件与业务活动之间存在映射不确定性的情况下，传统方法可能产生低信息量结果和高计算成本。

Method: 提出了一种神经符号方法，结合序列标记模型和基于AAF的推理器，以高效的方式生成候选事件解释。

Result: 实验结果表明，该方法能够利用先验知识弥补数据稀缺性，适用于标注和优化成本受限的场景。

Conclusion: 该方法在减少数据标注和计算成本的同时，提高了事件解释的准确性和可解释性，符合绿色AI的可持续发展目标。

Abstract: Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>

### [110] [Pseudo-Boolean d-DNNF Compilation for Expressive Feature Modeling Constructs](https://arxiv.org/abs/2505.05976)
*Chico Sundermann,Stefan Vill,Elias Kuiter,Sebastian Krieter,Thomas Thüm,Matthias Tichy*

Main category: cs.AI

TLDR: 论文提出了一种基于伪布尔编码的方法，用于处理特征模型中复杂的依赖关系，显著提升了表达性约束的处理效率。


<details>
  <summary>Details</summary>
Motivation: 特征模型中的复杂构造（如基数约束）难以转换为CNF形式，限制了自动化推理工具的适用性。

Method: 采用伪布尔编码替代布尔编码，并提出将伪布尔公式编译为布尔d-DNNF的新方法。

Result: 实验表明，该方法在处理表达性约束时显著优于基于CNF的现有方法，且对基本构造也具竞争力。

Conclusion: 伪布尔编码方法缩短了表达性构造与高效自动化推理之间的差距，提升了特征模型的分析效率。

Abstract: Configurable systems typically consist of reusable assets that have
dependencies between each other. To specify such dependencies, feature models
are commonly used. As feature models in practice are often complex, automated
reasoning is typically employed to analyze the dependencies. Here, the de facto
standard is translating the feature model to conjunctive normal form (CNF) to
enable employing off-the-shelf tools, such as SAT or #SAT solvers. However,
modern feature-modeling dialects often contain constructs, such as cardinality
constraints, that are ill-suited for conversion to CNF. This mismatch between
the input of reasoning engines and the available feature-modeling dialects
limits the applicability of the more expressive constructs. In this work, we
shorten this gap between expressive constructs and scalable automated
reasoning. Our contribution is twofold: First, we provide a pseudo-Boolean
encoding for feature models, which facilitates smaller representations of
commonly employed constructs compared to Boolean encoding. Second, we propose a
novel method to compile pseudo-Boolean formulas to Boolean d-DNNF. With the
compiled d-DNNFs, we can resort to a plethora of efficient analyses already
used in feature modeling. Our empirical evaluation shows that our proposal
substantially outperforms the state-of-the-art based on CNF inputs for
expressive constructs. For every considered dataset representing different
feature models and feature-modeling constructs, the feature models can be
significantly faster translated to pseudo-Boolean than to CNF. Overall,
deriving d-DNNFs from a feature model with the targeted expressive constraints
can be substantially accelerated using our pseudo-Boolean approach.
Furthermore, our approach is competitive on feature models with only basic
constructs.

</details>

### [111] [ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding](https://arxiv.org/abs/2505.06020)
*Shuai Wang,Ivona Najdenkoska,Hongyi Zhu,Stevan Rudinac,Monika Kackovic,Nachoem Wijnberg,Marcel Worring*

Main category: cs.AI

TLDR: ArtRAG是一个无需训练的框架，结合结构化知识和检索增强生成（RAG），用于多视角艺术品解释，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 理解视觉艺术需要超越对象识别的多视角推理，现有MLLMs在艺术品解释中缺乏文化、历史和风格的细微捕捉。

Method: ArtRAG通过构建艺术上下文知识图谱（ACKG），结合多粒度检索器选择相关子图，指导MLLMs生成文化丰富的艺术描述。

Result: 在SemArt和Artpedia数据集上，ArtRAG表现优于多个基线模型，人类评估确认其生成内容连贯且有文化洞察。

Conclusion: ArtRAG通过结构化知识和检索增强生成，显著提升了艺术品解释的质量和文化丰富性。

Abstract: Understanding visual art requires reasoning across multiple perspectives --
cultural, historical, and stylistic -- beyond mere object recognition. While
recent multimodal large language models (MLLMs) perform well on general image
captioning, they often fail to capture the nuanced interpretations that fine
art demands. We propose ArtRAG, a novel, training-free framework that combines
structured knowledge with retrieval-augmented generation (RAG) for
multi-perspective artwork explanation. ArtRAG automatically constructs an Art
Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing
entities such as artists, movements, themes, and historical events into a rich,
interpretable graph. At inference time, a multi-granular structured retriever
selects semantically and topologically relevant subgraphs to guide generation.
This enables MLLMs to produce contextually grounded, culturally informed art
descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG
outperforms several heavily trained baselines. Human evaluations further
confirm that ArtRAG generates coherent, insightful, and culturally enriched
interpretations.

</details>

### [112] [Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects](https://arxiv.org/abs/2505.06030)
*Tobias Preintner,Weixuan Yuan,Qi Huang,Adrian König,Thomas Bäck,Elena Raponi,Niki van Stein*

Main category: cs.AI

TLDR: 提出了一种生成反事实示例的方法，用于解释模型在3D对象引用任务中的错误预测。


<details>
  <summary>Details</summary>
Motivation: 解决模型在复杂语言描述和3D对象空间关系下的错误预测问题，增强对模型行为的理解。

Method: 通过生成与原始描述相似但能导致正确预测的反事实示例，揭示模型弱点。

Result: 反事实示例保持了原始描述的结构和语义，揭示了模型偏差和描述缺陷。

Conclusion: 该方法帮助实践者更好地理解模型行为，并为模型改进提供依据。

Abstract: Combining natural language and geometric shapes is an emerging research area
with multiple applications in robotics and language-assisted design. A crucial
task in this domain is object referent identification, which involves selecting
a 3D object given a textual description of the target. Variability in language
descriptions and spatial relationships of 3D objects makes this a complex task,
increasing the need to better understand the behavior of neural network models
in this domain. However, limited research has been conducted in this area.
Specifically, when a model makes an incorrect prediction despite being provided
with a seemingly correct object description, practitioners are left wondering:
"Why is the model wrong?". In this work, we present a method answering this
question by generating counterfactual examples. Our method takes a
misclassified sample, which includes two objects and a text description, and
generates an alternative yet similar formulation that would have resulted in a
correct prediction by the model. We have evaluated our approach with data from
the ShapeTalk dataset along with three distinct models. Our counterfactual
examples maintain the structure of the original description, are semantically
similar and meaningful. They reveal weaknesses in the description, model bias
and enhance the understanding of the models behavior. Theses insights help
practitioners to better interact with systems as well as engineers to improve
models.

</details>

### [113] [Seqret: Mining Rule Sets from Event Sequences](https://arxiv.org/abs/2505.06049)
*Aleena Siji,Joscha Cüppers,Osman Ali Mian,Jilles Vreeken*

Main category: cs.AI

TLDR: 该论文提出了一种名为Seqret的方法，用于从事件序列数据中发现条件和非条件依赖关系，生成简洁且非冗余的规则集。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽略条件依赖，仅关注顺序模式，而该研究旨在同时发现条件和非条件依赖，以更全面地描述事件序列关系。

Method: 通过形式化为最小描述长度问题，提出Seqret方法，在巨大且无结构的搜索空间中高效发现高质量规则集。

Result: 实验表明，Seqret能有效恢复合成数据集的真实依赖关系，并从真实数据集中发现有用规则。

Conclusion: Seqret方法在发现事件序列依赖关系方面优于现有技术，具有实用价值。

Abstract: Summarizing event sequences is a key aspect of data mining. Most existing
methods neglect conditional dependencies and focus on discovering sequential
patterns only. In this paper, we study the problem of discovering both
conditional and unconditional dependencies from event sequence data. We do so
by discovering rules of the form $X \rightarrow Y$ where $X$ and $Y$ are
sequential patterns. Rules like these are simple to understand and provide a
clear description of the relation between the antecedent and the consequent. To
discover succinct and non-redundant sets of rules we formalize the problem in
terms of the Minimum Description Length principle. As the search space is
enormous and does not exhibit helpful structure, we propose the Seqret method
to discover high-quality rule sets in practice. Through extensive empirical
evaluation we show that unlike the state of the art, Seqret ably recovers the
ground truth on synthetic datasets and finds useful rules from real datasets.

</details>

### [114] [Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs](https://arxiv.org/abs/2505.06096)
*Sam Bush,Matthew DeLorenzo,Phat Tieu,Jeyavijayan Rajendran*

Main category: cs.AI

TLDR: 论文提出了一种评估Verilog训练的LLM生成受版权保护代码风险的基准，并发布了一个开源Verilog数据集FreeSet以减少侵权风险。通过微调框架训练的FreeV模型侵权率最低（3%），且在Verilog生成功能上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在硬件设计任务（如生成Verilog代码）中存在能力限制，且现有数据集规模小、版权检查不足，可能导致微调后的LLM侵权。

Method: 提出评估侵权风险的基准，并发布开源数据集FreeSet（含220k文件）及自动化框架。通过持续预训练微调Llama模型，得到FreeV。

Result: FreeV侵权率仅3%，为最低；Verilog生成功能提升，VerilogEval pass@10率提高10%以上。

Conclusion: FreeV在减少侵权风险的同时提升了Verilog生成能力，为硬件设计任务提供了更可靠的LLM解决方案。

Abstract: Limitations in Large Language Model (LLM) capabilities for hardware design
tasks, such as generating functional Verilog codes, have motivated various
fine-tuning optimizations utilizing curated hardware datasets from open-source
repositories. However, these datasets remain limited in size and contain
minimal checks on licensing for reuse, resulting in potential copyright
violations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to
estimate the risk of Verilog-trained LLMs to generate copyright-protected
codes. To minimize this risk, we present an open-source Verilog dataset,
FreeSet, containing over 220k files, along with the automated dataset curation
framework utilized to provide additional guarantees of fair-use Verilog data.
We then execute an LLM fine-tuning framework consisting of continual
pre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our
results indicate that FreeV demonstrates the smallest risk of
copyright-infringement among prior works, with only a 3% violation rate.
Furthermore, experimental results demonstrate improvements in Verilog
generation functionality over its baseline model, improving VerilogEval pass@10
rates by over 10%.

</details>

### [115] [Neuro-Symbolic Concepts](https://arxiv.org/abs/2505.06191)
*Jiayuan Mao,Joshua B. Tenenbaum,Jiajun Wu*

Main category: cs.AI

TLDR: 提出了一种基于概念的中心化范式，构建能够持续学习和灵活推理的智能体。


<details>
  <summary>Details</summary>
Motivation: 通过神经符号概念的组合，实现高效学习和任务解决，适用于多领域任务。

Method: 利用类型化的神经符号概念（如对象、关系、动作），结合符号程序和神经网络表示。

Result: 框架具有数据高效性、组合泛化能力、持续学习和零样本迁移优势。

Conclusion: 概念中心化框架为多领域任务提供了高效且灵活的解决方案。

Abstract: This article presents a concept-centric paradigm for building agents that can
learn continually and reason flexibly. The concept-centric agent utilizes a
vocabulary of neuro-symbolic concepts. These concepts, such as object,
relation, and action concepts, are grounded on sensory inputs and actuation
outputs. They are also compositional, allowing for the creation of novel
concepts through their structural combination. To facilitate learning and
reasoning, the concepts are typed and represented using a combination of
symbolic programs and neural network representations. Leveraging such
neuro-symbolic concepts, the agent can efficiently learn and recombine them to
solve various tasks across different domains, ranging from 2D images, videos,
3D scenes, and robotic manipulation tasks. This concept-centric framework
offers several advantages, including data efficiency, compositional
generalization, continual learning, and zero-shot transfer.

</details>

### [116] [L2R: Learning to Reduce Search Space for Generalizable Neural Routing Solver](https://arxiv.org/abs/2503.03137)
*Changliang Zhou,Xi Lin,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TLDR: 提出了一种基于学习的搜索空间缩减方法，用于解决大规模神经组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经组合优化方法在大规模问题上泛化能力不足，计算复杂度高且结构模式捕捉效率低。

Method: 动态选择候选节点，基于学习模式减少搜索空间。

Result: 在100节点实例上训练后，能泛化到百万节点的TSP和CVRP问题。

Conclusion: 该方法显著提升了大规模问题的求解效率和质量。

Abstract: Constructive neural combinatorial optimization (NCO) has attracted growing
research attention due to its ability to solve complex routing problems without
relying on handcrafted rules. However, existing NCO methods face significant
challenges in generalizing to large-scale problems due to high computational
complexity and inefficient capture of structural patterns. To address this
issue, we propose a novel learning-based search space reduction method that
adaptively selects a small set of promising candidate nodes at each step of the
constructive NCO process. Unlike traditional methods that rely on fixed
heuristics, our selection model dynamically prioritizes nodes based on learned
patterns, significantly reducing the search space while maintaining solution
quality. Experimental results demonstrate that our method, trained solely on
100-node instances from uniform distribution, generalizes remarkably well to
large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) instances with up to 1 million nodes from the uniform
distribution and over 80K nodes from other distributions.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [117] [Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving](https://arxiv.org/abs/2505.05487)
*Shrinivas Pundlik,Seonggyu Choe,Patrick Baker,Chen-Yuan Lee,Naser Al-Madi,Alex R. Bowers,Gang Luo*

Main category: cs.CV

TLDR: 论文描述了从自然驾驶研究中提取和表征驾驶员在交叉路口头部扫描的方法，通过自动化处理车辆速度、GPS、视频等数据，开发了定制工具和AI模型，取得了高准确率的结果。


<details>
  <summary>Details</summary>
Motivation: 自然驾驶研究产生大量多样化数据，需要自动化处理以提取驾驶员行为特征，尤其是交叉路口的头部扫描行为。

Method: 开发定制工具标记交叉路口，同步数据和视频，使用AI模型检测头部姿态和场景对象，结合规则算法推断交叉路口类型和动作。

Result: 处理190个交叉路口，标志和动作检测准确率分别为100%和94%，车辆进入交叉路口的误差较小，边界重叠率高。

Conclusion: 自动化方法能高效处理自然驾驶数据，为驾驶员行为研究提供可靠工具。

Abstract: Naturalistic driving studies use devices in participants' own vehicles to
record daily driving over many months. Due to diverse and extensive amounts of
data recorded, automated processing is necessary. This report describes methods
to extract and characterize driver head scans at intersections from data
collected from an in-car recording system that logged vehicle speed, GPS
location, scene videos, and cabin videos. Custom tools were developed to mark
the intersections, synchronize location and video data, and clip the cabin and
scene videos for +/-100 meters from the intersection location. A
custom-developed head pose detection AI model for wide angle head turns was run
on the cabin videos to estimate the driver head pose, from which head scans >20
deg were computed in the horizontal direction. The scene videos were processed
using a YOLO object detection model to detect traffic lights, stop signs,
pedestrians, and other vehicles on the road. Turning maneuvers were
independently detected using vehicle self-motion patterns. Stop lines on the
road surface were detected using changing intensity patterns over time as the
vehicle moved. The information obtained from processing the scene videos, along
with the speed data was used in a rule-based algorithm to infer the
intersection type, maneuver, and bounds. We processed 190 intersections from 3
vehicles driven in cities and suburban areas from Massachusetts and California.
The automated video processing algorithm correctly detected intersection
signage and maneuvers in 100% and 94% of instances, respectively. The median
[IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9]
meters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and
estimated intersection bounds was 0.88[0.82-0.93].

</details>

### [118] [From Events to Enhancement: A Survey on Event-Based Imaging Technologies](https://arxiv.org/abs/2505.05488)
*Yunfan Lu,Xiaogang Xu,Pengteng Li,Yusheng Wang,Yi Cui,Huizai Yao,Hui Xiong*

Main category: cs.CV

TLDR: 本文综述了事件相机在成像领域的最新进展与挑战，包括其物理模型、特性、图像/视频增强任务以及高级应用，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 事件相机因其高动态范围和低延迟特性成为成像领域的颠覆性技术，但目前缺乏对其最新进展和挑战的全面研究，限制了其在通用成像应用中的潜力。

Method: 首先介绍事件传感器的物理模型和特性，随后分析图像/视频增强任务与事件的交互，并探讨利用事件捕捉更丰富光信息的高级任务。

Result: 总结了事件相机在成像任务中的优势与应用，如光场估计、多视图生成和光度学等，并提出了未来研究方向。

Conclusion: 事件相机在成像领域具有巨大潜力，但仍需解决新挑战和开放性问题，以推动其广泛应用。

Abstract: Event cameras offering high dynamic range and low latency have emerged as
disruptive technologies in imaging. Despite growing research on leveraging
these benefits for different imaging tasks, a comprehensive study of recently
advances and challenges are still lacking. This limits the broader
understanding of how to utilize events in universal imaging applications. In
this survey, we first introduce a physical model and the characteristics of
different event sensors as the foundation. Following this, we highlight the
advancement and interaction of image/video enhancement tasks with events.
Additionally, we explore advanced tasks, which capture richer light information
with events, \eg~light field estimation, multi-view generation, and
photometric. Finally, we discuss new challenges and open questions offering a
perspective for this rapidly evolving field. More continuously updated
resources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging

</details>

### [119] [MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection](https://arxiv.org/abs/2505.05491)
*TianYi Yu*

Main category: cs.CV

TLDR: 提出了一种名为MDDFNet的新型交通标志检测网络，通过动态双融合模块和Mamba主干网络解决特征提取单一和多尺度检测问题。


<details>
  <summary>Details</summary>
Motivation: 解决交通标志检测中特征提取单一和多尺度检测的挑战。

Method: 结合动态双融合模块（增强特征多样性）和Mamba主干网络（全局与局部特征自适应融合）。

Result: 在TT100K数据集上表现优于现有方法，保持实时性。

Conclusion: MDDFNet能有效检测小型交通标志。

Abstract: The Detection of small objects, especially traffic signs, is a critical
sub-task in object detection and autonomous driving. Despite signficant
progress in previous research, two main challenges remain. First, the issue of
feature extraction being too singular. Second, the detection process struggles
to efectively handle objects of varying sizes or scales. These problems are
also prevalent in general object detection tasks. To address these challenges,
we propose a novel object detection network, Mamba-based Dynamic Dual Fusion
Network (MDDFNet), for traffic sign detection. The network integrates a dynamic
dual fusion module and a Mamba-based backbone to simultaneously tackle the
aforementioned issues. Specifically, the dynamic dual fusion module utilizes
multiple branches to consolidate various spatial and semantic information, thus
enhancing feature diversity. The Mamba-based backbone leverages global feature
fusion and local feature interaction, combining features in an adaptive manner
to generate unique classification characteristics. Extensive experiments
conducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that
MDDFNet outperforms other state-of-the-art detectors, maintaining real-time
processing capabilities of single-stage models while achieving superior
performance. This confirms the efectiveness of MDDFNet in detecting small
traffic signs.

</details>

### [120] [DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision](https://arxiv.org/abs/2505.05492)
*Ignacy Stępka,Lukasz Sztukiewicz,Michał Wiliński,Jerzy Stefanowski*

Main category: cs.CV

TLDR: DetoxAI是一个开源Python库，旨在通过后处理去偏技术提升深度学习视觉分类器的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性解决方案多针对表格数据，难以适用于基于视觉的深度学习分类任务。

Method: DetoxAI实现了先进的去偏算法、公平性指标和可视化工具，支持通过干预内部表征进行去偏。

Result: 提供了基于归因的可视化工具和定量公平性指标，展示偏见的缓解效果。

Conclusion: DetoxAI为工程师和研究人员提供了实用工具，填补了视觉分类任务公平性研究的空白。

Abstract: While machine learning fairness has made significant progress in recent
years, most existing solutions focus on tabular data and are poorly suited for
vision-based classification tasks, which rely heavily on deep learning. To
bridge this gap, we introduce DetoxAI, an open-source Python library for
improving fairness in deep learning vision classifiers through post-hoc
debiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness
metrics, and visualization tools. It supports debiasing via interventions in
internal representations and includes attribution-based visualization tools and
quantitative algorithmic fairness metrics to show how bias is mitigated. This
paper presents the motivation, design, and use cases of DetoxAI, demonstrating
its tangible value to engineers and researchers.

</details>

### [121] [Learning 3D Persistent Embodied World Models](https://arxiv.org/abs/2505.05495)
*Siyuan Zhou,Yilun Du,Yuncong Yang,Lei Han,Peihao Chen,Dit-Yan Yeung,Chuang Gan*

Main category: cs.CV

TLDR: 提出了一种具有显式记忆的持久性世界模型，用于解决现有视频模型在长时程规划中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型缺乏对场景的记忆能力，无法在部分观察的复杂环境中进行一致的长时程规划。

Method: 采用视频扩散模型预测未来RGB-D视频，并将其聚合为持久性3D地图，以此为基础生成视频世界模型。

Result: 该模型能够准确模拟已观察和未观察的世界部分，提升了下游任务中的规划和策略学习效果。

Conclusion: 提出的持久性世界模型显著改善了长时程模拟的准确性，适用于复杂环境中的智能体任务。

Abstract: The ability to simulate the effects of future actions on the world is a
crucial ability of intelligent embodied agents, enabling agents to anticipate
the effects of their actions and make plans accordingly. While a large body of
existing work has explored how to construct such world models using video
models, they are often myopic in nature, without any memory of a scene not
captured by currently observed images, preventing agents from making consistent
long-horizon plans in complex environments where many parts of the scene are
partially observed. We introduce a new persistent embodied world model with an
explicit memory of previously generated content, enabling much more consistent
long-horizon simulation. During generation time, our video diffusion model
predicts RGB-D video of the future observations of the agent. This generation
is then aggregated into a persistent 3D map of the environment. By conditioning
the video model on this 3D spatial map, we illustrate how this enables video
world models to faithfully simulate both seen and unseen parts of the world.
Finally, we illustrate the efficacy of such a world model in downstream
embodied applications, enabling effective planning and policy learning.

</details>

### [122] [Preliminary Explorations with GPT-4o(mni) Native Image Generation](https://arxiv.org/abs/2505.05501)
*Pu Cao,Feng Zhou,Junyi Ji,Qingye Kong,Zhixiang Lv,Mingjian Zhang,Xuekun Zhao,Siqi Wu,Yinghui Lin,Qing Song,Lu Yang*

Main category: cs.CV

TLDR: GPT-4o展示了强大的多模态生成能力，但在空间推理、知识密集型任务和一致性预测方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 探索GPT-4o在多模态任务中的能力，评估其在图像生成和其他任务中的表现。

Method: 构建任务分类法并设计测试样本，对GPT-4o在六类任务中进行定性评估。

Result: GPT-4o在通用合成任务中表现优异，但在空间推理、知识密集型任务和一致性预测方面存在不足。

Conclusion: GPT-4o在多模态生成方面取得显著进展，但尚不适用于专业或安全关键领域。

Abstract: Recently, the visual generation ability by GPT-4o(mni) has been unlocked by
OpenAI. It demonstrates a very remarkable generation capability with excellent
multimodal condition understanding and varied task instructions. In this paper,
we aim to explore the capabilities of GPT-4o across various tasks. Inspired by
previous study, we constructed a task taxonomy along with a carefully curated
set of test samples to conduct a comprehensive qualitative test. Benefiting
from GPT-4o's powerful multimodal comprehension, its image-generation process
demonstrates abilities surpassing those of traditional image-generation tasks.
Thus, regarding the dimensions of model capabilities, we evaluate its
performance across six task categories: traditional image generation tasks,
discriminative tasks, knowledge-based generation, commonsense-based generation,
spatially-aware image generation, and temporally-aware image generation. These
tasks not only assess the quality and conditional alignment of the model's
outputs but also probe deeper into GPT-4o's understanding of real-world
concepts. Our results reveal that GPT-4o performs impressively well in
general-purpose synthesis tasks, showing strong capabilities in text-to-image
generation, visual stylization, and low-level image processing. However,
significant limitations remain in its ability to perform precise spatial
reasoning, instruction-grounded generation, and consistent temporal prediction.
Furthermore, when faced with knowledge-intensive or domain-specific scenarios,
such as scientific illustrations or mathematical plots, the model often
exhibits hallucinations, factual errors, or structural inconsistencies. These
findings suggest that while GPT-4o marks a substantial advancement in unified
multimodal generation, there is still a long way to go before it can be
reliably applied to professional or safety-critical domains.

</details>

### [123] [Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation](https://arxiv.org/abs/2505.05505)
*Yiming Qin,Zhu Xu,Yang Liu*

Main category: cs.CV

TLDR: HCoG方法通过分解长描述并分块生成，解决了复杂属性3D对象生成的难题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D模型对复杂属性对象生成效果不佳，主要因文本编码器理解能力有限和遮挡部分生成顺序问题。

Method: 利用大语言模型分解长描述为块，按遮挡顺序从内到外生成，通过目标区域定位和3D高斯核优化绑定属性。

Result: HCoG生成了结构连贯、属性准确的复杂3D对象。

Conclusion: HCoG是一种自动化方法，显著提升了复杂属性3D对象的生成质量。

Abstract: Recent text-to-3D models can render high-quality assets, yet they still
stumble on objects with complex attributes. The key obstacles are: (1) existing
text-to-3D approaches typically lift text-to-image models to extract semantics
via text encoders, while the text encoder exhibits limited comprehension
ability for long descriptions, leading to deviated cross-attention focus,
subsequently wrong attribute binding in generated results. (2) Occluded object
parts demand a disciplined generation order and explicit part disentanglement.
Though some works introduce manual efforts to alleviate the above issues, their
quality is unstable and highly reliant on manual information. To tackle above
problems, we propose a automated method Hierarchical-Chain-of-Generation
(HCoG). It leverages a large language model to decompose the long description
into blocks representing different object parts, and orders them from inside
out according to occlusions, forming a hierarchical chain. Within each block we
first coarsely create components, then precisely bind attributes via
target-region localization and corresponding 3D Gaussian kernel optimization.
Between blocks, we introduce Gaussian Extension and Label Elimination to
seamlessly generate new parts by extending new Gaussian kernels, re-assigning
semantic labels, and eliminating unnecessary kernels, ensuring that only
relevant parts are added without disrupting previously optimized parts.
Experiments confirm that HCoG yields structurally coherent, attribute-faithful
3D objects with complex attributes. The code is available at
https://github.com/Wakals/GASCOL .

</details>

### [124] [Occupancy World Model for Robots](https://arxiv.org/abs/2505.05512)
*Zhang Zhang,Qiang Zhang,Wei Cui,Shuai Shi,Yijie Guo,Gang Han,Wen Zhao,Jingkai Sun,Jiahang Cao,Jiaxu Wang,Hao Cheng,Xiaozhu Ju,Zhengping Che,Renjing Xu,Jian Tang*

Main category: cs.CV

TLDR: 提出了一种名为RoboOccWorld的新框架，用于预测室内3D占用场景的动态演化，结合时空感受野和自回归变换器，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注室外结构化道路场景，而忽略了室内机器人场景的动态预测需求。

Method: 提出Conditional Causal State Attention (CCSA)和Hybrid Spatio-Temporal Aggregation (HSTA)，结合时空感受野和自回归变换器。

Result: RoboOccWorld在室内3D占用场景预测任务中表现优于现有方法。

Conclusion: RoboOccWorld为室内场景动态预测提供了有效解决方案，代码即将发布。

Abstract: Understanding and forecasting the scene evolutions deeply affect the
exploration and decision of embodied agents. While traditional methods simulate
scene evolutions through trajectory prediction of potential instances, current
works use the occupancy world model as a generative framework for describing
fine-grained overall scene dynamics. However, existing methods cluster on the
outdoor structured road scenes, while ignoring the exploration of forecasting
3D occupancy scene evolutions for robots in indoor scenes. In this work, we
explore a new framework for learning the scene evolutions of observed
fine-grained occupancy and propose an occupancy world model based on the
combined spatio-temporal receptive field and guided autoregressive transformer
to forecast the scene evolutions, called RoboOccWorld. We propose the
Conditional Causal State Attention (CCSA), which utilizes camera poses of next
state as conditions to guide the autoregressive transformer to adapt and
understand the indoor robotics scenarios. In order to effectively exploit the
spatio-temporal cues from historical observations, Hybrid Spatio-Temporal
Aggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive
field based on multi-scale spatio-temporal windows. In addition, we restructure
the OccWorld-ScanNet benchmark based on local annotations to facilitate the
evaluation of the indoor 3D occupancy scene evolution prediction task.
Experimental results demonstrate that our RoboOccWorld outperforms
state-of-the-art methods in indoor 3D occupancy scene evolution prediction
task. The code will be released soon.

</details>

### [125] [Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach](https://arxiv.org/abs/2505.05513)
*Muhammad Junaid Asif,Hamza Khan,Rabia Tehseen,Syed Tahir Hussain Rizvi,Mujtaba Asad,Shazia Saqib,Rana Fayyaz Ahmad*

Main category: cs.CV

TLDR: 论文提出了一种基于卷积神经网络（CNN）的自动分类框架，用于区分不同品种的大米，通过性能指标验证了模型的高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大米是全球重要的主食，其质量检查与分类传统上依赖人工，效率低且易出错，因此需要自动化的解决方案。

Method: 采用卷积神经网络（CNN）模型，结合LIME和SHAP等解释性技术，对大米品种进行分类。

Result: 模型在准确率、召回率、精确度和F1分数上表现优异，ROC曲线下面积完美，混淆矩阵显示误分类极少。

Conclusion: 提出的CNN框架能高效准确分类大米品种，解释性技术增强了模型的可信度和透明度。

Abstract: Rice is an essential staple food worldwide that is important in promoting
international trade, economic growth, and nutrition. Asian countries such as
China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their
significant contribution to the cultivation and utilization of rice. These
nations are also known for cultivating different rice grains, including short
and long grains. These sizes are further classified as basmati, jasmine, kainat
saila, ipsala, arborio, etc., catering to diverse culinary preferences and
cultural traditions. For both local and international trade, inspecting and
maintaining the quality of rice grains to satisfy customers and preserve a
country's reputation is necessary. Manual quality check and classification is
quite a laborious and time-consuming process. It is also highly prone to
mistakes. Therefore, an automatic solution must be proposed for the effective
and efficient classification of different varieties of rice grains. This
research paper presents an automatic framework based on a convolutional neural
network (CNN) for classifying different varieties of rice grains. We evaluated
the proposed model based on performance metrics such as accuracy, recall,
precision, and F1-Score. The CNN model underwent rigorous training and
validation, achieving a remarkable accuracy rate and a perfect area under each
class's Receiver Operating Characteristic (ROC) curve. The confusion matrix
analysis confirmed the model's effectiveness in distinguishing between the
different rice varieties, indicating minimal misclassifications. Additionally,
the integration of explainability techniques such as LIME (Local Interpretable
Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided
valuable insights into the model's decision-making process, revealing how
specific features of the rice grains influenced classification outcomes.

</details>

### [126] [Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions](https://arxiv.org/abs/2505.05517)
*Hongyi Chen,Yunchao Yao,Yufei Ye,Zhixuan Xu,Homanga Bharadhwaj,Jiashun Wang,Shubham Tulsiani,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.CV

TLDR: 论文提出了一种从网络图像中提取人类抓取信息的方法，用于训练功能性抓取模型，避免了昂贵的机器人演示需求。通过重建3D手-物体交互网格并利用模拟器扩展数据集，模型在仿真和现实中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注静态抓取或依赖昂贵的机器人演示，而网络图像提供了自然的功能性抓取信息，可作为替代数据源。

Method: 从RGB图像重建3D手-物体交互网格，将人手动作迁移到机器人手，并与精确3D物体模型对齐。利用模拟器生成更多可行抓取数据。

Result: 模型在仿真中达到75.8%的成功率（已知物体）和61.8%（所有物体），模拟器增强后提升至83.4%。现实测试成功率为85%。

Conclusion: 网络图像和模拟器结合可高效训练功能性抓取模型，显著优于基线方法。

Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands
to manipulate objects effectively. However, most prior work either focuses on
power grasping, which simply involves holding an object still, or relies on
costly teleoperated robot demonstrations to teach robots how to grasp each
object functionally. Instead, we propose extracting human grasp information
from web images since they depict natural and functional object interactions,
thereby bypassing the need for curated demonstrations. We reconstruct human
hand-object interaction (HOI) 3D meshes from RGB images, retarget the human
hand to multi-finger robot hands, and align the noisy object mesh with its
accurate 3D shape. We show that these relatively low-quality HOI data from
inexpensive web sources can effectively train a functional grasping model. To
further expand the grasp dataset for seen and unseen objects, we use the
initially-trained grasping policy with web data in the IsaacGym simulator to
generate physically feasible grasps while preserving functionality. We train
the grasping model on 10 object categories and evaluate it on 9 unseen objects,
including challenging items such as syringes, pens, spray bottles, and tongs,
which are underrepresented in existing datasets. The model trained on the web
HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across
all objects in simulation, with a 6.7% improvement in success rate and a 1.8x
increase in functionality ratings over baselines. Simulator-augmented data
further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the
LEAP Hand achieves a 85% success rate. Project website is at:
https://webgrasp.github.io/.

</details>

### [127] [Real-Time Privacy Preservation for Robot Visual Perception](https://arxiv.org/abs/2505.05519)
*Minkyu Choi,Yunhao Yang,Neel P. Bhatt,Kushagra Gupta,Sahil Shah,Aditya Rai,David Fridovich-Keil,Ufuk Topcu,Sandeep P. Chinchali*

Main category: cs.CV

TLDR: PCVS是一种实时视频流隐私保护方法，通过逻辑规范和模糊处理敏感对象，确保隐私保护，同时提供理论概率下界。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方法无法完全隐藏敏感对象且不适用于实时视频流。

Method: PCVS结合逻辑规范和检测模型，模糊处理敏感对象，并使用符合预测方法更新理论概率下界。

Result: PCVS在多个数据集上达到95%以上的规范满足率，且实际表现优于理论下界。

Conclusion: PCVS在实时机器人操作中有效保护隐私且不影响机器人功能。

Abstract: Many robots (e.g., iRobot's Roomba) operate based on visual observations from
live video streams, and such observations may inadvertently include
privacy-sensitive objects, such as personal identifiers. Existing approaches
for preserving privacy rely on deep learning models, differential privacy, or
cryptography. They lack guarantees for the complete concealment of all
sensitive objects. Guaranteeing concealment requires post-processing techniques
and thus is inadequate for real-time video streams. We develop a method for
privacy-constrained video streaming, PCVS, that conceals sensitive objects
within real-time video streams. PCVS takes a logical specification constraining
the existence of privacy-sensitive objects, e.g., never show faces when a
person exists. It uses a detection model to evaluate the existence of these
objects in each incoming frame. Then, it blurs out a subset of objects such
that the existence of the remaining objects satisfies the specification. We
then propose a conformal prediction approach to (i) establish a theoretical
lower bound on the probability of the existence of these objects in a sequence
of frames satisfying the specification and (ii) update the bound with the
arrival of each subsequent frame. Quantitative evaluations show that PCVS
achieves over 95 percent specification satisfaction rate in multiple datasets,
significantly outperforming other methods. The satisfaction rate is
consistently above the theoretical bounds across all datasets, indicating that
the established bounds hold. Additionally, we deploy PCVS on robots in
real-time operation and show that the robots operate normally without being
compromised when PCVS conceals objects.

</details>

### [128] [GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation](https://arxiv.org/abs/2505.05520)
*Chengwei Ye,Huanzhen Zhang,Yufei Lin,Kangsheng Wang,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TLDR: GaMNet结合NMamba模块和多尺度CNN，高效分割脑胶质瘤，提升准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer模型在脑胶质瘤分割中缺乏上下文建模或计算量大，难以实时应用于移动医疗设备。

Method: 提出GaMNet，整合NMamba模块进行全局建模，多尺度CNN提取局部特征，并使用Gabor滤波器提升可解释性。

Result: GaMNet在减少参数和计算时间的同时，实现了高分割精度，显著降低假阳性和假阴性。

Conclusion: GaMNet优于现有方法，提升了临床诊断的可靠性。

Abstract: Gliomas are aggressive brain tumors that pose serious health risks. Deep
learning aids in lesion segmentation, but CNN and Transformer-based models
often lack context modeling or demand heavy computation, limiting real-time use
on mobile medical devices. We propose GaMNet, integrating the NMamba module for
global modeling and a multi-scale CNN for efficient local feature extraction.
To improve interpretability and mimic the human visual system, we apply Gabor
filters at multiple scales. Our method achieves high segmentation accuracy with
fewer parameters and faster computation. Extensive experiments show GaMNet
outperforms existing methods, notably reducing false positives and negatives,
which enhances the reliability of clinical diagnosis.

</details>

### [129] [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
*Hanxun Huang,Sarah Erfani,Yige Li,Xingjun Ma,James Bailey*

Main category: cs.CV

TLDR: X-Transfer是一种新型攻击方法，揭示了CLIP模型的通用对抗脆弱性，通过动态选择代理模型实现超强迁移性。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在多种任务中广泛应用，但其对抗扰动的脆弱性成为关键问题。

Method: 提出X-Transfer方法，通过代理缩放策略生成通用对抗扰动（UAP），实现跨数据、跨域、跨模型和跨任务的迁移性。

Result: X-Transfer显著优于现有UAP方法，为CLIP模型的对抗迁移性设定了新基准。

Conclusion: X-Transfer展示了CLIP模型的对抗脆弱性，为未来防御研究提供了重要参考。

Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly
adopted for diverse downstream tasks and integrated into large vision-language
models (VLMs), their susceptibility to adversarial perturbations has emerged as
a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel
attack method that exposes a universal adversarial vulnerability in CLIP.
X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of
deceiving various CLIP encoders and downstream VLMs across different samples,
tasks, and domains. We refer to this property as \textbf{super
transferability}--a single perturbation achieving cross-data, cross-domain,
cross-model, and cross-task adversarial transferability simultaneously. This is
achieved through \textbf{surrogate scaling}, a key innovation of our approach.
Unlike existing methods that rely on fixed surrogate models, which are
computationally intensive to scale, X-Transfer employs an efficient surrogate
scaling strategy that dynamically selects a small subset of suitable surrogates
from a large search space. Extensive evaluations demonstrate that X-Transfer
significantly outperforms previous state-of-the-art UAP methods, establishing a
new benchmark for adversarial transferability across CLIP models. The code is
publicly available in our
\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.

</details>

### [130] [OXSeg: Multidimensional attention UNet-based lip segmentation using semi-supervised lip contours](https://arxiv.org/abs/2505.05531)
*Hanie Moghaddasi,Christina Chambers,Sarah N. Mattson,Jeffrey R. Wozniak,Claire D. Coles,Raja Mukherjee,Michael Suttie*

Main category: cs.CV

TLDR: 提出了一种结合注意力UNet和多维输入的唇部分割方法，解决了传统方法对图像质量和标注数据的依赖问题，并在胎儿酒精综合征（FAS）诊断中取得了高精度。


<details>
  <summary>Details</summary>
Motivation: 唇部分割在多个领域至关重要，但现有方法受限于训练数据中的唇部轮廓标注以及图像质量、光照和肤色的影响。

Method: 采用局部二值模式提取面部图像的微观模式，构建多维输入，并通过顺序注意力UNet重建唇部轮廓。提出了一种基于解剖标志的掩膜生成方法以提高分割精度。

Result: 上唇分割的平均Dice分数为84.75%，像素精度为99.77%。使用GAN分类器对FAS的识别准确率达到98.55%。

Conclusion: 该方法显著提升了唇部分割的准确性，尤其是在Cupid's bow区域，并为FAS的唇部特征研究提供了新视角。

Abstract: Lip segmentation plays a crucial role in various domains, such as lip
synchronization, lipreading, and diagnostics. However, the effectiveness of
supervised lip segmentation is constrained by the availability of lip contour
in the training phase. A further challenge with lip segmentation is its
reliance on image quality , lighting, and skin tone, leading to inaccuracies in
the detected boundaries. To address these challenges, we propose a sequential
lip segmentation method that integrates attention UNet and multidimensional
input. We unravel the micro-patterns in facial images using local binary
patterns to build multidimensional inputs. Subsequently, the multidimensional
inputs are fed into sequential attention UNets, where the lip contour is
reconstructed. We introduce a mask generation method that uses a few anatomical
landmarks and estimates the complete lip contour to improve segmentation
accuracy. This mask has been utilized in the training phase for lip
segmentation. To evaluate the proposed method, we use facial images to segment
the upper lips and subsequently assess lip-related facial anomalies in subjects
with fetal alcohol syndrome (FAS). Using the proposed lip segmentation method,
we achieved a mean dice score of 84.75%, and a mean pixel accuracy of 99.77% in
upper lip segmentation. To further evaluate the method, we implemented
classifiers to identify those with FAS. Using a generative adversarial network
(GAN), we reached an accuracy of 98.55% in identifying FAS in one of the study
populations. This method could be used to improve lip segmentation accuracy,
especially around Cupid's bow, and shed light on distinct lip-related
characteristics of FAS.

</details>

### [131] [Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments](https://arxiv.org/abs/2505.05540)
*Pranav Guruprasad,Yangyue Wang,Sudipta Chowdhury,Harshvardhan Sikka*

Main category: cs.CV

TLDR: MultiNet v0.2是一个用于评估VLA和VLM模型在OOD环境中零样本泛化能力的基准测试，揭示了模型性能的关键限制和改进方向。


<details>
  <summary>Details</summary>
Motivation: 系统评估VLA模型的零样本泛化能力，特别是在OOD环境中的表现，目前研究较少。

Method: 通过MultiNet v0.2基准测试，评估了包括GPT-4o、GPT-4.1、OpenVLA等在内的多种VLA和VLM模型在Procgen任务中的表现。

Result: 发现所有模型在OOD任务中泛化能力有限，VLA模型表现最佳，VLM模型通过精确提示工程可显著提升性能。

Conclusion: VLA模型在泛化能力上优于其他模型，但模型性能对提示工程敏感，需进一步优化。

Abstract: Vision-language-action (VLA) models represent an important step toward
general-purpose robotic systems by integrating visual perception, language
understanding, and action execution. However, systematic evaluation of these
models, particularly their zero-shot generalization capabilities in
out-of-distribution (OOD) environments, remains limited. In this paper, we
introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and
analyze the generalization performance of state-of-the-art VLM and VLA
models-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse
procedural tasks from the Procgen benchmark. Our analysis reveals several
critical insights: (1) all evaluated models exhibit significant limitations in
zero-shot generalization to OOD tasks, with performance heavily influenced by
factors such as action representation and task complexit; (2) VLAs generally
outperform other models due to their robust architectural design; and (3) VLM
variants demonstrate substantial improvements when constrained appropriately,
highlighting the sensitivity of model performance to precise prompt
engineering.

</details>

### [132] [Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models](https://arxiv.org/abs/2505.05573)
*Mikhail Chaichuk,Sushant Gautam,Steven Hicks,Elena Tutubalina*

Main category: cs.CV

TLDR: 本文研究了从文本生成医学图像的方法，比较了微调大型预训练模型与训练小型领域特定模型的优劣，并提出了一种名为MSDM的优化模型。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI中数据稀缺问题，同时保护患者隐私。

Method: 比较两种方法：微调大型预训练模型（FLUX, Kandinsky）与训练小型领域特定模型（MSDM）。MSDM整合了临床文本编码器、变分自编码器和交叉注意力机制。

Result: 大型模型生成图像保真度更高，但MSDM在较低计算成本下实现了可比质量。

Conclusion: MSDM在计算效率和生成质量之间取得了平衡，适合医疗领域的实际应用。

Abstract: The generation of realistic medical images from text descriptions has
significant potential to address data scarcity challenges in healthcare AI
while preserving patient privacy. This paper presents a comprehensive study of
text-to-image synthesis in the medical domain, comparing two distinct
approaches: (1) fine-tuning large pre-trained latent diffusion models and (2)
training small, domain-specific models. We introduce a novel model named MSDM,
an optimized architecture based on Stable Diffusion that integrates a clinical
text encoder, variational autoencoder, and cross-attention mechanisms to better
align medical text prompts with generated images. Our study compares two
approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus
training compact domain-specific models (MSDM). Evaluation across colonoscopy
(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models
achieve higher fidelity, our optimized MSDM delivers comparable quality with
lower computational costs. Quantitative metrics and qualitative evaluations by
medical experts reveal strengths and limitations of each approach.

</details>

### [133] [Steepest Descent Density Control for Compact 3D Gaussian Splatting](https://arxiv.org/abs/2505.05587)
*Peihao Wang,Yuehao Wang,Dilin Wang,Sreyas Mohan,Zhiwen Fan,Lemeng Wu,Ruisi Cai,Yu-Ying Yeh,Zhangyang Wang,Qiang Liu,Rakesh Ranjan*

Main category: cs.CV

TLDR: 3D高斯泼溅（3DGS）是一种高效的实时高分辨率新视角合成技术，但点云冗余问题导致内存和性能问题。本文提出SteepGS框架，通过优化密度控制减少50%高斯点，提升效率。


<details>
  <summary>Details</summary>
Motivation: 3DGS的点云冗余问题导致内存占用高、性能下降，限制了在资源受限设备上的应用。

Method: 提出理论框架分析密度控制，确定高斯分裂的必要条件、最小后代数量、参数更新方向及透明度归一化。基于此，开发SteepGS，采用最陡密度控制策略。

Result: SteepGS减少50%高斯点，保持渲染质量，显著提升效率和可扩展性。

Conclusion: SteepGS通过理论优化密度控制，解决了3DGS的点云冗余问题，为资源受限设备提供了高效解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for
real-time, high-resolution novel view synthesis. By representing scenes as a
mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for
efficient rendering and reconstruction. To optimize scene coverage and capture
fine details, 3DGS employs a densification algorithm to generate additional
points. However, this process often leads to redundant point clouds, resulting
in excessive memory usage, slower performance, and substantial storage demands
- posing significant challenges for deployment on resource-constrained devices.
To address this limitation, we propose a theoretical framework that demystifies
and improves density control in 3DGS. Our analysis reveals that splitting is
crucial for escaping saddle points. Through an optimization-theoretic approach,
we establish the necessary conditions for densification, determine the minimal
number of offspring Gaussians, identify the optimal parameter update direction,
and provide an analytical solution for normalizing off-spring opacity. Building
on these insights, we introduce SteepGS, incorporating steepest density
control, a principled strategy that minimizes loss while maintaining a compact
point cloud. SteepGS achieves a ~50% reduction in Gaussian points without
compromising rendering quality, significantly enhancing both efficiency and
scalability.

</details>

### [134] [ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation](https://arxiv.org/abs/2505.05589)
*Jingzhong Lin,Yuanyuan Qi,Xinru Li,Wenxuan Huang,Xiangfeng Xu,Bangyan Li,Xuejiao Wang,Gaoqi He*

Main category: cs.CV

TLDR: ReactDance是一个基于扩散模型的新框架，用于高保真的反应性舞蹈生成，解决了现有方法在交互保真度、同步性和时间一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于强调全局约束和优化，忽视了局部信息（如细粒度空间交互和局部时间上下文），导致交互保真度和时间一致性不足。

Method: ReactDance引入了GRFSQ（多尺度解耦运动表示）和BLC（局部块因果掩码采样策略），并结合LDCFG（层解耦无分类器引导）实现多尺度控制。

Result: 在标准基准测试中，ReactDance表现优于现有方法，达到最先进性能。

Conclusion: ReactDance通过多尺度表示和局部上下文策略，显著提升了反应性舞蹈生成的质量和可控性。

Abstract: Reactive dance generation (RDG) produces follower movements conditioned on
guiding dancer and music while ensuring spatial coordination and temporal
coherence. However, existing methods overemphasize global constraints and
optimization, overlooking local information, such as fine-grained spatial
interactions and localized temporal context. Therefore, we present ReactDance,
a novel diffusion-based framework for high-fidelity RDG with long-term
coherence and multi-scale controllability. Unlike existing methods that
struggle with interaction fidelity, synchronization, and temporal consistency
in duet synthesis, our approach introduces two key innovations: 1)Group
Residual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion
representation that captures interaction semantics from coarse body rhythms to
fine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling
strategy eliminating error accumulation in long sequence generation via local
block causal masking and periodic positional encoding. Built on the decoupled
multi-scale GRFSQ representation, we implement a diffusion model
withLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control
over motion semantics across scales. Extensive experiments on standard
benchmarks demonstrate that ReactDance surpasses existing methods, achieving
state-of-the-art performance.

</details>

### [135] [QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization](https://arxiv.org/abs/2505.05591)
*Yueh-Cheng Liu,Lukas Höllein,Matthias Nießner,Angela Dai*

Main category: cs.CV

TLDR: QuickSplat利用数据驱动先验生成密集初始化，加速2D高斯溅射优化，提升大尺度室内场景重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于体积渲染的方法优化速度慢，难以处理低纹理区域。

Method: 学习数据驱动先验生成密集初始化，联合估计场景参数更新与高斯分布密度。

Result: 加速8倍，深度误差降低48%。

Conclusion: QuickSplat显著提升重建速度和几何精度。

Abstract: Surface reconstruction is fundamental to computer vision and graphics,
enabling applications in 3D modeling, mixed reality, robotics, and more.
Existing approaches based on volumetric rendering obtain promising results, but
optimize on a per-scene basis, resulting in a slow optimization that can
struggle to model under-observed or textureless regions. We introduce
QuickSplat, which learns data-driven priors to generate dense initializations
for 2D gaussian splatting optimization of large-scale indoor scenes. This
provides a strong starting point for the reconstruction, which accelerates the
convergence of the optimization and improves the geometry of flat wall
structures. We further learn to jointly estimate the densification and update
of the scene parameters during each iteration; our proposed densifier network
predicts new Gaussians based on the rendering gradients of existing ones,
removing the needs of heuristics for densification. Extensive experiments on
large-scale indoor scene reconstruction demonstrate the superiority of our
data-driven optimization. Concretely, we accelerate runtime by 8x, while
decreasing depth errors by up to 48% in comparison to state of the art methods.

</details>

### [136] [Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling](https://arxiv.org/abs/2505.05599)
*Seraj Al Mahmud Mostafa,Chenxi Wang,Jia Yue,Yuta Hozumi,Jianwu Wang*

Main category: cs.CV

TLDR: 论文提出YOLO-DCAP，一种改进的YOLOv5模型，用于解决卫星图像中物体定位的挑战，包括多尺度特征和全局空间关注，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 卫星图像中物体定位因高变异性、低分辨率和噪声干扰而困难，研究针对三种数据集（GW、Bore、OE）提出解决方案。

Method: YOLO-DCAP引入多尺度扩张残差卷积（MDRC）和注意力辅助空间池化（AaSP）模块，优化特征提取和选择。

Result: 实验显示YOLO-DCAP在mAP50和IoU上分别比基准模型提升20.95%和32.23%，优于现有方法。

Conclusion: YOLO-DCAP在多种卫星数据集上表现稳健且通用，代码已开源。

Abstract: Object localization in satellite imagery is particularly challenging due to
the high variability of objects, low spatial resolution, and interference from
noise and dominant features such as clouds and city lights. In this research,
we focus on three satellite datasets: upper atmospheric Gravity Waves (GW),
mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique
challenges. These challenges include the variability in the scale and
appearance of the main object patterns, where the size, shape, and feature
extent of objects of interest can differ significantly. To address these
challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed
to improve object localization in these complex scenarios. YOLO-DCAP
incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture
multi-scale features at scale with varying dilation rates, and an
Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant
spatial regions, enhancing feature selection. These structural improvements
help to better localize objects in satellite imagery. Experimental results
demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model
and state-of-the-art approaches, achieving an average improvement of 20.95% in
mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively
over state-of-the-art alternatives, consistently across all three satellite
datasets. These consistent gains across all three satellite datasets highlight
the robustness and generalizability of the proposed approach. Our code is open
sourced at
https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.

</details>

### [137] [A Preliminary Study for GPT-4o on Image Restoration](https://arxiv.org/abs/2505.05621)
*Hao Yang,Yan Yang,Ruikun Zhang,Liyuan Pan*

Main category: cs.CV

TLDR: GPT-4o在图像修复任务中表现优异，但存在像素级结构保真度问题。通过将其输出作为视觉先验，可显著提升现有去雾网络的性能。


<details>
  <summary>Details</summary>
Motivation: 研究GPT-4o在图像修复领域的潜力及其对现有技术的改进。

Method: 系统评估GPT-4o在多种修复任务中的表现，并以去雾、去雨和低光增强为例，验证其作为视觉先验的效果。

Result: GPT-4o生成的图像视觉效果好，但结构保真度不足；作为先验可显著提升去雾网络性能。

Conclusion: GPT-4o为图像修复提供了新思路，未来可加速图像生成领域的创新。

Abstract: OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an
autoregressive architecture, has demonstrated unprecedented performance in
image generation. In this work, we investigate its potential impact on the
image restoration community. We present the first systematic evaluation of
GPT-4o across diverse restoration tasks. Our experiments reveal that, although
restoration outputs from GPT-4o are visually appealing, they often suffer from
pixel-level structural fidelity when compared to ground-truth images. Common
issues are variations in image proportions, shifts in object positions and
quantities, and changes in viewpoint.To address it, taking image dehazing,
derainning, and low-light enhancement as representative case studies, we show
that GPT-4o's outputs can serve as powerful visual priors, substantially
enhancing the performance of existing dehazing networks. It offers practical
guidelines and a baseline framework to facilitate the integration of GPT-4o
into future image restoration pipelines. We hope the study on GPT-4o image
restoration will accelerate innovation in the broader field of image generation
areas. To support further research, we will release GPT-4o-restored images from
over 10 widely used image restoration datasets.

</details>

### [138] [Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models](https://arxiv.org/abs/2505.05626)
*Aarti Ghatkesar,Uddeshya Upadhyay,Ganesh Venkatesh*

Main category: cs.CV

TLDR: 论文提出了一种方法，旨在增强多模态大语言模型（MLLMs）对视觉内容的理解，并确保视觉信息能更有效地指导语言生成。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉与语言的深度融合上表现不足，往往过度依赖语言先验，未能充分利用视觉输入。

Method: 通过分析MLLMs内部如何构建对图像区域的理解，并引入技术来增强这种能力，同时确保视觉理解能主动引导语言生成。

Result: 模型在视觉依赖的词汇预测能力上表现出色，并在视觉挑战任务中提升了10个百分点。

Conclusion: 该方法显著提升了MLLMs的多模态理解能力，为视觉与语言的深度融合提供了有效途径。

Abstract: Achieving deep alignment between vision and language remains a central
challenge for Multimodal Large Language Models (MLLMs). These models often fail
to fully leverage visual input, defaulting to strong language priors. Our
approach first provides insights into how MLLMs internally build visual
understanding of image regions and then introduces techniques to amplify this
capability. Specifically, we explore techniques designed both to deepen the
model's understanding of visual content and to ensure that these visual
insights actively guide language generation. We demonstrate the superior
multimodal understanding of our resultant model through a detailed upstream
analysis quantifying its ability to predict visually-dependent tokens as well
as 10 pt boost on visually challenging tasks.

</details>

### [139] [VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models](https://arxiv.org/abs/2505.05635)
*Faizan Farooq Khan,Jun Chen,Youssef Mohamed,Chun-Mei Feng,Mohamed Elhoseiny*

Main category: cs.CV

TLDR: 论文提出了一种名为VR-RAG的开放词汇鸟类识别框架，结合视觉重排序和检索增强生成技术，显著提升了识别新物种的能力。


<details>
  <summary>Details</summary>
Motivation: 开放词汇识别在计算机视觉中具有挑战性，尤其是在自然界中不断发现新物种的背景下。传统方法在开放词汇场景下表现不佳。

Method: 提出VR-RAG框架，整合维基百科的结构化文本知识，并通过视觉相似性重排序候选物种。

Result: 实验表明，VR-RAG在五个基准测试中平均性能提升15.4%，优于传统视觉语言模型。

Conclusion: 该研究通过结合百科全书知识和视觉识别，推动了开放词汇识别的发展，为生物多样性监测提供了灵活、可扩展的解决方案。

Abstract: Open-vocabulary recognition remains a challenging problem in computer vision,
as it requires identifying objects from an unbounded set of categories. This is
particularly relevant in nature, where new species are discovered every year.
In this work, we focus on open-vocabulary bird species recognition, where the
goal is to classify species based on their descriptions without being
constrained to a predefined set of taxonomic categories. Traditional benchmarks
like CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary
paradigm, limiting their applicability to real-world scenarios where novel
species continually emerge. We show that the performance of current systems
when evaluated under settings closely aligned with open-vocabulary drops by a
huge margin. To address this gap, we propose a scalable framework integrating
structured textual knowledge from Wikipedia articles of 11,202 bird species
distilled via GPT-4o into concise, discriminative summaries. We propose Visual
Re-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented
generation framework that uses visual similarities to rerank the top m
candidates retrieved by a set of multimodal vision language encoders. This
allows for the recognition of unseen taxa. Extensive experiments across five
established classification benchmarks show that our approach is highly
effective. By integrating VR-RAG, we improve the average performance of
state-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five
benchmarks. Our approach outperforms conventional VLM-based approaches, which
struggle with unseen species. By bridging the gap between encyclopedic
knowledge and visual recognition, our work advances open-vocabulary
recognition, offering a flexible, scalable solution for biodiversity monitoring
and ecological research.

</details>

### [140] [Semantic Style Transfer for Enhancing Animal Facial Landmark Detection](https://arxiv.org/abs/2505.05640)
*Anadil Hussein,Anna Zamansky,George Martvel*

Main category: cs.CV

TLDR: 研究探讨了神经风格迁移（NST）在提升动物面部关键点检测器训练中的应用，通过改进生成图像质量和标注对齐，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统NST多用于艺术转换，本研究探索其在动物面部关键点检测中的潜力，以提升模型鲁棒性和准确性。

Method: 采用裁剪面部图像进行风格迁移以增强结构一致性；提出监督风格迁移（SST）解决标注对齐问题；通过风格迁移图像增强数据集。

Result: 裁剪面部图像风格迁移提升生成质量；SST保留98%基线准确率；风格迁移数据增强优于传统方法。

Conclusion: 语义风格迁移是提升面部关键点检测模型性能的有效策略，可推广至其他物种和模型。

Abstract: Neural Style Transfer (NST) is a technique for applying the visual
characteristics of one image onto another while preserving structural content.
Traditionally used for artistic transformations, NST has recently been adapted,
e.g., for domain adaptation and data augmentation. This study investigates the
use of this technique for enhancing animal facial landmark detectors training.
As a case study, we use a recently introduced Ensemble Landmark Detector for 48
anatomical cat facial landmarks and the CatFLW dataset it was trained on,
making three main contributions. First, we demonstrate that applying style
transfer to cropped facial images rather than full-body images enhances
structural consistency, improving the quality of generated images. Secondly,
replacing training images with style-transferred versions raised challenges of
annotation misalignment, but Supervised Style Transfer (SST) - which selects
style sources based on landmark accuracy - retained up to 98% of baseline
accuracy. Finally, augmenting the dataset with style-transferred images further
improved robustness, outperforming traditional augmentation methods. These
findings establish semantic style transfer as an effective augmentation
strategy for enhancing the performance of facial landmark detection models for
animals and beyond. While this study focuses on cat facial landmarks, the
proposed method can be generalized to other species and landmark detection
models.

</details>

### [141] [The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction](https://arxiv.org/abs/2505.05644)
*Tom Sander,Moritz Tenthoff,Kay Wohlfarth,Christian Wöhler*

Main category: cs.CV

TLDR: 多模态学习应用于行星科学，提出统一Transformer架构，支持多源数据间的灵活转换，解决3D重建和反射参数估计问题。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在行星科学中应用较少，本文旨在通过多模态学习解决月球图像的反射参数估计和3D重建问题。

Method: 提出单一统一的Transformer架构，学习灰度图像、数字高程模型、表面法线和反照率图等多源数据的共享表示。

Result: 模型能学习四种模态间的物理合理关系，支持从灰度图像预测DEM和反照率图，实现3D重建和参数解耦。

Conclusion: 未来可通过增加输入模态扩展任务，如光度归一化和共配准。

Abstract: Multimodal learning is an emerging research topic across multiple disciplines
but has rarely been applied to planetary science. In this contribution, we
identify that reflectance parameter estimation and image-based 3D
reconstruction of lunar images can be formulated as a multimodal learning
problem. We propose a single, unified transformer architecture trained to learn
shared representations between multiple sources like grayscale images, digital
elevation models, surface normals, and albedo maps. The architecture supports
flexible translation from any input modality to any target modality. Predicting
DEMs and albedo maps from grayscale images simultaneously solves the task of 3D
reconstruction of planetary surfaces and disentangles photometric parameters
and height information. Our results demonstrate that our foundation model
learns physically plausible relations across these four modalities. Adding more
input modalities in the future will enable tasks such as photometric
normalization and co-registration.

</details>

### [142] [Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval](https://arxiv.org/abs/2505.05666)
*Alexander Most,Joseph Winjum,Ayan Biswas,Shawn Jones,Nishath Rajiv Ranasinghe,Dan O'Malley,Manish Bhattarai*

Main category: cs.CV

TLDR: 比较视觉基础的RAG（ColPali）与传统OCR基础的RAG系统，发现视觉方法在特定文档表现好，而OCR方法泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型（LLMs）的可靠性，探讨视觉与OCR方法在RAG中的优劣。

Method: 系统比较视觉基础与OCR基础的RAG系统，引入语义答案评估基准。

Result: 视觉RAG在特定文档表现优，OCR RAG泛化能力更强。

Conclusion: 需权衡计算效率与语义准确性，为生产环境提供选择建议。

Abstract: Retrieval-Augmented Generation (RAG) has become a popular technique for
enhancing the reliability and utility of Large Language Models (LLMs) by
grounding responses in external documents. Traditional RAG systems rely on
Optical Character Recognition (OCR) to first process scanned documents into
text. However, even state-of-the-art OCRs can introduce errors, especially in
degraded or complex documents. Recent vision-language approaches, such as
ColPali, propose direct visual embedding of documents, eliminating the need for
OCR. This study presents a systematic comparison between a vision-based RAG
system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2
(90B) and Nougat OCR across varying document qualities. Beyond conventional
retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark
to assess end-to-end question-answering performance. Our findings indicate that
while vision-based RAG performs well on documents it has been fine-tuned on,
OCR-based RAG is better able to generalize to unseen documents of varying
quality. We highlight the key trade-offs between computational efficiency and
semantic accuracy, offering practical guidance for RAG practitioners in
selecting between OCR-dependent and vision-based document retrieval systems in
production environments.

</details>

### [143] [TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling](https://arxiv.org/abs/2505.05672)
*Gengyan Li,Paulo Gotardo,Timo Bolkart,Stephan Garbin,Kripasindhu Sarkar,Abhimitra Meka,Alexandros Lattas,Thabo Beeler*

Main category: cs.CV

TLDR: 提出了一种基于3D高斯泼溅的高细节3D头部虚拟化身模型，通过改进变形场和拟合方法，显著提升了动画保真度和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有动画化3D头部虚拟化身在运动估计不准确和内存限制下，常导致细节丢失和渲染质量下降。

Method: 结合基于网格的3D可变形模型和连续UVD切线空间中的3D高斯泼溅，引入新型UVD变形场以捕捉局部细微运动。

Result: 模型在4K分辨率下渲染时，显著增加了3D高斯数量并提升了建模质量。

Conclusion: 新型可变形高斯编码和拟合方法有效保留了外观细节，同时捕捉了面部运动和高频特征。

Abstract: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have
recently enabled animatable 3D head avatars that are rendered under arbitrary
viewpoints with impressive photorealism. Today, such photoreal avatars are seen
as a key component in emerging applications in telepresence, extended reality,
and entertainment. Building a photoreal avatar requires estimating the complex
non-rigid motion of different facial components as seen in input video images;
due to inaccurate motion estimation, animatable models typically present a loss
of fidelity and detail when compared to their non-animatable counterparts,
built from an individual facial expression. Also, recent state-of-the-art
models are often affected by memory limitations that reduce the number of 3D
Gaussians used for modeling, leading to lower detail and quality. To address
these problems, we present a new high-detail 3D head avatar model that improves
upon the state of the art, largely increasing the number of 3D Gaussians and
modeling quality for rendering at 4K resolution. Our high-quality model is
reconstructed from multiview input video and builds on top of a mesh-based 3D
morphable model, which provides a coarse deformation layer for the head.
Photoreal appearance is modelled by 3D Gaussians embedded within the continuous
UVD tangent space of this mesh, allowing for more effective densification where
most needed. Additionally, these Gaussians are warped by a novel UVD
deformation field to capture subtle, localized motion. Our key contribution is
the novel deformable Gaussian encoding and overall fitting procedure that
allows our head model to preserve appearance detail, while capturing facial
motion and other transient high-frequency features such as skin wrinkling.

</details>

### [144] [InstanceGen: Image Generation with Instance-level Instructions](https://arxiv.org/abs/2505.05678)
*Etai Sella,Yanir Kleiman,Hadar Averbuch-Elor*

Main category: cs.CV

TLDR: 提出了一种结合图像结构引导和LLM指令的方法，以改进复杂提示下的文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 预训练文本到图像模型在复杂提示下难以捕捉多对象和实例级属性的语义，需要额外结构约束。

Method: 利用图像生成模型提供的细粒度结构初始化，结合LLM的实例级指令，生成符合文本提示的图像。

Result: 生成的图像能更好地遵循文本提示，包括对象数量、实例级属性和空间关系。

Conclusion: 通过结合图像结构引导和LLM指令，显著提升了复杂提示下的生成效果。

Abstract: Despite rapid advancements in the capabilities of generative models,
pretrained text-to-image models still struggle in capturing the semantics
conveyed by complex prompts that compound multiple objects and instance-level
attributes. Consequently, we are witnessing growing interests in integrating
additional structural constraints, %leveraging additional structural inputs
typically in the form of coarse bounding boxes, to better guide the generation
process in such challenging cases. In this work, we take the idea of structural
guidance a step further by making the observation that contemporary image
generation models can directly provide a plausible \emph{fine-grained}
structural initialization. We propose a technique that couples this image-based
structural guidance with LLM-based instance-level instructions, yielding output
images that adhere to all parts of the text prompt, including object counts,
instance-level attributes, and spatial relations between instances.

</details>

### [145] [Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos](https://arxiv.org/abs/2505.05681)
*Giulio Cesare Mastrocinque Santo,Patrícia Izar,Irene Delval,Victor de Napole Gregolin,Nina S. T. Hirata*

Main category: cs.CV

TLDR: 论文提出了一种基于预训练视频-文本基础模型的微调方法，用于从野生卷尾猴视频中检索有用片段，通过数据清洗和微调显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 研究野生灵长类动物行为的视频通常无标签且噪声大，需要高效的计算模型帮助研究者检索有用片段。

Method: 采用两阶段方法：1) 数据清洗管道提取对齐的视频-文本对；2) 使用LoRA微调预训练的X-CLIP模型。

Result: Hits@5指标提升167%（16帧）和114%（8帧），NDCG@K显示模型能有效排序行为，而原始模型无法做到。

Conclusion: 方法显著提升了野生卷尾猴视频的检索性能，代码将公开。

Abstract: Video recordings of nonhuman primates in their natural habitat are a common
source for studying their behavior in the wild. We fine-tune pre-trained
video-text foundational models for the specific domain of capuchin monkeys,
with the goal of developing useful computational models to help researchers to
retrieve useful clips from videos. We focus on the challenging problem of
training a model based solely on raw, unlabeled video footage, using weak audio
descriptions sometimes provided by field collaborators. We leverage recent
advances in Multimodal Large Language Models (MLLMs) and Vision-Language Models
(VLMs) to address the extremely noisy nature of both video and audio content.
Specifically, we propose a two-folded approach: an agentic data treatment
pipeline and a fine-tuning process. The data processing pipeline automatically
extracts clean and semantically aligned video-text pairs from the raw videos,
which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model
through Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of
$167\%$ for the 16 frames model and an uplift of $114\%$ for the 8 frame model
on our domain data. Moreover, based on $NDCG@K$ results, our model is able to
rank well most of the considered behaviors, while the tested raw pre-trained
models are not able to rank them at all. The code will be made available upon
acceptance.

</details>

### [146] [HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder](https://arxiv.org/abs/2505.05710)
*Wooyoung Jeong,Hyun Jae Park,Seonghun Jeong,Jong Wook Jang,Tae Hoon Lim,Dae Seoung Kim*

Main category: cs.CV

TLDR: HyperspectralMAE是一种基于Transformer的基础模型，通过双掩码策略（空间和光谱维度）预训练，结合谐波傅里叶位置嵌入和重建目标，实现了高维高光谱数据的鲁棒表示。


<details>
  <summary>Details</summary>
Motivation: 高光谱数据的高维性在空间和光谱维度上带来挑战，需要一种能够学习跨维度缺失信息重建的模型。

Method: 采用双掩码策略（50%空间块和50%光谱带掩码），结合谐波傅里叶位置嵌入和MSE+SAM重建目标进行预训练。

Result: 模型在Indian Pines基准测试中达到最先进的迁移学习准确率，验证了双掩码预训练的有效性。

Conclusion: 双掩码和波长感知嵌入提升了高光谱图像重建和下游分析能力。

Abstract: Hyperspectral imagery provides rich spectral detail but poses unique
challenges because of its high dimensionality in both spatial and spectral
domains. We propose \textit{HyperspectralMAE}, a Transformer-based foundation
model for hyperspectral data that employs a \textit{dual masking} strategy:
during pre-training we randomly occlude 50\% of spatial patches and 50\% of
spectral bands. This forces the model to learn representations capable of
reconstructing missing information across both dimensions. To encode spectral
order, we introduce learnable harmonic Fourier positional embeddings based on
wavelength. The reconstruction objective combines mean-squared error (MSE) with
the spectral angle mapper (SAM) to balance pixel-level accuracy and
spectral-shape fidelity.
  The resulting model contains about $1.8\times10^{8}$ parameters and produces
768-dimensional embeddings, giving it sufficient capacity for transfer
learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --
NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra)
and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixel
spectra) -- and fine-tuned it for land-cover classification on the Indian Pines
benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning
accuracy on Indian Pines, confirming that masked dual-dimensional pre-training
yields robust spectral-spatial representations. These results demonstrate that
dual masking and wavelength-aware embeddings advance hyperspectral image
reconstruction and downstream analysis.

</details>

### [147] [DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer](https://arxiv.org/abs/2505.05711)
*Ho-Joong Kim,Yearang Lee,Jung-Ho Hong,Seong-Whan Lee*

Main category: cs.CV

TLDR: 论文提出了一种名为DiGIT的时序动作检测方法，通过多扩张门控编码器和中心-相邻区域集成解码器，解决了现有查询检测器在时序动作检测中的冗余信息和上下文捕捉不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的检测器直接沿用目标检测架构，难以应对时序动作检测的独特挑战，如多尺度特征冗余和上下文捕捉不足。

Method: 提出多扩张门控编码器替代多尺度可变形注意力编码器，减少冗余信息；引入中心-相邻区域集成解码器，优化采样策略以捕捉关键信息。

Result: DiGIT在THUMOS14、ActivityNet v1.3和HACS-Segment数据集上达到最优性能。

Conclusion: DiGIT通过改进编码器和解码器设计，显著提升了时序动作检测的性能。

Abstract: In this paper, we examine a key limitation in query-based detectors for
temporal action detection (TAD), which arises from their direct adaptation of
originally designed architectures for object detection. Despite the
effectiveness of the existing models, they struggle to fully address the unique
challenges of TAD, such as the redundancy in multi-scale features and the
limited ability to capture sufficient temporal context. To address these
issues, we propose a multi-dilated gated encoder and central-adjacent region
integrated decoder for temporal action detection transformer (DiGIT). Our
approach replaces the existing encoder that consists of multi-scale deformable
attention and feedforward network with our multi-dilated gated encoder. Our
proposed encoder reduces the redundant information caused by multi-level
features while maintaining the ability to capture fine-grained and long-range
temporal information. Furthermore, we introduce a central-adjacent region
integrated decoder that leverages a more comprehensive sampling strategy for
deformable cross-attention to capture the essential information. Extensive
experiments demonstrate that DiGIT achieves state-of-the-art performance on
THUMOS14, ActivityNet v1.3, and HACS-Segment. Code is available at:
https://github.com/Dotori-HJ/DiGIT

</details>

### [148] [Semantic-Space-Intervened Diffusive Alignment for Visual Classification](https://arxiv.org/abs/2505.05721)
*Zixuan Li,Lei Meng,Guoqing Chao,Wei Wu,Xiaoshuo Yan,Yimeng Yang,Zhuang Qi,Xiangxu Meng*

Main category: cs.CV

TLDR: 论文提出了一种名为SeDA的新方法，通过语义空间干预和双阶段扩散框架，解决了视觉与文本特征对齐的困难，提升了跨模态分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过一步映射将视觉特征投影到文本特征分布，但由于两种模态在样本分布和特征值范围上的差异，难以实现有效对齐。

Method: SeDA引入语义空间作为桥梁，采用双阶段扩散框架：第一阶段通过扩散控制语义学习器建模视觉特征的语义空间；第二阶段通过扩散控制语义翻译器学习文本特征分布，并结合逐步特征交互网络。

Result: 实验表明，SeDA在多种场景下优于现有方法，实现了更强的跨模态特征对齐。

Conclusion: SeDA通过语义空间干预和渐进对齐，有效解决了跨模态对齐问题，提升了分类性能。

Abstract: Cross-modal alignment is an effective approach to improving visual
classification. Existing studies typically enforce a one-step mapping that uses
deep neural networks to project the visual features to mimic the distribution
of textual features. However, they typically face difficulties in finding such
a projection due to the two modalities in both the distribution of class-wise
samples and the range of their feature values. To address this issue, this
paper proposes a novel Semantic-Space-Intervened Diffusive Alignment method,
termed SeDA, models a semantic space as a bridge in the visual-to-textual
projection, considering both types of features share the same class-level
information in classification. More importantly, a bi-stage diffusion framework
is developed to enable the progressive alignment between the two modalities.
Specifically, SeDA first employs a Diffusion-Controlled Semantic Learner to
model the semantic features space of visual features by constraining the
interactive features of the diffusion model and the category centers of visual
features. In the later stage of SeDA, the Diffusion-Controlled Semantic
Translator focuses on learning the distribution of textual features from the
semantic space. Meanwhile, the Progressive Feature Interaction Network
introduces stepwise feature interactions at each alignment step, progressively
integrating textual information into mapped features. Experimental results show
that SeDA achieves stronger cross-modal feature alignment, leading to superior
performance over existing methods across multiple scenarios.

</details>

### [149] [You Are Your Best Teacher: Semi-Supervised Surgical Point Tracking with Cycle-Consistent Self-Distillation](https://arxiv.org/abs/2505.05722)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik Lensch*

Main category: cs.CV

TLDR: SurgTracker是一种半监督框架，通过自蒸馏和循环一致性约束，将合成数据训练的点跟踪器适应于手术视频，显著提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 由于领域偏移和标注数据缺乏，合成数据训练的模型在手术视频等复杂场景中表现不佳，需要一种有效的适应方法。

Method: 采用固定教师模型生成在线伪标签，并通过循环一致性过滤不一致轨迹，确保几何一致性和稳定监督。

Result: 在STIR基准测试中，仅使用80个未标记视频，SurgTracker显著提升了跟踪性能。

Conclusion: SurgTracker为高偏移、数据稀缺领域提供了一种简单高效的适应方法。

Abstract: Synthetic datasets have enabled significant progress in point tracking by
providing large-scale, densely annotated supervision. However, deploying these
models in real-world domains remains challenging due to domain shift and lack
of labeled data-issues that are especially severe in surgical videos, where
scenes exhibit complex tissue deformation, occlusion, and lighting variation.
While recent approaches adapt synthetic-trained trackers to natural videos
using teacher ensembles or augmentation-heavy pseudo-labeling pipelines, their
effectiveness in high-shift domains like surgery remains unexplored. This work
presents SurgTracker, a semi-supervised framework for adapting
synthetic-trained point trackers to surgical video using filtered
self-distillation. Pseudo-labels are generated online by a fixed
teacher-identical in architecture and initialization to the student-and are
filtered using a cycle consistency constraint to discard temporally
inconsistent trajectories. This simple yet effective design enforces geometric
consistency and provides stable supervision throughout training, without the
computational overhead of maintaining multiple teachers. Experiments on the
STIR benchmark show that SurgTracker improves tracking performance using only
80 unlabeled videos, demonstrating its potential for robust adaptation in
high-shift, data-scarce domains.

</details>

### [150] [Dome-DETR: DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection](https://arxiv.org/abs/2505.05741)
*Zhangchi Hu,Peixi Wu,Jie Chen,Huyue Zhu,Yijun Wang,Yansong Peng,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TLDR: Dome-DETR是一种高效的小目标检测框架，通过密度导向的特征查询操作减少冗余计算，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有小目标检测方法存在特征利用效率低和计算成本高的问题，Dome-DETR旨在解决这些问题。

Method: 提出DeFE生成紧凑前景掩码，结合MWAS稀疏注意力机制和PAQI自适应查询分配策略。

Result: 在AI-TOD-V2和VisDrone数据集上分别提升3.3 AP和2.5 AP，同时保持低计算复杂度。

Conclusion: Dome-DETR通过高效特征利用和查询分配，实现了小目标检测的先进性能。

Abstract: Tiny object detection plays a vital role in drone surveillance, remote
sensing, and autonomous systems, enabling the identification of small targets
across vast landscapes. However, existing methods suffer from inefficient
feature leverage and high computational costs due to redundant feature
processing and rigid query allocation. To address these challenges, we propose
Dome-DETR, a novel framework with Density-Oriented Feature-Query Manipulation
for Efficient Tiny Object Detection. To reduce feature redundancies, we
introduce a lightweight Density-Focal Extractor (DeFE) to produce clustered
compact foreground masks. Leveraging these masks, we incorporate Masked Window
Attention Sparsification (MWAS) to focus computational resources on the most
informative regions via sparse attention. Besides, we propose Progressive
Adaptive Query Initialization (PAQI), which adaptively modulates query density
across spatial areas for better query allocation. Extensive experiments
demonstrate that Dome-DETR achieves state-of-the-art performance (+3.3 AP on
AI-TOD-V2 and +2.5 AP on VisDrone) while maintaining low computational
complexity and a compact model size. Code will be released upon acceptance.

</details>

### [151] [kFuse: A novel density based agglomerative clustering](https://arxiv.org/abs/2505.05748)
*Huan Yan,Junjie Hu*

Main category: cs.CV

TLDR: 本文提出了一种基于密度的凝聚聚类方法kFuse，解决了传统方法需要额外参数和聚类结果不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 传统凝聚聚类方法需要额外参数且对数据集敏感，同时受限于连接距离的计算方式，导致结果不稳定。

Method: kFuse包含四个关键步骤：基于自然邻居的子簇划分、边界连通性计算、密度相似性评估和合并规则制定。

Result: 实验证明kFuse在合成和真实数据集上表现优异，显著提升了聚类准确性。

Conclusion: kFuse通过综合考虑邻近样本、距离和密度，显著改进了凝聚聚类的性能。

Abstract: Agglomerative clustering has emerged as a vital tool in data analysis due to
its intuitive and flexible characteristics. However, existing agglomerative
clustering methods often involve additional parameters for sub-cluster
partitioning and inter-cluster similarity assessment. This necessitates
different parameter settings across various datasets, which is undoubtedly
challenging in the absence of prior knowledge. Moreover, existing agglomerative
clustering techniques are constrained by the calculation method of connection
distance, leading to unstable clustering results. To address these issues, this
paper introduces a novel density-based agglomerative clustering method, termed
kFuse. kFuse comprises four key components: (1) sub-cluster partitioning based
on natural neighbors; (2) determination of boundary connectivity between
sub-clusters through the computation of adjacent samples and shortest
distances; (3) assessment of density similarity between sub-clusters via the
calculation of mean density and variance; and (4) establishment of merging
rules between sub-clusters based on boundary connectivity and density
similarity. kFuse requires the specification of the number of clusters only at
the final merging stage. Additionally, by comprehensively considering adjacent
samples, distances, and densities among different sub-clusters, kFuse
significantly enhances accuracy during the merging phase, thereby greatly
improving its identification capability. Experimental results on both synthetic
and real-world datasets validate the effectiveness of kFuse.

</details>

### [152] [Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data](https://arxiv.org/abs/2505.05752)
*Amin Ghafourian,Andrew Lee,Dechen Gao,Tyler Beer,Kin Yen,Iman Soltani*

Main category: cs.CV

TLDR: 本文提出了一种基于点云数据的自动化几何测量和合规性评估框架，结合深度学习和几何处理技术，用于基础设施调查。以ADA合规性为例验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化在提高基础设施调查和合规性评估的效率、准确性和可扩展性方面具有重要作用。

Method: 结合深度学习检测与分割、几何和信号处理技术，自动化完成调查任务。

Result: 实验结果表明，该方法准确可靠，显著减少人工工作量并提高一致性。

Conclusion: 该框架为基础设施调查和自动化施工评估提供了基础，推动了点云数据在这些领域的广泛应用。

Abstract: Automation can play a prominent role in improving efficiency, accuracy, and
scalability in infrastructure surveying and assessing construction and
compliance standards. This paper presents a framework for automation of
geometric measurements and compliance assessment using point cloud data. The
proposed approach integrates deep learning-based detection and segmentation, in
conjunction with geometric and signal processing techniques, to automate
surveying tasks. As a proof of concept, we apply this framework to
automatically evaluate the compliance of curb ramps with the Americans with
Disabilities Act (ADA), demonstrating the utility of point cloud data in survey
automation. The method leverages a newly collected, large annotated dataset of
curb ramps, made publicly available as part of this work, to facilitate robust
model training and evaluation. Experimental results, including comparison with
manual field measurements of several ramps, validate the accuracy and
reliability of the proposed method, highlighting its potential to significantly
reduce manual effort and improve consistency in infrastructure assessment.
Beyond ADA compliance, the proposed framework lays the groundwork for broader
applications in infrastructure surveying and automated construction evaluation,
promoting wider adoption of point cloud data in these domains. The annotated
database, manual ramp survey data, and developed algorithms are publicly
available on the project's GitHub page:
https://github.com/Soltanilara/SurveyAutomation.

</details>

### [153] [A review of advancements in low-light image enhancement using deep learning](https://arxiv.org/abs/2505.05759)
*Fangxue Liu,Lei Fan*

Main category: cs.CV

TLDR: 本文综述了2020年以来基于深度学习的低光照图像增强方法，分析了其机制及对下游视觉任务的影响，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 低光照环境下计算机视觉算法性能下降，缺乏对深度学习增强方法的系统综述，本文旨在填补这一空白。

Method: 详细阐述2020年以来各种低光照图像增强方法的操作机制，辅以清晰图示，并分析其对下游任务的影响。

Result: 总结了不同增强技术的优缺点及其对视觉任务的提升效果。

Conclusion: 本文为选择低光照图像增强技术和优化视觉任务性能提供了参考，并指出了未来研究方向。

Abstract: In low-light environments, the performance of computer vision algorithms
often deteriorates significantly, adversely affecting key vision tasks such as
segmentation, detection, and classification. With the rapid advancement of deep
learning, its application to low-light image processing has attracted
widespread attention and seen significant progress in recent years. However,
there remains a lack of comprehensive surveys that systematically examine how
recent deep-learning-based low-light image enhancement methods function and
evaluate their effectiveness in enhancing downstream vison tasks. To address
this gap, this review provides a detailed elaboration on how various recent
approaches (from 2020) operate and their enhancement mechanisms, supplemented
with clear illustrations. It also investigates the impact of different
enhancement techniques on subsequent vision tasks, critically analyzing their
strengths and limitations. Additionally, it proposes future research
directions. This review serves as a useful reference for determining low-light
image enhancement techniques and optimizing vision task performance in
low-light conditions.

</details>

### [154] [Describe Anything in Medical Images](https://arxiv.org/abs/2505.05804)
*Xi Xiao,Yunbei Zhang,Thanh-Huy Nguyen,Ba-Thinh Lam,Janet Wang,Jihun Hamm,Tianyang Wang,Xingjian Li,Xiao Wang,Hao Xu,Tianming Liu,Min Xu*

Main category: cs.CV

TLDR: MedDAM是一个针对医学图像的局部描述生成框架，通过专家设计的提示词和评估基准，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有局部描述模型在医学图像领域应用不足，而医学诊断依赖区域细节。

Method: MedDAM利用大型视觉语言模型，结合专家设计的提示词和评估协议。

Result: 在多个数据集上优于GPT-4o等模型，验证了区域语义对齐的重要性。

Conclusion: MedDAM为临床视觉语言整合提供了有前景的基础。

Abstract: Localized image captioning has made significant progress with models like the
Describe Anything Model (DAM), which can generate detailed region-specific
descriptions without explicit region-text supervision. However, such
capabilities have yet to be widely applied to specialized domains like medical
imaging, where diagnostic interpretation relies on subtle regional findings
rather than global understanding. To mitigate this gap, we propose MedDAM, the
first comprehensive framework leveraging large vision-language models for
region-specific captioning in medical images. MedDAM employs medical
expert-designed prompts tailored to specific imaging modalities and establishes
a robust evaluation benchmark comprising a customized assessment protocol, data
pre-processing pipeline, and specialized QA template library. This benchmark
evaluates both MedDAM and other adaptable large vision-language models,
focusing on clinical factuality through attribute-level verification tasks,
thereby circumventing the absence of ground-truth region-caption pairs in
medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and
SkinCon datasets demonstrate MedDAM's superiority over leading peers (including
GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and
OMG-LLaVA) in the task, revealing the importance of region-level semantic
alignment in medical image understanding and establishing MedDAM as a promising
foundation for clinical vision-language integration.

</details>

### [155] [Image Segmentation via Variational Model Based Tailored UNet: A Deep Variational Framework](https://arxiv.org/abs/2505.05806)
*Kaili Qi,Wenli Yang,Ye Li,Zhongyi Huang*

Main category: cs.CV

TLDR: 提出了一种结合变分模型和UNet的混合框架VM_TUNet，兼具数学可解释性和自适应特征学习能力。


<details>
  <summary>Details</summary>
Motivation: 传统变分模型计算成本高且参数敏感，而深度学习模型缺乏理论可解释性，结合两者优势。

Method: 将四阶修正Cahn-Hilliard方程与UNet结合，引入数据驱动算子和TFPM方法。

Result: 在基准数据集上表现优于现有方法，尤其在精细边界分割上。

Conclusion: VM_TUNet成功结合了变分模型和深度学习的优势，提升了分割性能。

Abstract: Traditional image segmentation methods, such as variational models based on
partial differential equations (PDEs), offer strong mathematical
interpretability and precise boundary modeling, but often suffer from
sensitivity to parameter settings and high computational costs. In contrast,
deep learning models such as UNet, which are relatively lightweight in
parameters, excel in automatic feature extraction but lack theoretical
interpretability and require extensive labeled data. To harness the
complementary strengths of both paradigms, we propose Variational Model Based
Tailored UNet (VM_TUNet), a novel hybrid framework that integrates the
fourth-order modified Cahn-Hilliard equation with the deep learning backbone of
UNet, which combines the interpretability and edge-preserving properties of
variational methods with the adaptive feature learning of neural networks.
Specifically, a data-driven operator is introduced to replace manual parameter
tuning, and we incorporate the tailored finite point method (TFPM) to enforce
high-precision boundary preservation. Experimental results on benchmark
datasets demonstrate that VM_TUNet achieves superior segmentation performance
compared to existing approaches, especially for fine boundary delineation.

</details>

### [156] [Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition](https://arxiv.org/abs/2505.05829)
*Zhiyuan Chen,Keyi Li,Yifan Jia,Le Ye,Yufei Ma*

Main category: cs.CV

TLDR: 提出了一种无需训练的DiT加速方法，通过增量校准缓存和通道感知SVD，显著减少计算量并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiT）在图像生成中表现出色，但迭代性质导致计算复杂度高，现有缓存方法缺乏校正可能降低质量。

Method: 提出增量校准缓存方法，利用预训练模型生成校准参数，并通过通道感知SVD处理异常激活。

Result: 实验显示，在相似计算资源下优于现有缓存方法，减少45%计算量，IS提升12，FID增加小于0.06。

Conclusion: 该方法高效且无需额外训练，显著提升DiT的实用性。

Abstract: Diffusion transformer (DiT) models have achieved remarkable success in image
generation, thanks for their exceptional generative capabilities and
scalability. Nonetheless, the iterative nature of diffusion models (DMs)
results in high computation complexity, posing challenges for deployment.
Although existing cache-based acceleration methods try to utilize the inherent
temporal similarity to skip redundant computations of DiT, the lack of
correction may induce potential quality degradation. In this paper, we propose
increment-calibrated caching, a training-free method for DiT acceleration,
where the calibration parameters are generated from the pre-trained model
itself with low-rank approximation. To deal with the possible correction
failure arising from outlier activations, we introduce channel-aware Singular
Value Decomposition (SVD), which further strengthens the calibration effect.
Experimental results show that our method always achieve better performance
than existing naive caching methods with a similar computation resource budget.
When compared with 35-step DDIM, our method eliminates more than 45%
computation and improves IS by 12 at the cost of less than 0.06 FID increase.
Code is available at https://github.com/ccccczzy/icc.

</details>

### [157] [Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression](https://arxiv.org/abs/2505.05834)
*Chunlai Dong,Haochao Ying,Qibo Qiu,Jinhong Wang,Danny Chen,Jian Wu*

Main category: cs.CV

TLDR: 论文提出了一种名为DFPG的双层次模糊学习框架，通过补丁引导从模糊的序数标签中学习精确的特征分级边界，并利用补丁级监督。


<details>
  <summary>Details</summary>
Motivation: 当前方法仅依赖图像级序数标签，忽略了细粒度的补丁级特征，而人类专家则依赖补丁级特征进行决策。

Method: 设计了补丁标记和过滤策略，使模型仅依赖图像级序数标签即可关注补丁级特征；提出双层次模糊学习模块，从补丁和通道两个角度处理标签模糊性。

Result: 在多个图像序数回归数据集上的实验验证了方法的优越性，特别是在难以分类的类别中区分样本的能力。

Conclusion: DFPG框架通过补丁级监督和模糊学习，有效提升了序数回归的性能。

Abstract: Ordinal regression bridges regression and classification by assigning objects
to ordered classes. While human experts rely on discriminative patch-level
features for decisions, current approaches are limited by the availability of
only image-level ordinal labels, overlooking fine-grained patch-level
characteristics. In this paper, we propose a Dual-level Fuzzy Learning with
Patch Guidance framework, named DFPG that learns precise feature-based grading
boundaries from ambiguous ordinal labels, with patch-level supervision.
Specifically, we propose patch-labeling and filtering strategies to enable the
model to focus on patch-level features exclusively with only image-level
ordinal labels available. We further design a dual-level fuzzy learning module,
which leverages fuzzy logic to quantitatively capture and handle label
ambiguity from both patch-wise and channel-wise perspectives. Extensive
experiments on various image ordinal regression datasets demonstrate the
superiority of our proposed method, further confirming its ability in
distinguishing samples from difficult-to-classify categories. The code is
available at https://github.com/ZJUMAI/DFPG-ord.

</details>

### [158] [Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry](https://arxiv.org/abs/2505.05845)
*Guohao Lin,Shidong Pan,Rasul Khanbayov,Changxi Yang,Ani Khaloian-Sarnaghi,Andriy Kovryga*

Main category: cs.CV

TLDR: 提出了一种基于机器学习的轻量级全自动管道，用于木材中结节的检测与配对，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统手动标注木材结节费时费力，亟需自动化解决方案以提升木材加工的效率和质量。

Method: 采用YOLOv8l进行结节检测，并通过多维特征提取和三元神经网络实现结节配对。

Result: 检测阶段mAP@0.5达0.887，配对阶段准确率达0.85。

Conclusion: 实验验证了方案的可行性，展示了AI在木材科学与工业中的潜力。

Abstract: Knots in wood are critical to both aesthetics and structural integrity,
making their detection and pairing essential in timber processing. However,
traditional manual annotation was labor-intensive and inefficient,
necessitating automation. This paper proposes a lightweight and fully automated
pipeline for knot detection and pairing based on machine learning techniques.
In the detection stage, high-resolution surface images of wooden boards were
collected using industrial-grade cameras, and a large-scale dataset was
manually annotated and preprocessed. After the transfer learning, the YOLOv8l
achieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were
analyzed and paired based on multidimensional feature extraction. A triplet
neural network was used to map the features into a latent space, enabling
clustering algorithms to identify and pair corresponding knots. The triplet
network with learnable weights achieved a pairing accuracy of 0.85. Further
analysis revealed that he distances from the knot's start and end points to the
bottom of the wooden board, and the longitudinal coordinates play crucial roles
in achieving high pairing accuracy. Our experiments validate the effectiveness
of the proposed solution, demonstrating the potential of AI in advancing wood
science and industry.

</details>

### [159] [RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects](https://arxiv.org/abs/2505.05848)
*Yue Yin,Enze Tao,Weijian Deng,Dylan Campbell*

Main category: cs.CV

TLDR: 论文介绍了RefRef数据集和基准，用于从姿态图像重建包含折射和反射物体的场景，并提出了一种基于几何和折射率的精确光线路径计算方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法主要针对不透明Lambertian物体，无法处理折射和反射材料，且缺乏相关数据集。

Method: 提出RefRef数据集（150个场景）和基准，以及基于几何和折射率的精确光线路径计算方法。

Result: 实验表明，现有方法在RefRef数据集上表现显著落后于提出的oracle方法。

Conclusion: 该任务和数据集具有挑战性，现有方法需进一步改进。

Abstract: Modern 3D reconstruction and novel view synthesis approaches have
demonstrated strong performance on scenes with opaque Lambertian objects.
However, most assume straight light paths and therefore cannot properly handle
refractive and reflective materials. Moreover, datasets specialized for these
effects are limited, stymieing efforts to evaluate performance and develop
suitable techniques. In this work, we introduce a synthetic RefRef dataset and
benchmark for reconstructing scenes with refractive and reflective objects from
posed images. Our dataset has 50 such objects of varying complexity, from
single-material convex shapes to multi-material non-convex shapes, each placed
in three different background types, resulting in 150 scenes. We also propose
an oracle method that, given the object geometry and refractive indices,
calculates accurate light paths for neural rendering, and an approach based on
this that avoids these assumptions. We benchmark these against several
state-of-the-art methods and show that all methods lag significantly behind the
oracle, highlighting the challenges of the task and dataset.

</details>

### [160] [PICD: Versatile Perceptual Image Compression with Diffusion Rendering](https://arxiv.org/abs/2505.05853)
*Tongda Xu,Jiahao Li,Bin Li,Yan Wang,Ya-Qin Zhang,Yan Lu*

Main category: cs.CV

TLDR: 提出了一种基于扩散渲染的感知屏幕图像压缩方法（PICD），能够同时处理屏幕内容和自然图像，显著提升文本压缩质量。


<details>
  <summary>Details</summary>
Motivation: 现有感知图像压缩方法在处理屏幕内容（尤其是文本）时会产生明显伪影，需要一种通用且高效的解决方案。

Method: 通过分离编码文本和图像，并利用扩散模型在三个层次（领域、适配器和实例）进行条件渲染，实现高质量压缩。

Result: PICD在文本准确性和感知质量上优于现有方法，同时适用于自然图像压缩。

Conclusion: PICD是一种高效的通用图像压缩方法，特别适合屏幕内容处理。

Abstract: Recently, perceptual image compression has achieved significant advancements,
delivering high visual quality at low bitrates for natural images. However, for
screen content, existing methods often produce noticeable artifacts when
compressing text. To tackle this challenge, we propose versatile perceptual
screen image compression with diffusion rendering (PICD), a codec that works
well for both screen and natural images. More specifically, we propose a
compression framework that encodes the text and image separately, and renders
them into one image using diffusion model. For this diffusion rendering, we
integrate conditional information into diffusion models at three distinct
levels: 1). Domain level: We fine-tune the base diffusion model using text
content prompts with screen content. 2). Adaptor level: We develop an efficient
adaptor to control the diffusion model using compressed image and text as
input. 3). Instance level: We apply instance-wise guidance to further enhance
the decoding process. Empirically, our PICD surpasses existing perceptual
codecs in terms of both text accuracy and perceptual quality. Additionally,
without text conditions, our approach serves effectively as a perceptual codec
for natural images.

</details>

### [161] [Decoupling Multi-Contrast Super-Resolution: Pairing Unpaired Synthesis with Implicit Representations](https://arxiv.org/abs/2505.05855)
*Hongyu Rui,Yinzhe Wu,Fanwen Wang,Jiahao Huang,Liutao Yang,Zi Wang,Guang Yang*

Main category: cs.CV

TLDR: 提出了一种新型模块化多对比超分辨率（MCSR）框架，无需配对训练数据，支持任意放大倍数，通过两阶段设计实现跨模态合成和超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: MRI的多对比特性为跨模态增强提供了机会，但现有方法需要配对数据且分辨率固定，难以适应临床环境。

Method: 分为两阶段：无配对跨模态合成（U-CMS）和无监督超分辨率（U-SR），利用隐式神经表示（INRs）实现分辨率增强。

Result: 在4倍和8倍放大下表现优于现有基线，具有更高的保真度和解剖一致性。

Conclusion: 该框架在真实临床环境中具有潜力，支持可扩展、个体化和数据高效的MCSR。

Abstract: Magnetic Resonance Imaging (MRI) is critical for clinical diagnostics but is
often limited by long acquisition times and low signal-to-noise ratios,
especially in modalities like diffusion and functional MRI. The multi-contrast
nature of MRI presents a valuable opportunity for cross-modal enhancement,
where high-resolution (HR) modalities can serve as references to boost the
quality of their low-resolution (LR) counterparts-motivating the development of
Multi-Contrast Super-Resolution (MCSR) techniques. Prior work has shown that
leveraging complementary contrasts can improve SR performance; however,
effective feature extraction and fusion across modalities with varying
resolutions remains a major challenge. Moreover, existing MCSR methods often
assume fixed resolution settings and all require large, perfectly paired
training datasets-conditions rarely met in real-world clinical environments. To
address these challenges, we propose a novel Modular Multi-Contrast
Super-Resolution (MCSR) framework that eliminates the need for paired training
data and supports arbitrary upscaling. Our method decouples the MCSR task into
two stages: (1) Unpaired Cross-Modal Synthesis (U-CMS), which translates a
high-resolution reference modality into a synthesized version of the target
contrast, and (2) Unsupervised Super-Resolution (U-SR), which reconstructs the
final output using implicit neural representations (INRs) conditioned on
spatial coordinates. This design enables scale-agnostic and anatomically
faithful reconstruction by bridging un-paired cross-modal synthesis with
unsupervised resolution enhancement. Experiments show that our method achieves
superior performance at 4x and 8x upscaling, with improved fidelity and
anatomical consistency over existing baselines. Our framework demonstrates
strong potential for scalable, subject-specific, and data-efficient MCSR in
real-world clinical settings.

</details>

### [162] [Towards Facial Image Compression with Consistency Preserving Diffusion Prior](https://arxiv.org/abs/2505.05870)
*Yimin Zhou,Yichong Xia,Bin Chen,Baoyi An,Haoqian Wang,Zhi Wang,Yaowei Wang,Zikun Zhou*

Main category: cs.CV

TLDR: 提出了一种基于稳定扩散先验的人脸图像压缩方法（FaSDiff），通过频率增强保持一致性，平衡人类视觉质量和机器视觉准确性。


<details>
  <summary>Details</summary>
Motivation: 现有人脸图像压缩方法在低比特率下重建质量不佳，且扩散基方法因高频信息保留不足导致下游应用性能差。

Method: FaSDiff采用高频敏感压缩器和混合低频增强模块，结合稳定扩散先验，优化视觉提示和语义一致性。

Result: 实验表明FaSDiff在人类视觉质量和机器视觉准确性上优于现有方法。

Conclusion: FaSDiff通过频率增强和扩散先验，显著提升了人脸图像压缩的性能。

Abstract: With the widespread application of facial image data across various domains,
the efficient storage and transmission of facial images has garnered
significant attention. However, the existing learned face image compression
methods often produce unsatisfactory reconstructed image quality at low bit
rates. Simply adapting diffusion-based compression methods to facial
compression tasks results in reconstructed images that perform poorly in
downstream applications due to insufficient preservation of high-frequency
information. To further explore the diffusion prior in facial image
compression, we propose Facial Image Compression with a Stable Diffusion Prior
(FaSDiff), a method that preserves consistency through frequency enhancement.
FaSDiff employs a high-frequency-sensitive compressor in an end-to-end
framework to capture fine image details and produce robust visual prompts.
Additionally, we introduce a hybrid low-frequency enhancement module that
disentangles low-frequency facial semantics and stably modulates the diffusion
prior alongside visual prompts. The proposed modules allow FaSDiff to leverage
diffusion priors for superior human visual perception while minimizing
performance loss in machine vision due to semantic inconsistency. Extensive
experiments show that FaSDiff outperforms state-of-the-art methods in balancing
human visual quality and machine vision accuracy. The code will be released
after the paper is accepted.

</details>

### [163] [Register and CLS tokens yield a decoupling of local and global features in large ViTs](https://arxiv.org/abs/2505.05892)
*Alexander Lappe,Martin A. Giese*

Main category: cs.CV

TLDR: DINOv2模型的注意力图存在缺陷，影响模型解释性和密集图像任务性能。通过引入寄存器令牌改善了注意力图，但全局信息仍主导局部特征。研究发现CLS令牌也有类似问题，需谨慎解释大型ViT的注意力图。


<details>
  <summary>Details</summary>
Motivation: 解决DINOv2模型注意力图中的缺陷，提升模型解释性和密集图像任务性能。

Method: 引入寄存器令牌存储全局信息，并分析其对全局与局部特征关系的影响。

Result: 寄存器令牌改善了注意力图，但全局信息主导局部特征；CLS令牌也有类似问题。

Conclusion: 需谨慎解释大型ViT的注意力图，明确寄存器令牌和CLS令牌的影响，以提升模型可解释性。

Abstract: Recent work has shown that the attention maps of the widely popular DINOv2
model exhibit artifacts, which hurt both model interpretability and performance
on dense image tasks. These artifacts emerge due to the model repurposing patch
tokens with redundant local information for the storage of global image
information. To address this problem, additional register tokens have been
incorporated in which the model can store such information instead. We
carefully examine the influence of these register tokens on the relationship
between global and local image features, showing that while register tokens
yield cleaner attention maps, these maps do not accurately reflect the
integration of local image information in large models. Instead, global
information is dominated by information extracted from register tokens, leading
to a disconnect between local and global features. Inspired by these findings,
we show that the CLS token itself, which can be interpreted as a register,
leads to a very similar phenomenon in models without explicit register tokens.
Our work shows that care must be taken when interpreting attention maps of
large ViTs. Further, by clearly attributing the faulty behaviour to register
and CLS tokens, we show a path towards more interpretable vision models.

</details>

### [164] [Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI](https://arxiv.org/abs/2505.05895)
*Benjamin Raphael Ernhofer,Daniil Prokhorov,Jannica Langner,Dominik Bollmann*

Main category: cs.CV

TLDR: 提出了一种基于视觉-语言框架的汽车信息娱乐系统交互方法，并发布了开源数据集和合成数据管道，通过微调模型在跨领域任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代汽车信息娱乐系统需要智能且自适应的解决方案来处理频繁的UI更新和多样化的设计变化。

Method: 采用Molmo-7B模型，结合LoRa微调和合成数据管道生成的推理，具备视觉定位和评估能力。

Result: 微调后的ELAM模型在AutomotiveUI-Bench-4K上表现优异，跨领域泛化能力提升5.2%，ScreenSpot任务平均准确率达80.4%。

Conclusion: 研究表明数据收集和微调可推动汽车UI理解的AI进展，方法成本低且适用于消费级GPU部署。

Abstract: Modern automotive infotainment systems require intelligent and adaptive
solutions to handle frequent User Interface (UI) updates and diverse design
variations. We introduce a vision-language framework for understanding and
interacting with automotive infotainment systems, enabling seamless adaptation
across different UI designs. To further support research in this field, we
release AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208
annotations. Additionally, we present a synthetic data pipeline to generate
training data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation
(LoRa) and incorporating reasoning generated by our pipeline, along with visual
grounding and evaluation capabilities. The fine-tuned Evaluative Large Action
Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and
dataset are available on Hugging Face) and demonstrating strong cross-domain
generalization, including a +5.2% improvement on ScreenSpot over the baseline
model. Notably, our approach achieves 80.4% average accuracy on ScreenSpot,
closely matching or even surpassing specialized models for desktop, mobile, and
web, such as ShowUI, despite being trained for the infotainment domain. This
research investigates how data collection and subsequent fine-tuning can lead
to AI-driven progress within automotive UI understanding and interaction. The
applied method is cost-efficient and fine-tuned models can be deployed on
consumer-grade GPUs.

</details>

### [165] [Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection](https://arxiv.org/abs/2505.05901)
*Hanzhe Liang,Aoran Wang,Jie Zhou,Xin Jin,Can Gao,Jinbao Wang*

Main category: cs.CV

TLDR: 论文提出了一种基于异常原因的3D异常检测框架MC4AD，通过生成内部和外部纠正力来改进检测效果，并结合多样异常生成模块和纠正力预测网络，实现了高效且轻量化的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅从结构角度识别异常，而本文从异常原因出发，通过模拟内部和外部纠正力来更全面地检测异常。

Method: 提出MC4AD框架，包括DA-Gen模块模拟异常，CFP-Net预测纠正力，并结合对称损失和整体损失优化模型。

Result: 在多个数据集上取得九项最优性能，同时模型参数最少且推理速度最快。

Conclusion: MC4AD框架在3D异常检测中表现出色，为工业质量控制提供了分层策略，并贡献了新的数据集。

Abstract: In this paper, we go beyond identifying anomalies only in structural terms
and think about better anomaly detection motivated by anomaly causes. Most
anomalies are regarded as the result of unpredictable defective forces from
internal and external sources, and their opposite forces are sought to correct
the anomalies. We introduced a Mechanics Complementary framework for 3D anomaly
detection (MC4AD) to generate internal and external Corrective forces for each
point. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to
simulate various anomalies. Then, we present a Corrective Force Prediction
Network (CFP-Net) with complementary representations for point-level
representation to simulate the different contributions of internal and external
corrective forces. A combined loss was proposed, including a new symmetric loss
and an overall loss, to constrain the corrective forces properly. As a
highlight, we consider 3D anomaly detection in industry more comprehensively,
creating a hierarchical quality control strategy based on a three-way decision
and contributing a dataset named Anomaly-IntraVariance with intraclass variance
to evaluate the model. On the proposed and existing five datasets, we obtained
nine state-of-the-art performers with the minimum parameters and the fastest
inference speed. The source is available at
https://github.com/hzzzzzhappy/MC4AD

</details>

### [166] [DFEN: Dual Feature Equalization Network for Medical Image Segmentation](https://arxiv.org/abs/2505.05913)
*Jianjian Yin,Yi Chen,Chengyu Li,Zhichao Zheng,Yanhui Gu,Junsheng Zhou*

Main category: cs.CV

TLDR: 提出了一种基于Swin Transformer和CNN的双重特征均衡网络，通过图像级和类别级特征均衡增强像素特征表示，提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法未考虑边界像素和低类别像素区域的上下文特征信息不均衡问题，导致误分类。

Method: 设计图像级和类别级特征均衡模块，结合Swin Transformer和CNN架构，增强像素特征表示。

Result: 在多个数据集（BUSI、ISIC2017、ACDC、PH²）上达到最优性能。

Conclusion: 双重特征均衡网络有效解决了特征信息不均衡问题，提升了分割精度。

Abstract: Current methods for medical image segmentation primarily focus on extracting
contextual feature information from the perspective of the whole image. While
these methods have shown effective performance, none of them take into account
the fact that pixels at the boundary and regions with a low number of class
pixels capture more contextual feature information from other classes, leading
to misclassification of pixels by unequal contextual feature information. In
this paper, we propose a dual feature equalization network based on the hybrid
architecture of Swin Transformer and Convolutional Neural Network, aiming to
augment the pixel feature representations by image-level equalization feature
information and class-level equalization feature information. Firstly, the
image-level feature equalization module is designed to equalize the contextual
information of pixels within the image. Secondly, we aggregate regions of the
same class to equalize the pixel feature representations of the corresponding
class by class-level feature equalization module. Finally, the pixel feature
representations are enhanced by learning weights for image-level equalization
feature information and class-level equalization feature information. In
addition, Swin Transformer is utilized as both the encoder and decoder, thereby
bolstering the ability of the model to capture long-range dependencies and
spatial correlations. We conducted extensive experiments on Breast Ultrasound
Images (BUSI), International Skin Imaging Collaboration (ISIC2017), Automated
Cardiac Diagnosis Challenge (ACDC) and PH$^2$ datasets. The experimental
results demonstrate that our method have achieved state-of-the-art performance.
Our code is publicly available at https://github.com/JianJianYin/DFEN.

</details>

### [167] [CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking](https://arxiv.org/abs/2505.05936)
*Weihong Li,Xiaoqiong Liu,Heng Fan,Libo Zhang*

Main category: cs.CV

TLDR: CGTrack是一种新型无人机跟踪器，通过结合显式和隐式技术扩展网络容量，解决了轻量级网络在无人机跟踪中容量不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决轻量级网络在无人机跟踪中因容量不足导致的性能下降问题，尤其是在遮挡和视角变化频繁的场景中。

Method: 提出分层特征级联（HFC）模块和轻量级门控中心头（LGCH），结合粗到细框架扩展网络容量。

Result: 在三个无人机跟踪基准测试中表现优异，达到最先进性能且运行速度快。

Conclusion: CGTrack通过高效的特征利用和门控机制，显著提升了无人机跟踪的性能和效率。

Abstract: Recent advancements in visual object tracking have markedly improved the
capabilities of unmanned aerial vehicle (UAV) tracking, which is a critical
component in real-world robotics applications. While the integration of
hierarchical lightweight networks has become a prevalent strategy for enhancing
efficiency in UAV tracking, it often results in a significant drop in network
capacity, which further exacerbates challenges in UAV scenarios, such as
frequent occlusions and extreme changes in viewing angles. To address these
issues, we introduce a novel family of UAV trackers, termed CGTrack, which
combines explicit and implicit techniques to expand network capacity within a
coarse-to-fine framework. Specifically, we first introduce a Hierarchical
Feature Cascade (HFC) module that leverages the spirit of feature reuse to
increase network capacity by integrating the deep semantic cues with the rich
spatial information, incurring minimal computational costs while enhancing
feature representation. Based on this, we design a novel Lightweight Gated
Center Head (LGCH) that utilizes gating mechanisms to decouple target-oriented
coordinates from previously expanded features, which contain dense local
discriminative information. Extensive experiments on three challenging UAV
tracking benchmarks demonstrate that CGTrack achieves state-of-the-art
performance while running fast. Code will be available at
https://github.com/Nightwatch-Fox11/CGTrack.

</details>

### [168] [Achieving 3D Attention via Triplet Squeeze and Excitation Block](https://arxiv.org/abs/2505.05943)
*Maan Alhazmi,Abdulrahman Altahhan*

Main category: cs.CV

TLDR: 论文提出了一种结合Triplet注意力与Squeeze-and-Excitation（TripSE）的新机制，并在ResNet18、DenseNet和ConvNeXt架构中验证其效果，显著提升了性能，尤其在ConvNeXt上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 重新确认CNN模型在视觉任务中的适用性，特别是在面部表情识别（FER）领域，通过引入新的注意力机制进一步提升性能。

Method: 提出四种TripSE变体，并将其集成到ResNet18、DenseNet和ConvNeXt架构中，评估其效果。

Result: TripSE显著提升了模型性能，ConvNeXt结合TripSE在FER2013数据集上达到78.27%的准确率，创下新纪录。

Conclusion: TripSE机制为CNN模型提供了有效的性能提升方案，尤其在ConvNeXt架构中表现突出，为FER任务提供了新的解决方案。

Abstract: The emergence of ConvNeXt and its variants has reaffirmed the conceptual and
structural suitability of CNN-based models for vision tasks, re-establishing
them as key players in image classification in general, and in facial
expression recognition (FER) in particular. In this paper, we propose a new set
of models that build on these advancements by incorporating a new set of
attention mechanisms that combines Triplet attention with
Squeeze-and-Excitation (TripSE) in four different variants. We demonstrate the
effectiveness of these variants by applying them to the ResNet18, DenseNet and
ConvNext architectures to validate their versatility and impact. Our study
shows that incorporating a TripSE block in these CNN models boosts their
performances, particularly for the ConvNeXt architecture, indicating its
utility. We evaluate the proposed mechanisms and associated models across four
datasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where
ConvNext with TripSE achieves state-of-the-art results with an accuracy of
\textbf{78.27\%} on the popular FER2013 dataset, a new feat for this dataset.

</details>

### [169] [Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition](https://arxiv.org/abs/2505.06002)
*Congqi Cao,Peiheng Han,Yueran zhang,Yating Yu,Qinyi Lv,Lingtong Min,Yanning zhang*

Main category: cs.CV

TLDR: 论文提出Task-Adapter++，一种参数高效的双重适应方法，用于解决预训练模型在少样本动作识别中的问题，包括泛化能力下降、任务信息不足、语义顺序忽略和跨模态对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在少样本动作识别中存在直接微调损害泛化能力、任务信息探索不足、语义顺序忽略和跨模态对齐忽略时间耦合等问题。

Method: 提出Task-Adapter++，包括任务特定图像编码器适配、LLM生成的子动作描述、语义顺序适配器和细粒度跨模态对齐策略。

Result: 在5个基准测试中取得最先进性能。

Conclusion: Task-Adapter++有效解决了现有问题，性能优越，代码已开源。

Abstract: Large-scale pre-trained models have achieved remarkable success in language
and image tasks, leading an increasing number of studies to explore the
application of pre-trained image models, such as CLIP, in the domain of
few-shot action recognition (FSAR). However, current methods generally suffer
from several problems: 1) Direct fine-tuning often undermines the
generalization capability of the pre-trained model; 2) The exploration of
task-specific information is insufficient in the visual tasks; 3) The semantic
order information is typically overlooked during text modeling; 4) Existing
cross-modal alignment techniques ignore the temporal coupling of multimodal
information. To address these, we propose Task-Adapter++, a parameter-efficient
dual adaptation method for both image and text encoders. Specifically, to make
full use of the variations across different few-shot learning tasks, we design
a task-specific adaptation for the image encoder so that the most
discriminative information can be well noticed during feature extraction.
Furthermore, we leverage large language models (LLMs) to generate detailed
sequential sub-action descriptions for each action class, and introduce
semantic order adapters into the text encoder to effectively model the
sequential relationships between these sub-actions. Finally, we develop an
innovative fine-grained cross-modal alignment strategy that actively maps
visual features to reside in the same temporal stage as semantic descriptions.
Extensive experiments fully demonstrate the effectiveness and superiority of
the proposed method, which achieves state-of-the-art performance on 5
benchmarks consistently. The code is open-sourced at
https://github.com/Jaulin-Bage/Task-Adapter-pp.

</details>

### [170] [From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection](https://arxiv.org/abs/2505.06003)
*Moritz Vandenhirtz,Julia E. Vogt*

Main category: cs.CV

TLDR: 提出一种通过实例级稀疏化输入图像实现可解释预测的方法，结合语义区域和动态稀疏度，生成更易理解的预测结果。


<details>
  <summary>Details</summary>
Motivation: 理解机器学习模型的决策过程对任务、数据和模型失败原因有重要价值。

Method: 在语义区域而非像素级学习掩码，并动态确定每个实例的稀疏度。

Result: 在半合成和自然图像数据集上，该方法生成的预测比现有基准更易理解。

Conclusion: 该方法通过语义区域和动态稀疏度实现了更直观的可解释预测。

Abstract: Understanding the decision-making process of machine learning models provides
valuable insights into the task, the data, and the reasons behind a model's
failures. In this work, we propose a method that performs inherently
interpretable predictions through the instance-wise sparsification of input
images. To align the sparsification with human perception, we learn the masking
in the space of semantically meaningful pixel regions rather than on
pixel-level. Additionally, we introduce an explicit way to dynamically
determine the required level of sparsity for each instance. We show empirically
on semi-synthetic and natural image datasets that our inherently interpretable
classifier produces more meaningful, human-understandable predictions than
state-of-the-art benchmarks.

</details>

### [171] [Document Image Rectification Bases on Self-Adaptive Multitask Fusion](https://arxiv.org/abs/2505.06038)
*Heng Li,Xiangping Wu,Qingcai Chen*

Main category: cs.CV

TLDR: 提出了一种自适应的多任务融合网络SalmRec，用于文档图像矫正，通过任务间特征聚合和门控机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了多任务间的互补性和交互作用，影响了文档矫正效果。

Method: 设计了自适应的多任务融合网络SalmRec，包含任务间特征聚合模块和门控机制。

Result: 在三个基准数据集上显著提升了矫正性能，消融实验验证了模块的有效性。

Conclusion: SalmRec通过任务间特征互补和门控机制，有效提升了文档图像矫正的精度。

Abstract: Deformed document image rectification is essential for real-world document
understanding tasks, such as layout analysis and text recognition. However,
current multi-task methods -- such as background removal, 3D coordinate
prediction, and text line segmentation -- often overlook the complementary
features between tasks and their interactions. To address this gap, we propose
a self-adaptive learnable multi-task fusion rectification network named
SalmRec. This network incorporates an inter-task feature aggregation module
that adaptively improves the perception of geometric distortions, enhances
feature complementarity, and reduces negative interference. We also introduce a
gating mechanism to balance features both within global tasks and between local
tasks effectively. Experimental results on two English benchmarks (DIR300 and
DocUNet) and one Chinese benchmark (DocReal) demonstrate that our method
significantly improves rectification performance. Ablation studies further
highlight the positive impact of different tasks on dewarping and the
effectiveness of our proposed module.

</details>

### [172] [Towards Better Cephalometric Landmark Detection with Diffusion Data Generation](https://arxiv.org/abs/2505.06055)
*Dongqian Guo,Wencheng Han,Pang Lyu,Yuxi Zhou,Jianbing Shen*

Main category: cs.CV

TLDR: 提出了一种无需人工干预的生成多样化头影测量X射线图像及其标注的创新方法，通过扩散模型生成图像并结合医学文本提示，显著提升了深度学习模型的检测性能。


<details>
  <summary>Details</summary>
Motivation: 头影测量标志点检测对正畸诊断和治疗规划至关重要，但数据稀缺和人工标注成本高限制了深度学习方法的有效性。

Method: 基于解剖先验构建标注，利用扩散模型生成真实X射线图像，并结合医学文本提示控制生成样本属性。

Result: 实验表明，使用生成数据训练模型将成功检测率（SDR）提升6.5%，达到82.2%。

Conclusion: 该方法通过生成多样化数据显著提升了头影测量标志点检测的准确性，为相关领域提供了有效解决方案。

Abstract: Cephalometric landmark detection is essential for orthodontic diagnostics and
treatment planning. Nevertheless, the scarcity of samples in data collection
and the extensive effort required for manual annotation have significantly
impeded the availability of diverse datasets. This limitation has restricted
the effectiveness of deep learning-based detection methods, particularly those
based on large-scale vision models. To address these challenges, we have
developed an innovative data generation method capable of producing diverse
cephalometric X-ray images along with corresponding annotations without human
intervention. To achieve this, our approach initiates by constructing new
cephalometric landmark annotations using anatomical priors. Then, we employ a
diffusion-based generator to create realistic X-ray images that correspond
closely with these annotations. To achieve precise control in producing samples
with different attributes, we introduce a novel prompt cephalometric X-ray
image dataset. This dataset includes real cephalometric X-ray images and
detailed medical text prompts describing the images. By leveraging these
detailed prompts, our method improves the generation process to control
different styles and attributes. Facilitated by the large, diverse generated
data, we introduce large-scale vision detection models into the cephalometric
landmark detection task to improve accuracy. Experimental results demonstrate
that training with the generated data substantially enhances the performance.
Compared to methods without using the generated data, our approach improves the
Success Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and
data are available at: https://um-lab.github.io/cepha-generation

</details>

### [173] [Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation](https://arxiv.org/abs/2505.06068)
*Kunpeng Qiu,Zhiqiang Gao,Zhiying Zhou,Mingjie Sun,Yongxin Guo*

Main category: cs.CV

TLDR: Siamese-Diffusion模型通过双组件结构和噪声一致性损失提升医学图像分割的形态保真度，显著提高了分割性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分割中受限于标注数据稀缺，传统方法生成的图像保真度低，影响分割模型的鲁棒性。

Method: 提出Siamese-Diffusion模型，包含Mask-Diffusion和Image-Diffusion，通过噪声一致性损失提升形态保真度。

Result: 在Polyps和ISIC2018数据集上，Siamese-Diffusion显著提升了SANet和UNet的分割性能。

Conclusion: Siamese-Diffusion通过双组件结构和噪声一致性损失有效解决了数据稀缺和形态保真度问题，提升了分割模型的性能。

Abstract: Deep learning has revolutionized medical image segmentation, yet its full
potential remains constrained by the paucity of annotated datasets. While
diffusion models have emerged as a promising approach for generating synthetic
image-mask pairs to augment these datasets, they paradoxically suffer from the
same data scarcity challenges they aim to mitigate. Traditional mask-only
models frequently yield low-fidelity images due to their inability to
adequately capture morphological intricacies, which can critically compromise
the robustness and reliability of segmentation models. To alleviate this
limitation, we introduce Siamese-Diffusion, a novel dual-component model
comprising Mask-Diffusion and Image-Diffusion. During training, a Noise
Consistency Loss is introduced between these components to enhance the
morphological fidelity of Mask-Diffusion in the parameter space. During
sampling, only Mask-Diffusion is used, ensuring diversity and scalability.
Comprehensive experiments demonstrate the superiority of our method.
Siamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps,
while UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at
GitHub.

</details>

### [174] [Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles](https://arxiv.org/abs/2505.06113)
*Anupkumar Bochare*

Main category: cs.CV

TLDR: 提出了一种仅使用摄像头的感知框架，通过结合YOLOv11目标检测和DepthAnythingV2深度估计，生成鸟瞰图（BEV），在OpenLane-V2和NuScenes数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶感知系统依赖昂贵的LiDAR传感器，本文旨在通过摄像头实现低成本且高精度的环境感知。

Method: 扩展Lift-Splat-Shoot架构，结合YOLOv11目标检测和DepthAnythingV2单目深度估计，处理多摄像头输入。

Result: 在OpenLane-V2和NuScenes数据集上，道路分割准确率达85%，车辆检测率为85-90%，平均位置误差1.2米。

Conclusion: 深度学习可从摄像头输入中提取丰富空间信息，实现低成本且高精度的自动驾驶导航。

Abstract: Autonomous vehicle perception systems have traditionally relied on costly
LiDAR sensors to generate precise environmental representations. In this paper,
we propose a camera-only perception framework that produces Bird's Eye View
(BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines
YOLOv11-based object detection with DepthAnythingV2 monocular depth estimation
across multi-camera inputs to achieve comprehensive 360-degree scene
understanding. We evaluate our approach on the OpenLane-V2 and NuScenes
datasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle
detection rates when compared against LiDAR ground truth, with average
positional errors limited to 1.2 meters. These results highlight the potential
of deep learning to extract rich spatial information using only camera inputs,
enabling cost-efficient autonomous navigation without sacrificing accuracy.

</details>

### [175] [Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation](https://arxiv.org/abs/2505.06117)
*Dongying Li,Binyi Su,Hua Zhang,Yong Li,Haiyong Chen*

Main category: cs.CV

TLDR: 提出了一种基于稳定扩散（SD）的光伏缺陷图像生成器PDIG，通过语义概念嵌入和轻量级工业风格适配器提升生成质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 光伏缺陷检测数据稀缺，现有生成模型存在不稳定、多样性不足和领域偏移问题。

Method: 结合语义概念嵌入（SCE）和轻量级工业风格适配器（LISA），并引入文本-图像双空间约束（TIDSC）提升生成质量。

Result: PDIG在FID指标上优于第二名19.16分，显著提升下游缺陷检测性能。

Conclusion: PDIG通过结合SD的强大先验和领域适配技术，有效解决了光伏缺陷数据稀缺问题，生成图像质量高且多样。

Abstract: Accurate defect detection of photovoltaic (PV) cells is critical for ensuring
quality and efficiency in intelligent PV manufacturing systems. However, the
scarcity of rich defect data poses substantial challenges for effective model
training. While existing methods have explored generative models to augment
datasets, they often suffer from instability, limited diversity, and domain
shifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image
Generator based on Stable Diffusion (SD). PDIG leverages the strong priors
learned from large-scale datasets to enhance generation quality under limited
data. Specifically, we introduce a Semantic Concept Embedding (SCE) module that
incorporates text-conditioned priors to capture the relational concepts between
defect types and their appearances. To further enrich the domain distribution,
we design a Lightweight Industrial Style Adaptor (LISA), which injects
industrial defect characteristics into the SD model through cross-disentangled
attention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)
module, enforcing the quality of generated images via positional consistency
and spatial smoothing alignment. Extensive experiments demonstrate that PDIG
achieves superior realism and diversity compared to state-of-the-art methods.
Specifically, our approach improves Frechet Inception Distance (FID) by 19.16
points over the second-best method and significantly enhances the performance
of downstream defect detection tasks.

</details>

### [176] [BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation](https://arxiv.org/abs/2505.06133)
*Hongming Wang,Yifeng Wu,Huimin Huang,Hongtao Wu,Jia-Xuan Jiang,Xiaodong Zhang,Hao Zheng,Xian Wu,Yefeng Zheng,Jinping Xu,Jing Cheng*

Main category: cs.CV

TLDR: 提出了一种名为BrainSegDMLF的全自动脑部病变分割模型，解决了现有方法在多模态数据利用、小病变检测和自动分割方面的不足。


<details>
  <summary>Details</summary>
Motivation: 脑部病变分割在医学图像分析中具有挑战性，现有方法在多模态数据利用、小病变检测和自动分割方面存在局限性。

Method: 开发了BrainSegDMLF模型，包含动态模态交互融合模块（DMIF）、逐层上采样解码器和自动分割掩码生成功能。

Result: 模型能够更全面地利用多模态数据，提高小病变检测能力，并实现全自动分割。

Conclusion: BrainSegDMLF模型在脑部病变分割任务中表现出色，解决了现有方法的不足。

Abstract: The segmentation of substantial brain lesions is a significant and
challenging task in the field of medical image segmentation. Substantial brain
lesions in brain imaging exhibit high heterogeneity, with indistinct boundaries
between lesion regions and normal brain tissue. Small lesions in single slices
are difficult to identify, making the accurate and reproducible segmentation of
abnormal regions, as well as their feature description, highly complex.
Existing methods have the following limitations: 1) They rely solely on
single-modal information for learning, neglecting the multi-modal information
commonly used in diagnosis. This hampers the ability to comprehensively acquire
brain lesion information from multiple perspectives and prevents the effective
integration and utilization of multi-modal data inputs, thereby limiting a
holistic understanding of lesions. 2) They are constrained by the amount of
data available, leading to low sensitivity to small lesions and difficulty in
detecting subtle pathological changes. 3) Current SAM-based models rely on
external prompts, which cannot achieve automatic segmentation and, to some
extent, affect diagnostic efficiency.To address these issues, we have developed
a large-scale fully automated segmentation model specifically designed for
brain lesion segmentation, named BrainSegDMLF. This model has the following
features: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and
integrates multi-modal data during the encoding process, providing the SAM
encoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling
Decoder, enabling the model to extract rich low-level and high-level features
even with limited data, thereby detecting the presence of small lesions. 3)
Automatic segmentation masks, allowing the model to generate lesion masks
automatically without requiring manual prompts.

</details>

### [177] [MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks](https://arxiv.org/abs/2505.06152)
*Wenqi Zeng,Yuqi Sun,Chenxi Ma,Weimin Tan,Bo Yan*

Main category: cs.CV

TLDR: 论文提出了MM-Skin数据集和SkinVL模型，用于提升皮肤病视觉语言模型的诊断能力。


<details>
  <summary>Details</summary>
Motivation: 当前皮肤病视觉语言模型缺乏专业文本描述支持，限制了其诊断分析的精确性。

Method: 构建了包含3种成像模态和10k高质量图像-文本对的MM-Skin数据集，并开发了皮肤病专用模型SkinVL。

Result: SkinVL在多个任务中表现优于通用和医学视觉语言模型。

Conclusion: MM-Skin和SkinVL为皮肤病临床辅助工具的发展提供了重要支持。

Abstract: Medical vision-language models (VLMs) have shown promise as clinical
assistants across various medical fields. However, specialized dermatology VLM
capable of delivering professional and detailed diagnostic analysis remains
underdeveloped, primarily due to less specialized text descriptions in current
dermatology multimodal datasets. To address this issue, we propose MM-Skin, the
first large-scale multimodal dermatology dataset that encompasses 3 imaging
modalities, including clinical, dermoscopic, and pathological and nearly 10k
high-quality image-text pairs collected from professional textbooks. In
addition, we generate over 27k diverse, instruction-following vision question
answering (VQA) samples (9 times the size of current largest dermatology VQA
dataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a
dermatology-specific VLM designed for precise and nuanced skin disease
interpretation. Comprehensive benchmark evaluations of SkinVL on VQA,
supervised fine-tuning (SFT) and zero-shot classification tasks across 8
datasets, reveal its exceptional performance for skin diseases in comparison to
both general and medical VLM models. The introduction of MM-Skin and SkinVL
offers a meaningful contribution to advancing the development of clinical
dermatology VLM assistants. MM-Skin is available at
https://github.com/ZwQ803/MM-Skin

</details>

### [178] [DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models](https://arxiv.org/abs/2505.06166)
*Radu Alexandru Rosu,Keyu Wu,Yao Feng,Youyi Zheng,Michael J. Black*

Main category: cs.CV

TLDR: 提出DiffLocks框架，从单张图像生成详细3D头发几何，克服现有方法在多样发型和缺乏配对数据上的限制。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成3D头发几何时因数据不足和低维表示导致的细节缺失和发型限制问题。

Method: 自动化创建大规模合成头发数据集（40K发型），并训练基于扩散-Transformer的模型，直接从图像生成3D发丝。

Result: DiffLocks首次实现从单张图像重建高度卷曲发型（如非洲发型），无需后处理。

Conclusion: DiffLocks通过合成数据和扩散模型，显著提升了单图像3D头发重建的多样性和细节表现。

Abstract: We address the task of generating 3D hair geometry from a single image, which
is challenging due to the diversity of hairstyles and the lack of paired
image-to-3D hair data. Previous methods are primarily trained on synthetic data
and cope with the limited amount of such data by using low-dimensional
intermediate representations, such as guide strands and scalp-level embeddings,
that require post-processing to decode, upsample, and add realism. These
approaches fail to reconstruct detailed hair, struggle with curly hair, or are
limited to handling only a few hairstyles. To overcome these limitations, we
propose DiffLocks, a novel framework that enables detailed reconstruction of a
wide variety of hairstyles directly from a single image. First, we address the
lack of 3D hair data by automating the creation of the largest synthetic hair
dataset to date, containing 40K hairstyles. Second, we leverage the synthetic
hair dataset to learn an image-conditioned diffusion-transfomer model that
generates accurate 3D strands from a single frontal image. By using a
pretrained image backbone, our method generalizes to in-the-wild images despite
being trained only on synthetic data. Our diffusion model predicts a scalp
texture map in which any point in the map contains the latent code for an
individual hair strand. These codes are directly decoded to 3D strands without
post-processing techniques. Representing individual strands, instead of guide
strands, enables the transformer to model the detailed spatial structure of
complex hairstyles. With this, DiffLocks can recover highly curled hair, like
afro hairstyles, from a single image for the first time. Data and code is
available at https://radualexandru.github.io/difflocks/

</details>

### [179] [Adapting a Segmentation Foundation Model for Medical Image Classification](https://arxiv.org/abs/2505.06217)
*Pengfei Gu,Haoteng Tang,Islam A. Ebeid,Jose A. Nunez,Fabian Vazquez,Diego Adame,Marcus Zhan,Huimin Li,Bin Fu,Danny Z. Chen*

Main category: cs.CV

TLDR: 本文提出了一种新框架，将SAM模型用于医学图像分类，通过冻结SAM编码器权重并结合空间局部通道注意力机制（SLCA），提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM在图像分割任务中表现出色，但其在医学图像分类中的应用尚未充分探索，本文旨在填补这一空白。

Method: 利用SAM编码器提取特征，结合SLCA机制计算空间局部注意力权重，增强分类模型对关键区域的关注。

Result: 在三个公开医学图像分类数据集上的实验表明，该方法有效且数据高效。

Conclusion: 提出的框架成功将SAM适应于医学图像分类，显著提升了性能。

Abstract: Recent advancements in foundation models, such as the Segment Anything Model
(SAM), have shown strong performance in various vision tasks, particularly
image segmentation, due to their impressive zero-shot segmentation
capabilities. However, effectively adapting such models for medical image
classification is still a less explored topic. In this paper, we introduce a
new framework to adapt SAM for medical image classification. First, we utilize
the SAM image encoder as a feature extractor to capture segmentation-based
features that convey important spatial and contextual details of the image,
while freezing its weights to avoid unnecessary overhead during training. Next,
we propose a novel Spatially Localized Channel Attention (SLCA) mechanism to
compute spatially localized attention weights for the feature maps. The
features extracted from SAM's image encoder are processed through SLCA to
compute attention weights, which are then integrated into deep learning
classification models to enhance their focus on spatially relevant or
meaningful regions of the image, thus improving classification performance.
Experimental results on three public medical image classification datasets
demonstrate the effectiveness and data-efficiency of our approach.

</details>

### [180] [VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction](https://arxiv.org/abs/2505.06219)
*Noah Frahm,Dongxu Zhao,Andrea Dunn Beltran,Ron Alterovitz,Jan-Michael Frahm,Junier Oliva,Roni Sengupta*

Main category: cs.CV

TLDR: 论文提出了一种名为VIN-NBV的新方法，通过预测视图对重建质量的直接改进来选择最佳视图，优于传统覆盖最大化方法。


<details>
  <summary>Details</summary>
Motivation: 现有NBV算法依赖场景先验知识或额外图像捕获，且覆盖最大化策略在复杂场景中无法直接提升重建质量。

Method: 提出View Introspection Network (VIN)，通过3D感知特征化和模仿学习预测视图改进分数，结合贪婪策略选择最佳视图。

Result: VIN-NBV在捕获次数或运动时间受限时，重建质量比基线方法提高约30%。

Conclusion: VIN-NBV通过直接预测重建质量改进，显著提升了复杂场景下的3D重建效率。

Abstract: Next Best View (NBV) algorithms aim to acquire an optimal set of images using
minimal resources, time, or number of captures to enable efficient 3D
reconstruction of a scene. Existing approaches often rely on prior scene
knowledge or additional image captures and often develop policies that maximize
coverage. Yet, for many real scenes with complex geometry and self-occlusions,
coverage maximization does not lead to better reconstruction quality directly.
In this paper, we propose the View Introspection Network (VIN), which is
trained to predict the reconstruction quality improvement of views directly,
and the VIN-NBV policy. A greedy sequential sampling-based policy, where at
each acquisition step, we sample multiple query views and choose the one with
the highest VIN predicted improvement score. We design the VIN to perform
3D-aware featurization of the reconstruction built from prior acquisitions, and
for each query view create a feature that can be decoded into an improvement
score. We then train the VIN using imitation learning to predict the
reconstruction improvement score. We show that VIN-NBV improves reconstruction
quality by ~30% over a coverage maximization baseline when operating with
constraints on the number of acquisitions or the time in motion.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [181] [CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations](https://arxiv.org/abs/2505.04999)
*Anthony Liang,Pavel Czempin,Matthew Hong,Yutai Zhou,Erdem Biyik,Stephen Tu*

Main category: cs.RO

TLDR: 论文提出了一种名为CLAM的方法，通过无监督学习从无标签的观察数据中学习连续潜在动作标签，解决了模仿学习中需要大量标记专家数据的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习需要大量标记的专家数据，成本高昂且限制了训练规模。利用无标签观察数据（如视频演示）学习潜在动作标签是一种有前景的解决方案。

Method: 设计了连续潜在动作模型（CLAM），采用连续潜在动作标签而非离散表示，并联合训练动作解码器，确保潜在动作空间可以轻松映射到真实动作。

Result: 在DMControl（运动）和MetaWorld（操作）基准测试以及真实WidowX机器人上，CLAM显著优于现有方法，任务成功率提高了2-3倍。

Conclusion: CLAM能够在不依赖标记专家数据的情况下学习高性能策略，为复杂连续控制任务提供了一种有效的解决方案。

Abstract: Learning robot policies using imitation learning requires collecting large
amounts of costly action-labeled expert demonstrations, which fundamentally
limits the scale of training data. A promising approach to address this
bottleneck is to harness the abundance of unlabeled observations-e.g., from
video demonstrations-to learn latent action labels in an unsupervised way.
However, we find that existing methods struggle when applied to complex robot
tasks requiring fine-grained motions. We design continuous latent action models
(CLAM) which incorporate two key ingredients we find necessary for learning to
solve complex continuous control tasks from unlabeled observation data: (a)
using continuous latent action labels instead of discrete representations, and
(b) jointly training an action decoder to ensure that the latent action space
can be easily grounded to real actions with relatively few labeled examples.
Importantly, the labeled examples can be collected from non-optimal play data,
enabling CLAM to learn performant policies without access to any action-labeled
expert data. We demonstrate on continuous control benchmarks in DMControl
(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot
arm that CLAM significantly outperforms prior state-of-the-art methods,
remarkably with a 2-3x improvement in task success rate compared to the best
baseline. Videos and code can be found at clamrobot.github.io.

</details>

### [182] [Adaptive Stress Testing Black-Box LLM Planners](https://arxiv.org/abs/2505.05665)
*Neeloy Chakraborty,John Pohovey,Melkior Ornik,Katherine Driggs-Campbell*

Main category: cs.RO

TLDR: 论文提出了一种利用自适应压力测试（AST）和蒙特卡洛树搜索（MCTS）检测大语言模型（LLMs）幻觉的方法，以提高安全关键场景中的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs在决策任务中表现出色，但其幻觉行为可能带来风险，尤其是在安全关键场景中，因此需要有效检测这些失败。

Method: 通过手动案例研究验证不同扰动形式对LLMs的影响，并提出基于AST和MCTS的扰动空间搜索方法，生成高不确定性场景和提示。

Result: 实验表明，离线分析可生成影响模型不确定性的提示，并为实时信任评估提供支持。

Conclusion: 该方法能有效检测LLMs的幻觉行为，提升其在安全关键任务中的可靠性。

Abstract: Large language models (LLMs) have recently demonstrated success in
generalizing across decision-making tasks including planning, control and
prediction, but their tendency to hallucinate unsafe and undesired outputs
poses risks. We argue that detecting such failures is necessary, especially in
safety-critical scenarios. Existing black-box methods often detect
hallucinations by identifying inconsistencies across multiple samples. Many of
these approaches typically introduce prompt perturbations like randomizing
detail order or generating adversarial inputs, with the intuition that a
confident model should produce stable outputs. We first perform a manual case
study showing that other forms of perturbations (e.g., adding noise, removing
sensor details) cause LLMs to hallucinate in a driving environment. We then
propose a novel method for efficiently searching the space of prompt
perturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search
(MCTS). Our AST formulation enables discovery of scenarios and prompts that
cause language models to act with high uncertainty. By generating MCTS prompt
perturbation trees across diverse scenarios, we show that offline analyses can
be used at runtime to automatically generate prompts that influence model
uncertainty, and to inform real-time trust assessments of an LLM.

</details>

### [183] [Flight Validation of Learning-Based Trajectory Optimization for the Astrobee Free-Flyer](https://arxiv.org/abs/2505.05588)
*Somrita Banerjee,Abhishek Cauligi,Marco Pavone*

Main category: cs.RO

TLDR: 论文展示了在ISS上使用机器学习加速轨迹优化的飞行实验结果，首次实现了基于学习的控制。


<details>
  <summary>Details</summary>
Motivation: 轨迹优化在太空应用中因高计算需求受限，需探索加速方法。

Method: 结合GuSTO框架和离线训练的神经网络，生成初始轨迹以加速优化。

Result: 成功在ISS上实现基于学习的控制，加速了实时优化。

Conclusion: 该方法为资源受限的太空平台提供了更快的实时优化路径。

Abstract: Although widely used in commercial and industrial robotics, trajectory
optimization has seen limited use in space applications due to its high
computational demands. In this work, we present flight results from experiments
with the Astrobee free-flying robot on board the International Space Station
(ISS), that demonstrate how machine learning can accelerate on-board trajectory
optimization while preserving theoretical solver guarantees. To the best of the
authors' knowledge, this is the first-ever demonstration of learning-based
control on the ISS. Our approach leverages the GuSTO sequential convex
programming framework and uses a neural network, trained offline, to map
problem parameters to effective initial ``warm-start'' trajectories, paving the
way for faster real-time optimization on resource-constrained space platforms.

</details>

### [184] [CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory](https://arxiv.org/abs/2505.05622)
*Weichen Zhang,Chen Gao,Shiquan Yu,Ruiying Peng,Baining Zhao,Qian Zhang,Jinqiang Cui,Xinlei Chen,Yong Li*

Main category: cs.RO

TLDR: 论文提出了一种基于大型语言模型（LLM）的无人机导航代理CityNavAgent，通过分层语义规划模块（HSPM）和全局记忆模块，显著降低了城市空中视觉与语言导航（VLN）的复杂性，并在实验中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的地面VLN代理在室内外环境中表现良好，但在空中VLN中因缺乏预定义导航图和长距离探索中动作空间指数级扩展而表现不佳。

Method: 设计了分层语义规划模块（HSPM）将长距离任务分解为不同语义层次的子目标，并开发了全局记忆模块存储历史轨迹以简化导航。

Result: 实验表明，CityNavAgent在连续城市环境中实现了最先进的性能，各模块的有效性也得到了验证。

Conclusion: CityNavAgent通过LLM赋能和模块化设计，显著提升了空中VLN的性能，为复杂环境中的无人机导航提供了有效解决方案。

Abstract: Aerial vision-and-language navigation (VLN), requiring drones to interpret
natural language instructions and navigate complex urban environments, emerges
as a critical embodied AI challenge that bridges human-robot interaction, 3D
spatial reasoning, and real-world deployment. Although existing ground VLN
agents achieved notable results in indoor and outdoor settings, they struggle
in aerial VLN due to the absence of predefined navigation graphs and the
exponentially expanding action space in long-horizon exploration. In this work,
we propose \textbf{CityNavAgent}, a large language model (LLM)-empowered agent
that significantly reduces the navigation complexity for urban aerial VLN.
Specifically, we design a hierarchical semantic planning module (HSPM) that
decomposes the long-horizon task into sub-goals with different semantic levels.
The agent reaches the target progressively by achieving sub-goals with
different capacities of the LLM. Additionally, a global memory module storing
historical trajectories into a topological graph is developed to simplify
navigation for visited targets. Extensive benchmark experiments show that our
method achieves state-of-the-art performance with significant improvement.
Further experiments demonstrate the effectiveness of different modules of
CityNavAgent for aerial VLN in continuous city environments. The code is
available at \href{https://github.com/VinceOuti/CityNavAgent}{link}.

</details>

### [185] [Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks](https://arxiv.org/abs/2505.05638)
*Mohamed-Khalil Bouzidi,Christian Schlauch,Nicole Scheuerer,Yue Yao,Nadja Klein,Daniel Göhring,Jörg Reichardt*

Main category: cs.RO

TLDR: 研究发现，运动预测模型的开放环精度提升并不总能转化为闭环驾驶性能的改善，模型参数减少86%时仍可能表现更优。


<details>
  <summary>Details</summary>
Motivation: 评估运动预测模型在自动驾驶系统中的实际表现，探索预测精度与闭环驾驶性能的关系。

Method: 系统评估了最先进的运动预测模型与运动规划器的交互，并研究了模型参数减少的影响。

Result: 开放环精度与闭环驾驶性能无直接关联，模型参数减少86%时仍可能表现更优。

Conclusion: 预测模型的闭环性能不仅依赖精度，还需考虑时间一致性与规划器兼容性，轻量化模型可能更优。

Abstract: Fueled by motion prediction competitions and benchmarks, recent years have
seen the emergence of increasingly large learning based prediction models, many
with millions of parameters, focused on improving open-loop prediction accuracy
by mere centimeters. However, these benchmarks fail to assess whether such
improvements translate to better performance when integrated into an autonomous
driving stack. In this work, we systematically evaluate the interplay between
state-of-the-art motion predictors and motion planners. Our results show that
higher open-loop accuracy does not always correlate with better closed-loop
driving behavior and that other factors, such as temporal consistency of
predictions and planner compatibility, also play a critical role. Furthermore,
we investigate downsized variants of these models, and, surprisingly, find that
in some cases models with up to 86% fewer parameters yield comparable or even
superior closed-loop driving performance. Our code is available at
https://github.com/continental/pred2plan.

</details>

### [186] [Towards Embodiment Scaling Laws in Robot Locomotion](https://arxiv.org/abs/2505.05753)
*Bo Ai,Liu Dai,Nico Bohlinger,Dichen Li,Tongzhou Mu,Zhanxin Wu,K. Fay,Henrik I. Christensen,Jan Peters,Hao Su*

Main category: cs.RO

TLDR: 研究通过增加训练实体数量提升泛化能力，验证了实体缩放定律，并在真实世界中实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 开发能在多样化任务、环境和实体中运作的通用智能体是机器人和人工智能领域的重大挑战。

Method: 通过程序生成约1000种不同实体的数据集，训练通用策略以处理多样化的观察和动作空间。

Result: 增加训练实体数量能提升泛化能力，且实体缩放比数据缩放更有效。最佳策略能在真实世界中零样本迁移到新实体。

Conclusion: 研究为通用具身智能迈出一步，对可配置机器人的自适应控制和形态与控制的协同设计具有潜在意义。

Abstract: Developing generalist agents that can operate across diverse tasks,
environments, and physical embodiments is a grand challenge in robotics and
artificial intelligence. In this work, we focus on the axis of embodiment and
investigate embodiment scaling laws$\unicode{x2013}$the hypothesis that
increasing the number of training embodiments improves generalization to unseen
ones. Using robot locomotion as a test bed, we procedurally generate a dataset
of $\sim$1,000 varied embodiments, spanning humanoids, quadrupeds, and
hexapods, and train generalist policies capable of handling diverse observation
and action spaces on random subsets. We find that increasing the number of
training embodiments improves generalization to unseen ones, and scaling
embodiments is more effective in enabling embodiment-level generalization than
scaling data on small, fixed sets of embodiments. Notably, our best policy,
trained on the full dataset, zero-shot transfers to novel embodiments in the
real world, such as Unitree Go2 and H1. These results represent a step toward
general embodied intelligence, with potential relevance to adaptive control for
configurable robots, co-design of morphology and control, and beyond.

</details>

### [187] [Multi-Agent Systems for Robotic Autonomy with LLMs](https://arxiv.org/abs/2505.05762)
*Junhong Chen,Ziqi Yang,Haoyuan G Xu,Dandan Zhang,George Mylonas*

Main category: cs.RO

TLDR: 提出了一种基于大语言模型（LLMs）的多智能体框架，用于机器人任务分析、机械设计和路径生成，并通过实验验证了其可行性和潜力。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs提升机器人系统开发的效率和可访问性，尤其是在任务分析、设计和控制策略生成方面。

Method: 构建了一个包含任务分析师、机器人设计师和强化学习设计师的多智能体框架，输出多模态结果（如代码文件或技术报告）。

Result: 实验表明，系统能够根据任务输入设计可行的机器人及控制策略，展示了其在研究和工业应用中的潜力。

Conclusion: 该框架为机器人系统开发提供了高效且易用的解决方案，具有广泛的应用前景。

Abstract: Since the advent of Large Language Models (LLMs), various research based on
such models have maintained significant academic attention and impact,
especially in AI and robotics. In this paper, we propose a multi-agent
framework with LLMs to construct an integrated system for robotic task
analysis, mechanical design, and path generation. The framework includes three
core agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.
Outputs are formatted as multimodal results, such as code files or technical
reports, for stronger understandability and usability. To evaluate
generalizability comparatively, we conducted experiments with models from both
GPT and DeepSeek. Results demonstrate that the proposed system can design
feasible robots with control strategies when appropriate task inputs are
provided, exhibiting substantial potential for enhancing the efficiency and
accessibility of robotic system development in research and industrial
applications.

</details>

### [188] [Learning to Drive Anywhere with Model-Based Reannotation11](https://arxiv.org/abs/2505.05592)
*Noriaki Hirose,Lydia Ignatova,Kyle Stachowicz,Catherine Glossop,Sergey Levine,Dhruv Shah*

Main category: cs.RO

TLDR: 论文提出MBRA框架，利用模型重新标注被动收集的数据，训练出长距离导航策略LogoNav，在未见环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决机器人视觉导航策略泛化性不足的问题，因高质量训练数据规模有限。

Method: 利用被动收集的数据（如众包遥操作数据和YouTube视频），通过MBRA框架重新标注或生成高质量动作，训练LogoNav策略。

Result: LogoNav在未见室内外环境中导航超过300米，表现优于现有方法，并在多城市、多机器人测试中验证其泛化性。

Conclusion: MBRA和LogoNav显著提升了视觉导航策略的泛化性和实用性，适用于复杂现实场景。

Abstract: Developing broadly generalizable visual navigation policies for robots is a
significant challenge, primarily constrained by the availability of
large-scale, diverse training data. While curated datasets collected by
researchers offer high quality, their limited size restricts policy
generalization. To overcome this, we explore leveraging abundant, passively
collected data sources, including large volumes of crowd-sourced teleoperation
data and unlabeled YouTube videos, despite their potential for lower quality or
missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework
that utilizes a learned short-horizon, model-based expert model to relabel or
generate high-quality actions for these passive datasets. This relabeled data
is then distilled into LogoNav, a long-horizon navigation policy conditioned on
visual goals or GPS waypoints. We demonstrate that LogoNav, trained using
MBRA-processed data, achieves state-of-the-art performance, enabling robust
navigation over distances exceeding 300 meters in previously unseen indoor and
outdoor environments. Our extensive real-world evaluations, conducted across a
fleet of robots (including quadrupeds) in six cities on three continents,
validate the policy's ability to generalize and navigate effectively even
amidst pedestrians in crowded settings.

</details>

### [189] [UniVLA: Learning to Act Anywhere with Task-centric Latent Actions](https://arxiv.org/abs/2505.06111)
*Qingwen Bu,Yanting Yang,Jisong Cai,Shenyuan Gao,Guanghui Ren,Maoqing Yao,Ping Luo,Hongyang Li*

Main category: cs.RO

TLDR: UniVLA是一种新的跨具身视觉-语言-动作（VLA）策略学习框架，通过任务中心化动作表示和潜在动作模型，利用多源数据提升机器人策略的通用性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量动作标注数据，局限于单一物理规格，难以在不同具身和环境间迁移知识。UniVLA旨在解决这些问题。

Method: 从视频中提取任务中心化动作表示，结合语言指令和DINO特征空间建立潜在动作模型，利用互联网规模视频数据训练通用策略。

Result: 在多个操作和导航基准测试及实际机器人部署中取得最优性能，计算和数据需求显著低于OpenVLA。

Conclusion: UniVLA展示了高效、可扩展的机器人策略学习潜力，支持异构数据（包括人类视频）的持续性能提升。

Abstract: A generalist robot should perform effectively across various environments.
However, most existing approaches heavily rely on scaling action-annotated data
to enhance their capabilities. Consequently, they are often limited to single
physical specification and struggle to learn transferable knowledge across
different embodiments and environments. To confront these limitations, we
propose UniVLA, a new framework for learning cross-embodiment
vision-language-action (VLA) policies. Our key innovation is to derive
task-centric action representations from videos with a latent action model.
This enables us to exploit extensive data across a wide spectrum of embodiments
and perspectives. To mitigate the effect of task-irrelevant dynamics, we
incorporate language instructions and establish a latent action model within
the DINO feature space. Learned from internet-scale videos, the generalist
policy can be deployed to various robots through efficient latent action
decoding. We obtain state-of-the-art results across multiple manipulation and
navigation benchmarks, as well as real-robot deployments. UniVLA achieves
superior performance over OpenVLA with less than 1/20 of pretraining compute
and 1/10 of downstream data. Continuous performance improvements are observed
as heterogeneous data, even including human videos, are incorporated into the
training pipeline. The results underscore UniVLA's potential to facilitate
scalable and efficient robot policy learning.

</details>

### [190] [Efficient Sensorimotor Learning for Open-world Robot Manipulation](https://arxiv.org/abs/2505.06136)
*Yifeng Zhu*

Main category: cs.RO

TLDR: 论文提出了一种基于高效感知运动学习的方法，解决开放世界机器人操作问题，利用有限演示数据中的规律性实现数据高效学习。


<details>
  <summary>Details</summary>
Motivation: 解决机器人面对新物体、场景或任务时的快速适应问题，避免预编程或预训练的高成本。

Method: 通过利用演示数据中的规律性，提出三种方法：赋予机器人对象中心先验、空间理解能力和技能复用能力。

Result: 实现了从少量演示数据中学习通用操作策略，并能从视频观察中模仿技能和持续学习多任务。

Conclusion: 为低成本数据收集和快速适应的通用机器人助手奠定了基础，推动了智能机器人助手在日常场景中的应用。

Abstract: This dissertation considers Open-world Robot Manipulation, a manipulation
problem where a robot must generalize or quickly adapt to new objects, scenes,
or tasks for which it has not been pre-programmed or pre-trained. This
dissertation tackles the problem using a methodology of efficient sensorimotor
learning. The key to enabling efficient sensorimotor learning lies in
leveraging regular patterns that exist in limited amounts of demonstration
data. These patterns, referred to as ``regularity,'' enable the data-efficient
learning of generalizable manipulation skills. This dissertation offers a new
perspective on formulating manipulation problems through the lens of
regularity. Building upon this notion, we introduce three major contributions.
First, we introduce methods that endow robots with object-centric priors,
allowing them to learn generalizable, closed-loop sensorimotor policies from a
small number of teleoperation demonstrations. Second, we introduce methods that
constitute robots' spatial understanding, unlocking their ability to imitate
manipulation skills from in-the-wild video observations. Last but not least, we
introduce methods that enable robots to identify reusable skills from their
past experiences, resulting in systems that can continually imitate multiple
tasks in a sequential manner. Altogether, the contributions of this
dissertation help lay the groundwork for building general-purpose personal
robots that can quickly adapt to new situations or tasks with low-cost data
collection and interact easily with humans. By enabling robots to learn and
generalize from limited data, this dissertation takes a step toward realizing
the vision of intelligent robotic assistants that can be seamlessly integrated
into everyday scenarios.

</details>

### [191] [3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks](https://arxiv.org/abs/2505.05800)
*Vineet Bhat,Yu-Hsiang Lan,Prashanth Krishnamurthy,Ramesh Karri,Farshad Khorrami*

Main category: cs.RO

TLDR: 论文提出3D-CAVLA模型，通过整合深度感知和任务导向的感兴趣区域检测，提升视觉-语言-动作模型在3D场景中的上下文感知能力，显著提高了任务成功率和零样本适应能力。


<details>
  <summary>Details</summary>
Motivation: 机器人需要在3D空间中学习关节空间轨迹以实现物体操作，但现有视觉-语言模型在场景上下文感知方面存在不足。

Method: 提出3D-CAVLA模型，整合链式思维推理、深度感知和任务导向的感兴趣区域检测，增强模型对3D场景的理解。

Result: 在LIBERO仿真环境中，模型平均任务成功率达98.1%，零样本任务中绝对提升8.8%。

Conclusion: 3D-CAVLA通过增强场景感知能力，显著提升了机器人操作的性能和适应性，并开源代码和数据集以推动社区研究。

Abstract: Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint
space trajectory of a robot manipulator. Robots must possess semantic and
visual perception abilities to transform real-world mappings of their workspace
into the low-level control necessary for object manipulation. Recent work has
demonstrated the capabilities of fine-tuning large Vision-Language Models
(VLMs) to learn the mapping between RGB images, language instructions, and
joint space control. These models typically take as input RGB images of the
workspace and language instructions, and are trained on large datasets of
teleoperated robot demonstrations. In this work, we explore methods to improve
the scene context awareness of a popular recent Vision-Language-Action model by
integrating chain-of-thought reasoning, depth perception, and task-oriented
region of interest detection. Our experiments in the LIBERO simulation
environment show that our proposed model, 3D-CAVLA, improves the success rate
across various LIBERO task suites, achieving an average success rate of
98.1$\%$. We also evaluate the zero-shot capabilities of our method,
demonstrating that 3D scene awareness leads to robust learning and adaptation
for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of
8.8$\%$ on unseen tasks. We will open-source our code and the unseen tasks
dataset to promote community-driven research here: https://3d-cavla.github.io

</details>

### [192] [Let Humanoids Hike! Integrative Skill Development on Complex Trails](https://arxiv.org/abs/2505.06218)
*Kwan-Yee Lin,Stella X. Yu*

Main category: cs.RO

TLDR: LEGO-H是一个学习框架，用于训练人形机器人在复杂小径上徒步，结合视觉感知、决策和运动执行，通过技术创新实现自主徒步。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人研究在徒步任务上存在碎片化和不足，缺乏长期目标和情境感知，需要整合技能以应对复杂地形。

Method: 提出LEGO-H框架，包括1）时间视觉变换器变体，结合分层强化学习；2）潜在关节运动模式表示与分层度量学习，增强特权学习方案。

Result: 实验表明LEGO-H能处理多样化的物理和环境挑战，无需依赖预定义运动模式，展现多功能性和鲁棒性。

Conclusion: 徒步任务成为体现自主性的重要测试平台，LEGO-H为未来人形机器人发展提供了基准。

Abstract: Hiking on complex trails demands balance, agility, and adaptive
decision-making over unpredictable terrain. Current humanoid research remains
fragmented and inadequate for hiking: locomotion focuses on motor skills
without long-term goals or situational awareness, while semantic navigation
overlooks real-world embodiment and local terrain variability. We propose
training humanoids to hike on complex trails, driving integrative skill
development across visual perception, decision making, and motor execution. We
develop a learning framework, LEGO-H, that enables a vision-equipped humanoid
robot to hike complex trails autonomously. We introduce two technical
innovations: 1) A temporal vision transformer variant - tailored into
Hierarchical Reinforcement Learning framework - anticipates future local goals
to guide movement, seamlessly integrating locomotion with goal-directed
navigation. 2) Latent representations of joint movement patterns, combined with
hierarchical metric learning - enhance Privileged Learning scheme - enable
smooth policy transfer from privileged training to onboard execution. These
components allow LEGO-H to handle diverse physical and environmental challenges
without relying on predefined motion patterns. Experiments across varied
simulated trails and robot morphologies highlight LEGO-H's versatility and
robustness, positioning hiking as a compelling testbed for embodied autonomy
and LEGO-H as a baseline for future humanoid development.

</details>

### [193] [TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations](https://arxiv.org/abs/2505.06079)
*Shuaiyi Huang,Mara Levy,Anubhav Gupta,Daniel Ekpo,Ruijie Zheng,Abhinav Shrivastava*

Main category: cs.RO

TLDR: TREND框架通过结合少量专家演示和三重教学策略，有效减少偏好反馈中的噪声，提升强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决偏好反馈中的噪声问题，提高偏好强化学习的准确性。

Method: 使用三重教学策略训练三个奖励模型，互相传递小损失偏好对以更新参数。

Result: 在机器人操作任务中，即使噪声高达40%，成功率仍可达90%。

Conclusion: TREND框架在噪声环境下表现出高效鲁棒性，仅需少量专家演示即可实现高性能。

Abstract: Preference feedback collected by human or VLM annotators is often noisy,
presenting a significant challenge for preference-based reinforcement learning
that relies on accurate preference labels. To address this challenge, we
propose TREND, a novel framework that integrates few-shot expert demonstrations
with a tri-teaching strategy for effective noise mitigation. Our method trains
three reward models simultaneously, where each model views its small-loss
preference pairs as useful knowledge and teaches such useful pairs to its peer
network for updating the parameters. Remarkably, our approach requires as few
as one to three expert demonstrations to achieve high performance. We evaluate
TREND on various robotic manipulation tasks, achieving up to 90% success rates
even with noise levels as high as 40%, highlighting its effective robustness in
handling noisy preference feedback. Project page:
https://shuaiyihuang.github.io/publications/TREND.

</details>

### [194] [Physics-informed Temporal Difference Metric Learning for Robot Motion Planning](https://arxiv.org/abs/2505.05691)
*Ruiqi Ni,Zherong Pan,Ahmed H Qureshi*

Main category: cs.RO

TLDR: 本文提出了一种新型的自监督时间差分度量学习方法，用于更准确地解决Eikonal方程，并在复杂和未见过的运动规划任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法在复杂环境中表现不佳，无法维持Eikonal方程的关键性质（如最优值函数和测地距离）。

Method: 提出了一种结合时间差分学习和度量学习的方法，通过有限区域内的Bellman最优性原则避免局部极小值，并保留Eikonal方程的测地性质。

Result: 该方法在复杂环境和未见环境中显著优于现有自监督学习方法，适用于2到12自由度的机器人配置。

Conclusion: 新方法通过更准确地解决Eikonal方程，提升了运动规划任务的性能和泛化能力。

Abstract: The motion planning problem involves finding a collision-free path from a
robot's starting to its target configuration. Recently, self-supervised
learning methods have emerged to tackle motion planning problems without
requiring expensive expert demonstrations. They solve the Eikonal equation for
training neural networks and lead to efficient solutions. However, these
methods struggle in complex environments because they fail to maintain key
properties of the Eikonal equation, such as optimal value functions and
geodesic distances. To overcome these limitations, we propose a novel
self-supervised temporal difference metric learning approach that solves the
Eikonal equation more accurately and enhances performance in solving complex
and unseen planning tasks. Our method enforces Bellman's principle of
optimality over finite regions, using temporal difference learning to avoid
spurious local minima while incorporating metric learning to preserve the
Eikonal equation's essential geodesic properties. We demonstrate that our
approach significantly outperforms existing self-supervised learning methods in
handling complex environments and generalizing to unseen environments, with
robot configurations ranging from 2 to 12 degrees of freedom (DOF).

</details>

### [195] [Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization](https://arxiv.org/abs/2505.05851)
*Janik Kaden,Maximilian Hilger,Tim Schreiter,Marius Schaab,Thomas Graichen,Andrey Rudenko,Ulrich Heinkel,Achim J. Lilienthal*

Main category: cs.RO

TLDR: 研究探讨了利用超宽带（UWB）定位技术作为可扩展的替代方案，用于在拥挤和易遮挡环境中捕捉人体运动。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地融入人类环境，理解和预测人体运动对安全高效的交互至关重要。传统运动捕捉系统存在硬件复杂、校准繁琐、遮挡问题和成本高等限制。

Method: 结合UWB定位技术、眼动追踪、机器人LiDAR和雷达传感器，并在模拟博物馆环境中记录多模态数据，包括运动捕捉数据作为地面真实值。

Result: 提供了超过130分钟的多模态数据，展示了UWB技术在复杂环境中的潜力。

Conclusion: 研究为超越视觉系统的可扩展运动数据收集奠定了基础，适用于仓库、机场等大型复杂环境。

Abstract: With robots increasingly integrating into human environments, understanding
and predicting human motion is essential for safe and efficient interactions.
Modern human motion and activity prediction approaches require high quality and
quantity of data for training and evaluation, usually collected from motion
capture systems, onboard or stationary sensors. Setting up these systems is
challenging due to the intricate setup of hardware components, extensive
calibration procedures, occlusions, and substantial costs. These constraints
make deploying such systems in new and large environments difficult and limit
their usability for in-the-wild measurements. In this paper we investigate the
possibility to apply the novel Ultra-Wideband (UWB) localization technology as
a scalable alternative for human motion capture in crowded and occlusion-prone
environments. We include additional sensing modalities such as eye-tracking,
onboard robot LiDAR and radar sensors, and record motion capture data as ground
truth for evaluation and comparison. The environment imitates a museum setup,
with up to four active participants navigating toward random goals in a natural
way, and offers more than 130 minutes of multi-modal data. Our investigation
provides a step toward scalable and accurate motion data collection beyond
vision-based systems, laying a foundation for evaluating sensing modalities
like UWB in larger and complex environments like warehouses, airports, or
convention centers.

</details>

### [196] [Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based Approach](https://arxiv.org/abs/2505.06182)
*Tim Schneider,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TLDR: TAP是一个基于强化学习和Transformer的任务无关主动感知框架，用于解决部分可观测环境中的挑战，并在触觉感知任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 人类通过触觉探索感知物体属性，机器人领域需要类似技术以补充视觉感知，但现有方法在部分可观测环境中存在挑战。

Method: TAP整合了Soft Actor-Critic和CrossQ算法，联合训练感知模块和决策策略，采用任务无关设计。

Result: TAP在触觉MNIST数字识别和触觉姿态估计任务中取得高精度。

Conclusion: TAP是一个通用且高效的框架，有望推动机器人主动触觉感知的发展。

Abstract: Humans make extensive use of haptic exploration to map and identify the
properties of the objects that we touch. In robotics, active tactile perception
has emerged as an important research domain that complements vision for tasks
such as object classification, shape reconstruction, and manipulation. This
work introduces TAP (Task-agnostic Active Perception) -- a novel framework that
leverages reinforcement learning (RL) and transformer-based architectures to
address the challenges posed by partially observable environments. TAP
integrates Soft Actor-Critic (SAC) and CrossQ algorithms within a unified
optimization objective, jointly training a perception module and
decision-making policy. By design, TAP is completely task-agnostic and can, in
principle, generalize to any active perception problem. We evaluate TAP across
diverse tasks, including toy examples and realistic applications involving
haptic exploration of 3D models from the Tactile MNIST benchmark. Experiments
demonstrate the efficacy of TAP, achieving high accuracies on the Tactile MNIST
haptic digit recognition task and a tactile pose estimation task. These
findings underscore the potential of TAP as a versatile and generalizable
framework for advancing active tactile perception in robotics.

</details>

<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [197] [Towards order of magnitude X-ray dose reduction in breast cancer imaging using phase contrast and deep denoising](https://arxiv.org/abs/2505.05812)
*Ashkan Pakzad,Robert Turnbull,Simon J. Mutch,Thomas A. Leatham,Darren Lockie,Jane Fox,Beena Kumar,Daniel Häsermann,Christopher J. Hall,Anton Maksimenko,Benedicta D. Arhatari,Yakov I. Nesterets,Amir Entezam,Seyedamir T. Taba,Patrick C. Brennan,Timur E. Gureyev,Harry M. Quiney*

Main category: physics.med-ph

TLDR: 深度学习图像去噪结合相位对比CT技术，可将乳腺癌成像的辐射剂量降低16倍以上，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌筛查方法（如X线乳腺摄影）存在敏感性和特异性不足的问题，且患者不适感强。相位对比CT（PCT）虽能提供高质量图像，但高剂量辐射对乳腺组织有害。

Method: 研究采用PCT技术对全新鲜乳腺切除样本成像，并应用深度学习图像去噪技术。

Result: 实验表明，该方法可将辐射剂量降低16倍以上，且图像质量（如空间分辨率和对比噪声比）未受影响，专家评估也证实了其有效性。

Conclusion: 该技术为未来在同步辐射设施中开展活体患者PCT乳腺癌成像奠定了基础。

Abstract: Breast cancer is the most frequently diagnosed human cancer in the United
States at present. Early detection is crucial for its successful treatment.
X-ray mammography and digital breast tomosynthesis are currently the main
methods for breast cancer screening. However, both have known limitations in
terms of their sensitivity and specificity to breast cancers, while also
frequently causing patient discomfort due to the requirement for breast
compression. Breast computed tomography is a promising alternative, however, to
obtain high-quality images, the X-ray dose needs to be sufficiently high. As
the breast is highly radiosensitive, dose reduction is particularly important.
Phase-contrast computed tomography (PCT) has been shown to produce
higher-quality images at lower doses and has no need for breast compression. It
is demonstrated in the present study that, when imaging full fresh mastectomy
samples with PCT, deep learning-based image denoising can further reduce the
radiation dose by a factor of 16 or more, without any loss of image quality.
The image quality has been assessed both in terms of objective metrics, such as
spatial resolution and contrast-to-noise ratio, as well as in an observer study
by experienced medical imaging specialists and radiologists. This work was
carried out in preparation for live patient PCT breast cancer imaging,
initially at specialized synchrotron facilities.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [198] [What Is Next for LLMs? Next-Generation AI Computing Hardware Using Photonic Chips](https://arxiv.org/abs/2505.05794)
*Renjie Li,Wenjie Wei,Qi Xin,Xiaoli Liu,Sixuan Mao,Erik Ma,Zijian Chen,Malu Zhang,Haizhou Li,Zhaoyu Zhang*

Main category: cs.AR

TLDR: 本文探讨了光子计算硬件在支持下一代生成式AI（如大型语言模型）中的潜力，分析了其优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）对计算硬件的需求激增，传统冯·诺依曼架构难以满足其能耗和性能要求，因此需要探索新型计算范式。

Method: 综述了集成光子神经网络架构（如马赫-曾德尔干涉仪网格、激光器、波长复用微环谐振器）和其他神经形态设备（如脉冲神经网络电路和混合自旋电子-光子突触），并分析了如何将LLM的动态矩阵运算映射到这些硬件上。

Result: 光子计算系统在吞吐量和能效上可能远超电子处理器，但在内存和超大数据集存储方面仍需突破。

Conclusion: 光子硬件为LLM提供了潜在的高效解决方案，但需解决内存和存储等关键挑战才能实现规模化应用。

Abstract: Large language models (LLMs) are rapidly pushing the limits of contemporary
computing hardware. For example, training GPT-3 has been estimated to consume
around 1300 MWh of electricity, and projections suggest future models may
require city-scale (gigawatt) power budgets. These demands motivate exploration
of computing paradigms beyond conventional von Neumann architectures. This
review surveys emerging photonic hardware optimized for next-generation
generative AI computing. We discuss integrated photonic neural network
architectures (e.g., Mach-Zehnder interferometer meshes, lasers,
wavelength-multiplexed microring resonators) that perform ultrafast matrix
operations. We also examine promising alternative neuromorphic devices,
including spiking neural network circuits and hybrid spintronic-photonic
synapses, which combine memory and processing. The integration of
two-dimensional materials (graphene, TMDCs) into silicon photonic platforms is
reviewed for tunable modulators and on-chip synaptic elements.
Transformer-based LLM architectures (self-attention and feed-forward layers)
are analyzed in this context, identifying strategies and challenges for mapping
dynamic matrix multiplications onto these novel hardware substrates. We then
dissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and
LLaMA, highlighting their architectural similarities and differences. We
synthesize state-of-the-art components, algorithms, and integration methods,
highlighting key advances and open issues in scaling such systems to mega-sized
LLM models. We find that photonic computing systems could potentially surpass
electronic processors by orders of magnitude in throughput and energy
efficiency, but require breakthroughs in memory, especially for long-context
windows and long token sequences, and in storage of ultra-large datasets.

</details>

### [199] [LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization](https://arxiv.org/abs/2505.05893)
*Seunghee Han,Soongyu Choi,Joo-Young Kim*

Main category: cs.AR

TLDR: LightNobel是一个软硬件协同设计的加速器，通过Token-wise Adaptive Activation Quantization（AAQ）和多精度可重构矩阵处理单元（RMPU）等技术，显著提升了蛋白质结构预测模型（PPM）的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构预测模型（如AlphaFold2和ESMFold）在处理长氨基酸序列时面临内存和计算资源的瓶颈，限制了其实际应用。

Method: 提出AAQ技术，利用PPM激活中的独特模式进行细粒度量化；硬件层面集成RMPU和VVPU，高效执行AAQ。

Result: LightNobel在速度和能效上分别比NVIDIA A100和H100 GPU提升8.44x、8.41x和37.29x、43.35x，同时内存需求降低120.05x。

Conclusion: LightNobel成功解决了PPM的可扩展性问题，为长序列蛋白质分析提供了高效解决方案。

Abstract: Recent advances in Protein Structure Prediction Models (PPMs), such as
AlphaFold2 and ESMFold, have revolutionized computational biology by achieving
unprecedented accuracy in predicting three-dimensional protein folding
structures. However, these models face significant scalability challenges,
particularly when processing proteins with long amino acid sequences (e.g.,
sequence length > 1,000). The primary bottleneck that arises from the
exponential growth in activation sizes is driven by the unique data structure
in PPM, which introduces an additional dimension that leads to substantial
memory and computational demands. These limitations have hindered the effective
scaling of PPM for real-world applications, such as analyzing large proteins or
complex multimers with critical biological and pharmaceutical relevance.
  In this paper, we present LightNobel, the first hardware-software co-designed
accelerator developed to overcome scalability limitations on the sequence
length in PPM. At the software level, we propose Token-wise Adaptive Activation
Quantization (AAQ), which leverages unique token-wise characteristics, such as
distogram patterns in PPM activations, to enable fine-grained quantization
techniques without compromising accuracy. At the hardware level, LightNobel
integrates the multi-precision reconfigurable matrix processing unit (RMPU) and
versatile vector processing unit (VVPU) to enable the efficient execution of
AAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup
and 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100
GPUs, respectively, while maintaining negligible accuracy loss. It also reduces
the peak memory requirement up to 120.05x in PPM, enabling scalable processing
for proteins with long sequences.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [200] [Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints](https://arxiv.org/abs/2505.05957)
*Peter Röseler,Oliver Schaudt,Helmut Berg,Christian Bauckhage,Matthias Koch*

Main category: quant-ph

TLDR: 论文提出了一种量子卷积神经网络（QCNN）的编码方案，显著降低了输入维度，并在NISQ设备上实现了高精度图像分类。


<details>
  <summary>Details</summary>
Motivation: 量子计算为神经网络架构提供了新的可能性，但当前NISQ设备的硬件限制阻碍了QCNN的实际应用。

Method: 引入编码方案减少输入维度，提出基于可表达性、纠缠和复杂度的自动化框架设计QCNN的构建模块。

Result: 在IBM Heron r2量子处理器上实现96.08%的分类准确率，超越传统方法的71.74%。

Conclusion: 研究验证了量子计算在图像分类中的潜力，为QCNN的实际应用提供了可行方案。

Abstract: While classical convolutional neural networks (CNNs) have revolutionized
image classification, the emergence of quantum computing presents new
opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)
leverage quantum mechanical properties and hold potential to outperform
classical approaches. However, their implementation on current noisy
intermediate-scale quantum (NISQ) devices remains challenging due to hardware
limitations. In our research, we address this challenge by introducing an
encoding scheme that significantly reduces the input dimensionality. We
demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to
directly process $28\times 28$ pixel MNIST images, eliminating the need for
classical dimensionality reduction pre-processing. Additionally, we propose an
automated framework based on expressibility, entanglement, and complexity
characteristics to identify the building blocks of QCNNs, parameterized quantum
circuits (PQCs). Our approach demonstrates advantages in accuracy and
convergence speed with a similar parameter count compared to both hybrid QCNNs
and classical CNNs. We validated our experiments on IBM's Heron r2 quantum
processor, achieving $96.08\%$ classification accuracy, surpassing the
$71.74\%$ benchmark of traditional approaches under identical training
conditions. These results represent one of the first implementations of image
classifications on real quantum hardware and validate the potential of quantum
computing in this area.

</details>

<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [201] [GenAI in Entrepreneurship: a systematic review of generative artificial intelligence in entrepreneurship research: current issues and future directions](https://arxiv.org/abs/2505.05523)
*Anna Kusetogullari,Huseyin Kusetogullari,Martin Andersson,Tony Gorschek*

Main category: econ.GN

TLDR: 本文通过系统文献综述分析了生成式人工智能（GenAI）对创业研究的影响，识别出五大主题集群，并讨论了未来研究方向、文献缺口及伦理问题。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI对创业条件和行业动态的潜在影响，填补创业研究中GenAI主题的知识空白。

Method: 采用自然语言处理和无监督机器学习技术（TF-IDF向量化、PCA和层次聚类）分析了83篇同行评审文章。

Result: 识别出五大主题集群：数字转型与行为模型、GenAI增强的教育与学习系统、可持续创新与战略AI影响、商业模式与市场趋势、数据驱动的创业技术趋势。

Conclusion: 呼吁更多宏观层面的研究，关注GenAI作为创业外部推动者的作用，以及有效监管框架的研究。

Abstract: Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are recognized to have significant effects on industry and business dynamics,
not least because of their impact on the preconditions for entrepreneurship.
There is still a lack of knowledge of GenAI as a theme in entrepreneurship
research. This paper presents a systematic literature review aimed at
identifying and analyzing the evolving landscape of research on the effects of
GenAI on entrepreneurship. We analyze 83 peer-reviewed articles obtained from
leading academic databases: Web of Science and Scopus. Using natural language
processing and unsupervised machine learning techniques with TF-IDF
vectorization, Principal Component Analysis (PCA), and hierarchical clustering,
five major thematic clusters are identified: (1) Digital Transformation and
Behavioral Models, (2) GenAI-Enhanced Education and Learning Systems, (3)
Sustainable Innovation and Strategic AI Impact, (4) Business Models and Market
Trends, and (5) Data-Driven Technological Trends in Entrepreneurship. Based on
the review, we discuss future research directions, gaps in the current
literature, as well as ethical concerns raised in the literature. We highlight
the need for more macro-level research on GenAI and LLMs as external enablers
for entrepreneurship and for research on effective regulatory frameworks that
facilitate business experimentation, innovation, and further technology
development.

</details>

<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [202] [AI-powered virtual eye: perspective, challenges and opportunities](https://arxiv.org/abs/2505.05516)
*Yue Wu,Yibo Guo,Yulong Yan,Jiancheng Yang,Xin Zhou,Ching-Yu Cheng,Danli Shi,Mingguang He*

Main category: q-bio.TO

TLDR: AI驱动的“虚拟眼”平台，通过多模态、多尺度模型模拟人眼结构与功能，有望革新眼科医疗与研究。


<details>
  <summary>Details</summary>
Motivation: 利用AI、成像和多组学技术构建高保真数字人眼模型，推动个性化眼科护理和疾病研究。

Method: 整合多模态数据、生成式AI、基础模型和代理架构，开发动态预测与反馈的统一模型。

Result: 提出发展路线图，强调大规模数据集和交互界面的重要性。

Conclusion: 尽管存在可解释性、伦理等挑战，虚拟眼有望革新眼科领域。

Abstract: We envision the "virtual eye" as a next-generation, AI-powered platform that
uses interconnected foundation models to simulate the eye's intricate structure
and biological function across all scales. Advances in AI, imaging, and
multiomics provide a fertile ground for constructing a universal, high-fidelity
digital replica of the human eye. This perspective traces the evolution from
early mechanistic and rule-based models to contemporary AI-driven approaches,
integrating in a unified model with multimodal, multiscale, dynamic predictive
capabilities and embedded feedback mechanisms. We propose a development roadmap
emphasizing the roles of large-scale multimodal datasets, generative AI,
foundation models, agent-based architectures, and interactive interfaces.
Despite challenges in interpretability, ethics, data processing and evaluation,
the virtual eye holds the potential to revolutionize personalized ophthalmic
care and accelerate research into ocular health and disease.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [203] [Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition](https://arxiv.org/abs/2505.05768)
*Weiyi Zhang,Peranut Chotcomwongse,Yinwen Li,Pusheng Xu,Ruijie Yao,Lianhao Zhou,Yuxuan Zhou,Hui Feng,Qiping Zhou,Xinyue Wang,Shoujin Huang,Zihao Jin,Florence H. T. Chung,Shujun Wang,Yalin Zheng,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: eess.IV

TLDR: 该研究首次探索了基于治疗前分层的糖尿病黄斑水肿（DME）治疗反应预测，并通过APTOS竞赛推动AI在个性化治疗中的应用。


<details>
  <summary>Details</summary>
Motivation: DME治疗反应差异大，需个性化策略，但缺乏预测方法。

Method: 组织APTOS竞赛，利用OCT图像数据集，通过AI模型预测抗VEGF治疗反应。

Result: 竞赛吸引了170支团队，最终41支进入决赛，最佳团队AUC达80.06%。

Conclusion: AI在DME个性化治疗中具有潜力，可辅助临床决策。

Abstract: Diabetic macular edema (DME) significantly contributes to visual impairment
in diabetic patients. Treatment responses to intravitreal therapies vary,
highlighting the need for patient stratification to predict therapeutic
benefits and enable personalized strategies. To our knowledge, this study is
the first to explore pre-treatment stratification for predicting DME treatment
responses. To advance this research, we organized the 2nd Asia-Pacific
Tele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The
competition focused on improving predictive accuracy for anti-VEGF therapy
responses using ophthalmic OCT images. We provided a dataset containing tens of
thousands of OCT images from 2,000 patients with labels across four sub-tasks.
This paper details the competition's structure, dataset, leading methods, and
evaluation metrics. The competition attracted strong scientific community
participation, with 170 teams initially registering and 41 reaching the final
round. The top-performing team achieved an AUC of 80.06%, highlighting the
potential of AI in personalized DME treatment and clinical decision-making.

</details>

### [204] [Image Restoration via Multi-domain Learning](https://arxiv.org/abs/2505.05504)
*Xingyu Jiang,Ning Gao,Xiuhui Zhang,Hongkun Dou,Shaowen Fu,Xiaoqing Zhong,Hongjue Li,Yue Deng*

Main category: eess.IV

TLDR: 论文提出了一种新的图像修复框架SWFormer，通过多域学习改进Transformer，结合空间-小波-傅里叶多域结构和多尺度学习，显著提升了修复性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 自然图像因大气和成像条件导致多种退化现象，现有Transformer方法模型复杂且缺乏对退化共性的研究。

Method: 提出SWFormer框架，包括Token Mixer中的空间-小波-傅里叶多域结构和Feed-Forward Network中的多尺度学习。

Result: 在十种修复任务中表现优于现有方法，平衡了性能、参数大小、计算成本和推理延迟。

Conclusion: SWFormer为图像修复提供了一种高效且通用的解决方案。

Abstract: Due to adverse atmospheric and imaging conditions, natural images suffer from
various degradation phenomena. Consequently, image restoration has emerged as a
key solution and garnered substantial attention. Although recent Transformer
architectures have demonstrated impressive success across various restoration
tasks, their considerable model complexity poses significant challenges for
both training and real-time deployment. Furthermore, instead of investigating
the commonalities among different degradations, most existing restoration
methods focus on modifying Transformer under limited restoration priors. In
this work, we first review various degradation phenomena under multi-domain
perspective, identifying common priors. Then, we introduce a novel restoration
framework, which integrates multi-domain learning into Transformer.
Specifically, in Token Mixer, we propose a Spatial-Wavelet-Fourier multi-domain
structure that facilitates local-region-global multi-receptive field modeling
to replace vanilla self-attention. Additionally, in Feed-Forward Network, we
incorporate multi-scale learning to fuse multi-domain features at different
resolutions. Comprehensive experimental results across ten restoration tasks,
such as dehazing, desnowing, motion deblurring, defocus deblurring, rain
streak/raindrop removal, cloud removal, shadow removal, underwater enhancement
and low-light enhancement, demonstrate that our proposed model outperforms
state-of-the-art methods and achieves a favorable trade-off among restoration
performance, parameter size, computational cost and inference latency. The code
is available at: https://github.com/deng-ai-lab/SWFormer.

</details>

### [205] [StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation](https://arxiv.org/abs/2505.05509)
*Yi Liu,Xinyi Liu,Panwang Xia,Qiong Wu,Yi Wan,Yongjun Zhang*

Main category: eess.IV

TLDR: StereoINR提出了一种新的立体图像超分辨率方法，通过连续隐式表示和跨视图信息融合，突破了固定尺度限制并提升了几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有立体超分辨率方法缺乏跨视图几何一致性和固定尺度限制，难以自适应利用多视图信息。

Method: 提出StereoINR，将立体图像对建模为连续隐式表示，结合空间扭曲和交叉注意力机制实现跨视图信息融合。

Result: 实验表明，StereoINR在训练分布内外尺度上均表现优异，几何一致性显著提升。

Conclusion: StereoINR为任意尺度立体超分辨率提供了统一解决方案，性能优于现有方法。

Abstract: Stereo image super-resolution (SSR) aims to enhance high-resolution details
by leveraging information from stereo image pairs. However, existing stereo
super-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook
cross-view geometric consistency and are limited to fixed-scale upsampling. The
key issue is that previous upsampling methods use convolution to independently
process deep features of different views, lacking cross-view and non-local
information perception, making it difficult to select beneficial information
from multi-view scenes adaptively. In this work, we propose Stereo Implicit
Neural Representation (StereoINR), which innovatively models stereo image pairs
as continuous implicit representations. This continuous representation breaks
through the scale limitations, providing a unified solution for arbitrary-scale
stereo super-resolution reconstruction of left-right views. Furthermore, by
incorporating spatial warping and cross-attention mechanisms, StereoINR enables
effective cross-view information fusion and achieves significant improvements
in pixel-level geometric consistency. Extensive experiments across multiple
datasets show that StereoINR outperforms out-of-training-distribution scale
upsampling and matches state-of-the-art SSR methods within
training-distribution scales.

</details>

### [206] [Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility](https://arxiv.org/abs/2505.05518)
*Jaeyoung Huh,Ankur Kapoor,Young-Ho Kim*

Main category: eess.IV

TLDR: 提出了一种AI驱动的跟踪模型，用于在ICE成像中持续跟踪治疗设备尖端，结合临床和合成数据增强模型鲁棒性，实验结果显示角度误差较低。


<details>
  <summary>Details</summary>
Motivation: 在EP和SHD干预中，手动ICE导管操作需要频繁调整，导致治疗设备尖端持续可见性难以维持。

Method: 采用混合数据集生成策略，结合临床ICE序列和合成数据增强，使用预训练的超声基础模型和基于transformer的网络进行特征提取和预测。

Result: 模型实现了3.32度的入射角误差和12.76度的旋转角误差。

Conclusion: 该AI框架为实时机器人ICE导管调整奠定了基础，未来将扩展临床数据集以增强模型泛化能力。

Abstract: Intra-cardiac Echocardiography (ICE) plays a critical role in
Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by
providing real-time visualization of intracardiac structures. However,
maintaining continuous visibility of the therapy device tip remains a challenge
due to frequent adjustments required during manual ICE catheter manipulation.
To address this, we propose an AI-driven tracking model that estimates the
device tip incident angle and passing point within the ICE imaging plane,
ensuring continuous visibility and facilitating robotic ICE catheter control.
  A key innovation of our approach is the hybrid dataset generation strategy,
which combines clinical ICE sequences with synthetic data augmentation to
enhance model robustness. We collected ICE images in a water chamber setup,
equipping both the ICE catheter and device tip with electromagnetic (EM)
sensors to establish precise ground-truth locations. Synthetic sequences were
created by overlaying catheter tips onto real ICE images, preserving motion
continuity while simulating diverse anatomical scenarios. The final dataset
consists of 5,698 ICE-tip image pairs, ensuring comprehensive training
coverage.
  Our model architecture integrates a pretrained ultrasound (US) foundation
model, trained on 37.4M echocardiography images, for feature extraction. A
transformer-based network processes sequential ICE frames, leveraging
historical passing points and incident angles to improve prediction accuracy.
  Experimental results demonstrate that our method achieves 3.32 degree entry
angle error, 12.76 degree rotation angle error. This AI-driven framework lays
the foundation for real-time robotic ICE catheter adjustments, minimizing
operator workload while ensuring consistent therapy device visibility. Future
work will focus on expanding clinical datasets to further enhance model
generalization.

</details>

### [207] [Score-based Self-supervised MRI Denoising](https://arxiv.org/abs/2505.05631)
*Jiachen Tu,Yaokun Shi,Fan Lam*

Main category: eess.IV

TLDR: C2S是一种基于分数的自监督学习框架，用于MRI去噪，通过广义去噪分数匹配损失直接从噪声数据中学习，并在多噪声水平和多对比度下表现出色。


<details>
  <summary>Details</summary>
Motivation: MRI去噪中，监督学习方法需要高信噪比标签，而自监督方法易过度平滑细节。C2S旨在解决标签稀缺问题并提升去噪性能。

Method: 提出广义去噪分数匹配（GDSM）损失，直接从噪声数据中学习；引入噪声水平重参数化和细节细化扩展。

Result: 在M4Raw和fastMRI数据集上，C2S在自监督方法中表现最佳，与监督方法竞争。

Conclusion: C2S是一种有效的自监督MRI去噪框架，适用于多噪声水平和多对比度场景。

Abstract: Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging
tool that provides unparalleled soft tissue contrast and anatomical detail.
Noise contamination, especially in accelerated and/or low-field acquisitions,
can significantly degrade image quality and diagnostic accuracy. Supervised
learning based denoising approaches have achieved impressive performance but
require high signal-to-noise ratio (SNR) labels, which are often unavailable.
Self-supervised learning holds promise to address the label scarcity issue, but
existing self-supervised denoising methods tend to oversmooth fine spatial
features and often yield inferior performance than supervised methods. We
introduce Corruption2Self (C2S), a novel score-based self-supervised framework
for MRI denoising. At the core of C2S is a generalized denoising score matching
(GDSM) loss, which extends denoising score matching to work directly with noisy
observations by modeling the conditional expectation of higher-SNR images given
further corrupted observations. This allows the model to effectively learn
denoising across multiple noise levels directly from noisy data. Additionally,
we incorporate a reparameterization of noise levels to stabilize training and
enhance convergence, and introduce a detail refinement extension to balance
noise reduction with the preservation of fine spatial features. Moreover, C2S
can be extended to multi-contrast denoising by leveraging complementary
information across different MRI contrasts. We demonstrate that our method
achieves state-of-the-art performance among self-supervised methods and
competitive results compared to supervised counterparts across varying noise
conditions and MRI contrasts on the M4Raw and fastMRI dataset.

</details>

### [208] [UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes](https://arxiv.org/abs/2505.05643)
*Mark C. Eid,Ana I. L. Namburete,João F. Henriques*

Main category: eess.IV

TLDR: UltraGauss是一种专为超声设计的3D重建框架，通过高斯泼溅技术提升效率和准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统2D超声图像操作依赖性强、变异性高的问题，同时克服现有3D重建方法计算成本高、与超声物理不兼容的缺点。

Method: 提出UltraGauss框架，基于高斯泼溅技术，模拟探头平面交点和声学图像形成，优化GPU并行化和数值稳定性。

Result: 在真实临床数据上，5分钟内实现最佳重建，20分钟内SSIM达0.99，专家评估显示其重建效果最真实。

Conclusion: UltraGauss为超声3D重建提供了高效、准确的解决方案，具有临床应用潜力。

Abstract: Ultrasound imaging is widely used due to its safety, affordability, and
real-time capabilities, but its 2D interpretation is highly operator-dependent,
leading to variability and increased cognitive demand. 2D-to-3D reconstruction
mitigates these challenges by providing standardized volumetric views, yet
existing methods are often computationally expensive, memory-intensive, or
incompatible with ultrasound physics. We introduce UltraGauss: the first
ultrasound-specific Gaussian Splatting framework, extending view synthesis
techniques to ultrasound wave propagation. Unlike conventional
perspective-based splatting, UltraGauss models probe-plane intersections in 3D,
aligning with acoustic image formation. We derive an efficient rasterization
boundary formulation for GPU parallelization and introduce a numerically stable
covariance parametrization, improving computational efficiency and
reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves
state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20
minutes on a single GPU. A survey of expert clinicians confirms UltraGauss'
reconstructions are the most realistic among competing methods. Our CUDA
implementation will be released upon publication.

</details>

### [209] [V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models](https://arxiv.org/abs/2505.05659)
*Guilherme Vieira Neto,Marcos Eduardo Valle*

Main category: eess.IV

TLDR: V-EfficientNets是EfficientNet的扩展，专为处理向量值数据设计，在医学图像分类任务中表现优异，准确率达99.46%。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理多维数据时忽略了通道间关系，而向量值神经网络能更好地处理此类数据。

Method: 通过联合优化网络宽度、深度和分辨率，扩展EfficientNet以处理向量值数据。

Result: 在ALL-IDB2数据集上达到99.46%的准确率，参数更少且优于现有模型。

Conclusion: V-EfficientNets在效率和性能上均优于传统方法，适用于向量值数据任务。

Abstract: EfficientNet models are convolutional neural networks optimized for parameter
allocation by jointly balancing network width, depth, and resolution. Renowned
for their exceptional accuracy, these models have become a standard for image
classification tasks across diverse computer vision benchmarks. While
traditional neural networks learn correlations between feature channels during
training, vector-valued neural networks inherently treat multidimensional data
as coherent entities, taking for granted the inter-channel relationships. This
paper introduces vector-valued EfficientNets (V-EfficientNets), a novel
extension of EfficientNet designed to process arbitrary vector-valued data. The
proposed models are evaluated on a medical image classification task, achieving
an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute
lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency,
significantly reducing parameters while outperforming state-of-the-art models,
including the original EfficientNet. The source code is available at
https://github.com/mevalle/v-nets.

</details>

### [210] [Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology](https://arxiv.org/abs/2505.05689)
*Fuyao Chen,Yuexi Du,Tal Zeevi,Nicha C. Dvornek,John A. Onofrey*

Main category: eess.IV

TLDR: 该论文提出了一种基于对称卷积核的无监督分割方法，用于提取鲁棒且等变的组织病理学生物标志物，以解决传统机器学习模型在旋转和反射不变性上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统病理学分析依赖人工，效率低且存在变异性；现有机器学习模型缺乏旋转和反射不变性，限制了其在组织病理学中的泛化能力。

Method: 通过新型对称卷积核进行无监督分割，提取等变生物标志物，并在前列腺组织微阵列图像上验证。

Result: 该方法提取的生物标志物在旋转鲁棒性和泛化性上优于标准卷积核模型。

Conclusion: 该方法有望提升数字病理学中机器学习模型的准确性、一致性和鲁棒性，扩展至其他癌症的诊断和预后。

Abstract: Histopathology evaluation of tissue specimens through microscopic examination
is essential for accurate disease diagnosis and prognosis. However, traditional
manual analysis by specially trained pathologists is time-consuming,
labor-intensive, cost-inefficient, and prone to inter-rater variability,
potentially affecting diagnostic consistency and accuracy. As digital pathology
images continue to proliferate, there is a pressing need for automated analysis
to address these challenges. Recent advancements in artificial
intelligence-based tools such as machine learning (ML) models, have
significantly enhanced the precision and efficiency of analyzing
histopathological slides. However, despite their impressive performance, ML
models are invariant only to translation, lacking invariance to rotation and
reflection. This limitation restricts their ability to generalize effectively,
particularly in histopathology, where images intrinsically lack meaningful
orientation. In this study, we develop robust, equivariant histopathological
biomarkers through a novel symmetric convolutional kernel via unsupervised
segmentation. The approach is validated using prostate tissue micro-array (TMA)
images from 50 patients in the Gleason 2019 Challenge public dataset. The
biomarkers extracted through this approach demonstrate enhanced robustness and
generalizability against rotation compared to models using standard convolution
kernels, holding promise for enhancing the accuracy, consistency, and
robustness of ML models in digital pathology. Ultimately, this work aims to
improve diagnostic and prognostic capabilities of histopathology beyond
prostate cancer through equivariant imaging.

</details>

### [211] [Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference](https://arxiv.org/abs/2505.05703)
*Haoyang Pei,Ding Xia,Xiang Xu,William Moore,Yao Wang,Hersh Chandarana,Li Feng*

Main category: eess.IV

TLDR: 提出了一种混合学习框架，结合自监督和监督学习，用于MRI图像重建，解决了高加速率下自监督学习性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习需要高质量参考图像，而自监督学习在高加速率下性能下降。混合学习旨在克服这些限制。

Method: 分两阶段：自监督学习生成伪参考图像，监督学习进一步优化重建性能。

Result: 在螺旋UTE肺MRI和3D T1脑映射中，混合学习在图像质量和定量准确性上优于其他方法。

Conclusion: 混合学习为低质量或不完整参考数据下的MRI重建提供了有效解决方案，有望推动深度学习MRI的临床应用。

Abstract: Purpose: Deep learning has demonstrated strong potential for MRI
reconstruction, but conventional supervised learning methods require
high-quality reference images, which are often unavailable in practice.
Self-supervised learning offers an alternative, yet its performance degrades at
high acceleration rates. To overcome these limitations, we propose hybrid
learning, a novel two-stage training framework that combines self-supervised
and supervised learning for robust image reconstruction.
  Methods: Hybrid learning is implemented in two sequential stages. In the
first stage, self-supervised learning is employed to generate improved images
from noisy or undersampled reference data. These enhanced images then serve as
pseudo-ground truths for the second stage, which uses supervised learning to
refine reconstruction performance and support higher acceleration rates. We
evaluated hybrid learning in two representative applications: (1) accelerated
0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of
the brain without access to fully sampled ground truth.
  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image
quality over both self-supervised and conventional supervised methods across
different acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping,
hybrid learning achieved superior T1 quantification accuracy across a wide
dynamic range, outperforming self-supervised learning in all tested conditions.
  Conclusions: Hybrid learning provides a practical and effective solution for
training deep MRI reconstruction networks when only low-quality or incomplete
reference data are available. It enables improved image quality and accurate
quantitative mapping across different applications and field strengths,
representing a promising technique toward broader clinical deployment of deep
learning-based MRI.

</details>

### [212] [The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review](https://arxiv.org/abs/2505.06118)
*Jingguo Qu,Xinyang Han,Man-Lik Chui,Yao Pu,Simon Takadiyi Gunda,Ziman Chen,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Ying*

Main category: eess.IV

TLDR: 本文综述了深度学习在淋巴结分割中的应用，探讨了不同架构的优缺点，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统淋巴结分割方法受限于人工标注和操作者水平，深度学习为提高准确性提供了新可能。

Method: 评估了卷积神经网络、编码器-解码器网络和变换器等架构在多模态医学影像分析中的应用。

Result: 尽管有进展，仍面临淋巴结形状多样、标注数据稀缺及跨模态方法不足等挑战。

Conclusion: 首次全面综述深度学习在淋巴结分割中的应用，并探索了多模态融合、迁移学习等未来方向。

Abstract: Automatic lymph node segmentation is the cornerstone for advances in computer
vision tasks for early detection and staging of cancer. Traditional
segmentation methods are constrained by manual delineation and variability in
operator proficiency, limiting their ability to achieve high accuracy. The
introduction of deep learning technologies offers new possibilities for
improving the accuracy of lymph node image analysis. This study evaluates the
application of deep learning in lymph node segmentation and discusses the
methodologies of various deep learning architectures such as convolutional
neural networks, encoder-decoder networks, and transformers in analyzing
medical imaging data across different modalities. Despite the advancements, it
still confronts challenges like the shape diversity of lymph nodes, the
scarcity of accurately labeled datasets, and the inadequate development of
methods that are robust and generalizable across different imaging modalities.
To the best of our knowledge, this is the first study that provides a
comprehensive overview of the application of deep learning techniques in lymph
node segmentation task. Furthermore, this study also explores potential future
research directions, including multimodal fusion techniques, transfer learning,
and the use of large-scale pre-trained models to overcome current limitations
while enhancing cancer diagnosis and treatment planning strategies.

</details>

### [213] [S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology Reconstruction via Echocardiogram](https://arxiv.org/abs/2505.06105)
*Xilin Gong,Yongkai Chen,Shushan Wu,Fang Wang,Ping Ma,Wenxuan Zhong*

Main category: eess.IV

TLDR: 提出了一种名为S2MNet的深度学习框架，通过整合六张常规2D超声心动图切片，重建连续且高保真的3D心脏模型。


<details>
  <summary>Details</summary>
Motivation: 尽管超声心动图因其无创性、实时性和成本效益而广泛应用，但其2D视图限制了心脏解剖和功能的全面评估。3D超声心动图存在分辨率低、可用性有限和成本高的问题。

Method: 通过模拟六张2D超声心动图图像，并引入基于变形场的方法，避免3D重建中的空间不连续或结构伪影。

Result: 验证显示，重建的左心室体积与医生测量的GLPS（应呈负相关）强相关，证实了方法的可靠性。

Conclusion: S2MNet提供了一种可靠且高效的3D心脏模型重建方法，克服了现有技术的局限性。

Abstract: Echocardiogram is the most commonly used imaging modality in cardiac
assessment duo to its non-invasive nature, real-time capability, and
cost-effectiveness. Despite its advantages, most clinical echocardiograms
provide only two-dimensional views, limiting the ability to fully assess
cardiac anatomy and function in three dimensions. While three-dimensional
echocardiography exists, it often suffers from reduced resolution, limited
availability, and higher acquisition costs. To overcome these challenges, we
propose a deep learning framework S2MNet that reconstructs continuous and
high-fidelity 3D heart models by integrating six slices of routinely acquired
2D echocardiogram views. Our method has three advantages. First, our method
avoid the difficulties on training data acquasition by simulate six of 2D
echocardiogram images from corresponding slices of a given 3D heart mesh.
Second, we introduce a deformation field-based method, which avoid spatial
discontinuities or structural artifacts in 3D echocardiogram reconstructions.
We validate our method using clinically collected echocardiogram and
demonstrate that our estimated left ventricular volume, a key clinical
indicator of cardiac function, is strongly correlated with the doctor measured
GLPS, a clinical measurement that should demonstrate a negative correlation
with LVE in medical theory. This association confirms the reliability of our
proposed 3D construction method.

</details>

### [214] [Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation](https://arxiv.org/abs/2505.06210)
*Diego Adame,Jose A. Nunez,Fabian Vazquez,Nayeli Gurrola,Huimin Li,Haoteng Tang,Bin Fu,Pengfei Gu*

Main category: eess.IV

TLDR: 提出了一种名为Topo-VM-UNetV2的新方法，通过将拓扑特征编码到基于Mamba的VM-UNetV2模型中，改进了息肉分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和Transformer的息肉分割模型在长距离依赖建模或计算复杂度上存在不足，而Mamba模型虽然解决了这些问题，但无法有效捕捉拓扑特征。

Method: 分两阶段：1) 使用VM-UNetV2生成概率图并计算拓扑注意力图；2) 将拓扑注意力图集成到VM-UNetV2的SDI模块中，形成Topo-SDI模块。

Result: 在五个公开数据集上的实验证明了方法的有效性。

Conclusion: Topo-VM-UNetV2能够有效结合拓扑特征，提升息肉分割的边界准确性。

Abstract: Convolutional neural network (CNN) and Transformer-based architectures are
two dominant deep learning models for polyp segmentation. However, CNNs have
limited capability for modeling long-range dependencies, while Transformers
incur quadratic computational complexity. Recently, State Space Models such as
Mamba have been recognized as a promising approach for polyp segmentation
because they not only model long-range interactions effectively but also
maintain linear computational complexity. However, Mamba-based architectures
still struggle to capture topological features (e.g., connected components,
loops, voids), leading to inaccurate boundary delineation and polyp
segmentation. To address these limitations, we propose a new approach called
Topo-VM-UNetV2, which encodes topological features into the Mamba-based
state-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of
two stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for
the training and test images, which are then used to compute topology attention
maps. Specifically, we first compute persistence diagrams of the PMs, then we
generate persistence score maps by assigning persistence values (i.e., the
difference between death and birth times) of each topological feature to its
birth location, finally we transform persistence scores into attention weights
using the sigmoid function. Stage 2: These topology attention maps are
integrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to
form a topology-guided semantics and detail infusion (Topo-SDI) module for
enhancing the segmentation results. Extensive experiments on five public polyp
segmentation datasets demonstrate the effectiveness of our proposed method. The
code will be made publicly available.

</details>

<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [215] [Minimal Sequent Calculus for Teaching First-Order Logic: Lessons Learned](https://arxiv.org/abs/2505.05988)
*Jørgen Villadsen*

Main category: cs.LO

TLDR: MiniCalc是一个基于最小序列演算的网页应用，用于教授一阶逻辑，并可选择在Isabelle证明助手中验证证明。


<details>
  <summary>Details</summary>
Motivation: 开发MiniCalc的目的是为教学提供一个直观的工具，帮助学生理解一阶逻辑，并通过验证功能增强学习效果。

Method: MiniCalc基于最小序列演算设计，支持在Isabelle中验证证明。

Result: 通过近年来的使用，MiniCalc在教学实践中取得了积极的效果。

Conclusion: MiniCalc是一个有效的教学工具，能够提升学生对一阶逻辑的理解和证明能力。

Abstract: MiniCalc is a web app for teaching first-order logic based on a minimal
sequent calculus. As an option the proofs can be verified in the Isabelle proof
assistant. We present the lessons learned using the tool in recent years at our
university.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [216] [MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills](https://arxiv.org/abs/2505.06176)
*Niladri Shekhar Dutt,Duygu Ceylan,Niloy J. Mitra*

Main category: cs.GR

TLDR: 论文探讨了如何利用多模态大语言模型（MLLM）进行照片修饰，通过训练模型理解图像处理操作，生成可解释且保留原始对象细节的编辑序列。


<details>
  <summary>Details</summary>
Motivation: 传统照片修饰工具虽然保守但专业，但操作复杂；生成式编辑虽灵活但可能改变对象身份。研究旨在结合两者的优势，利用MLLM实现可控且专业的修饰。

Method: 训练MLLM通过视觉谜题理解图像处理操作，并基于专家编辑照片合成推理数据集，生成编辑序列。

Result: 实验表明，该方法在可解释性和身份保留方面优于现有生成式和传统修饰方法。

Conclusion: MLLM能够有效规划照片修饰操作，提供可理解且可控的编辑方案。

Abstract: Retouching is an essential task in post-manipulation of raw photographs.
Generative editing, guided by text or strokes, provides a new tool accessible
to users but can easily change the identity of the original objects in
unacceptable and unpredictable ways. In contrast, although traditional
procedural edits, as commonly supported by photoediting tools (e.g., Gimp,
Lightroom), are conservative, they are still preferred by professionals.
Unfortunately, professional quality retouching involves many individual
procedural editing operations that is challenging to plan for most novices. In
this paper, we ask if a multimodal large language model (MLLM) can be taught to
critique raw photographs, suggest suitable remedies, and finally realize them
with a given set of pre-authored procedural image operations. We demonstrate
that MLLMs can be first made aware of the underlying image processing
operations, by training them to solve specially designed visual puzzles.
Subsequently, such an operation-aware MLLM can both plan and propose edit
sequences. To facilitate training, given a set of expert-edited photos, we
synthesize a reasoning dataset by procedurally manipulating the expert edits
and then grounding a pretrained LLM on the visual adjustments, to synthesize
reasoning for finetuning. The proposed retouching operations are, by
construction, understandable by the users, preserve object details and
resolution, and can be optionally overridden. We evaluate our setup on a
variety of test examples and show advantages, in terms of explainability and
identity preservation, over existing generative and other procedural
alternatives. Code, data, models, and supplementary results can be found via
our project website at https://monetgpt.github.io.

</details>

### [217] [Anymate: A Dataset and Baselines for Learning 3D Object Rigging](https://arxiv.org/abs/2505.06227)
*Yufan Deng,Yuhao Zhang,Chen Geng,Shangzhe Wu,Jiajun Wu*

Main category: cs.GR

TLDR: 论文提出了Anymate数据集和基于学习的自动绑定框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统自动绑定方法依赖几何启发式，难以处理复杂几何体；现有数据驱动方法受限于训练数据规模。

Method: 提出包含23万3D资产的Anymate数据集，并设计基于学习的自动绑定框架，包含关节、连接性和蒙皮权重预测三个模块。

Result: 模型在数据集上表现显著优于现有方法。

Conclusion: Anymate数据集和框架为未来自动绑定和蒙皮方法提供了基准。

Abstract: Rigging and skinning are essential steps to create realistic 3D animations,
often requiring significant expertise and manual effort. Traditional attempts
at automating these processes rely heavily on geometric heuristics and often
struggle with objects of complex geometry. Recent data-driven approaches show
potential for better generality, but are often constrained by limited training
data. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets
paired with expert-crafted rigging and skinning information -- 70 times larger
than existing datasets. Using this dataset, we propose a learning-based
auto-rigging framework with three sequential modules for joint, connectivity,
and skinning weight prediction. We systematically design and experiment with
various architectures as baselines for each module and conduct comprehensive
evaluations on our dataset to compare their performance. Our models
significantly outperform existing methods, providing a foundation for comparing
future methods in automated rigging and skinning. Code and dataset can be found
at https://anymate3d.github.io/.

</details>

<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [218] [Trading Under Uncertainty: A Distribution-Based Strategy for Futures Markets Using FutureQuant Transformer](https://arxiv.org/abs/2505.05595)
*Wenhao Guo,Yuda Wang,Zeqiao Huang,Changjiang Zhang,Shumin ma*

Main category: q-fin.TR

TLDR: FutureQuant Transformer模型利用注意力机制预测期货价格范围和波动性，优于传统点预测模型，提升交易策略和风险管理，平均每30分钟交易收益0.1193%。


<details>
  <summary>Details</summary>
Motivation: 传统期货交易中，大量数据和实时限价订单簿（LOB）等变量使价格预测复杂化，需要更高效的预测方法。

Method: 采用注意力机制的FutureQuant Transformer模型，分析市场模式，结合RSI、ATR和布林带等因子进行预测。

Result: 模型显著提升风险管理，平均每30分钟交易收益0.1193%，优于现有先进模型。

Conclusion: FutureQuant Transformer在期货交易预测分析中实现了重大突破。

Abstract: In the complex landscape of traditional futures trading, where vast data and
variables like real-time Limit Order Books (LOB) complicate price predictions,
we introduce the FutureQuant Transformer model, leveraging attention mechanisms
to navigate these challenges. Unlike conventional models focused on point
predictions, the FutureQuant model excels in forecasting the range and
volatility of future prices, thus offering richer insights for trading
strategies. Its ability to parse and learn from intricate market patterns
allows for enhanced decision-making, significantly improving risk management
and achieving a notable average gain of 0.1193% per 30-minute trade over
state-of-the-art models with a simple algorithm using factors such as RSI, ATR,
and Bollinger Bands. This innovation marks a substantial leap forward in
predictive analytics within the volatile domain of futures trading.

</details>

### [219] [FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions](https://arxiv.org/abs/2505.05784)
*Yang Li,Zhi Chen,Steve Yang*

Main category: q-fin.TR

TLDR: FlowHFT是一种基于流匹配策略的模仿学习框架，通过从多个专家模型中学习策略，能够动态适应不同市场环境，并在复杂或极端市场条件下优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统高频交易模型依赖历史数据假设未来市场状态相似，但实际市场动态多变且波动频繁，限制了单一模型的有效性。

Method: 提出FlowHFT框架，结合流匹配策略和网格搜索微调机制，从多个专家模型中学习并优化交易策略。

Result: FlowHFT在多种市场环境中表现优于单一专家模型，适应性强且性能稳定。

Conclusion: FlowHFT通过动态学习和优化策略，显著提升了高频交易在复杂市场环境中的适应性和性能。

Abstract: High-frequency trading (HFT) is an investing strategy that continuously
monitors market states and places bid and ask orders at millisecond speeds.
Traditional HFT approaches fit models with historical data and assume that
future market states follow similar patterns. This limits the effectiveness of
any single model to the specific conditions it was trained for. Additionally,
these models achieve optimal solutions only under specific market conditions,
such as assumptions about stock price's stochastic process, stable order flow,
and the absence of sudden volatility. Real-world markets, however, are dynamic,
diverse, and frequently volatile. To address these challenges, we propose the
FlowHFT, a novel imitation learning framework based on flow matching policy.
FlowHFT simultaneously learns strategies from numerous expert models, each
proficient in particular market scenarios. As a result, our framework can
adaptively adjust investment decisions according to the prevailing market
state. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism.
This allows it to refine strategies and achieve superior performance even in
complex or extreme market scenarios where expert strategies may be suboptimal.
We test FlowHFT in multiple market environments. We first show that flow
matching policy is applicable in stochastic market environments, thus enabling
FlowHFT to learn trading strategies under different market conditions. Notably,
our single framework consistently achieves performance superior to the best
expert for each market condition.

</details>

<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [220] [A Common Interface for Automatic Differentiation](https://arxiv.org/abs/2505.05542)
*Guillaume Dalle,Adrian Hill*

Main category: cs.MS

TLDR: DifferentiationInterface.jl 是一个 Julia 包，提供统一接口访问多种自动微分后端，便于比较和模块化开发。


<details>
  <summary>Details</summary>
Motivation: 针对科学机器学习任务中自定义代码较多的情况，选择合适的自动微分系统至关重要。

Method: 通过内置的预处理机制，利用各后端的优势，分摊一次性计算成本。

Result: 支持复杂功能（如稀疏处理）且不增加用户负担。

Conclusion: 该工具为科学机器学习任务提供了灵活高效的自动微分解决方案。

Abstract: For scientific machine learning tasks with a lot of custom code, picking the
right Automatic Differentiation (AD) system matters. Our Julia package
DifferentiationInterface.jl provides a common frontend to a dozen AD backends,
unlocking easy comparison and modular development. In particular, its built-in
preparation mechanism leverages the strengths of each backend by amortizing
one-time computations. This is key to enabling sophisticated features like
sparsity handling without putting additional burdens on the user.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [221] [Unsupervised Blind Speech Separation with a Diffusion Prior](https://arxiv.org/abs/2505.05657)
*Zhongweiyang Xu,Xulin Fan,Zhong-Qiu Wang,Xilin Jiang,Romit Roy Choudhury*

Main category: eess.AS

TLDR: ArrayDPS是一种无监督、阵列无关的生成方法，用于解决盲语音分离问题，通过扩散后验采样和优化近似房间声学及麦克风间的相对传递函数，性能优于无监督基线方法。


<details>
  <summary>Details</summary>
Motivation: 盲语音分离（BSS）是一个具有挑战性的盲逆问题，因为麦克风阵列几何结构、房间脉冲响应（RIR）和语音源均未知。

Method: ArrayDPS基于扩散后验采样（DPS），通过优化问题近似似然函数，估计房间声学和相对传递函数，结合扩散先验迭代采样，最终分离语音源。

Result: ArrayDPS在SDR指标上优于所有无监督基线方法，与监督方法相当。

Conclusion: ArrayDPS是一种高效的无监督方法，无需麦克风阵列信息即可实现语音分离，性能接近监督方法。

Abstract: Blind Speech Separation (BSS) aims to separate multiple speech sources from
audio mixtures recorded by a microphone array. The problem is challenging
because it is a blind inverse problem, i.e., the microphone array geometry, the
room impulse response (RIR), and the speech sources, are all unknown. We
propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic,
and generative manner. The core idea builds on diffusion posterior sampling
(DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must
approximate the likelihood by formulating a separate optimization problem. The
solution to the optimization approximates room acoustics and the relative
transfer functions between microphones. These approximations, along with the
diffusion priors, iterate through the ArrayDPS sampling process and ultimately
yield separated voice sources. We only need a simple single-speaker speech
diffusion model as a prior along with the mixtures recorded at the microphones;
no microphone array information is necessary. Evaluation results show that
ArrayDPS outperforms all baseline unsupervised methods while being comparable
to supervised methods in terms of SDR. Audio demos are provided at:
https://arraydps.github.io/ArrayDPSDemo/.

</details>

<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [222] [A Machine-Learning Compositional Study of Exoplanetary Material Accreted Onto Five Helium-Atmosphere White Dwarfs with $\texttt{cecilia}$](https://arxiv.org/abs/2505.06228)
*Mariona Badenas-Agusti,Siyi Xu,Andrew Vanderburg,Kishalay De,Patrick Dufour,Laura K. Rogers,Susana Hoyos,Simon Blouin,Javier Viaña,Amy Bonsor,Ben Zuckerman*

Main category: astro-ph.EP

TLDR: 论文首次应用机器学习流程$	exttt{cecilia}$，通过联合贝叶斯拟合分析五颗金属污染的白矮星光谱，测量其大气中多种元素的丰度，结果与CI球粒陨石一致，并发现氧富集现象。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用机器学习技术分析缺乏元素丰度数据的金属污染白矮星，以揭示其污染物来源及组成。

Method: 采用$	exttt{cecilia}$机器学习流程，联合贝叶斯拟合$	extit{SDSS}$和$	extit{Keck/ESI}$光谱数据，测量元素丰度。

Result: 成功测量了2至6种元素的丰度，精度与传统方法相当（约0.20 dex）；污染物组成与CI球粒陨石一致，部分白矮星存在显著氧富集。

Conclusion: $	exttt{cecilia}$有望支持未来大规模白矮星光谱分析，提升对系外物质组成的统计认识。

Abstract: We present the first application of the Machine Learning (ML) pipeline
$\texttt{cecilia}$ to determine the physical parameters and photospheric
composition of five metal-polluted He-atmosphere white dwarfs without
well-characterised elemental abundances. To achieve this, we perform a joint
and iterative Bayesian fit to their $\textit{SDSS}$ (R=2,000) and
$\textit{Keck/ESI}$ (R=4,500) optical spectra, covering the wavelength range
from about 3,800\r{A} to 9,000\r{A}. Our analysis measures the abundances of at
least two $-$and up to six$-$ chemical elements in their atmospheres with a
predictive accuracy similar to that of conventional WD analysis techniques
($\approx$0.20 dex). The white dwarfs with the largest number of detected heavy
elements are SDSS J0859$+$5732 and SDSS J2311$-$0041, which simultaneously
exhibit O, Mg, Si, Ca, and Fe in their $\textit{Keck/ESI}$ spectra. For all
systems, we find that the bulk composition of their pollutants is largely
consistent with those of primitive CI chondrites to within 1-2$\sigma$. We also
find evidence of statistically significant ($>2\sigma$) oxygen excesses for
SDSS J0859$+$5732 and SDSS J2311$-$0041, which could point to the accretion of
oxygen-rich exoplanetary material. In the future, as wide-field astronomical
surveys deliver millions of public WD spectra to the scientific community,
$\texttt{cecilia}$ aspires to unlock population-wide studies of polluted WDs,
therefore helping to improve our statistical knowledge of extrasolar
compositions.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [223] [Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications](https://arxiv.org/abs/2505.05736)
*Da Wu,Zhanliang Wang,Quan Nguyen,Zhuoran Xu,Kai Wang*

Main category: q-bio.QM

TLDR: MINT框架通过偏好优化将单模态大模型与多模态生物医学数据对齐，提升单模态输入任务的性能。


<details>
  <summary>Details</summary>
Motivation: 高质量多模态生物医学数据稀缺，限制了预训练大模型在专业任务中的微调效果。

Method: MINT采用ORPO框架，利用上游多模态模型生成偏好数据，优化下游单模态模型。

Result: 在罕见遗传病预测和组织分类任务中，MINT显著优于其他方法。

Conclusion: MINT通过偏好优化有效将多模态知识迁移至单模态模型。

Abstract: The scarcity of high-quality multimodal biomedical data limits the ability to
effectively fine-tune pretrained Large Language Models (LLMs) for specialized
biomedical tasks. To address this challenge, we introduce MINT (Multimodal
Integrated kNowledge Transfer), a framework that aligns unimodal large decoder
models with domain-specific decision patterns from multimodal biomedical data
through preference optimization. While MINT supports different optimization
techniques, we primarily implement it with the Odds Ratio Preference
Optimization (ORPO) framework as its backbone. This strategy enables the
aligned LLMs to perform predictive tasks using text-only or image-only inputs
while retaining knowledge learnt from multimodal data. MINT leverages an
upstream multimodal machine learning (MML) model trained on high-quality
multimodal data to transfer domain-specific insights to downstream text-only or
image-only LLMs. We demonstrate its effectiveness through two key applications:
(1) Rare genetic disease prediction from texts, where MINT uses a multimodal
encoder model, trained on facial photos and clinical notes, to generate a
preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite
relying on text input only, the MINT-derived model outperforms models trained
with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue
type classification using cell nucleus images, where MINT uses a
vision-language foundation model as the preference generator, containing
knowledge learnt from both text and histopathological images to align
downstream image-only models. The resulting MINT-derived model significantly
improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type
classification. In summary, MINT provides an effective strategy to align
unimodal LLMs with high-quality multimodal expertise through preference
optimization.

</details>

<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [224] [Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models](https://arxiv.org/abs/2505.06107)
*Faeze Ghorbanpour,Thiago Zordan Malaguth,Aliakbar Akbaritabar*

Main category: cs.DL

TLDR: 论文提出了一种基于姓名的国籍检测方法，解决了数字足迹数据中缺乏国籍信息的问题，并应用于学者迁移研究。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，大多数网络和数字足迹数据不包含国籍信息，这给迁移研究带来了挑战，尤其是左截断问题。

Method: 利用从维基百科收集的260万姓名-国籍对作为训练数据，采用基于字符的机器学习模型进行国籍分类。

Result: 模型在粗粒度分类中F1得分为84%，细粒度国家级别为67%。实证研究表明，使用姓名来源国比学术来源国更能准确估计回流规模。

Conclusion: 该方法有效解决了左截断问题，适用于其他基于数字足迹的迁移研究。

Abstract: Most web and digital trace data do not include information about an
individual's nationality due to privacy concerns. The lack of data on
nationality can create challenges for migration research. It can lead to a
left-censoring issue since we are uncertain about the migrant's country of
origin. Once we observe an emigration event, if we know the nationality, we can
differentiate it from return migration. We propose methods to detect the
nationality with the least available data, i.e., full names. We use the
detected nationality in comparison with the country of academic origin, which
is a common approach in studying the migration of researchers. We gathered 2.6
million unique name-nationality pairs from Wikipedia and categorized them into
families of nationalities with three granularity levels to use as our training
data. Using a character-based machine learning model, we achieved a weighted F1
score of 84% for the broadest and 67% for the most granular, country-level
categorization. In our empirical study, we used the trained and tested model to
assign nationality to 8+ million scholars' full names in Scopus data. Our
results show that using the country of first publication as a proxy for
nationality underestimates the size of return flows, especially for countries
with a more diverse academic workforce, such as the USA, Australia, and Canada.
We found that around 48% of emigration from the USA was return migration once
we used the country of name origin, in contrast to 33% based on academic
origin. In the most recent period, 79% of scholars whose affiliation has
consistently changed from the USA to China, and are considered emigrants, have
Chinese names in contrast to 41% with a Chinese academic origin. Our proposed
methods for addressing left-censoring issues are beneficial for other research
that uses digital trace data to study migration.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [225] [An Overview of the Prospects and Challenges of Using Artificial Intelligence for Energy Management Systems in Microgrids](https://arxiv.org/abs/2505.05498)
*Noor ul Misbah Khanum,Hayssam Dahrouj,Ramesh C. Bansal,Hissam Mouayad Tawfik*

Main category: eess.SY

TLDR: AI在微电网能源管理系统中的应用及其未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决微电网中可再生能源预测、网络安全、成本控制等问题，推动可持续能源发展。

Method: 探讨AI在微电网能源管理系统中的适用性和效率。

Result: AI能优化微电网能源管理，未来可结合区块链、IoT等技术。

Conclusion: AI驱动的EMS前景广阔，需进一步研究自愈微电网、数据隐私等问题。

Abstract: Microgrids have emerged as a pivotal solution in the quest for a sustainable
and energy-efficient future. While microgrids offer numerous advantages, they
are also prone to issues related to reliably forecasting renewable energy
demand and production, protecting against cyberattacks, controlling operational
costs, optimizing power flow, and regulating the performance of energy
management systems (EMS). Tackling these energy management challenges is
essential to facilitate microgrid applications and seamlessly incorporate
renewable energy resources. Artificial intelligence (AI) has recently
demonstrated immense potential for optimizing energy management in microgrids,
providing efficient and reliable solutions. This paper highlights the combined
benefits of enabling AI-based methodologies in the energy management systems of
microgrids by examining the applicability and efficiency of AI-based EMS in
achieving specific technical and economic objectives. The paper also points out
several future research directions that promise to spearhead AI-driven EMS,
namely the development of self-healing microgrids, integration with blockchain
technology, use of Internet of things (IoT), and addressing interpretability,
data privacy, scalability, and the prospects to generative AI in the context of
future AI-based EMS.

</details>

### [226] [Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency](https://arxiv.org/abs/2505.05796)
*Xinyu Liang,Frits de Nijs,Buser Say,Hao Wang*

Main category: eess.SY

TLDR: 提出了一种基于人机交互（HITL）的人工智能框架，通过实时用户反馈和电价波动优化HVAC系统性能，显著降低能耗成本并提升舒适度。


<details>
  <summary>Details</summary>
Motivation: 传统HVAC系统无法动态适应实时电价变化和用户舒适偏好，导致能耗高且舒适度不足。

Method: 结合用户实时反馈和强化学习，动态优化HVAC运行，无需预设信息。

Result: 模拟显示该方法显著降低能耗成本，同时保持或提升舒适度。

Conclusion: 该框架为HVAC系统提供了一种可扩展的解决方案，平衡了个人偏好与经济、环境目标。

Abstract: Heating, Ventilation, and Air Conditioning (HVAC) systems account for
approximately 38% of building energy consumption globally, making them one of
the most energy-intensive services. The increasing emphasis on energy
efficiency and sustainability, combined with the need for enhanced occupant
comfort, presents a significant challenge for traditional HVAC systems. These
systems often fail to dynamically adjust to real-time changes in electricity
market rates or individual comfort preferences, leading to increased energy
costs and reduced comfort. In response, we propose a Human-in-the-Loop (HITL)
Artificial Intelligence framework that optimizes HVAC performance by
incorporating real-time user feedback and responding to fluctuating electricity
prices. Unlike conventional systems that require predefined information about
occupancy or comfort levels, our approach learns and adapts based on ongoing
user input. By integrating the occupancy prediction model with reinforcement
learning, the system improves operational efficiency and reduces energy costs
in line with electricity market dynamics, thereby contributing to demand
response initiatives. Through simulations, we demonstrate that our method
achieves significant cost reductions compared to baseline approaches while
maintaining or enhancing occupant comfort. This feedback-driven approach
ensures personalized comfort control without the need for predefined settings,
offering a scalable solution that balances individual preferences with economic
and environmental goals.

</details>

### [227] [Leveraging Multi-Task Learning for Multi-Label Power System Security Assessment](https://arxiv.org/abs/2505.06207)
*Muhy Eddin Za'ter,Amir Sajad,Bri-Mathias Hodge*

Main category: eess.SY

TLDR: 本文提出了一种基于多任务学习（MTL）的电力系统安全评估新方法，将问题重新定义为多标签分类任务。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在电力系统安全评估中存在准确性和可解释性不足的问题，需要一种更高效的方法。

Method: 采用共享编码器和多个解码器的MTL框架，同时评估静态、电压、暂态和小信号稳定性。

Result: 在IEEE 68总线系统上的实验表明，该方法在性能上优于现有最先进方法。

Conclusion: 提出的MTL框架显著提升了电力系统安全评估的准确性和可解释性。

Abstract: This paper introduces a novel approach to the power system security
assessment using Multi-Task Learning (MTL), and reformulating the problem as a
multi-label classification task. The proposed MTL framework simultaneously
assesses static, voltage, transient, and small-signal stability, improving both
accuracy and interpretability with respect to the most state of the art machine
learning methods. It consists of a shared encoder and multiple decoders,
enabling knowledge transfer between stability tasks. Experiments on the IEEE
68-bus system demonstrate a measurable superior performance of the proposed
method compared to the extant state-of-the-art approaches.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [228] [Learning-Augmented Algorithms for Boolean Satisfiability](https://arxiv.org/abs/2505.06146)
*Idan Attias,Xing Gao,Lev Reyzin*

Main category: cs.DS

TLDR: 论文研究了在布尔可满足性问题（SAT）中使用机器学习预测（建议）来优化算法性能的方法，提出了两种建议形式（子集建议和标签建议），并展示了其在决策和优化问题中的显著改进。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用机器学习提供的部分信息（建议）来改进经典SAT问题的算法性能，以超越最坏情况分析的局限性。

Method: 采用两种建议形式：子集建议（随机提供部分变量的最优赋值）和标签建议（提供所有变量的噪声预测），并将其应用于PPSZ算法和近似算法中。

Result: 在决策问题中，子集建议显著加速了PPSZ算法的运行时间；在优化问题中，子集建议改进了近似算法的近似比，标签建议则在特定情况下实现了接近最优的近似。

Conclusion: 机器学习建议可以有效提升SAT问题的算法性能，为未来研究提供了新的方向。

Abstract: Learning-augmented algorithms are a prominent recent development in beyond
worst-case analysis. In this framework, a problem instance is provided with a
prediction (``advice'') from a machine-learning oracle, which provides partial
information about an optimal solution, and the goal is to design algorithms
that leverage this advice to improve worst-case performance. We study the
classic Boolean satisfiability (SAT) decision and optimization problems within
this framework using two forms of advice. ``Subset advice" provides a random
$\epsilon$ fraction of the variables from an optimal assignment, whereas
``label advice" provides noisy predictions for all variables in an optimal
assignment.
  For the decision problem $k$-SAT, by using the subset advice we accelerate
the exponential running time of the PPSZ family of algorithms due to Paturi,
Pudlak, Saks and Zane, which currently represent the state of the art in the
worst case. We accelerate the running time by a multiplicative factor of
$2^{-c}$ in the base of the exponent, where $c$ is a function of $\epsilon$ and
$k$. For the optimization problem, we show how to incorporate subset advice in
a black-box fashion with any $\alpha$-approximation algorithm, improving the
approximation ratio to $\alpha + (1 - \alpha)\epsilon$. Specifically, we
achieve approximations of $0.94 + \Omega(\epsilon)$ for MAX-$2$-SAT, $7/8 +
\Omega(\epsilon)$ for MAX-$3$-SAT, and $0.79 + \Omega(\epsilon)$ for MAX-SAT.
Moreover, for label advice, we obtain near-optimal approximation for instances
with large average degree, thereby generalizing recent results on MAX-CUT and
MAX-$2$-LIN.

</details>

<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [229] [Machine learning automorphic forms for black holes](https://arxiv.org/abs/2505.05549)
*Vishnu Jejjala,Suresh Nampuri,Dumisani Nxumalo,Pratik Roy,Abinash Swain*

Main category: hep-th

TLDR: 利用神经网络预测模形式和雅可比形式的模权重，证明机器学习在识别引力系统中模对称性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习是否能够通过傅里叶系数预测模权重，从而揭示引力系统中数据的模对称性组织方式。

Method: 训练前馈神经网络，基于Dedekind eta函数、Eisenstein级数和雅可比theta函数的傅里叶系数，预测模权重。

Result: 负权重模形式和准模形式预测准确率高，正权重和复杂雅可比theta函数组合准确率较低。

Conclusion: 机器学习可用于识别模对称性，为量子引力中对称性的自动检测和验证提供可能。

Abstract: Modular, Jacobi, and mock-modular forms serve as generating functions for BPS
black hole degeneracies. By training feed-forward neural networks on Fourier
coefficients of automorphic forms derived from the Dedekind eta function,
Eisenstein series, and Jacobi theta functions, we demonstrate that machine
learning techniques can accurately predict modular weights from truncated
expansions. Our results reveal strong performance for negative weight modular
and quasi-modular forms, particularly those arising in exact black hole
counting formulae, with lower accuracy for positive weights and more
complicated combinations of Jacobi theta functions. This study establishes a
proof of concept for using machine learning to identify how data is organized
in terms of modular symmetries in gravitational systems and suggests a pathway
toward automated detection and verification of symmetries in quantum gravity.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [230] [Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous Information Networks](https://arxiv.org/abs/2505.05989)
*Hongye Zheng,Yue Xing,Lipeng Zhu,Xu Han,Junliang Du,Wanyu Cui*

Main category: cs.IR

TLDR: 该研究提出了一种多跳路径感知的推荐框架，用于异构信息网络中的路径建模，通过路径选择、语义表示和注意力融合三阶段建模用户偏好，实验证明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决异构信息网络中路径建模的问题，探索多跳路径在推荐系统中的潜力。

Method: 采用三阶段方法：路径选择（过滤冗余信息）、语义表示（编码实体和关系）、注意力融合（加权路径生成用户兴趣表示）。

Result: 在Amazon-Book等数据集上显著优于现有模型，验证了多跳路径在高阶交互语义捕捉中的有效性。

Conclusion: 该方法为异构推荐场景提供了更具表达力的建模范式，兼具理论和实践价值。

Abstract: This study focuses on the problem of path modeling in heterogeneous
information networks and proposes a multi-hop path-aware recommendation
framework. The method centers on multi-hop paths composed of various types of
entities and relations. It models user preferences through three stages: path
selection, semantic representation, and attention-based fusion. In the path
selection stage, a path filtering mechanism is introduced to remove redundant
and noisy information. In the representation learning stage, a sequential
modeling structure is used to jointly encode entities and relations, preserving
the semantic dependencies within paths. In the fusion stage, an attention
mechanism assigns different weights to each path to generate a global user
interest representation. Experiments conducted on real-world datasets such as
Amazon-Book show that the proposed method significantly outperforms existing
recommendation models across multiple evaluation metrics, including HR@10,
Recall@10, and Precision@10. The results confirm the effectiveness of multi-hop
paths in capturing high-order interaction semantics and demonstrate the
expressive modeling capabilities of the framework in heterogeneous
recommendation scenarios. This method provides both theoretical and practical
value by integrating structural information modeling in heterogeneous networks
with recommendation algorithm design. It offers a more expressive and flexible
paradigm for learning user preferences in complex data environments.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [231] [An Empirical Study of Fuzz Harness Degradation](https://arxiv.org/abs/2505.06177)
*Philipp Görz,Joschua Schilling,Thorsten Holz,Marcel Böhme*

Main category: cs.SE

TLDR: 研究了Google的OSS-Fuzz平台中510个C/C++项目的模糊测试工具（harness）是否随项目更新而维护，以及其覆盖率和错误发现能力是否会随时间退化。


<details>
  <summary>Details</summary>
Motivation: 探讨模糊测试工具在项目持续演化过程中是否得到同步更新，以及其性能是否会因缺乏维护而退化。

Method: 分析了OSS-Fuzz平台中510个开源C/C++项目的模糊测试工具，研究了其覆盖率和错误发现能力的变化。

Result: 发现模糊测试工具的整体覆盖率保持稳定，且即使未更新，其错误发现能力仍能持久；但也存在个别覆盖率退化的情况，并对其原因进行了分类。

Conclusion: 研究为OSS-Fuzz和Fuzz Introspector提供了检测模糊测试工具退化的指标支持。

Abstract: The purpose of continuous fuzzing platforms is to enable fuzzing for software
projects via \emph{fuzz harnesses} -- but as the projects continue to evolve,
are these harnesses updated in lockstep, or do they run out of date? If these
harnesses remain unmaintained, will they \emph{degrade} over time in terms of
coverage achieved or number of bugs found? This is the subject of our study.
  We study Google's OSS-Fuzz continuous fuzzing platform containing harnesses
for 510 open-source C/C++ projects, many of which are security-critical. A
harness is the glue code between the fuzzer and the project, so it needs to
adapt to changes in the project. It is often added by a project maintainer or
as part of a, sometimes short-lived, testing effort.
  Our analysis shows a consistent overall fuzzer coverage percentage for
projects in OSS-Fuzz and a surprising longevity of the bug-finding capability
of harnesses even without explicit updates, as long as they still build.
However, we also identify and manually examine individual cases of harness
coverage degradation and categorize their root causes. Furthermore, we
contribute to OSS-Fuzz and Fuzz Introspector to support metrics to detect
harness degradation in OSS-Fuzz projects guided by this research.

</details>

### [232] [PyResBugs: A Dataset of Residual Python Bugs for Natural Language-Driven Fault Injection](https://arxiv.org/abs/2505.05777)
*Domenico Cotroneo,Giuseppe De Rosa,Pietro Liguori*

Main category: cs.SE

TLDR: PyResBugs是一个包含Python框架中残留缺陷的精选数据集，每个缺陷都配有修复版本和多级自然语言描述，支持自然语言驱动的故障注入。


<details>
  <summary>Details</summary>
Motivation: 传统测试中未检测到的缺陷在生产环境中暴露，需要高质量数据集以推动AI驱动的自动化测试研究。

Method: 收集Python框架中的残留缺陷，配对修复版本，并标注多级自然语言描述。

Result: 提供了支持自然语言驱动故障注入的高质量数据集，填补了软件故障注入技术与实际代表性的差距。

Conclusion: PyResBugs为Python系统的AI驱动自动化测试研究提供了宝贵资源。

Abstract: This paper presents PyResBugs, a curated dataset of residual bugs, i.e.,
defects that persist undetected during traditional testing but later surface in
production, collected from major Python frameworks. Each bug in the dataset is
paired with its corresponding fault-free (fixed) version and annotated with
multi-level natural language (NL) descriptions. These NL descriptions enable
natural language-driven fault injection, offering a novel approach to
simulating real-world faults in software systems. By bridging the gap between
software fault injection techniques and real-world representativeness,
PyResBugs provides researchers with a high-quality resource for advancing
AI-driven automated testing in Python systems.

</details>

### [233] [PRIMG : Efficient LLM-driven Test Generation Using Mutant Prioritization](https://arxiv.org/abs/2505.05584)
*Mohamed Salah Bouafif,Mohammad Hamdaqa,Edward Zulkoski*

Main category: cs.SE

TLDR: PRIMG框架通过集成突变优先级和测试用例生成模块，显著减少测试套件规模，同时保持高突变覆盖率。


<details>
  <summary>Details</summary>
Motivation: 解决突变测试中因生成大量测试用例导致的计算开销问题。

Method: PRIMG结合突变优先级模块（基于机器学习预测突变体有用性）和测试用例生成模块（利用LLM生成并迭代优化测试用例）。

Result: 实验表明PRIMG能显著减少测试套件规模，保持高突变覆盖率，且优先级模块优于随机选择。

Conclusion: PRIMG有效提升测试用例质量和效率，解决了LLM生成测试的局限性。

Abstract: Mutation testing is a widely recognized technique for assessing and enhancing
the effectiveness of software test suites by introducing deliberate code
mutations. However, its application often results in overly large test suites,
as developers generate numerous tests to kill specific mutants, increasing
computational overhead. This paper introduces PRIMG (Prioritization and
Refinement Integrated Mutation-driven Generation), a novel framework for
incremental and adaptive test case generation for Solidity smart contracts.
PRIMG integrates two core components: a mutation prioritization module, which
employs a machine learning model trained on mutant subsumption graphs to
predict the usefulness of surviving mutants, and a test case generation module,
which utilizes Large Language Models (LLMs) to generate and iteratively refine
test cases to achieve syntactic and behavioral correctness.
  We evaluated PRIMG on real-world Solidity projects from Code4Arena to assess
its effectiveness in improving mutation scores and generating high-quality test
cases. The experimental results demonstrate that PRIMG significantly reduces
test suite size while maintaining high mutation coverage. The prioritization
module consistently outperformed random mutant selection, enabling the
generation of high-impact tests with reduced computational effort. Furthermore,
the refining process enhanced the correctness and utility of LLM-generated
tests, addressing their inherent limitations in handling edge cases and complex
program logic.

</details>

### [234] [Enhancing Large Language Models with Faster Code Preprocessing for Vulnerability Detection](https://arxiv.org/abs/2505.05600)
*José Gonçalves,Miguel Silva,Eva Maia,Isabel Praça*

Main category: cs.SE

TLDR: SCoPE2是一个改进的代码预处理工具，显著提升了处理时间和漏洞检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于代码功能实现形式多样，需要高效、跨语言的标准化工具以支持漏洞检测。

Method: 基于SCoPE框架改进，引入SCoPE2，比较处理时间和内存使用，并评估其对LLM漏洞检测的影响。

Result: SCoPE2减少97.3%的处理时间，并提升LLM的F1分数。

Conclusion: SCoPE2通过优化预处理方法，显著提升了漏洞检测的效率和效果。

Abstract: The application of Artificial Intelligence has become a powerful approach to
detecting software vulnerabilities. However, effective vulnerability detection
relies on accurately capturing the semantic structure of code and its
contextual relationships. Given that the same functionality can be implemented
in various forms, a preprocessing tool that standardizes code representation is
important. This tool must be efficient, adaptable across programming languages,
and capable of supporting new transformations. To address this challenge, we
build on the existing SCoPE framework and introduce SCoPE2, an enhanced version
with improved performance. We compare both versions in terms of processing time
and memory usage and evaluate their impact on a Large Language Model (LLM) for
vulnerability detection. Our results show a 97.3\% reduction in processing time
with SCoPE2, along with an improved F1-score for the LLM, solely due to the
refined preprocessing approach.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [235] [An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers](https://arxiv.org/abs/2505.05828)
*Alba María Mármol-Romero,Manuel García-Vega,Miguel Ángel García-Cumbreras,Arturo Montejo-Ráez*

Main category: cs.HC

TLDR: 论文介绍了一种基于聊天机器人的系统，通过自我披露技术提高西班牙年轻人对某些心理障碍的认识。


<details>
  <summary>Details</summary>
Motivation: 旨在通过聊天机器人系统提高青少年对心理障碍的认识，并建立更具同理心的沟通。

Method: 结合封闭和开放对话，使用GPT-3语言模型进行自由交流，根据用户敏感性调整对话内容。

Result: 系统受到年轻人欢迎，有助于提高对心理障碍的认识。

Conclusion: 聊天机器人系统在青少年心理障碍意识提升方面具有潜力。

Abstract: This paper presents a chatbot-based system to engage young Spanish people in
the awareness of certain mental disorders through a self-disclosure technique.
The study was carried out in a population of teenagers aged between 12 and 18
years. The dialogue engine mixes closed and open conversations, so certain
controlled messages are sent to focus the chat on a specific disorder, which
will change over time. Once a set of trial questions is answered, the system
can initiate the conversation on the disorder under the focus according to the
user's sensibility to that disorder, in an attempt to establish a more
empathetic communication. Then, an open conversation based on the GPT-3
language model is initiated, allowing the user to express themselves with more
freedom. The results show that these systems are of interest to young people
and could help them become aware of certain mental disorders.

</details>

### [236] [Would You Rely on an Eerie Agent? A Systematic Review of the Impact of the Uncanny Valley Effect on Trust in Human-Agent Interaction](https://arxiv.org/abs/2505.05543)
*Ahdiyeh Alipour,Tilo Hartmann,Maryam Alimardani*

Main category: cs.HC

TLDR: 本文系统回顾了‘恐怖谷效应’（UVE）对人类信任人工代理的影响，分析了53项实证研究，揭示了研究方法与测量工具的局限性，并提出了新的分类框架。


<details>
  <summary>Details</summary>
Motivation: 随着人工代理在日常生活中的普及，理解人类如何感知和信任这些代理变得至关重要。然而，现有研究对UVE与信任关系的定义和操作化存在不一致性，缺乏系统性理解。

Method: 遵循PRISMA指南，系统搜索并分析了53项实证研究，分类考察了代理类型、交互方式、方法学及测量工具。

Result: 研究发现大多数研究依赖静态图像或假设场景，缺乏实时交互，且主要使用主观信任测量。

Conclusion: 本文首次系统梳理了UVE与信任的关系，提出了新的测量框架，为未来研究奠定了基础。

Abstract: Trust is a fundamental component of human-agent interaction. With the
increasing presence of artificial agents in daily life, it is essential to
understand how people perceive and trust these agents. One of the key
challenges affecting this perception is the Uncanny Valley Effect (UVE), where
increasingly human-like artificial beings can be perceived as eerie or
repelling. Despite growing interest in trust and the UVE, existing research
varies widely in terms of how these concepts are defined and operationalized.
This inconsistency raises important questions about how and under what
conditions the UVE influences trust in agents. A systematic understanding of
their relationship is currently lacking. This review aims to examine the impact
of the UVE on human trust in agents and to identify methodological patterns,
limitations, and gaps in the existing empirical literature. Following PRISMA
guidelines, a systematic search identified 53 empirical studies that
investigated both UVE-related constructs and trust or trust-related outcomes.
Studies were analyzed based on a structured set of categories, including types
of agents and interactions, methodological and measurement approaches, and key
findings. The results of our systematic review reveal that most studies rely on
static images or hypothetical scenarios with limited real-time interaction, and
the majority use subjective trust measures. This review offers a novel
framework for classifying trust measurement approaches with regard to the
best-practice criteria for empirically investigating the UVE. As the first
systematic attempt to map the intersection of UVE and trust, this review
contributes to a deeper understanding of their interplay and offers a
foundation for future research. Keywords: the uncanny valley effect, trust,
human-likeness, affinity response, human-agent interaction

</details>

### [237] [Extending Stress Detection Reproducibility to Consumer Wearable Sensors](https://arxiv.org/abs/2505.05694)
*Ohida Binte Amin,Varun Mishra,Tinashe M. Tapera,Robert Volpe,Aarti Sathyanarayana*

Main category: cs.HC

TLDR: 研究评估了消费级可穿戴设备在压力检测中的表现，发现不同设备和模型性能差异显著，结合HRV和EDA可提升预测效果，但硬件与模型兼容性仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 评估消费级可穿戴设备在压力检测中的可重复性和性能，填补现有研究中对设备间差异和模型通用性研究的不足。

Method: 通过实验室环境下的标准化压力诱导任务，比较研究级设备（Biopac MP160、Polar H10、Empatica E4）与消费级设备（Garmin Forerunner 55s）的表现。

Result: Biopac MP160表现最佳，Garmin Forerunner 55s在部分任务中与研究级设备相当，但Empatica E4存在硬件兼容性问题。结合HRV和EDA可提升预测效果。

Conclusion: 消费级设备在压力检测中具有潜力，但需解决硬件与模型的兼容性问题，以提升通用性和实用性。

Abstract: Wearable sensors are widely used to collect physiological data and develop
stress detection models. However, most studies focus on a single dataset,
rarely evaluating model reproducibility across devices, populations, or study
conditions. We previously assessed the reproducibility of stress detection
models across multiple studies, testing models trained on one dataset against
others using heart rate (with R-R interval) and electrodermal activity (EDA).
In this study, we extended our stress detection reproducibility to consumer
wearable sensors. We compared validated research-grade devices, to consumer
wearables - Biopac MP160, Polar H10, Empatica E4, to the Garmin Forerunner 55s,
assessing device-specific stress detection performance by conducting a new
stress study on undergraduate students. Thirty-five students completed three
standardized stress-induction tasks in a lab setting. Biopac MP160 performed
the best, being consistent with our expectations of it as the gold standard,
though performance varied across devices and models. Combining heart rate
variability (HRV) and EDA enhanced stress prediction across most scenarios.
However, Empatica E4 showed variability; while HRV and EDA improved stress
detection in leave-one-subject-out (LOSO) evaluations (AUROC up to 0.953),
device-specific limitations led to underperformance when tested with our
pre-trained stress detection tool (AUROC 0.723), highlighting generalizability
challenges related to hardware-model compatibility. Garmin Forerunner 55s
demonstrated strong potential for real-world stress monitoring, achieving the
best mental arithmetic stress detection performance in LOSO (AUROC up to 0.961)
comparable to research-grade devices like Polar H10 (AUROC 0.954), and Empatica
E4 (AUROC 0.905 with HRV-only model and AUROC 0.953 with HRV+EDA model), with
the added advantage of consumer-friendly wearability for free-living contexts.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [238] [FedAvgen: Metadata for Model Aggregation In Communication Systems](https://arxiv.org/abs/2505.05486)
*Anthony Kiggundu,Dennis Krummacker,Hans D. Schotten*

Main category: cs.NE

TLDR: 论文提出了一种基于元启发式算法（FedAvgen）的联邦学习方法，通过将预训练模型与其权重空间关联为表型和基因型，优化全局模型聚合。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中因设备多样性导致的模型聚合挑战，提高全局模型的泛化能力。

Method: 采用FedAvgen算法，将预训练模型与权重空间关联为表型和基因型，模拟遗传进化过程进行全局平均。

Result: 与FedAvg和FedSGD相比，FedAvgen在模型聚合中表现出更优的性能。

Conclusion: FedAvgen为联邦学习提供了一种有效的模型聚合方法，尤其适用于设备多样性场景。

Abstract: To improve business efficiency and minimize costs, Artificial Intelligence
(AI) practitioners have adopted a shift from formulating models from scratch
towards sharing pretrained models. The pretrained models are then aggregated
into a global model with higher generalization capabilities, which is
afterwards distributed to the client devices. This approach is known as
federated learning and inherently utilizes different techniques to select the
candidate client models averaged to obtain the global model. This approach, in
the case of communication systems, faces challenges arising from the
existential diversity in device profiles. The multiplicity in profiles
motivates our conceptual assessment of a metaheuristic algorithm (FedAvgen),
which relates each pretrained model with its weight space as metadata, to a
phenotype and genotype, respectively. This parent-child genetic evolution
characterizes the global averaging step in federated learning. We then compare
the results of our approach to two widely adopted baseline federated learning
algorithms like Federated Averaging (FedAvg) and Federated Stochastic Gradient
Descent (FedSGD).

</details>

### [239] [Evolutionary thoughts: integration of large language models and evolutionary algorithms](https://arxiv.org/abs/2505.05756)
*Antonio Jimeno Yepes,Pieter Barnard*

Main category: cs.NE

TLDR: 论文提出了一种结合大语言模型（LLMs）和进化算法（EAs）的高效评估框架，以解决LLMs在复杂场景中的幻觉问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂、新颖场景中容易产生幻觉或陷入局部解，而EAs能有效探索复杂搜索空间，但计算成本高。

Method: 引入高效评估框架，结合LLMs生成优质候选解，优化进化搜索策略。

Result: 实验证明，该方法能生成更优解，提升搜索效率。

Conclusion: 结合LLMs和EAs的方法有效解决了复杂场景中的优化问题，具有实际应用潜力。

Abstract: Large Language Models (LLMs) have unveiled remarkable capabilities in
understanding and generating both natural language and code, but LLM reasoning
is prone to hallucination and struggle with complex, novel scenarios, often
getting stuck on partial or incorrect solutions. However, the inherent ability
of Evolutionary Algorithms (EAs) to explore extensive and complex search spaces
makes them particularly effective in scenarios where traditional optimization
methodologies may falter. However, EAs explore a vast search space when applied
to complex problems.
  To address the computational bottleneck of evaluating large populations,
particularly crucial for complex evolutionary tasks, we introduce a highly
efficient evaluation framework. This implementation maintains compatibility
with existing primitive definitions, ensuring the generation of valid
individuals.
  Using LLMs, we propose an enhanced evolutionary search strategy that enables
a more focused exploration of expansive solution spaces. LLMs facilitate the
generation of superior candidate solutions, as evidenced by empirical results
demonstrating their efficacy in producing improved outcomes.

</details>

### [240] [Evolutionary Optimization for the Classification of Small Molecules Regulating the Circadian Rhythm Period: A Reliable Assessment](https://arxiv.org/abs/2505.05485)
*Antonio Arauzo-Azofra,Jose Molina-Baena,Maria Luque-Rodriguez*

Main category: cs.NE

TLDR: 该研究利用进化优化技术提升对调节生物钟的小分子分类性能，结果表明该方法优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 生物钟紊乱与多种健康问题相关，识别影响生物钟周期的小分子对靶向治疗至关重要。

Method: 应用进化算法优化特征选择和分类性能，评估多种机器学习分类器的准确性和泛化能力。

Result: 进化优化方法提高了分类准确性，减少了过拟合，且通过方差惩罚因子增强了模型可靠性。

Conclusion: 进化优化是分类调节生物钟小分子的有效策略，提升了预测性能和模型稳健性。

Abstract: The circadian rhythm plays a crucial role in regulating biological processes,
and its disruption is linked to various health issues. Identifying small
molecules that influence the circadian period is essential for developing
targeted therapies. This study explores the use of evolutionary optimization
techniques to enhance the classification of these molecules. We applied an
evolutionary algorithm to optimize feature selection and classification
performance. Several machine learning classifiers were employed, and
performance was evaluated using accuracy and generalization ability. The
findings demonstrate that the proposed evolutionary optimization method
improves classification accuracy and reduces overfitting compared to baseline
models. Additionally, the use of variance in accuracy as a penalty factor may
enhance the model's reliability for real-world applications. Our study confirms
that evolutionary optimization is an effective strategy for classifying small
molecules regulating the circadian rhythm. The proposed approach not only
improves predictive performance but also ensures a more robust model.

</details>

### [241] [Akkumula: Evidence accumulation driver models with Spiking Neural Networks](https://arxiv.org/abs/2505.05489)
*Alberto Morando*

Main category: cs.NE

TLDR: Akkumula是一个基于深度学习的证据积累建模框架，用于驾驶员模型，解决了现有模型缺乏标准化、适应性和扩展性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员模型缺乏标准化构建方法，且适应性差、优化效率低，限制了其应用。

Method: 采用深度学习技术，基于脉冲神经网络（Spiking Neural Networks）模拟生物大脑的证据积累过程，并结合车辆传感器数据训练。

Result: 模型在测试场实验中表现良好，能准确拟合车辆控制（刹车、加速、转向）的时间过程。

Conclusion: Akkumula框架具有高效处理大数据、适应多样化驾驶场景的能力，同时保持了核心机制的透明度。

Abstract: Processes of evidence accumulation for motor control contribute to the
ecological validity of driver models. According to established theories of
cognition, drivers make control adjustments when a process of accumulation of
perceptual inputs reaches a decision boundary. Unfortunately, there is not a
standard way for building such models, limiting their use. Current
implementations are hand-crafted, lack adaptability, and rely on inefficient
optimization techniques that do not scale well with large datasets. This paper
introduces Akkumula, an evidence accumulation modelling framework built using
deep learning techniques to leverage established coding libraries, gradient
optimization, and large batch training. The core of the library is based on
Spiking Neural Networks, whose operation mimic the evidence accumulation
process in the biological brain. The model was tested on data collected during
a test-track experiment. Results are promising. The model fits well the time
course of vehicle control (brake, accelerate, steering) based on vehicle sensor
data. The perceptual inputs are extracted by a dedicated neural network,
increasing the context-awareness of the model in dynamic scenarios. Akkumula
integrates with existing machine learning architectures, benefits from
continuous advancements in deep learning, efficiently processes large datasets,
adapts to diverse driving scenarios, and maintains a degree of transparency in
its core mechanisms.

</details>

### [242] [How to Train Your Metamorphic Deep Neural Network](https://arxiv.org/abs/2505.05510)
*Thomas Sommariva,Simone Calderara,Angelo Porrello*

Main category: cs.NE

TLDR: NeuMeta通过连续权重流形生成不同宽度和深度的神经网络，但原方法仅适用于模型最后一层。本文提出一种训练算法，扩展NeuMeta以实现全网络变形，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 原NeuMeta方法仅适用于模型最后一层，限制了其广泛应用。本文旨在扩展其能力，实现全网络变形。

Method: 采用分块增量训练、INR初始化和替换批归一化策略，以最小化精度损失。

Result: 生成的变形网络在多种压缩比下保持竞争力，为深度模型的高效部署提供可扩展方案。

Conclusion: 本文提出的训练算法成功扩展了NeuMeta的能力，实现了全网络变形，且代码已开源。

Abstract: Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural
networks of varying width and depth. Based on Implicit Neural Representation
(INR), NeuMeta learns a continuous weight manifold, enabling the direct
generation of compressed models, including those with configurations not seen
during training. While promising, the original formulation of NeuMeta proves
effective only for the final layers of the undelying model, limiting its
broader applicability. In this work, we propose a training algorithm that
extends the capabilities of NeuMeta to enable full-network metamorphosis with
minimal accuracy degradation. Our approach follows a structured recipe
comprising block-wise incremental training, INR initialization, and strategies
for replacing batch normalization. The resulting metamorphic networks maintain
competitive accuracy across a wide range of compression ratios, offering a
scalable solution for adaptable and efficient deployment of deep models. The
code is available at: https://github.com/TSommariva/HTTY_NeuMeta.

</details>

### [243] [Economic Analysis and Optimization of Energy Storage Configuration for Park Power Systems Based on Random Forest and Genetic Algorithm](https://arxiv.org/abs/2505.05511)
*Yanghui Song,Aoqi Li,Lilei Huo*

Main category: cs.NE

TLDR: 研究分析了不同条件下公园的经济表现，重点考察了储能系统部署前后的运营成本和电力负载平衡。通过随机森林模型和遗传算法优化，发现储能配置能显著降低成本并提高经济效益。


<details>
  <summary>Details</summary>
Motivation: 探讨储能系统对公园经济表现的影响，优化能源配置以实现可持续电力系统发展。

Method: 使用随机森林模型分析无储能时的经济表现，模拟储能部署后的运营，并用遗传算法优化储能配置。

Result: 储能部署后，风光弃电量减少，运营成本降低；遗传算法优化后，公园A、B、C的经济指标均提升。

Conclusion: 优化储能配置可降低成本、提高经济效益，促进电力系统可持续发展。

Abstract: This study aims to analyze the economic performance of various parks under
different conditions, particularly focusing on the operational costs and power
load balancing before and after the deployment of energy storage systems.
Firstly, the economic performance of the parks without energy storage was
analyzed using a random forest model. Taking Park A as an example, it was found
that the cost had the greatest correlation with electricity purchase, followed
by photovoltaic output, indicating that solar and wind power output are key
factors affecting economic performance. Subsequently, the operation of the
parks after the configuration of a 50kW/100kWh energy storage system was
simulated, and the total cost and operation strategy of the energy storage
system were calculated. The results showed that after the deployment of energy
storage, the amount of wind and solar power curtailment in each park decreased,
and the operational costs were reduced. Finally, a genetic algorithm was used
to optimize the energy storage configuration of each park. The energy storage
operation strategy was optimized through fitness functions, crossover
operations, and mutation operations. After optimization, the economic
indicators of Parks A, B, and C all improved. The research results indicate
that by optimizing energy storage configuration, each park can reduce costs,
enhance economic benefits, and achieve sustainable development of the power
system.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [244] [On the Price of Differential Privacy for Spectral Clustering over Stochastic Block Models](https://arxiv.org/abs/2505.05816)
*Antti Koskela,Mohamed Seif,Andrea J. Goldsmith*

Main category: cs.SI

TLDR: 研究隐私保护的谱聚类方法，用于随机块模型中的社区检测，重点探讨边差分隐私与社区恢复准确性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 探讨在随机块模型（SBMs）中，如何在保护隐私的同时实现准确的社区检测，特别是通过边差分隐私（DP）方法。

Method: 提出基于边差分隐私的私有算法，用于社区恢复，并分析隐私预算与准确性之间的关系。

Result: 建立了信息论条件，确保方法在边差分隐私下的准确性，为社区恢复提供了理论保证。

Conclusion: 研究为隐私保护下的社区检测提供了有效的算法和理论支持，平衡了隐私与准确性。

Abstract: We investigate privacy-preserving spectral clustering for community detection
within stochastic block models (SBMs). Specifically, we focus on edge
differential privacy (DP) and propose private algorithms for community
recovery. Our work explores the fundamental trade-offs between the privacy
budget and the accurate recovery of community labels. Furthermore, we establish
information-theoretic conditions that guarantee the accuracy of our methods,
providing theoretical assurances for successful community recovery under edge
DP.

</details>

### [245] [From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling](https://arxiv.org/abs/2505.06184)
*Vahid Rahimzadeh,Ali Hamzehpour,Azadeh Shakery,Masoud Asadpour*

Main category: cs.SI

TLDR: 提出了一种基于大语言模型（LLM）的两阶段用户画像方法，通过领域定义语句生成可解释的自然语言用户画像，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用户画像技术存在可迁移性差、特征不可解释、依赖大量标注数据或预定义类别等问题，需要一种更灵活、可解释的方法。

Method: 采用两阶段方法：1）半监督过滤结合领域知识库；2）生成抽象（合成描述）和抽取（代表性推文选择）用户画像。

Result: 实验结果显示，该方法比现有LLM和传统方法性能提升9.8%，生成灵活、可适应且可解释的用户画像。

Conclusion: 该方法通过LLM的固有知识减少对标注数据的依赖，适用于多领域，为下游社交网络任务提供了高效工具。

Abstract: Social media user profiling through content analysis is crucial for tasks
like misinformation detection, engagement prediction, hate speech monitoring,
and user behavior modeling. However, existing profiling techniques, including
tweet summarization, attribute-based profiling, and latent representation
learning, face significant limitations: they often lack transferability,
produce non-interpretable features, require large labeled datasets, or rely on
rigid predefined categories that limit adaptability. We introduce a novel large
language model (LLM)-based approach that leverages domain-defining statements,
which serve as key characteristics outlining the important pillars of a domain
as foundations for profiling. Our two-stage method first employs
semi-supervised filtering with a domain-specific knowledge base, then generates
both abstractive (synthesized descriptions) and extractive (representative
tweet selections) user profiles. By harnessing LLMs' inherent knowledge with
minimal human validation, our approach is adaptable across domains while
reducing the need for large labeled datasets. Our method generates
interpretable natural language user profiles, condensing extensive user data
into a scale that unlocks LLMs' reasoning and knowledge capabilities for
downstream social network tasks. We contribute a Persian political Twitter (X)
dataset and an LLM-based evaluation framework with human validation.
Experimental results show our method significantly outperforms state-of-the-art
LLM-based and traditional methods by 9.8%, demonstrating its effectiveness in
creating flexible, adaptable, and interpretable user profiles.

</details>

### [246] [A Noise-Resilient Semi-Supervised Graph Autoencoder for Overlapping Semantic Community Detection](https://arxiv.org/abs/2505.05965)
*Abdelfateh Bekkair,Slimane Bellaouar,Slimane Oulad-Naoui*

Main category: cs.SI

TLDR: 提出了一种半监督图自编码器，结合图多头注意力和模块化最大化，用于鲁棒检测重叠社区。


<details>
  <summary>Details</summary>
Motivation: 解决在噪声真实环境中整合拓扑、节点属性和先验信息的挑战。

Method: 采用半监督图自编码器，融合结构、属性和先验知识，并显式处理节点特征噪声。

Result: 模型在重叠社区检测中表现优于现有方法（NMI和F1-score提升），且在60%特征损坏下仍保持稳定性能。

Conclusion: 整合属性语义和结构模式对复杂网络中准确社区发现至关重要。

Abstract: Community detection in networks with overlapping structures remains a
significant challenge, particularly in noisy real-world environments where
integrating topology, node attributes, and prior information is critical. To
address this, we propose a semi-supervised graph autoencoder that combines
graph multi-head attention and modularity maximization to robustly detect
overlapping communities. The model learns semantic representations by fusing
structural, attribute, and prior knowledge while explicitly addressing noise in
node features. Key innovations include a noise-resistant architecture and a
semantic semi-supervised design optimized for community quality through
modularity constraints. Experiments demonstrate superior performance the model
outperforms state-of-the-art methods in overlapping community detection
(improvements in NMI and F1-score) and exhibits exceptional robustness to
attribute noise, maintaining stable performance under 60\% feature corruption.
These results highlight the importance of integrating attribute semantics and
structural patterns for accurate community discovery in complex networks.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [247] [Optimal Regret of Bernoulli Bandits under Global Differential Privacy](https://arxiv.org/abs/2505.05613)
*Achraf Azize,Yulian Wu,Junya Honda,Francesco Orabona,Shinji Ito,Debabrota Basu*

Main category: stat.ML

TLDR: 论文研究了在差分隐私（DP）下的随机多臂老虎机问题，改进了现有的遗憾下界和上界，并提出了一种新的信息论方法。


<details>
  <summary>Details</summary>
Motivation: 随着序列学习算法在现实生活中的广泛应用，如何在保证数据隐私的同时维持其效用成为一个重要问题。

Method: 通过提出新的信息论量，改进了Bernoulli老虎机在全局DP下的遗憾下界，并设计了两种渐近最优的DP算法（DP-KLUCB和DP-IMED）。

Result: 新算法在Bernoulli老虎机上的遗憾与改进后的下界渐近匹配，否定了过去认为必须遗忘历史奖励才能设计最优DP算法的猜想。

Conclusion: 论文通过耦合Laplace噪声与随机变量的浓度不等式，为DP下的随机老虎机问题提供了更紧的界限和更优的算法。

Abstract: As sequential learning algorithms are increasingly applied to real life,
ensuring data privacy while maintaining their utilities emerges as a timely
question. In this context, regret minimisation in stochastic bandits under
$\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike
bandits without DP, there is a significant gap between the best-known regret
lower and upper bound in this setting, though they "match" in order. Thus, we
revisit the regret lower and upper bounds of $\epsilon$-global DP algorithms
for Bernoulli bandits and improve both. First, we prove a tighter regret lower
bound involving a novel information-theoretic quantity characterising the
hardness of $\epsilon$-global DP in stochastic bandits. Our lower bound
strictly improves on the existing ones across all $\epsilon$ values. Then, we
choose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED,
and propose their DP versions using a unified blueprint, i.e., (a) running in
arm-dependent phases, and (b) adding Laplace noise to achieve privacy. For
Bernoulli bandits, we analyse the regrets of these algorithms and show that
their regrets asymptotically match our lower bound up to a constant arbitrary
close to 1. This refutes the conjecture that forgetting past rewards is
necessary to design optimal bandit algorithms under global DP. At the core of
our algorithms lies a new concentration inequality for sums of Bernoulli
variables under Laplace mechanism, which is a new DP version of the Chernoff
bound. This result is universally useful as the DP literature commonly treats
the concentrations of Laplace noise and random variables separately, while we
couple them to yield a tighter bound.

</details>

<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [248] [DaringFed: A Dynamic Bayesian Persuasion Pricing for Online Federated Learning under Two-sided Incomplete Information](https://arxiv.org/abs/2505.05842)
*Yun Xin,Jianfeng Lu,Shuqin Cao,Gang Li,Haozhao Wang,Guanghui Wen*

Main category: cs.GT

TLDR: DaringFed提出了一种动态贝叶斯说服定价机制，用于在线联邦学习中的激励设计，解决了动态双边不完全信息下的资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 在线联邦学习（OFL）需要激励客户端参与，但动态双边不完全信息（TII）限制了激励机制的设计。

Method: 将服务器与客户端的交互建模为贝叶斯说服博弈，设计动态信号与定价分配机制，并证明其存在唯一纳什均衡。

Result: 在真实数据集上，DaringFed优化了16.99%的准确率和收敛速度；合成数据集实验验证了其提升服务器效用的有效性（12.6%）。

Conclusion: DaringFed在TII条件下有效激励客户端参与，显著提升了学习性能和服务器效用。

Abstract: Online Federated Learning (OFL) is a real-time learning paradigm that
sequentially executes parameter aggregation immediately for each random
arriving client. To motivate clients to participate in OFL, it is crucial to
offer appropriate incentives to offset the training resource consumption.
However, the design of incentive mechanisms in OFL is constrained by the
dynamic variability of Two-sided Incomplete Information (TII) concerning
resources, where the server is unaware of the clients' dynamically changing
computational resources, while clients lack knowledge of the real-time
communication resources allocated by the server. To incentivize clients to
participate in training by offering dynamic rewards to each arriving client, we
design a novel Dynamic Bayesian persuasion pricing for online Federated
learning (DaringFed) under TII. Specifically, we begin by formulating the
interaction between the server and clients as a dynamic signaling and pricing
allocation problem within a Bayesian persuasion game, and then demonstrate the
existence of a unique Bayesian persuasion Nash equilibrium. By deriving the
optimal design of DaringFed under one-sided incomplete information, we further
analyze the approximate optimal design of DaringFed with a specific bound under
TII. Finally, extensive evaluation conducted on real datasets demonstrate that
DaringFed optimizes accuracy and converges speed by 16.99%, while experiments
with synthetic datasets validate the convergence of estimate unknown values and
the effectiveness of DaringFed in improving the server's utility by up to
12.6%.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [249] [Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates](https://arxiv.org/abs/2505.05940)
*Rodrigo Diaz,Mark Sandler*

Main category: cs.SD

TLDR: 提出了一种基于JAX库的快速、可微分、GPU加速的模态框架，用于高效模拟非线性振动模型，并支持基于梯度的逆建模。


<details>
  <summary>Details</summary>
Motivation: 传统方法在非线性模型（如von Kármán板）中计算量大且不可微分，限制了逆建模和实时应用。

Method: 使用JAX库构建GPU加速的模态框架，实现高效模拟和梯度计算。

Result: 性能显著优于CPU和GPU实现，能通过逆建模恢复物理参数（如张力、刚度和几何形状）。

Conclusion: 方法虽对初始值敏感，但具有更高解释性和紧凑参数化，代码已开源。

Abstract: Modal methods for simulating vibrations of strings, membranes, and plates are
widely used in acoustics and physically informed audio synthesis. However,
traditional implementations, particularly for non-linear models like the von
K\'arm\'an plate, are computationally demanding and lack differentiability,
limiting inverse modelling and real-time applications. We introduce a fast,
differentiable, GPU-accelerated modal framework built with the JAX library,
providing efficient simulations and enabling gradient-based inverse modelling.
Benchmarks show that our approach significantly outperforms CPU and GPU-based
implementations, particularly for simulations with many modes. Inverse
modelling experiments demonstrate that our approach can recover physical
parameters, including tension, stiffness, and geometry, from both synthetic and
experimental data. Although fitting physical parameters is more sensitive to
initialisation compared to other methods, it provides greater interpretability
and more compact parameterisation. The code is released as open source to
support future research and applications in differentiable physical modelling
and sound synthesis.

</details>

### [250] [Learning Music Audio Representations With Limited Data](https://arxiv.org/abs/2505.06042)
*Christos Plachouras,Emmanouil Benetos,Johan Pauwels*

Main category: cs.SD

TLDR: 研究探讨了在有限数据条件下音乐音频表示模型的表现，发现某些情况下小数据集模型甚至随机模型的表现与大数据集模型相当，但手工特征在某些任务中仍占优。


<details>
  <summary>Details</summary>
Motivation: 解决音乐音频数据稀缺场景（如小众音乐、个性化创作）下深度学习模型的性能问题。

Method: 测试多种架构、训练范式和输入时长的音乐模型，在5到8000分钟的数据集上训练，并在多个音乐信息检索任务中评估。

Result: 某些条件下，有限数据或随机模型的表现与大数据集模型相当，但手工特征在某些任务中表现更好。

Conclusion: 有限数据条件下模型表现具有潜力，但手工特征仍不可忽视。

Abstract: Large deep-learning models for music, including those focused on learning
general-purpose music audio representations, are often assumed to require
substantial training data to achieve high performance. If true, this would pose
challenges in scenarios where audio data or annotations are scarce, such as for
underrepresented music traditions, non-popular genres, and personalized music
creation and listening. Understanding how these models behave in limited-data
scenarios could be crucial for developing techniques to tackle them.
  In this work, we investigate the behavior of several music audio
representation models under limited-data learning regimes. We consider music
models with various architectures, training paradigms, and input durations, and
train them on data collections ranging from 5 to 8,000 minutes long. We
evaluate the learned representations on various music information retrieval
tasks and analyze their robustness to noise. We show that, under certain
conditions, representations from limited-data and even random models perform
comparably to ones from large-dataset models, though handcrafted features
outperform all learned representations in some tasks.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [251] [OccuEMBED: Occupancy Extraction Merged with Building Energy Disaggregation for Occupant-Responsive Operation at Scale](https://arxiv.org/abs/2505.05478)
*Yufei Zhang,Andrew Sonta*

Main category: eess.SP

TLDR: 论文提出OccuEMBED框架，利用智能电表数据推断建筑占用情况和系统运行状态，以支持节能和电网灵活性。


<details>
  <summary>Details</summary>
Motivation: 建筑能耗和排放占全球重要比例，需高效运营。电网波动性增加，建筑需提供灵活性。但大规模集中运营中融入占用信息仍具挑战。

Method: 提出OccuEMBED框架，结合概率占用生成器和基于KAN的可解释负载分解器，嵌入占用模式和负载-占用-天气关系。

Result: 在合成和真实数据集中表现优异，F1分数超0.8，RMSE在0.1-0.2间。

Conclusion: OccuEMBED为规模化占用中心建筑管理系统奠定基础，适应能源系统变革。

Abstract: Buildings account for a significant share of global energy consumption and
emissions, making it critical to operate them efficiently. As electricity grids
become more volatile with renewable penetration, buildings must provide
flexibility to support grid stability. Building automation plays a key role in
enhancing efficiency and flexibility via centralized operations, but it must
prioritize occupant-centric strategies to balance energy and comfort targets.
However, incorporating occupant information into large-scale, centralized
building operations remains challenging due to data limitations. We investigate
the potential of using whole-building smart meter data to infer both occupancy
and system operations. Integrating these insights into data-driven building
energy analysis allows more occupant-centric energy-saving and flexibility at
scale. Specifically, we propose OccuEMBED, a unified framework for occupancy
inference and system-level load analysis. It combines two key components: a
probabilistic occupancy profile generator, and a controllable and interpretable
load disaggregator supported by Kolmogorov-Arnold Networks (KAN). This design
embeds knowledge of occupancy patterns and load-occupancy-weather relationships
into deep learning models. We conducted comprehensive evaluations to
demonstrate its effectiveness across synthetic and real-world datasets compared
to various occupancy inference baselines. OccuEMBED always achieved average F1
scores above 0.8 in discrete occupancy inference and RMSE within 0.1-0.2 for
continuous occupancy ratios. We further demonstrate how OccuEMBED integrates
with building load monitoring platforms to display occupancy profiles, analyze
system-level operations, and inform occupant-responsive strategies. Our model
lays a robust foundation in scaling occupant-centric building management
systems to meet the challenges of an evolving energy system.

</details>

### [252] [Improving Local Air Quality Predictions Using Transfer Learning on Satellite Data and Graph Neural Networks](https://arxiv.org/abs/2505.05479)
*Finn Gueterbock,Raul Santos-Rodriguez,Jeffrey N. Clark*

Main category: eess.SP

TLDR: 提出了一种基于迁移学习和卫星气象数据的NO2浓度预测方法，在数据稀缺地区提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 城市地区NO2监测网络稀疏，亟需低成本高效的空气质量监测方法。

Method: 结合GraphSAGE框架，利用自回归和迁移学习，预训练于伦敦数据并应用于布里斯托尔。

Result: 相比基线模型，NRMSE降低8.6%，Gradient RMSE降低32.6%。

Conclusion: 虚拟传感器为空气质量监测提供了经济高效的解决方案，有助于气候与健康干预。

Abstract: Air pollution is a significant global health risk, contributing to millions
of premature deaths annually. Nitrogen dioxide (NO2), a harmful pollutant,
disproportionately affects urban areas where monitoring networks are often
sparse. We propose a novel method for predicting NO2 concentrations at
unmonitored locations using transfer learning with satellite and meteorological
data. Leveraging the GraphSAGE framework, our approach integrates
autoregression and transfer learning to enhance predictive accuracy in
data-scarce regions like Bristol. Pre-trained on data from London, UK, our
model achieves a 8.6% reduction in Normalised Root Mean Squared Error (NRMSE)
and a 32.6% reduction in Gradient RMSE compared to a baseline model. This work
demonstrates the potential of virtual sensors for cost-effective air quality
monitoring, contributing to actionable insights for climate and health
interventions.

</details>

### [253] [ECGDeDRDNet: A deep learning-based method for Electrocardiogram noise removal using a double recurrent dense network](https://arxiv.org/abs/2505.05477)
*Sainan xiao,Wangdong Yang,Buwen Cao,Jintao Wu*

Main category: eess.SP

TLDR: 提出了一种基于深度学习的ECG去噪框架ECGDeDRDNet，采用双循环密集网络架构，结合LSTM和DenseNet块，通过双重循环机制提升噪声抑制效果。


<details>
  <summary>Details</summary>
Motivation: ECG信号常受基线漂移、肌肉伪影和电极运动等噪声干扰，影响诊断准确性，需高效去噪方法。

Method: 使用双循环方案，结合LSTM和DenseNet块处理ECG波形，并迭代反馈估计的干净ECG图像以利用时空特征。

Result: 在MIT-BIH数据集上，PSNR和SSIM优于传统图像去噪方法，SNR和RMSE优于经典ECG去噪技术。

Conclusion: ECGDeDRDNet通过双重循环架构有效利用时空信息，显著提升ECG去噪性能。

Abstract: Electrocardiogram (ECG) signals are frequently corrupted by noise, such as
baseline wander (BW), muscle artifacts (MA), and electrode motion (EM), which
significantly degrade their diagnostic utility. To address this issue, we
propose ECGDeDRDNet, a deep learning-based ECG Denoising framework leveraging a
Double Recurrent Dense Network architecture. In contrast to traditional
approaches, we introduce a double recurrent scheme to enhance information reuse
from both ECG waveforms and the estimated clean image. For ECG waveform
processing, our basic model employs LSTM layers cascaded with DenseNet blocks.
The estimated clean ECG image, obtained by subtracting predicted noise
components from the noisy input, is iteratively fed back into the model. This
dual recurrent architecture enables comprehensive utilization of both temporal
waveform features and spatial image details, leading to more effective noise
suppression. Experimental results on the MIT-BIH dataset demonstrate that our
method achieves superior performance compared to conventional image denoising
methods in terms of PSNR and SSIM while also surpassing classical ECG denoising
techniques in both SNR and RMSE.

</details>

### [254] [A New k-Space Model for Non-Cartesian Fourier Imaging](https://arxiv.org/abs/2505.05647)
*Chin-Cheng Chan,Justin P. Haldar*

Main category: eess.SP

TLDR: 论文提出了一种新的傅里叶域基展开模型，以替代传统的基于体素的图像重建方法，解决了计算成本高、收敛慢和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于体素的模型存在计算成本高、收敛慢和伪影等长期问题，且可能忽略了近似性、周期性和零空间特性等新问题。

Method: 提出了一种基于傅里叶域基展开的新模型，替代传统的图像域体素方法。

Result: 新模型在非笛卡尔MRI重建中展示了改进的图像质量（减少伪影）和/或降低的计算复杂度（更快计算和更好收敛）。

Conclusion: 新模型比传统方法更具鲁棒性，能够有效解决其局限性。

Abstract: For the past several decades, it has been popular to reconstruct Fourier
imaging data using model-based approaches that can easily incorporate physical
constraints and advanced regularization/machine learning priors. The most
common modeling approach is to represent the continuous image as a linear
combination of shifted "voxel" basis functions. Although well-studied and
widely-deployed, this voxel-based model is associated with longstanding
limitations, including high computational costs, slow convergence, and a
propensity for artifacts. In this work, we reexamine this model from a fresh
perspective, identifying new issues that may have been previously overlooked
(including undesirable approximation, periodicity, and nullspace
characteristics). Our insights motivate us to propose a new model that is more
resilient to the limitations (old and new) of the previous approach.
Specifically, the new model is based on a Fourier-domain basis expansion rather
than the standard image-domain voxel-based approach. Illustrative results,
which are presented in the context of non-Cartesian MRI reconstruction,
demonstrate that the new model enables improved image quality (reduced
artifacts) and/or reduced computational complexity (faster computations and
improved convergence).

</details>

### [255] [Turbo-ICL: In-Context Learning-Based Turbo Equalization](https://arxiv.org/abs/2505.06175)
*Zihang Song,Matteo Zecchin,Bipin Rajendran,Osvaldo Simeone*

Main category: eess.SP

TLDR: 本文提出了一种基于大语言模型（LLM）的新型上下文学习（ICL）框架，用于编码多输入多输出（MIMO）系统中的软输入软输出信道均衡。通过提示增强和迭代优化，该框架显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统线性假设在低分辨率量化等场景下失效，需要一种更灵活的方法来优化信道均衡性能。

Method: 提出基于Transformer和状态空间架构的ICL模型，利用提示增强和迭代解码反馈优化符号估计。

Result: 在传统方法失效的场景下，ICL均衡器表现优于基线方法，且Transformer模型在训练多样性有限时表现更优，状态空间模型在资源受限时更高效。

Conclusion: ICL框架为信道均衡提供了新的解决方案，尤其在复杂场景下表现出色，两种模型各有优势。

Abstract: This paper introduces a novel in-context learning (ICL) framework, inspired
by large language models (LLMs), for soft-input soft-output channel
equalization in coded multiple-input multiple-output (MIMO) systems. The
proposed approach learns to infer posterior symbol distributions directly from
a prompt of pilot signals and decoder feedback. A key innovation is the use of
prompt augmentation to incorporate extrinsic information from the decoder
output as additional context, enabling the ICL model to refine its symbol
estimates iteratively across turbo decoding iterations. Two model variants,
based on Transformer and state-space architectures, are developed and
evaluated. Extensive simulations demonstrate that, when traditional linear
assumptions break down, e.g., in the presence of low-resolution quantization,
ICL equalizers consistently outperform conventional model-based baselines, even
when the latter are provided with perfect channel state information. Results
also highlight the advantage of Transformer-based models under limited training
diversity, as well as the efficiency of state-space models in
resource-constrained scenarios.

</details>

### [256] [Multi-User Beamforming with Deep Reinforcement Learning in Sensing-Aided Communication](https://arxiv.org/abs/2505.05956)
*Xiyu Wang,Gilberto Berardinelli,Hei Victor Cheng,Petar Popovski,Ramoni Adeogun*

Main category: eess.SP

TLDR: 论文研究了在毫米波通信中通过动态管理波束分配优化感知辅助通信的问题，提出了多波束方案和基于深度强化学习的优化方法。


<details>
  <summary>Details</summary>
Motivation: 解决毫米波通信中因波束漂移导致的波束失效问题，利用感知技术减少开销并提高效率。

Method: 引入多波束方案，结合深度强化学习优化波束分配策略，并与基于近似CRLB的启发式方法对比。

Result: 深度强化学习方法在吞吐量上优于传统波束扫描和启发式方法，且对不同用户速度具有鲁棒性。

Conclusion: 动态波束分配和深度强化学习的结合能有效提升毫米波通信性能。

Abstract: Mobile users are prone to experience beam failure due to beam drifting in
millimeter wave (mmWave) communications. Sensing can help alleviate beam
drifting with timely beam changes and low overhead since it does not need user
feedback. This work studies the problem of optimizing sensing-aided
communication by dynamically managing beams allocated to mobile users. A
multi-beam scheme is introduced, which allocates multiple beams to the users
that need an update on the angle of departure (AoD) estimates and a single beam
to the users that have satisfied AoD estimation precision. A deep reinforcement
learning (DRL) assisted method is developed to optimize the beam allocation
policy, relying only upon the sensing echoes. For comparison, a heuristic
AoD-based method using approximated Cram\'er-Rao lower bound (CRLB) for
allocation is also presented. Both methods require neither user feedback nor
prior state evolution information. Results show that the DRL-assisted method
achieves a considerable gain in throughput than the conventional beam sweeping
method and the AoD-based method, and it is robust to different user speeds.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [257] [Understanding Stragglers in Large Model Training Using What-if Analysis](https://arxiv.org/abs/2505.05713)
*Jinkun Lin,Ziheng Jiang,Zuquan Song,Sida Zhao,Menghan Yu,Zhanghan Wang,Chenyuan Wang,Zuocheng Shi,Xiang Shi,Wei Jia,Zherui Liu,Shuguang Wang,Haibin Lin,Xiu Liu,Aurojit Panda,Jinyang Li*

Main category: cs.DC

TLDR: 本文研究了大型语言模型（LLM）训练中的拖慢问题，通过五个月的跟踪数据分析了拖慢现象的频率、影响、模式及潜在原因。


<details>
  <summary>Details</summary>
Motivation: LLM训练对分布式计算要求极高，拖慢问题可能导致训练停滞，但原因复杂且非单一硬件故障。

Method: 采用假设分析方法，模拟无拖慢情况并与实际数据对比。

Result: 分析了拖慢对训练任务的影响、时空模式及潜在原因。

Conclusion: 拖慢问题在LLM训练中普遍存在，需进一步研究以优化性能。

Abstract: Large language model (LLM) training is one of the most demanding distributed
computations today, often requiring thousands of GPUs with frequent
synchronization across machines. Such a workload pattern makes it susceptible
to stragglers, where the training can be stalled by few slow workers. At
ByteDance we find stragglers are not trivially always caused by hardware
failures, but can arise from multiple complex factors. This work aims to
present a comprehensive study on the straggler issues in LLM training, using a
five-month trace collected from our ByteDance LLM training cluster. The core
methodology is what-if analysis that simulates the scenario without any
stragglers and contrasts with the actual case. We use this method to study the
following questions: (1) how often do stragglers affect training jobs, and what
effect do they have on job performance; (2) do stragglers exhibit temporal or
spatial patterns; and (3) what are the potential root causes for stragglers?

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [258] [Structure & Quality: Conceptual and Formal Foundations for the Mind-Body Problem](https://arxiv.org/abs/2505.05481)
*Ryan Williams*

Main category: q-bio.NC

TLDR: 论文从结构与质量的关系探索意识难题，提出信息论方法量化二者互确定性，并建立Q-S空间分析其保真度，进而分类五种关系。


<details>
  <summary>Details</summary>
Motivation: 解决意识难题，避免传统物理与心理的二分法，探索更基础的结构与质量关系。

Method: 开发信息论度量，构建Q-S空间分析结构与质量的保真度，提出五种关系分类。

Result: 揭示了五种结构与质量关系的分类，为功能主义、涌现论等哲学争论提供新视角。

Conclusion: 建立了定性系统演化的理论约束框架，为后续研究（如《Qualia & Natural Selection》）奠定基础。

Abstract: This paper explores the hard problem of consciousness from a different
perspective. Instead of drawing distinctions between the physical and the
mental, an exploration of a more foundational relationship is examined: the
relationship between structure and quality.
  Information-theoretic measures are developed to quantify the mutual
determinability between structure and quality, including a novel Q-S space for
analyzing fidelity between the two domains. This novel space naturally points
toward a five-fold categorization of possible relationships between structural
and qualitative properties, illustrating each through conceptual and formal
models.
  The ontological implications of each category are examined, shedding light on
debates around functionalism, emergentism, idealism, panpsychism, and neutral
monism.
  This new line of inquiry has established a framework for deriving theoretical
constraints on qualitative systems undergoing evolution that is explored in my
companion paper, Qualia & Natural Selection.

</details>

### [259] [Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience](https://arxiv.org/abs/2505.05515)
*Zinan Liu,Haoran Li,Jingyi Lu,Gaoyuan Ma,Xu Hong,Giovanni Iacca,Arvind Kumar,Shaojun Tang,Lin Wang*

Main category: q-bio.NC

TLDR: 论文提出了一种受神经科学启发的自主AI推理框架，旨在通过模拟人脑功能提升AI的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有自主AI的推理方法缺乏与生物推理的明确对比和理论基础，限制了其通用性和认知对齐。

Method: 基于神经科学的三个定义，提出统一框架，涵盖感知、维度、逻辑和交互四种推理类型，并应用于分析现有AI方法。

Result: 框架为AI推理提供了理论基础和实践路线图，支持构建更具通用性和认知对齐的智能体。

Conclusion: 通过结合认知神经科学与AI，论文为智能系统的自主推理提供了新方向，并提出了未来研究方向。

Abstract: Autonomous AI is no longer a hard-to-reach concept, it enables the agents to
move beyond executing tasks to independently addressing complex problems,
adapting to change while handling the uncertainty of the environment. However,
what makes the agents truly autonomous? It is agentic reasoning, that is
crucial for foundation models to develop symbolic logic, statistical
correlations, or large-scale pattern recognition to process information, draw
inferences, and make decisions. However, it remains unclear why and how
existing agentic reasoning approaches work, in comparison to biological
reasoning, which instead is deeply rooted in neural mechanisms involving
hierarchical cognition, multimodal integration, and dynamic interactions. In
this work, we propose a novel neuroscience-inspired framework for agentic
reasoning. Grounded in three neuroscience-based definitions and supported by
mathematical and biological foundations, we propose a unified framework
modeling reasoning from perception to action, encompassing four core types,
perceptual, dimensional, logical, and interactive, inspired by distinct
functional roles observed in the human brain. We apply this framework to
systematically classify and analyze existing AI reasoning methods, evaluating
their theoretical foundations, computational designs, and practical
limitations. We also explore its implications for building more generalizable,
cognitively aligned agents in physical and virtual environments. Finally,
building on our framework, we outline future directions and propose new
neural-inspired reasoning methods, analogous to chain-of-thought prompting. By
bridging cognitive neuroscience and AI, this work offers a theoretical
foundation and practical roadmap for advancing agentic reasoning in intelligent
systems. The associated project can be found at:
https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [260] [An Automated LLM-based Pipeline for Asset-Level Database Creation to Assess Deforestation Impact](https://arxiv.org/abs/2505.05494)
*Avanija Menon,Ovidiu Serban*

Main category: cs.DB

TLDR: 研究提出了一种自动化数据提取流程，利用LLMs和IRZ-CoT提示技术，结合RAV验证过程，显著提升了数据提取和验证的准确性，适用于高森林砍伐风险的行业。


<details>
  <summary>Details</summary>
Motivation: 欧盟森林砍伐法规（EUDR）要求企业证明其产品不导致森林砍伐，但现有数据库缺乏详细数据，限制了合规性和环境建模的准确性。

Method: 研究开发了一种端到端自动化数据提取流程，采用IRZ-CoT提示技术和RAV验证过程，应用于SEC EDGAR文件中的高森林砍伐风险行业。

Result: 相比传统零样本提示方法，该流程在数据提取准确性和验证覆盖率上有显著提升。

Conclusion: 该研究推动了NLP驱动的自动化在法规合规、CSR和ESG领域的应用，具有广泛的行业适用性。

Abstract: The European Union Deforestation Regulation (EUDR) requires companies to
prove their products do not contribute to deforestation, creating a critical
demand for precise, asset-level environmental impact data. Current databases
lack the necessary detail, relying heavily on broad financial metrics and
manual data collection, which limits regulatory compliance and accurate
environmental modeling. This study presents an automated, end-to-end data
extraction pipeline that uses LLMs to create, clean, and validate structured
databases, specifically targeting sectors with a high risk of deforestation.
The pipeline introduces Instructional, Role-Based, Zero-Shot Chain-of-Thought
(IRZ-CoT) prompting to enhance data extraction accuracy and a
Retrieval-Augmented Validation (RAV) process that integrates real-time web
searches for improved data reliability. Applied to SEC EDGAR filings in the
Mining, Oil & Gas, and Utilities sectors, the pipeline demonstrates significant
improvements over traditional zero-shot prompting approaches, particularly in
extraction accuracy and validation coverage. This work advances NLP-driven
automation for regulatory compliance, CSR (Corporate Social Responsibility),
and ESG, with broad sectoral applicability.

</details>

<div id='math.RT'></div>

# math.RT [[Back]](#toc)

### [261] [Representation gaps of rigid planar diagram monoids](https://arxiv.org/abs/2505.05846)
*Willow Stewart,Daniel Tubbenhauer*

Main category: math.RT

TLDR: 本文定义了非枢轴版本的Temperley-Lieb、Motzkin和平面rook幺半群，并计算了其非平凡简单表示的大小的界限。通过比较表示间隙和间隙比，评估了这两类幺半群在密码学中的适用性，结论是非枢轴幺半群通常不适合密码学用途。


<details>
  <summary>Details</summary>
Motivation: 研究非枢轴幺半群的性质及其在密码学中的潜在应用。

Method: 定义非枢轴幺半群，计算其简单表示的大小界限，并通过表示间隙和间隙比进行比较。

Result: 非枢轴幺半群的表示间隙和间隙比表明其密码学适用性较差。

Conclusion: 非枢轴幺半群在密码学中表现不佳，不适合作为密码学工具。

Abstract: We define non-pivotal analogs of the Temperley-Lieb, Motzkin, and planar rook
monoids, and compute bounds for the sizes of their nontrivial simple
representations. From this, we assess the two types of monoids in their
relative suitability for use in cryptography by comparing their representation
gaps and gap ratios. We conclude that the non-pivotal monoids are generally
worse for cryptographic purposes.

</details>

<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [262] [Evolutionary ecology of words](https://arxiv.org/abs/2505.05863)
*Reiji Suzuki,Takaya Arita*

Main category: q-bio.PE

TLDR: 该论文提出了一种基于大型语言模型（LLM）的词汇进化生态模型，通过模拟代理之间的互动和词汇替换，展示了多样化和无限交互选项的涌现与演化。初步实验验证了物种的多样化和生态适应性。


<details>
  <summary>Details</summary>
Motivation: 扩展进化博弈论和基于代理的模型，利用LLM的丰富语言表达能力，模拟词汇的进化生态过程。

Method: 代理在空间环境中移动，通过LLM生成的词汇互动，胜者替换败者的词汇，并引入词汇突变机制。

Result: 实验显示物种多样化的涌现，包括渐进式和间断平衡式演化，最终形成生态适应性强的主导物种。

Conclusion: 模型成功展示了词汇进化的多样性，为进化生态学和语言模型结合提供了新视角。

Abstract: We propose a model for the evolutionary ecology of words as one attempt to
extend evolutionary game theory and agent-based models by utilizing the rich
linguistic expressions of Large Language Models (LLMs). Our model enables the
emergence and evolution of diverse and infinite options for interactions among
agents. Within the population, each agent possesses a short word (or phrase)
generated by an LLM and moves within a spatial environment. When agents become
adjacent, the outcome of their interaction is determined by the LLM based on
the relationship between their words, with the loser's word being replaced by
the winner's. Word mutations, also based on LLM outputs, may occur. We
conducted preliminary experiments assuming that ``strong animal species" would
survive. The results showed that from an initial population consisting of
well-known species, many species emerged both gradually and in a punctuated
equilibrium manner. Each trial demonstrated the unique evolution of diverse
populations, with one type of large species becoming dominant, such as
terrestrial animals, marine life, or extinct species, which were ecologically
specialized and adapted ones across diverse extreme habitats. We also conducted
a long-term experiment with a large population, demonstrating the emergence and
coexistence of diverse species.

</details>

<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [263] [Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities](https://arxiv.org/abs/2505.06085)
*Hiari Pizzini Cavagna,Daniele Cesarini,Andrea Bartolini*

Main category: cs.PF

TLDR: 本文评估了Tenstorrent Grayskull e75 RISC-V加速器在低精度下的线性代数运算性能，并与NVIDIA GPU和Intel处理器进行了比较。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（如大语言模型）需求的增长，需要优化计算效率和能耗的专用硬件架构。

Method: 详细分析了Grayskull的执行模型、网格大小、矩阵维度、数据格式和数值精度对计算效率的影响。

Result: NVIDIA GPU在原始性能上占优，但Grayskull在功耗与计算吞吐量之间表现出竞争力，BF16精度下峰值达1.55 TFLOPs/Watt。

Conclusion: Grayskull在低精度线性代数运算中展示了高效的能耗比，适合生成式AI应用。

Abstract: The increasing demand for generative AI as Large Language Models (LLMs)
services has driven the need for specialized hardware architectures that
optimize computational efficiency and energy consumption. This paper evaluates
the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic
linear algebra kernels at reduced numerical precision, a fundamental operation
in LLM computations. We present a detailed characterization of Grayskull's
execution model, gridsize, matrix dimensions, data formats, and numerical
precision impact computational efficiency. Furthermore, we compare Grayskull's
performance against state-of-the-art architectures with tensor acceleration,
including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).
Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a
competitive trade-off between power consumption and computational throughput,
reaching a peak of 1.55 TFLOPs/Watt with BF16.

</details>