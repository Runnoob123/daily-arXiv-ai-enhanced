<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 22]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.CL](#cs.CL) [Total: 22]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.CV](#cs.CV) [Total: 68]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.CY](#cs.CY) [Total: 4]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.GR](#cs.GR) [Total: 6]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [math.OC](#math.OC) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.MA](#cs.MA) [Total: 4]
- [math.HO](#math.HO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Pseudo Random Number Generator using Internet-of-Things Techniques on Portable Field-Programmable-Gate-Array Platform](https://arxiv.org/abs/2505.03741)
*Tee Hui Teo*

Main category: cs.CR

TLDR: 比较三种基于IoT的PRNG模型（Logistic Map、Double Pendulum和Multi-LFSR）的性能，发现Logistic Map和Double Pendulum适合高安全性应用，而Multi-LFSR适合嵌入式或实时应用。


<details>
  <summary>Details</summary>
Motivation: 研究不同PRNG模型在IoT平台上的性能差异，为选择适合的模型提供依据。

Method: 在FPGA平台上实现三种PRNG模型，比较其随机性、延迟、功耗、硬件资源使用等指标。

Result: Logistic Map和Double Pendulum提供高质量随机性但资源消耗高；Multi-LFSR资源高效但安全性较低。

Conclusion: 根据应用需求选择PRNG模型，高安全性选Logistic Map或Double Pendulum，高效能选Multi-LFSR。

Abstract: This paper conducts a comparative study of three IoT-based PRNG models,
including Logistic Map, Double Pendulum, and Multi-LFSR, implemented on an FPGA
platform. Comparisons are made across key performance metrics like randomness,
latency, power consumption, hardware resource usage, energy efficiency,
scalability, and application suitability. Compared to Multi-LFSR, Logistic Map,
and Double Pendulum Models provide perfect quality randomness, which is quite
apt for high-security grade applications; however, the requirements of these
models concerning power and hardware resources are also considerably high. By
contrast, the Multi-LFSR comes into its own due to its lower latency, power
consumption, and resource-efficient design. It is, therefore, suited for
embedded or real-time applications. Furthermore, environmental sensors will
also be introduced as entropy sources for the PRNGs to enhance the randomness
of the systems, particularly in IoT-enabled battery-powered FPGA platforms. The
experimental results confirm that the Multi-LFSR model has the highest energy
efficiency, while the Logistic Map and Double Pendulum outperform in generating
numbers with very high security. The study thus provides a deeper insight into
decision-making for selecting PRNG models.

</details>

### [2] [Hardware-Enabled Mechanisms for Verifying Responsible AI Development](https://arxiv.org/abs/2505.03742)
*Aidan O'Gara,Gabriel Kulp,Will Hodgkins,James Petrie,Vincent Immler,Aydin Aysu,Kanad Basu,Shivam Bhasin,Stjepan Picek,Ankur Srivastava*

Main category: cs.CR

TLDR: 硬件支持机制（HEMs）可促进AI开发的透明度和安全性，但需进一步研究解决实施问题。


<details>
  <summary>Details</summary>
Motivation: AI能力的提升带来机遇与风险，需通过技术手段确保AI开发的透明与安全。

Method: 利用硬件支持机制（HEMs）实现AI训练活动的可验证报告和政策执行。

Result: HEMs可提升透明度、安全性，并解决隐私和知识产权问题。

Conclusion: 需进一步研究以开发稳健、可扩展的HEMs解决方案。

Abstract: Advancements in AI capabilities, driven in large part by scaling up computing
resources used for AI training, have created opportunities to address major
global challenges but also pose risks of misuse. Hardware-enabled mechanisms
(HEMs) can support responsible AI development by enabling verifiable reporting
of key properties of AI training activities such as quantity of compute used,
training cluster configuration or location, as well as policy enforcement. Such
tools can promote transparency and improve security, while addressing privacy
and intellectual property concerns. Based on insights from an interdisciplinary
workshop, we identify open questions regarding potential implementation
approaches, emphasizing the need for further research to ensure robust,
scalable solutions.

</details>

### [3] [Implementation of Shor Algorithm: Factoring a 4096-Bit Integer Under Specific Constraints](https://arxiv.org/abs/2505.03743)
*Abel C. H. Chen*

Main category: cs.CR

TLDR: 论文探讨了量子芯片技术进步对减少量子计算错误率的影响，并研究了适用于实际应用的量子算法设计，重点改进了Shor算法的模计算效率，成功分解了4096位整数。


<details>
  <summary>Details</summary>
Motivation: 量子芯片技术的进步（如Willow）降低了量子计算错误率，推动了量子计算的实际应用，因此设计适用于现实场景的量子算法成为关键研究方向。

Method: 研究聚焦于Shor算法的实现，旨在提高模计算效率，并在特定约束下演示4096位整数的分解。

Result: 实验结果表明，与现有最优方法相比，算法效率显著提升，并能分解更长的整数。

Conclusion: 研究证明了改进的Shor算法在量子计算中的高效性和实用性，为量子计算的实际应用提供了支持。

Abstract: In recent years, advancements in quantum chip technology, such as Willow,
have contributed to reducing quantum computation error rates, potentially
accelerating the practical adoption of quantum computing. As a result, the
design of quantum algorithms suitable for real-world applications has become a
crucial research direction. This study focuses on the implementation of Shor
algorithm, aiming to improve modular computation efficiency and demonstrate the
factorization of a 4096-bit integer under specific constraints. Experimental
results, when compared with state-of-the-art (SOTA) methods, indicate a
significant improvement in efficiency while enabling the factorization of
longer integers.

</details>

### [4] [From Concept to Measurement: A Survey of How the Blockchain Trilemma Can Be Analyzed](https://arxiv.org/abs/2505.03768)
*Mansur Aliyu,Niclas Kannengießer,Sunyaev Ali*

Main category: cs.CR

TLDR: 论文探讨了区块链三难问题（去中心化、可扩展性和安全性）的权衡，提出了分析方法和度量标准，以帮助识别帕累托最优配置。


<details>
  <summary>Details</summary>
Motivation: 区块链系统需要在去中心化、可扩展性和安全性之间权衡，但缺乏明确的分析框架和度量标准。

Method: 综述了分析方法，明确了相关概念及其度量标准，并评估了这些标准在不同区块链系统中的适用性。

Result: 提供了理论框架，帮助更深入地分析区块链三难问题，并为实践者识别帕累托最优配置提供了支持。

Conclusion: 通过澄清概念和度量标准，为区块链系统的优化配置奠定了理论基础。

Abstract: To meet non-functional requirements, practitioners must identify
Pareto-optimal configurations of the degree of decentralization, scalability,
and security of blockchain systems. Maximizing all of these subconcepts is,
however, impossible due to the trade-offs highlighted by the blockchain
trilemma. We reviewed analysis approaches to identify constructs and their
operationalization through metrics for analyzing the blockchain trilemma
subconcepts and to assess the applicability of the operationalized constructs
to various blockchain systems. By clarifying these constructs and metrics, this
work offers a theoretical foundation for more sophisticated investigations into
how the blockchain trilemma manifests in blockchain systems, helping
practitioners identify Pareto-optimal configurations.

</details>

### [5] [AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection](https://arxiv.org/abs/2505.03796)
*Lokesh Koli,Shubham Kalra,Rohan Thakur,Anas Saifi,Karanpreet Singh*

Main category: cs.CR

TLDR: 本文提出了一种AI驱动的内部风险管理（IRM）系统，通过行为分析、动态风险评分和实时策略执行，显著提升了内部威胁的检测精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的检测系统难以应对内部威胁的隐蔽性和情境性，亟需更智能的解决方案。

Method: 采用混合评分机制，结合静态PRISM模型和基于自动编码器神经网络的AI模型，通过迭代反馈和持续学习优化性能。

Result: 系统将误报率降低59%，真阳性检测率提高30%，每日可处理1000万日志事件，响应时间缩短47%。

Conclusion: 该IRM系统为内部风险提供了可扩展的主动防御框架，未来将通过可解释AI等技术进一步提升其适应性和透明度。

Abstract: Insider threats pose a significant challenge to organizational security,
often evading traditional rule-based detection systems due to their subtlety
and contextual nature. This paper presents an AI-powered Insider Risk
Management (IRM) system that integrates behavioral analytics, dynamic risk
scoring, and real-time policy enforcement to detect and mitigate insider
threats with high accuracy and adaptability. We introduce a hybrid scoring
mechanism - transitioning from the static PRISM model to an adaptive AI-based
model utilizing an autoencoder neural network trained on expert-annotated user
activity data. Through iterative feedback loops and continuous learning, the
system reduces false positives by 59% and improves true positive detection
rates by 30%, demonstrating substantial gains in detection precision.
Additionally, the platform scales efficiently, processing up to 10 million log
events daily with sub-300ms query latency, and supports automated enforcement
actions for policy violations, reducing manual intervention. The IRM system's
deployment resulted in a 47% reduction in incident response times, highlighting
its operational impact. Future enhancements include integrating explainable AI,
federated learning, graph-based anomaly detection, and alignment with Zero
Trust principles to further elevate its adaptability, transparency, and
compliance-readiness. This work establishes a scalable and proactive framework
for mitigating emerging insider risks in both on-premises and hybrid
environments.

</details>

### [6] [Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning](https://arxiv.org/abs/2505.03817)
*Aditya Shinde,Prashant Doshi*

Main category: cs.CR

TLDR: 该论文提出了一种基于逆向强化学习（IRL）的系统级审计日志攻击者偏好建模方法，揭示了攻击者的行为偏好可作为独特的威胁归因特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖不断更新的攻击工具和技术来追踪威胁行为者，但攻击者的行为偏好具有内在稳定性，可作为更可靠的归因依据。

Method: 通过攻击溯源图从审计日志中提取状态-动作轨迹，利用IRL建模攻击者的未知行为偏好。

Result: 实验证明，低级别取证数据能自动揭示攻击者的主观偏好，这些偏好具有不变性，可作为独特的行为签名。

Conclusion: 攻击者的行为偏好是内在且稳定的，可作为威胁归因的新维度，提升网络安全防御能力。

Abstract: This paper presents a holistic approach to attacker preference modeling from
system-level audit logs using inverse reinforcement learning (IRL). Adversary
modeling is an important capability in cybersecurity that lets defenders
characterize behaviors of potential attackers, which enables attribution to
known cyber adversary groups. Existing approaches rely on documenting an
ever-evolving set of attacker tools and techniques to track known threat
actors. Although attacks evolve constantly, attacker behavioral preferences are
intrinsic and less volatile. Our approach learns the behavioral preferences of
cyber adversaries from forensics data on their tools and techniques. We model
the attacker as an expert decision-making agent with unknown behavioral
preferences situated in a computer host. We leverage attack provenance graphs
of audit logs to derive a state-action trajectory of the attack. We test our
approach on open datasets of audit logs containing real attack data. Our
results demonstrate for the first time that low-level forensics data can
automatically reveal an adversary's subjective preferences, which serves as an
additional dimension to modeling and documenting cyber adversaries. Attackers'
preferences tend to be invariant despite their different tools and indicate
predispositions that are inherent to the attacker. As such, these inferred
preferences can potentially serve as unique behavioral signatures of attackers
and improve threat attribution.

</details>

### [7] [A Comprehensive Analysis of Adversarial Attacks against Spam Filters](https://arxiv.org/abs/2505.03831)
*Esra Hotoğlu,Sevil Sen,Burcu Can*

Main category: cs.CR

TLDR: 研究探讨了对抗攻击对深度学习垃圾邮件检测系统的影响，评估了六种模型并提出新评分函数以提高安全性。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击日益复杂，威胁深度学习垃圾邮件过滤系统的有效性，需研究其影响并改进安全性。

Method: 使用真实数据集评估六种深度学习模型，分析词、字符、句子和AI生成段落级别的攻击，并引入新评分函数。

Result: 揭示了垃圾邮件过滤器的漏洞，新评分函数提高了对抗攻击的有效性。

Conclusion: 研究为改进垃圾邮件过滤器的安全性提供了重要见解，应对抗不断演变的对抗威胁。

Abstract: Deep learning has revolutionized email filtering, which is critical to
protect users from cyber threats such as spam, malware, and phishing. However,
the increasing sophistication of adversarial attacks poses a significant
challenge to the effectiveness of these filters. This study investigates the
impact of adversarial attacks on deep learning-based spam detection systems
using real-world datasets. Six prominent deep learning models are evaluated on
these datasets, analyzing attacks at the word, character sentence, and
AI-generated paragraph-levels. Novel scoring functions, including spam weights
and attention weights, are introduced to improve attack effectiveness. This
comprehensive analysis sheds light on the vulnerabilities of spam filters and
contributes to efforts to improve their security against evolving adversarial
threats.

</details>

### [8] [Economic Security of Multiple Shared Security Protocols](https://arxiv.org/abs/2505.03843)
*Abhimanyu Nag,Dhruv Bodani,Abhishek Kumar*

Main category: cs.CR

TLDR: 论文分析了多共享安全提供商（SSP）环境下的主动验证服务（AVS）安全问题，比较了两种架构模型（M和S），并提出了安全性和攻击成本的优化方案。


<details>
  <summary>Details</summary>
Motivation: 随着区块链生态中重质押协议的普及，AVS需要跨多个SSP运行，导致质押碎片化，增加了安全风险。本文旨在解决这一问题。

Method: 通过凸优化和博弈论方法，分析了两种模型（M和S），推导了效用边界、攻击成本条件和市场均衡。

Result: 模型M具有部署灵活性但易受低成本攻击，模型S通过单一验证集和聚合惩罚逻辑提供更强的安全保障。

Conclusion: 未来研究方向包括设计激励兼容的质押再平衡分配机制。

Abstract: As restaking protocols gain adoption across blockchain ecosystems, there is a
need for Actively Validated Services (AVSs) to span multiple Shared Security
Providers (SSPs). This leads to stake fragmentation which introduces new
complications where an adversary may compromise an AVS by targeting its weakest
SSP. In this paper, we formalize the Multiple SSP Problem and analyze two
architectures : an isolated fragmented model called Model $\mathbb{M}$ and a
shared unified model called Model $\mathbb{S}$, through a convex optimization
and game-theoretic lens. We derive utility bounds, attack cost conditions, and
market equilibrium that describes protocol security for both models. Our
results show that while Model $\mathbb{M}$ offers deployment flexibility, it
inherits lowest-cost attack vulnerabilities, whereas Model $\mathbb{S}$
achieves tighter security guarantees through single validator sets and
aggregated slashing logic. We conclude with future directions of work including
an incentive-compatible stake rebalancing allocation in restaking ecosystems.

</details>

### [9] [Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles](https://arxiv.org/abs/2505.03850)
*Hanlin Chen,Simin Chen,Wenyu Li,Wei Yang,Yiheng Feng*

Main category: cs.CR

TLDR: 本文提出了一种基于推理时间攻击的自动驾驶车辆影响分析方法，并通过仿真系统展示了此类攻击对自车及其他交通参与者安全的威胁。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）作为安全关键的网络物理系统，其网络安全及相关安全问题一直是重要研究课题。感知模块是最易受攻击的表面之一，因为驾驶员和AV无法控制外部环境。当前大多数研究关注感知正确性，而本文则聚焦推理时间攻击的影响。

Method: 提出了一种基于推理时间攻击的影响分析方法，并在仿真系统中进行了验证。

Result: 研究表明，推理时间攻击不仅威胁自车的安全，还可能对其他交通参与者造成危害。

Conclusion: 本文强调了推理时间攻击对自动驾驶车辆安全的潜在威胁，为相关安全研究提供了新的视角。

Abstract: As a safety-critical cyber-physical system, cybersecurity and related safety
issues for Autonomous Vehicles (AVs) have been important research topics for a
while. Among all the modules on AVs, perception is one of the most accessible
attack surfaces, as drivers and AVs have no control over the outside
environment. Most current work targeting perception security for AVs focuses on
perception correctness. In this work, we propose an impact analysis based on
inference time attacks for autonomous vehicles. We demonstrate in a simulation
system that such inference time attacks can also threaten the safety of both
the ego vehicle and other traffic participants.

</details>

### [10] [Data-Driven Falsification of Cyber-Physical Systems](https://arxiv.org/abs/2505.03863)
*Atanu Kundu,Sauvik Gon,Rajarshi Ray*

Main category: cs.CR

TLDR: 本文提出了一种框架，将CPS的伪造问题与DNN的伪造问题联系起来，并利用决策树的可解释性加速CPS的伪造。


<details>
  <summary>Details</summary>
Motivation: CPS在安全关键领域广泛应用，但其操作安全的正式验证至关重要。本文专注于寻找系统中的不安全执行而非证明其不存在。

Method: 构建CPS的替代模型（DNN或决策树），应用DNN伪造工具，并提出一种基于决策树解释的新型伪造算法。

Result: 框架在CPS中有效检测难以发现的反例，并在ARCH-COMP 2024基准测试中表现出色。

Conclusion: 该框架通过结合DNN伪造工具和决策树解释，显著提升了CPS伪造的效率和效果。

Abstract: Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as
healthcare, avionics, and autonomous vehicles. Formal verification of their
operational safety is, therefore, of utmost importance. In this paper, we
address the falsification problem, where the focus is on searching for an
unsafe execution in the system instead of proving their absence. The
contribution of this paper is a framework that (a) connects the falsification
of CPS with the falsification of deep neural networks (DNNs) and (b) leverages
the inherent interpretability of Decision Trees for faster falsification of
CPS. This is achieved by: (1) building a surrogate model of the CPS under test,
either as a DNN model or a Decision Tree, (2) application of various DNN
falsification tools to falsify CPS, and (3) a novel falsification algorithm
guided by the explanations of safety violations of the CPS model extracted from
its Decision Tree surrogate. The proposed framework has the potential to
exploit a repertoire of \emph{adversarial attack} algorithms designed to
falsify robustness properties of DNNs, as well as state-of-the-art
falsification algorithms for DNNs. Although the presented methodology is
applicable to systems that can be executed/simulated in general, we demonstrate
its effectiveness, particularly in CPS. We show that our framework, implemented
as a tool \textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS
that have linear and non-linear dynamics. Decision tree-guided falsification
shows promising results in efficiently finding multiple counterexamples in the
ARCH-COMP 2024 falsification benchmarks~\cite{khandait2024arch}.

</details>

### [11] [AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience](https://arxiv.org/abs/2505.03945)
*Shamnad Mohamed Shaffi,Sunish Vengathattil,Jezeena Nikarthil Sidhick,Resmi Vijayan*

Main category: cs.CR

TLDR: 本文探讨了人工智能如何通过预测分析、行为检测和加密技术提升云安全，同时指出了传统模型的不足及AI的改进。


<details>
  <summary>Details</summary>
Motivation: 由于云计算中威胁日益复杂，传统安全方案难以实时应对，AI被视为提升云安全的关键技术。

Method: 应用预测分析、行为检测和AI驱动的加密技术，分析AI如何改进传统安全模型。

Result: AI能有效提升云安全，但仍需解决隐私、偏见和合规性问题，未来可与区块链等技术结合。

Conclusion: AI在云安全中具有潜力，但需进一步研究其可靠性、模块化和伦理问题。

Abstract: Cloud security concerns have been greatly realized in recent years due to the
increase of complicated threats in the computing world. Many traditional
solutions do not work well in real-time to detect or prevent more complex
threats. Artificial intelligence is today regarded as a revolution in
determining a protection plan for cloud data architecture through machine
learning, statistical visualization of computing infrastructure, and detection
of security breaches followed by counteraction. These AI-enabled systems make
work easier as more network activities are scrutinized, and any anomalous
behavior that might be a precursor to a more serious breach is prevented. This
paper examines ways AI can enhance cloud security by applying predictive
analytics, behavior-based security threat detection, and AI-stirring
encryption. It also outlines the problems of the previous security models and
how AI overcomes them. For a similar reason, issues like data privacy, biases
in the AI model, and regulatory compliance are also covered. So, AI improves
the protection of cloud computing contexts; however, more efforts are needed in
the subsequent phases to extend the technology's reliability, modularity, and
ethical aspects. This means that AI can be blended with other new computing
technologies, including blockchain, to improve security frameworks further. The
paper discusses the current trends in securing cloud data architecture using AI
and presents further research and application directions.

</details>

### [12] [Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs [Technical Report]](https://arxiv.org/abs/2505.04014)
*David Chu,Aditya Balasubramanian,Dee Bao,Natacha Crooks,Heidi Howard,Lucky E. Katahanas,Soujanya Ponnapalli*

Main category: cs.CR

TLDR: Rollbaccine是一种设备映射器，通过确保磁盘一致性自动为所有应用程序提供回滚攻击防护，性能开销低且优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 现代TEEs无法防止磁盘回滚攻击，现有解决方案要么仅支持部分应用，要么需要代码修改。

Method: Rollbaccine拦截并复制磁盘写入操作，利用备份恢复丢失状态，并通过弱多线程语义最小化开销。

Result: 在PostgreSQL和HDFS等实际应用中，Rollbaccine仅增加19%的开销，性能优于现有方案208倍。

Conclusion: Rollbaccine为所有应用程序提供自动回滚防护，且性能高效。

Abstract: Today, users can "lift-and-shift" unmodified applications into modern,
VM-based Trusted Execution Environments (TEEs) in order to gain hardware-based
security guarantees. However, TEEs do not protect applications against disk
rollback attacks, where persistent storage can be reverted to an earlier state
after a crash; existing rollback resistance solutions either only support a
subset of applications or require code modification. Our key insight is that
restoring disk consistency after a rollback attack guarantees rollback
resistance for any application. We present Rollbaccine, a device mapper that
provides automatic rollback resistance for all applications by provably
preserving disk consistency. Rollbaccine intercepts and replicates writes to
disk, restores lost state from backups during recovery, and minimizes overheads
by taking advantage of the weak, multi-threaded semantics of disk operations.
Across benchmarks over two real applications (PostgreSQL and HDFS) and two file
systems (ext4 and xfs), Rollbaccine adds only 19% overhead, except for the
fsync-heavy Filebench Varmail. In addition, Rollbaccine outperforms the
state-of-the-art, non-automatic rollback resistant solution by $208\times$.

</details>

### [13] [MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models](https://arxiv.org/abs/2505.04015)
*Soheil Zibakhsh Shabgahi,Yaman Jandali,Farinaz Koushanfar*

Main category: cs.CR

TLDR: MergeGuard是一种新颖的方法，用于减轻AI木马攻击，通过线性化和合并全连接层，同时提高模型的通用性和性能。


<details>
  <summary>Details</summary>
Motivation: AI木马攻击会导致带有触发器的输入被错误分类，对由不可信第三方训练的模型构成威胁。

Method: 提出一种后训练方法，线性化和合并全连接层。

Result: 在Transformer模型上的概念验证表明，MergeGuard保持模型准确性，同时降低木马攻击成功率，优于常见的微调方法。

Conclusion: MergeGuard是一种有效的后训练木马攻击缓解方法。

Abstract: This paper proposes MergeGuard, a novel methodology for mitigation of AI
Trojan attacks. Trojan attacks on AI models cause inputs embedded with triggers
to be misclassified to an adversary's target class, posing a significant threat
to model usability trained by an untrusted third party. The core of MergeGuard
is a new post-training methodology for linearizing and merging fully connected
layers which we show simultaneously improves model generalizability and
performance. Our Proof of Concept evaluation on Transformer models demonstrates
that MergeGuard maintains model accuracy while decreasing trojan attack success
rate, outperforming commonly used (post-training) Trojan mitigation by
fine-tuning methodologies.

</details>

### [14] [SolPhishHunter: Towards Detecting and Understanding Phishing on Solana](https://arxiv.org/abs/2505.04094)
*Ziwei Li,Zigui Jiang,Ming Fang,Jiaxin Chen,Zhiying Wu,Jiajing Wu,Lun Zhang,Zibin Zheng*

Main category: cs.CR

TLDR: 论文研究了Solana区块链平台上的新型钓鱼攻击（SolPhish），定义了三种类型，并开发了检测工具SolPhishHunter，发现了8,058例攻击，造成约110万美元损失。


<details>
  <summary>Details</summary>
Motivation: Solana平台的快速增长吸引了恶意行为者，其独特的账户和交易设计催生了新型钓鱼攻击（SolPhish），需研究其类型和影响。

Method: 定义了三种SolPhish类型，开发了检测工具SolPhishHunter，并分析了8,058例攻击的分布、影响及攻击者特征。

Result: 检测到8,058例SolPhish攻击，造成约110万美元损失，并构建了首个Solana钓鱼相关数据集SolPhishDataset。

Conclusion: 研究揭示了SolPhish的危害性，为社区提供了检测工具和数据集，有助于未来防御措施的发展。

Abstract: Solana is a rapidly evolving blockchain platform that has attracted an
increasing number of users. However, this growth has also drawn the attention
of malicious actors, with some phishers extending their reach into the Solana
ecosystem. Unlike platforms such as Ethereum, Solana has distinct designs of
accounts and transactions, leading to the emergence of new types of phishing
transactions that we term SolPhish. We define three types of SolPhish and
develop a detection tool called SolPhishHunter. Utilizing SolPhishHunter, we
detect a total of 8,058 instances of SolPhish and conduct an empirical analysis
of these detected cases. Our analysis explores the distribution and impact of
SolPhish, the characteristics of the phishers, and the relationships among
phishing gangs. Particularly, the detected SolPhish transactions have resulted
in nearly \$1.1 million in losses for victims. We report our detection results
to the community and construct SolPhishDataset, the \emph{first} Solana
phishing-related dataset in academia.

</details>

### [15] [LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling](https://arxiv.org/abs/2505.04101)
*AbdulAziz AbdulGhaffar,Ashraf Matrawy*

Main category: cs.CR

TLDR: 该论文探讨了大型语言模型（LLMs）在网络安全性中的适用性，特别是通过STRIDE威胁建模案例研究，发现需要对LLMs进行调整和微调以适应网络安全用例。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在网络安全性中有广泛应用，但LLMs在此领域的适用性研究几乎空白，论文旨在填补这一空白。

Method: 使用四种提示技术和五种LLMs对5G威胁进行STRIDE分类，并通过评估结果分析LLMs的行为。

Result: 数值结果和详细见解表明，LLMs在建模某些威胁时存在行为差异，需要调整和微调。

Conclusion: LLMs在网络安全中具有潜力，但需进一步优化以适应特定用例。

Abstract: Artificial Intelligence (AI) is expected to be an integral part of
next-generation AI-native 6G networks. With the prevalence of AI, researchers
have identified numerous use cases of AI in network security. However, there
are almost nonexistent studies that analyze the suitability of Large Language
Models (LLMs) in network security. To fill this gap, we examine the suitability
of LLMs in network security, particularly with the case study of STRIDE threat
modeling. We utilize four prompting techniques with five LLMs to perform STRIDE
classification of 5G threats. From our evaluation results, we point out key
findings and detailed insights along with the explanation of the possible
underlying factors influencing the behavior of LLMs in the modeling of certain
threats. The numerical results and the insights support the necessity for
adjusting and fine-tuning LLMs for network security use cases.

</details>

### [16] [A Framework to Prevent Biometric Data Leakage in the Immersive Technologies Domain](https://arxiv.org/abs/2505.04123)
*Keshav Sood,Iynkaran Natgunanathan,Uthayasanker Thayasivam,Vithurabiman Senthuran,Xiaoning Zhang,Shui Yu*

Main category: cs.CR

TLDR: 论文探讨了沉浸式技术中的数据隐私风险，并提出了一种技术框架来防止敏感数据泄露。


<details>
  <summary>Details</summary>
Motivation: 沉浸式技术（如3D头显）存在隐私泄露风险，如通过语音命令和面部动态捕捉用户敏感信息，目前相关研究较少。

Method: 开发了一个简单的技术框架，用于减轻沉浸式技术中的敏感数据（如生物特征数据）泄露问题。

Result: 通过六个数据集的性能评估，证明所提解决方案有效且可行。

Conclusion: 该技术框架能有效防止沉浸式技术中的隐私泄露问题。

Abstract: Doubtlessly, the immersive technologies have potential to ease people's life
and uplift economy, however the obvious data privacy risks cannot be ignored.
For example, a participant wears a 3D headset device which detects
participant's head motion to track the pose of participant's head to match the
orientation of camera with participant's eyes positions in the real-world. In a
preliminary study, researchers have proved that the voice command features on
such headsets could lead to major privacy leakages. By analyzing the facial
dynamics captured with the motion sensors, the headsets suffer security
vulnerabilities revealing a user's sensitive speech without user's consent. The
psychography data (such as voice command features, facial dynamics, etc.) is
sensitive data and it should not be leaked out of the device without users
consent else it is a privacy breach. To the best of our literature review, the
work done in this particular research problem is very limited. Motivated from
this, we develop a simple technical framework to mitigate sensitive data (or
biometric data) privacy leaks in immersive technology domain. The performance
evaluation is conducted in a robust way using six data sets, to show that the
proposed solution is effective and feasible to prevent this issue.

</details>

### [17] [Privacy Challenges In Image Processing Applications](https://arxiv.org/abs/2505.04181)
*Maneesha,Bharat Gupta,Rishabh Sethi,Charvi Adita Das*

Main category: cs.CR

TLDR: 本文探讨了图像处理中的隐私问题，并综述了差分隐私、安全多方计算、同态加密和匿名化等隐私保护技术，同时指出了隐私与效用平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着图像处理系统的普及，隐私问题日益突出，尤其是医疗和监控等领域中的敏感数据。

Method: 综述了差分隐私、安全多方计算、同态加密和匿名化等技术。

Result: 指出了隐私保护与数据效用之间的平衡问题，并提出了未来研究方向。

Conclusion: 需要结合技术创新、伦理考量和政策框架，以应对图像处理快速发展中的隐私挑战。

Abstract: As image processing systems proliferate, privacy concerns intensify given the
sensitive personal information contained in images. This paper examines privacy
challenges in image processing and surveys emerging privacy-preserving
techniques including differential privacy, secure multiparty computation,
homomorphic encryption, and anonymization. Key applications with heightened
privacy risks include healthcare, where medical images contain patient health
data, and surveillance systems that can enable unwarranted tracking.
Differential privacy offers rigorous privacy guarantees by injecting controlled
noise, while MPC facilitates collaborative analytics without exposing raw data
inputs. Homomorphic encryption enables computations on encrypted data and
anonymization directly removes identifying elements. However, balancing privacy
protections and utility remains an open challenge. Promising future directions
identified include quantum-resilient cryptography, federated learning,
dedicated hardware, and conceptual innovations like privacy by design.
Ultimately, a holistic effort combining technological innovations, ethical
considerations, and policy frameworks is necessary to uphold the fundamental
right to privacy as image processing capabilities continue advancing rapidly.

</details>

### [18] [AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities](https://arxiv.org/abs/2505.04195)
*Minjae Seo,Wonwoo Choi,Myoungsung You,Seungwon Shin*

Main category: cs.CR

TLDR: AutoPatch是一个多智能体框架，用于修补LLM生成的易受攻击代码，特别是那些在LLM知识截止日期后引入的漏洞。它结合了检索增强生成（RAG）和结构化漏洞数据库，通过语义和污点分析识别相关CVE，并利用增强的Chain-of-Thought推理进行验证和修补。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码生成中存在漏洞修复的局限性，尤其是对新披露的CVE缺乏知识，且传统微调方法成本高昂。

Method: AutoPatch整合RAG与结构化漏洞数据库，结合语义和污点分析，使用增强的Chain-of-Thought推理构建提示进行验证和修补。

Result: AutoPatch在CVE匹配中达到90.4%准确率，漏洞验证F1分数为89.5%，修补准确率为95.0%，且成本效益比传统微调高50倍。

Conclusion: AutoPatch有效解决了LLM在漏洞修复中的局限性，显著提升了修补效率和成本效益。

Abstract: Large Language Models (LLMs) have emerged as promising tools in software
development, enabling automated code generation and analysis. However, their
knowledge is limited to a fixed cutoff date, making them prone to generating
code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets
is costly, and existing LLM-based approaches focus on oversimplified CWE
examples and require providing explicit bug locations to LLMs, limiting their
ability to patch complex real-world vulnerabilities. To address these
limitations, we propose AutoPatch, a multi-agent framework designed to patch
vulnerable LLM-generated code, particularly those introduced after the LLMs'
knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG)
with a structured database of recently disclosed vulnerabilities, comprising
525 code snippets derived from 75 high-severity CVEs across real-world systems
such as the Linux kernel and Chrome. AutoPatch combines semantic and taint
analysis to identify the most relevant CVE and leverages enhanced
Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification
and patching. Our unified similarity model, which selects the most relevant
vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch
attains 89.5 percent F1-score for vulnerability verification and 95.0 percent
accuracy in patching, while being over 50x more cost-efficient than traditional
fine-tuning approaches.

</details>

### [19] [On the Vulnerability of Underwater Magnetic Induction Communication](https://arxiv.org/abs/2505.04249)
*Muhammad Muzzammil,Waqas Aman,Irfan Ullah,Shang Zhigang,Saif Al-Kuwari,Zhou Tian,Marwa Qaraqe*

Main category: cs.CR

TLDR: 研究了水下磁感应通信中的窃听攻击，通过仿真和实验验证了其脆弱性，并提出了一种潜在的安全检测机制。


<details>
  <summary>Details</summary>
Motivation: 尽管磁感应通信被认为是一种安全的水下无线通信技术，但其仍存在被窃听的风险，需要定量评估这种脆弱性。

Method: 通过有限元仿真和实验室实验，分析了不同窃听配置对接收电压和保密容量的影响。

Result: 水下磁感应通信易受窃听攻击，但其脆弱性取决于窃听节点的位置和方向；同时发现了一种可用于检测恶意节点的独特接收行为。

Conclusion: 研究揭示了磁感应通信的脆弱性，并提出了一种潜在的安全机制来应对窃听攻击。

Abstract: Typical magnetic induction (MI) communication is commonly considered a secure
underwater wireless communication (UWC) technology due to its non-audible and
non-visible nature compared to acoustic and optical UWC technologies. However,
vulnerabilities in communication systems inevitably exist and may lead to
different types of attacks. In this paper, we investigate the eavesdropping
attack in underwater MI communication to quantitatively measure the system's
vulnerability under this attack. We consider different potential eavesdropping
configuration setups based on the positions and orientations of the
eavesdropper node to investigate how they impact the received voltage and
secrecy at the legitimate receiver node. To this end, we develop
finite-element-method-based simulation models for each configuration in an
underwater environment and evaluate the received voltage and the secrecy
capacity against different system parameters such as magnetic flux, magnetic
flux density, distance, and orientation sensitivity. Furthermore, we construct
an experimental setup within a laboratory environment to replicate the
simulation experiments. Both simulation and lab experimental confirm the
susceptibility of underwater MI communication to eavesdropping attacks.
However, this vulnerability is highly dependent on the position and orientation
of the coil between the eavesdropper and the legitimate transmitter. On the
positive side, we also observe a unique behavior in the received coil reception
that might be used to detect malicious node activities in the vicinity, which
might lead to a potential security mechanism against eavesdropping attacks.

</details>

### [20] [Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper](https://arxiv.org/abs/2505.04265)
*Abdulrahman S Almuhaidib,Azlan Mohd Zain,Zalmiyah Zakaria,Izyan Izzati Kamsani,Abdulaziz S Almuhaidib*

Main category: cs.CR

TLDR: 本文探讨了大型语言模型（LLMs）在漏洞评估（VA）报告验证中的潜力，提出了一种自动化分析方法以减少误报并提高效率。


<details>
  <summary>Details</summary>
Motivation: 随着网络战的日益复杂，需要新的解决方案。LLMs在网络安全中的应用主要集中在防御方面，而其在攻击性用途（如VA报告验证）的研究较少。本文旨在填补这一空白。

Method: 通过文献综述，提出了一种利用LLMs自动化分析和验证VA报告的新方法，以减少误报并提升效率。

Result: 结果表明，LLMs在自动化VA报告验证中具有潜力，能够提高准确性并减少人力投入。

Conclusion: 本文进一步证明了LLMs在攻防两方面的能力，为制定更合适的网络安全策略和工具提供了依据。

Abstract: This, with the ever-increasing sophistication of cyberwar, calls for novel
solutions. In this regard, Large Language Models (LLMs) have emerged as a
highly promising tool for defensive and offensive cybersecurity-related
strategies. While existing literature has focused much on the defensive use of
LLMs, when it comes to their offensive utilization, very little has been
reported-namely, concerning Vulnerability Assessment (VA) report validation.
Consequentially, this paper tries to fill that gap by investigating the
capabilities of LLMs in automating and improving the validation process of the
report of the VA. From the critical review of the related literature, this
paper hereby proposes a new approach to using the LLMs in the automation of the
analysis and within the validation process of the report of the VA that could
potentially reduce the number of false positives and generally enhance
efficiency. These results are promising for LLM automatization for improving
validation on reports coming from VA in order to improve accuracy while
reducing human effort and security postures. The contribution of this paper
provides further evidence about the offensive and defensive LLM capabilities
and therefor helps in devising more appropriate cybersecurity strategies and
tools accordingly.

</details>

### [21] [Guardians of the Web: The Evolution and Future of Website Information Security](https://arxiv.org/abs/2505.04308)
*Md Saiful Islam,Li Xiangdong*

Main category: cs.CR

TLDR: 本文探讨了网站信息安全的历史发展、当前实践及未来方向，强调了技术进步和国际合作的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着互联网的普及和网络威胁的复杂化，网站信息安全成为数字时代的核心问题，需要持续研究和创新。

Method: 通过历史回顾、现状分析和未来展望，总结了信息安全技术的演变和应对策略。

Result: 从早期的基础技术到当前的多层次防护措施，信息安全技术不断进步，未来将依赖AI、区块链等新兴技术。

Conclusion: 网站信息安全的未来发展需结合技术创新和国际合作，以应对不断演变的网络威胁。

Abstract: Website information security has become a critical concern in the digital
age. This article explores the evolution of website information security,
examining its historical development, current practices, and future directions.
The early beginnings from the 1960s to the 1980s laid the groundwork for modern
cybersecurity, with the development of ARPANET, TCP/IP, public-key
cryptography, and the first antivirus programs. The 1990s marked a
transformative era, driven by the commercialization of the Internet and the
emergence of web-based services. As the Internet grew, so did the range and
sophistication of cyber threats, leading to advancements in security
technologies such as the Secure Sockets Layer (SSL) protocol, password
protection, and firewalls. Current practices in website information security
involve a multi-layered approach, including encryption, secure coding
practices, regular security audits, and user education. The future of website
information security is expected to be shaped by emerging technologies such as
artificial intelligence, blockchain, and quantum computing, as well as the
increasing importance of international cooperation and standardization efforts.
As cyber threats continue to evolve, ongoing research and innovation in website
information security will be essential to protect sensitive information and
maintain trust in the digital world.

</details>

### [22] [Applied Post Quantum Cryptography: A Practical Approach for Generating Certificates in Industrial Environments](https://arxiv.org/abs/2505.04333)
*Nino Ricchizzi,Christian Schwinne,Jan Pelzl*

Main category: cs.CR

TLDR: 该论文分析了后量子密码（PQC）在工业环境中证书管理的集成问题，开发了一个基于Bouncy Castle的工具，支持多种证书类型，填补了开源工具的空白。


<details>
  <summary>Details</summary>
Motivation: 工业环境中设备的安全注册依赖于长期且互操作的证书，而PQC的过渡带来了挑战。

Method: 开发了一个基于Bouncy Castle的模块化工具，支持生成和验证经典、混合、复合及变色龙证书。

Result: 工具展示了与标准X.509工作流的兼容性，并支持无头操作和受限平台。

Conclusion: 该工具填补了开源解决方案的空白，促进了PQC迁移策略的研究和测试。

Abstract: The transition to post-quantum cryptography (PQC) presents significant
challenges for certificate-based identity management in industrial
environments, where secure onboarding of devices relies on long-lived and
interoperable credentials. This work analyzes the integration of PQC into X.509
certificate structures and compares existing tool support for classical,
hybrid, composite, and chameleon certificates. A gap is identified in available
open-source solutions, particularly for the generation and validation of hybrid
and composite certificates via command-line interfaces. To address this, a
proof-of-concept implementation based on the Bouncy Castle library is
developed. The tool supports the creation of classical, hybrid (Catalyst),
composite, and partially chameleon certificates using PQC algorithms such as
ML-DSA and SLH-DSA. It demonstrates compatibility with standard X.509 workflows
and aims to support headless operation and constrained platforms typical of
industrial systems. The implementation is modular, publicly available, and
intended to facilitate further research and testing of PQC migration strategies
in practice. A comparison with OpenSSL-based solutions highlights current
limitations in standardization, toolchain support, and algorithm coverage.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation](https://arxiv.org/abs/2505.03774)
*Tao Yin,Chen Zhao,Xiaoyan Liu,Minglai Shao*

Main category: cs.LG

TLDR: 该论文提出了一种用于异构图中的OOD检测方法（OODHG），通过能量传播机制和约束条件区分ID和OOD节点，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的图数据通常是异构的，且存在分布偏移问题，而现有研究多集中于同构图，异构图的OOD检测研究不足。

Method: 学习异构图节点的表示，计算能量值以检测OOD节点，并分类ID节点；引入基于元路径的能量传播机制和能量约束。

Result: 实验证明OODHG在OOD检测任务中优于基线模型，且在ID节点分类中表现准确。

Conclusion: OODHG是一种简单有效的异构图OOD检测方法，填补了该领域的研究空白。

Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node
and structural information from graph data. While current GNNs perform well in
node classification tasks within in-distribution (ID) settings, real-world
scenarios often present distribution shifts, leading to the presence of
out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and
challenging task. Most existing research focuses on homogeneous graphs, but
real-world graphs are often heterogeneous, consisting of diverse node and edge
types. This heterogeneity adds complexity and enriches the informational
content. To the best of our knowledge, OOD detection in heterogeneous graphs
remains an underexplored area. In this context, we propose a novel methodology
for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main
objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the
first task's results. Specifically, we learn representations for each node in
the heterogeneous graph, calculate energy values to determine whether nodes are
OOD, and then classify ID nodes. To leverage the structural information of
heterogeneous graphs, we introduce a meta-path-based energy propagation
mechanism and an energy constraint to enhance the distinction between ID and
OOD nodes. Extensive experimental findings substantiate the simplicity and
effectiveness of OODHG, demonstrating its superiority over baseline models in
OOD detection tasks and its accuracy in ID node classification.

</details>

### [24] [Hierarchical Multi-Label Generation with Probabilistic Level-Constraint](https://arxiv.org/abs/2505.03775)
*Linqing Chen,Weilei Wang,Wentao Wu,Hanmeng Zhong*

Main category: cs.LG

TLDR: 论文提出了一种生成式框架（HMG）结合概率层级约束（PLC），用于解决层次极端多标签分类问题，无需依赖聚类等预处理步骤，并能精确控制输出。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理层次极端多标签分类时，难以应对复杂的层级关系和大量标签，且生成方法无法精确控制输出。

Method: 将任务重新定义为层次多标签生成（HMG），并采用生成式框架结合概率层级约束（PLC）生成标签。

Result: 实验表明，该方法在HMG任务中达到新SOTA性能，且在输出控制方面优于先前研究。

Conclusion: 提出的HMG框架有效解决了层次多标签分类问题，并在生成和控制输出方面表现优异。

Abstract: Hierarchical Extreme Multi-Label Classification poses greater difficulties
compared to traditional multi-label classification because of the intricate
hierarchical connections of labels within a domain-specific taxonomy and the
substantial number of labels. Some of the prior research endeavors centered on
classifying text through several ancillary stages such as the cluster algorithm
and multiphase classification. Others made attempts to leverage the assistance
of generative methods yet were unable to properly control the output of the
generative model. We redefine the task from hierarchical multi-Label
classification to Hierarchical Multi-Label Generation (HMG) and employ a
generative framework with Probabilistic Level Constraints (PLC) to generate
hierarchical labels within a specific taxonomy that have complex hierarchical
relationships. The approach we proposed in this paper enables the framework to
generate all relevant labels across levels for each document without relying on
preliminary operations like clustering. Meanwhile, it can control the model
output precisely in terms of count, length, and level aspects. Experiments
demonstrate that our approach not only achieves a new SOTA performance in the
HMG task, but also has a much better performance in constrained the output of
model than previous research work.

</details>

### [25] [PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction](https://arxiv.org/abs/2505.03776)
*Hansi Denis,Siegfried Mercelis,Ngoc-Quang Luong*

Main category: cs.LG

TLDR: 论文提出了一种名为PAPN的新型路由预测方法，结合了局部和全局注意力机制，优化了最后一公里配送和第一公里取件的效率。


<details>
  <summary>Details</summary>
Motivation: 最后一公里配送和第一公里取件的优化对物流效率和成本控制至关重要，需要准确的路由预测系统。

Method: 采用Proximity Attention机制和Pointer Network解码器，结合全局多头注意力编码器，实现全局与局部注意力的混合建模。

Result: 在LaDE数据集上表现优异，优于现有监督学习方法，并与最佳强化学习方法DRL4Route竞争。

Conclusion: PAPN方法在路由预测任务中表现出显著潜力，为物流优化提供了有效解决方案。

Abstract: Optimization of the last-mile delivery and first-mile pickup of parcels is an
integral part of the broader logistics optimization pipeline as it entails both
cost and resource efficiency as well as a heightened service quality. Such
optimization requires accurate route and time prediction systems to adapt to
different scenarios in advance. This work tackles the first building block,
namely route prediction. This is done by introducing a novel Proximity
Attention mechanism in an encoder-decoder architecture utilizing a Pointer
Network in the decoding process (Proximity Attention Encoder and Pointer
Network decoder: PAPN) to leverage the underlying connections between the
different visitable pickup positions at each timestep. To this local attention
process is coupled global context computing via a multi-head attention
transformer encoder. The obtained global context is then mixed to an aggregated
version of the local embedding thus achieving a mix of global and local
attention for complete modeling of the problems. Proximity attention is also
used in the decoding process to skew predictions towards the locations with the
highest attention scores and thus using inter-connectivity of locations as a
base for next-location prediction. This method is trained, validated and tested
on a large industry-level dataset of real-world, large-scale last-mile delivery
and first-mile pickup named LaDE[1]. This approach shows noticeable promise,
outperforming all state-of-the-art supervised systems in terms of most metrics
used for benchmarking methods on this dataset while still being competitive
with the best-performing reinforcement learning method named DRL4Route[2].

</details>

### [26] [MolMole: Molecule Mining from Scientific Literature](https://arxiv.org/abs/2505.03777)
*LG AI Research,Sehyun Chun,Jiye Kim,Ahra Jo,Yeonsik Jo,Seungyul Oh,Seungjun Lee,Kwangrok Ryoo,Jongmin Lee,Seunghwan Kim,Byung Jun Kang,Soonyoung Lee,Jun Ha Park,Chanwoo Moon,Jiwon Ham,Haein Lee,Heejae Han,Jaeseung Byun,Soojong Do,Minju Ha,Dongyun Kim,Kyunghoon Bae,Woohyung Lim,Edward Hwayoung Lee,Yongmin Park,Jeongsang Yu,Gerrard Jeongwon Jo,Yeonjung Hong,Kyungjae Yoo,Sehui Han,Jaewan Lee,Changyoung Park,Kijeong Jeon,Sihyuk Yi*

Main category: cs.LG

TLDR: MolMole是一个基于视觉的深度学习框架，用于从科学文档中自动化提取分子结构和反应数据，统一了分子检测、反应图解析和光学化学结构识别。


<details>
  <summary>Details</summary>
Motivation: 科学文档中的化学数据格式多样且布局复杂，缺乏标准化的提取方法和评估指标。

Method: MolMole结合了分子检测、反应图解析和光学化学结构识别，并提出了新的测试集和评估指标。

Result: MolMole在基准测试和公共数据集上优于现有工具包。

Conclusion: MolMole框架和测试集将公开，为化学数据提取提供了高效解决方案。

Abstract: The extraction of molecular structures and reaction data from scientific
documents is challenging due to their varied, unstructured chemical formats and
complex document layouts. To address this, we introduce MolMole, a vision-based
deep learning framework that unifies molecule detection, reaction diagram
parsing, and optical chemical structure recognition (OCSR) into a single
pipeline for automating the extraction of chemical data directly from
page-level documents. Recognizing the lack of a standard page-level benchmark
and evaluation metric, we also present a testset of 550 pages annotated with
molecule bounding boxes, reaction labels, and MOLfiles, along with a novel
evaluation metric. Experimental results demonstrate that MolMole outperforms
existing toolkits on both our benchmark and public datasets. The benchmark
testset will be publicly available, and the MolMole toolkit will be accessible
soon through an interactive demo on the LG AI Research website. For commercial
inquiries, please contact us at
\href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.

</details>

### [27] [Dragonfly: a modular deep reinforcement learning library](https://arxiv.org/abs/2505.03778)
*Jonathan Viquerat,Paul Garnier,Amirhossein Bateni,Elie Hachem*

Main category: cs.LG

TLDR: Dragonfly是一个模块化的深度强化学习库，旨在简化实验和开发，支持JSON序列化和参数扫描，适用于CPU密集型环境，性能优于文献中的标准基准。


<details>
  <summary>Details</summary>
Motivation: 为了简化深度强化学习的实验和开发过程，减少代码维护成本，同时支持模块化和参数调整。

Method: 基于JSON序列化技术，允许模块交换和参数扫描，特别优化了CPU密集型环境的性能。

Result: 在标准代理和常见基准测试中，性能优于文献中的其他方法。

Conclusion: Dragonfly是一个高效、模块化的深度强化学习工具，适用于实验和开发，尤其在CPU密集型任务中表现优异。

Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in
order to ease experimentation and developments. It relies on a json
serialization that allows to swap building blocks and perform parameter sweep,
while minimizing code maintenance. Some of its features are specifically
designed for CPU-intensive environments, such as numerical simulations. Its
performance on standard agents using common benchmarks compares favorably with
the literature.

</details>

### [28] [Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites](https://arxiv.org/abs/2505.03779)
*Tao Liu,Tianyu Zhang,Yongxue Chen,Weiming Wang,Yu Jiang,Yuming Huang,Charlie C. L. Wang*

Main category: cs.LG

TLDR: 提出了一种基于神经网络的框架，用于同时优化纤维增强热塑性复合材料的结构拓扑、弯曲层和路径方向，以实现强各向异性强度并确保可制造性。


<details>
  <summary>Details</summary>
Motivation: 解决纤维增强复合材料在设计和制造过程中各向异性强度与可制造性之间的平衡问题。

Method: 使用三个隐式神经场表示几何形状、层序列和纤维方向，将设计和制造目标（如各向异性强度、结构体积、机器运动控制等）整合为可微优化过程。

Result: 实验表明，该方法生成的复合材料在破坏载荷上比顺序优化方法提高了33.1%。

Conclusion: 该框架成功实现了复合材料力学性能与可制造性的协同优化，适用于多轴3D打印。

Abstract: We propose a neural network-based computational framework for the
simultaneous optimization of structural topology, curved layers, and path
orientations to achieve strong anisotropic strength in fiber-reinforced
thermoplastic composites while ensuring manufacturability. Our framework
employs three implicit neural fields to represent geometric shape, layer
sequence, and fiber orientation. This enables the direct formulation of both
design and manufacturability objectives - such as anisotropic strength,
structural volume, machine motion control, layer curvature, and layer thickness
- into an integrated and differentiable optimization process. By incorporating
these objectives as loss functions, the framework ensures that the resultant
composites exhibit optimized mechanical strength while remaining its
manufacturability for filament-based multi-axis 3D printing across diverse
hardware platforms. Physical experiments demonstrate that the composites
generated by our co-optimization method can achieve an improvement of up to
33.1% in failure loads compared to composites with sequentially optimized
structures and manufacturing sequences.

</details>

### [29] [ALFRED: Ask a Large-language model For Reliable ECG Diagnosis](https://arxiv.org/abs/2505.03781)
*Jin Yu,JaeHo Park,TaeJun Park,Gyurin Kim,JiHyun Lee,Min Sung Lee,Joon-myoung Kwon,Jeong Min Son,Yong-Yeon Jo*

Main category: cs.LG

TLDR: 提出了一种基于RAG的零样本ECG诊断框架，结合专家知识以提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域（如ECG分析）中，仅依赖RAG可能无法生成可靠的结果，需要结合专家知识。

Method: 提出了一种基于RAG的零样本ECG诊断框架，整合专家知识。

Result: 在PTB-XL数据集上验证了框架的有效性，展示了结构化领域知识在ECG分析中的价值。

Conclusion: 该框架支持全面的ECG分析，具有广泛的应用潜力。

Abstract: Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers
high accuracy and convenience. However, generating reliable, evidence-based
results in specialized fields like healthcare remains a challenge, as RAG alone
may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG
for ECG analysis that incorporates expert-curated knowledge to enhance
diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset
demonstrates the framework's effectiveness, highlighting the value of
structured domain expertise in automated ECG interpretation. Our framework is
designed to support comprehensive ECG analysis, addressing diverse diagnostic
needs with potential applications beyond the tested dataset.

</details>

### [30] [A general physics-constrained method for the modelling of equation's closure terms with sparse data](https://arxiv.org/abs/2505.03783)
*Tian Chen,Shengping Liu,Li Liu,Heng Yong*

Main category: cs.LG

TLDR: 提出了一种基于Series-Parallel Multi-Network Architecture的新方法，结合PINNs和多源数据，用于稀疏数据下的闭包建模。


<details>
  <summary>Details</summary>
Motivation: 稀疏或不完整数据下构建通用闭包模型的挑战。

Method: 采用Series-Parallel多网络架构，结合PINNs和多源数据，独立建模未知闭包项。

Result: 模型能整合到PDE求解器中，提升复杂预测模拟的鲁棒性。

Conclusion: 新方法在工程应用中展现出广泛适用性和准确性。

Abstract: Accurate modeling of closure terms is a critical challenge in engineering and
scientific research, particularly when data is sparse (scarse or incomplete),
making widely applicable models difficult to develop. This study proposes a
novel approach for constructing closure models in such challenging scenarios.
We introduce a Series-Parallel Multi-Network Architecture that integrates
Physics-Informed Neural Networks (PINNs) to incorporate physical constraints
and heterogeneous data from multiple initial and boundary conditions, while
employing dedicated subnetworks to independently model unknown closure terms,
enhancing generalizability across diverse problems. These closure models are
integrated into an accurate Partial Differential Equation (PDE) solver,
enabling robust solutions to complex predictive simulations in engineering
applications.

</details>

### [31] [Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers](https://arxiv.org/abs/2505.03784)
*Ahmed A. Metwally,A. Ali Heydari,Daniel McDuff,Alexandru Solot,Zeinab Esmaeilpour,Anthony Z Faranesh,Menglian Zhou,David B. Savage,Conor Heneghan,Shwetak Patel,Cathy Speed,Javier L. Prieto*

Main category: cs.LG

TLDR: 该研究利用可穿戴设备和血液生物标志物数据，开发深度学习模型预测胰岛素抵抗，效果优于单一数据源，并展示了在肥胖和久坐人群中的高敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有胰岛素抵抗测量方法昂贵且不易获取，阻碍早期干预。

Method: 结合可穿戴设备数据和血液生物标志物，开发深度神经网络模型预测胰岛素抵抗。

Result: 模型预测效果优于单一数据源（R2=0.5, auROC=0.80），在肥胖和久坐人群中敏感性达93%。

Conclusion: 该研究为早期发现2型糖尿病风险人群提供了潜在工具，有助于实施预防策略。

Abstract: Insulin resistance, a precursor to type 2 diabetes, is characterized by
impaired insulin action in tissues. Current methods for measuring insulin
resistance, while effective, are expensive, inaccessible, not widely available
and hinder opportunities for early intervention. In this study, we remotely
recruited the largest dataset to date across the US to study insulin resistance
(N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%),
incorporating wearable device time series data and blood biomarkers, including
the ground-truth measure of insulin resistance, homeostatic model assessment
for insulin resistance (HOMA-IR). We developed deep neural network models to
predict insulin resistance based on readily available digital and blood
biomarkers. Our results show that our models can predict insulin resistance by
combining both wearable data and readily available blood biomarkers better than
either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%,
and specificity 84%). The model showed 93% sensitivity and 95% adjusted
specificity in obese and sedentary participants, a subpopulation most
vulnerable to developing type 2 diabetes and who could benefit most from early
intervention. Rigorous evaluation of model performance, including
interpretability, and robustness, facilitates generalizability across larger
cohorts, which is demonstrated by reproducing the prediction performance on an
independent validation cohort (N=72 participants). Additionally, we
demonstrated how the predicted insulin resistance can be integrated into a
large language model agent to help understand and contextualize HOMA-IR values,
facilitating interpretation and safe personalized recommendations. This work
offers the potential for early detection of people at risk of type 2 diabetes
and thereby facilitate earlier implementation of preventative strategies.

</details>

### [32] [mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging](https://arxiv.org/abs/2505.03785)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.LG

TLDR: mAIstro是一个基于LLM的开源多代理框架，用于医疗AI模型的端到端开发和部署，无需用户编码。


<details>
  <summary>Details</summary>
Motivation: 自动化医疗AI中的复杂工作流程，提供可复现和可扩展的临床与研究AI集成基础。

Method: 通过自然语言界面协调数据探索、特征提取、图像分割、分类和回归，支持开源和闭源LLM。

Result: 在16个开源数据集上成功执行任务，生成可解释的输出和验证模型。

Conclusion: mAIstro是首个能统一医疗应用中数据分析和AI模型开发的代理框架。

Abstract: Agentic systems built on large language models (LLMs) offer promising
capabilities for automating complex workflows in healthcare AI. We introduce
mAIstro, an open-source, autonomous multi-agentic framework for end-to-end
development and deployment of medical AI models. The system orchestrates
exploratory data analysis, radiomic feature extraction, image segmentation,
classification, and regression through a natural language interface, requiring
no coding from the user. Built on a modular architecture, mAIstro supports both
open- and closed-source LLMs, and was evaluated using a large and diverse set
of prompts across 16 open-source datasets, covering a wide range of imaging
modalities, anatomical regions, and data types. The agents successfully
executed all tasks, producing interpretable outputs and validated models. This
work presents the first agentic framework capable of unifying data analysis, AI
model development, and inference across varied healthcare applications,
offering a reproducible and extensible foundation for clinical and research AI
integration. The code is available at: https://github.com/eltzanis/mAIstro

</details>

### [33] [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/abs/2505.03786)
*Md Fahim Anjum*

Main category: cs.LG

TLDR: 研究比较了推理型与非推理型大语言模型在文本到SQL任务中的表现，发现推理模型在判别任务中表现更优，但在生成任务中表现较差。


<details>
  <summary>Details</summary>
Motivation: 探索推理型大语言模型在规划框架中作为判别器的潜力，并与传统非推理模型进行性能对比。

Method: 采用生成器-判别器框架，提出一种从推理链输出中提取软分数的新方法，用于细粒度候选排名。

Result: 蒸馏后的推理模型DeepSeek-R1-1.5B在F1分数和判别准确率上显著优于CodeLlama-7B和CodeLlama-13B，但在生成任务中表现不佳。

Conclusion: 推理模型在判别任务中潜力巨大，但在生成任务中表现有限，需进一步优化其在规划框架中的角色。

Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising
path for improving candidate evaluation in planning frameworks, but their
relative performance against traditional non-reasoning models remains largely
underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning
model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within
a generator-discriminator LLM planning framework for the text-to-SQL task. For
this, we introduce a novel method for extracting soft scores from the
chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking
of candidates. Our central hypothesis is that reasoning models are more
effective discriminators than non-reasoning LLMs. Our results show that
distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better
discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution
accuracy than CodeLlama-13B, despite having significantly fewer parameters.
Furthermore, we find that there is a limit to the logical capabilities of
reasoning models, and only providing more context or allowing more compute
budget for reasoning is not enough to improve their discrimination performance.
Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find
generation more challenging than discrimination and may underperform as
generators compared to smaller non-reasoning LLMs. Our work highlights the
potential of reasoning models as discriminators in agentic frameworks, far
outweighing their capabilities as generators, offering insights into their
optimal role within LLM planning infrastructures.

</details>

### [34] [ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification](https://arxiv.org/abs/2505.03787)
*Zuraiz Baig,Sidra Nasir,Rizwan Ahmed Khan,Muhammad Zeeshan Ul Haque*

Main category: cs.LG

TLDR: 论文提出两种轻量级1D卷积神经网络（ArrhythmiNet V1和V2），用于实时心律失常分类，兼具高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 心律失常是威胁生命的心脏事件的主要原因，现有ECG分析方法依赖人工且易出错，深度学习模型则缺乏可解释性和计算效率。

Method: 采用MobileNet的深度可分离卷积设计，提出两种轻量级1D卷积神经网络，并整合SHAP和Grad-CAM实现可解释性。

Result: 在MIT-BIH数据集上，V1和V2的分类准确率分别为0.99和0.98，模型体积小（302.18 KB和157.76 KB）。

Conclusion: 研究证明了在可穿戴设备中结合可解释性、准确性和计算效率的可行性，但仍需解决数据集多样性和泛化性问题。

Abstract: Cardiac arrhythmias are a leading cause of life-threatening cardiac events,
highlighting the urgent need for accurate and timely detection.
Electrocardiography (ECG) remains the clinical gold standard for arrhythmia
diagnosis; however, manual interpretation is time-consuming, dependent on
clinical expertise, and prone to human error. Although deep learning has
advanced automated ECG analysis, many existing models abstract away the
signal's intrinsic temporal and morphological features, lack interpretability,
and are computationally intensive-hindering their deployment on
resource-constrained platforms. In this work, we propose two novel lightweight
1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for
efficient, real-time arrhythmia classification on edge devices. Inspired by
MobileNet's depthwise separable convolutional design, these models maintain
memory footprints of just 302.18 KB and 157.76 KB, respectively, while
achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH
Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch
Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature
Ventricular Contraction. In order to ensure clinical transparency and
relevance, we integrate Shapley Additive Explanations and Gradient-weighted
Class Activation Mapping, enabling both local and global interpretability.
These techniques highlight physiologically meaningful patterns such as the QRS
complex and T-wave that contribute to the model's predictions. We also discuss
performance-efficiency trade-offs and address current limitations related to
dataset diversity and generalizability. Overall, our findings demonstrate the
feasibility of combining interpretability, predictive accuracy, and
computational efficiency in practical, wearable, and embedded ECG monitoring
systems.

</details>

### [35] [A new architecture of high-order deep neural networks that learn martingales](https://arxiv.org/abs/2505.03789)
*Syoiti Ninomiya,Yuming Ma*

Main category: cs.LG

TLDR: 提出一种基于高阶弱逼近算法的深度学习神经网络架构，用于高效学习鞅，并应用于金融衍生品定价。


<details>
  <summary>Details</summary>
Motivation: 通过深度学习模型高效学习鞅，解决金融衍生品定价问题。

Method: 采用显式Runge-Kutta类型的高阶弱逼近算法，通过迭代组合和线性组合目标SDE的向量场实现逼近。

Result: 新架构能够有效学习鞅，并在金融衍生品定价中表现出良好性能。

Conclusion: 该架构为深度学习在金融数学中的应用提供了新思路。

Abstract: A new deep-learning neural network architecture based on high-order weak
approximation algorithms for stochastic differential equations (SDEs) is
proposed. The architecture enables the efficient learning of martingales by
deep learning models. The behaviour of deep neural networks based on this
architecture, when applied to the problem of pricing financial derivatives, is
also examined. The core of this new architecture lies in the high-order weak
approximation algorithms of the explicit Runge--Kutta type, wherein the
approximation is realised solely through iterative compositions and linear
combinations of vector fields of the target SDEs.

</details>

### [36] [A Time-Series Data Augmentation Model through Diffusion and Transformer Integration](https://arxiv.org/abs/2505.03790)
*Yuren Zhang,Zhongnan Pu,Lei Jing*

Main category: cs.LG

TLDR: 提出了一种结合扩散模型和Transformer的方法，用于生成高质量的时间序列增强数据。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列数据增强的研究较少，而其他领域（如图像和语音）已有较多进展，因此需要填补这一空白。

Method: 使用扩散去噪模型生成初始时间步数据，再用Transformer预测后续动作，并通过加权损失函数实现收敛。

Result: 该方法能生成高质量增强数据，显著提升模型性能，优于传统方法或无数据增强的情况。

Conclusion: 该方法简单有效，为时间序列数据增强提供了新思路。

Abstract: With the development of Artificial Intelligence, numerous real-world tasks
have been accomplished using technology integrated with deep learning. To
achieve optimal performance, deep neural networks typically require large
volumes of data for training. Although advances in data augmentation have
facilitated the acquisition of vast datasets, most of this data is concentrated
in domains like images and speech. However, there has been relatively less
focus on augmenting time-series data. To address this gap and generate a
substantial amount of time-series data, we propose a simple and effective
method that combines the Diffusion and Transformer models. By utilizing an
adjusted diffusion denoising model to generate a large volume of initial
time-step action data, followed by employing a Transformer model to predict
subsequent actions, and incorporating a weighted loss function to achieve
convergence, the method demonstrates its effectiveness. Using the performance
improvement of the model after applying augmented data as a benchmark, and
comparing the results with those obtained without data augmentation or using
traditional data augmentation methods, this approach shows its capability to
produce high-quality augmented data.

</details>

### [37] [Practical Boolean Backpropagation](https://arxiv.org/abs/2505.03791)
*Simon Golbert*

Main category: cs.LG

TLDR: 提出了一种基于特定布尔门的纯布尔反向传播方法，无需数值计算，实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 布尔神经网络在硬件效率上优于实值模型，但纯布尔训练方法尚未充分探索。

Method: 使用单一特定布尔门，直接在布尔代数中进行反向传播，不涉及数值计算。

Result: 初步实验证实了该方法的可行性。

Conclusion: 纯布尔反向传播方法为布尔神经网络训练提供了新的可能性。

Abstract: Boolean neural networks offer hardware-efficient alternatives to real-valued
models. While quantization is common, purely Boolean training remains
underexplored. We present a practical method for purely Boolean backpropagation
for networks based on a single specific gate we chose, operating directly in
Boolean algebra involving no numerics. Initial experiments confirm its
feasibility.

</details>

### [38] [Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning](https://arxiv.org/abs/2505.03792)
*Lang Feng,Weihao Tan,Zhiyi Lyu,Longtao Zheng,Haiyang Xu,Ming Yan,Fei Huang,Bo An*

Main category: cs.LG

TLDR: 提出了一种名为CoSo的新方法，通过反事实推理动态评估文本动作中关键令牌的影响，优化在线强化学习中的探索效率。


<details>
  <summary>Details</summary>
Motivation: 在线微调视觉语言模型（VLM）代理时，开放文本动作空间和非端到端动作生成导致探索效率低下，亟需改进方法。

Method: CoSo利用反事实推理动态评估令牌对动作的因果影响，优先探索关键令牌，减少冗余令牌的影响。

Result: 理论分析和实验验证表明，CoSo在多种任务（如设备控制、游戏和具身AI）中显著提升探索效率和性能。

Conclusion: CoSo通过针对性探索优化了VLM代理的在线强化学习，具有广泛的应用潜力。

Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.

</details>

### [39] [LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection](https://arxiv.org/abs/2505.03793)
*Xinyue Zeng,Haohui Wang,Junhong Lin,Jun Wu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TLDR: 论文提出了一种新的理论框架LENSLLM，用于高效选择大型语言模型（LLM），通过Hessian-based PAC-Bayes泛化边界和NTK-based修正缩放模型，显著提升选择准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源限制，无法对所有候选LLM进行微调，因此需要一种高效的方法来理解LLM在微调中的动态行为及其泛化性能。

Method: 提出Hessian-based PAC-Bayes泛化边界理论，并设计基于NTK的修正缩放模型LENSLLM，用于预测LLM在不同任务中的性能。

Result: 在3个大规模基准测试中，LENSLLM达到91.1%的准确率，并减少88.5%的计算成本，优于5种现有方法。

Conclusion: LENSLLM为LLM选择提供了一种高效且准确的解决方案，并开源了模型和结果。

Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse
downstream tasks necessitates efficient model selection, given the
impracticality of fine-tuning all candidates due to computational constraints.
Despite the recent advances in LLM selection, a fundamental research question
largely remains nascent: how can we model the dynamic behaviors of LLMs during
fine-tuning, thereby enhancing our understanding of their generalization
performance across diverse downstream tasks? In this work, we propose a novel
theoretical framework that provides a proper lens to assess the generalization
capabilities of LLMs, thereby enabling accurate and efficient LLM selection for
downstream applications. In particular, we first derive a Hessian-based
PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and
then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling
Model that enables accurate performance predictions across diverse tasks while
maintaining computational efficiency. Extensive empirical results on 3
large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy
and reduces up to 88.5% computational cost in LLM selection, outperforming 5
state-of-the-art methods. We open-source our proposed LENSLLM model and
corresponding results at the Github link:
https://github.com/Susan571/LENSLLM.git.

</details>

### [40] [A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems](https://arxiv.org/abs/2505.03794)
*İrfan Işik,Ibrahim Karahan,Okan Erkaymaz*

Main category: cs.LG

TLDR: 提出了一种改进的前向后向分裂算法，具有两个惯性参数，用于在实希尔伯特空间中寻找使两个算子和为零的点。


<details>
  <summary>Details</summary>
Motivation: 解决现有算法在回归和数据分类问题中的性能不足。

Method: 改进的前向后向分裂算法，引入两个惯性参数，并在标准假设下证明弱收敛性。

Result: 实验结果表明，该算法在回归和数据分类问题上优于现有算法。

Conclusion: 提出的算法在性能上优于现有相关算法，具有实际应用潜力。

Abstract: This paper presents an improved forward-backward splitting algorithm with two
inertial parameters. It aims to find a point in the real Hilbert space at which
the sum of a co-coercive operator and a maximal monotone operator vanishes.
Under standard assumptions, our proposed algorithm demonstrates weak
convergence. We present numerous experimental results to demonstrate the
behavior of the developed algorithm by comparing it with existing algorithms in
the literature for regression and data classification problems. Furthermore,
these implementations suggest our proposed algorithm yields superior outcomes
when benchmarked against other relevant algorithms in existing literature.

</details>

### [41] [Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks](https://arxiv.org/abs/2505.03797)
*Andrew Millard,Joshua Murphy,Simon Maskell,Zheng Zhao*

Main category: cs.LG

TLDR: 本文提出了一种基于SMC的新训练方法，用于部分贝叶斯神经网络（pBNNs），通过引导提议和梯度马尔可夫核提升高维问题的可扩展性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 部分贝叶斯神经网络（pBNNs）在性能上与全贝叶斯神经网络相当，但仅部分参数为随机。现有SMC采样器虽能提供非参数概率估计，但需改进以适应高维问题。

Method: 采用基于SMC的训练方法，结合引导提议和梯度马尔可夫核，提升高维问题的可扩展性。

Result: 新方法在预测性能和最优损失上优于现有技术，且pBNNs在大批量训练时表现更优，显著减少训练时间。

Conclusion: 新SMC方法显著提升了pBNNs的性能和可扩展性，尤其适用于高维和大批量训练场景。

Abstract: Partial Bayesian neural networks (pBNNs) have been shown to perform
competitively with fully Bayesian neural networks while only having a subset of
the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as
the inference method for pBNNs gives a non-parametric probabilistic estimation
of the stochastic parameters, and has shown improved performance over
parametric methods. In this paper we introduce a new SMC-based training method
for pBNNs by utilising a guided proposal and incorporating gradient-based
Markov kernels, which gives us better scalability on high dimensional problems.
We show that our new method outperforms the state-of-the-art in terms of
predictive performance and optimal loss. We also show that pBNNs scale well
with larger batch sizes, resulting in significantly reduced training times and
often better performance.

</details>

### [42] [Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks](https://arxiv.org/abs/2505.04046)
*Xuyang Wang,Siyuan Duan,Qizhi Li,Guiduo Duan,Yuan Sun,Dezhong Peng*

Main category: cs.LG

TLDR: 提出了一种可靠解耦多视图学习框架（RDML），通过证据解耦学习、特征重新校准模块和视图级证据注意力机制，解决了多视图学习中的对抗不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 多视图学习在实际应用中面临对抗性扰动的威胁，导致模型不可靠，现有方法未考虑数据安全问题。

Method: 1. 证据解耦学习分解视图为干净和对抗部分；2. 特征重新校准模块减轻对抗扰动影响；3. 视图级证据注意力机制忽略不可修复的对抗干扰。

Result: 在多视图分类任务中，RDML显著优于现有方法。

Conclusion: RDML有效提升了多视图学习在对抗环境下的可靠性。

Abstract: Recently, trustworthy multi-view learning has attracted extensive attention
because evidence learning can provide reliable uncertainty estimation to
enhance the credibility of multi-view predictions. Existing trusted multi-view
learning methods implicitly assume that multi-view data is secure. In practice,
however, in safety-sensitive applications such as autonomous driving and
security monitoring, multi-view data often faces threats from adversarial
perturbations, thereby deceiving or disrupting multi-view learning models. This
inevitably leads to the adversarial unreliability problem (AUP) in trusted
multi-view learning. To overcome this tricky problem, we propose a novel
multi-view learning framework, namely Reliable Disentanglement Multi-view
Learning (RDML). Specifically, we first propose evidential disentanglement
learning to decompose each view into clean and adversarial parts under the
guidance of corresponding evidences, which is extracted by a pretrained
evidence extractor. Then, we employ the feature recalibration module to
mitigate the negative impact of adversarial perturbations and extract potential
informative features from them. Finally, to further ignore the irreparable
adversarial interferences, a view-level evidential attention mechanism is
designed. Extensive experiments on multi-view classification tasks with
adversarial attacks show that our RDML outperforms the state-of-the-art
multi-view learning methods by a relatively large margin.

</details>

### [43] [Position: Foundation Models Need Digital Twin Representations](https://arxiv.org/abs/2505.03798)
*Yiqing Shen,Hao Ding,Lalithkumar Seenivasan,Tianmin Shu,Mathias Unberath*

Main category: cs.LG

TLDR: 当前基础模型（FMs）依赖离散的token表示，限制了其对真实世界知识的理解和语义连贯性。本文提出数字孪生（DT）表示作为替代，以解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 当前FMs通过统计相关性学习知识，缺乏显式领域知识，导致跨模态语义连贯性差、时空动态捕捉不足和因果推理困难。

Method: 提出使用数字孪生（DT）表示，作为物理过程的虚拟复制品，替代token表示。

Result: DT表示能提供物理基础的表征，显式编码领域知识并保持真实世界过程的连续性。

Conclusion: 数字孪生表示有望解决当前FMs的局限性，提升其在复杂任务中的表现。

Abstract: Current foundation models (FMs) rely on token representations that directly
fragment continuous real-world multimodal data into discrete tokens. They limit
FMs to learning real-world knowledge and relationships purely through
statistical correlation rather than leveraging explicit domain knowledge.
Consequently, current FMs struggle with maintaining semantic coherence across
modalities, capturing fine-grained spatial-temporal dynamics, and performing
causal reasoning. These limitations cannot be overcome by simply scaling up
model size or expanding datasets. This position paper argues that the machine
learning community should consider digital twin (DT) representations, which are
outcome-driven digital representations that serve as building blocks for
creating virtual replicas of physical processes, as an alternative to the token
representation for building FMs. Finally, we discuss how DT representations can
address these challenges by providing physically grounded representations that
explicitly encode domain knowledge and preserve the continuous nature of
real-world processes.

</details>

### [44] [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/abs/2505.03799)
*Hyun Lee,Chris Yi,Maminur Islam,B. D. S. Aritra*

Main category: cs.LG

TLDR: 提出了一种名为SDM-InstructGLM的新型指令调优图语言模型框架，用于解决大语言模型在图相关任务中的可扩展性和效率问题，无需依赖图神经网络。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在图相关任务中的应用受限，主要由于可扩展性问题和缺乏处理图结构的专用机制。现有方法多依赖图神经网络，但直接在图结构中编码大语言模型的研究较少。

Method: 引入基于相似度和度的偏置随机游走机制，选择性采样和编码图信息，以提高标记效率并减少信息损失。

Result: 显著提升了节点分类和链接预测等图任务的性能，证明了仅用大语言模型处理图的可行性。

Conclusion: 为不依赖图神经网络的图学习方法开辟了新途径，利用大语言模型作为独立的图推理模型。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various
natural language processing tasks; however, their application to graph-related
problems remains limited, primarily due to scalability constraints and the
absence of dedicated mechanisms for processing graph structures. Existing
approaches predominantly integrate LLMs with Graph Neural Networks (GNNs),
using GNNs as feature encoders or auxiliary components. However, directly
encoding graph structures within LLMs has been underexplored, particularly in
the context of large-scale graphs where token limitations hinder effective
representation. To address these challenges, we propose SDM-InstructGLM, a
novel instruction-tuned Graph Language Model (InstructGLM) framework that
enhances scalability and efficiency without relying on GNNs. Our method
introduces a similarity-degree-based biased random walk mechanism, which
selectively samples and encodes graph information based on node-feature
similarity and degree centrality, ensuring an adaptive and structured
representation within the LLM. This approach significantly improves token
efficiency, mitigates information loss due to random sampling, and enhances
performance on graph-based tasks such as node classification and link
prediction. Furthermore, our results demonstrate the feasibility of LLM-only
graph processing, enabling scalable and interpretable Graph Language Models
(GLMs) optimized through instruction-based fine-tuning. This work paves the way
for GNN-free approaches to graph learning, leveraging LLMs as standalone graph
reasoning models. Our source code is available on GitHub.

</details>

### [45] [Large Language Model Compression with Global Rank and Sparsity Optimization](https://arxiv.org/abs/2505.03801)
*Changhai Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TLDR: 提出了一种两阶段LLM压缩方法，通过全局优化低秩和稀疏结构，解决了现有方法在权重分配和矩阵交互上的挑战。


<details>
  <summary>Details</summary>
Motivation: 压缩大型语言模型时，低秩和稀疏复合近似面临矩阵交互和权重分配的挑战，影响性能。

Method: 第一阶段使用鲁棒主成分分析分解权重矩阵为低秩和稀疏成分；第二阶段通过概率全局优化技术联合识别低秩和稀疏结构。

Result: 实验结果表明，该方法在稀疏化和复合近似方面显著优于现有技术。

Conclusion: 该方法能自动检测冗余并管理稀疏与低秩组件的交互，性能优越。

Abstract: Low-rank and sparse composite approximation is a natural idea to compress
Large Language Models (LLMs). However, such an idea faces two primary
challenges that adversely affect the performance of existing methods. The first
challenge relates to the interaction and cooperation between low-rank and
sparse matrices, while the second involves determining weight allocation across
different layers, as redundancy varies considerably among them. To address
these challenges, we propose a novel two-stage LLM compression method with the
capability of global rank and sparsity optimization. It is noteworthy that the
overall optimization space is vast, making comprehensive optimization
computationally prohibitive. Therefore, to reduce the optimization space, our
first stage utilizes robust principal component analysis to decompose the
weight matrices of LLMs into low-rank and sparse components, which span the low
dimensional and sparse spaces containing the resultant low-rank and sparse
matrices, respectively. In the second stage, we propose a probabilistic global
optimization technique to jointly identify the low-rank and sparse structures
within the above two spaces. The appealing feature of our approach is its
ability to automatically detect the redundancy across different layers and to
manage the interaction between the sparse and low-rank components. Extensive
experimental results indicate that our method significantly surpasses
state-of-the-art techniques for sparsification and composite approximation.

</details>

### [46] [Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth](https://arxiv.org/abs/2505.03802)
*Changhai Zhou,Yuhua Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TLDR: QLoRA结合低比特量化和LoRA实现内存友好的大语言模型微调。QR-Adaptor提出了一种联合搜索量化组件和低秩空间秩的策略，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效结合量化误差和低秩子空间的协同优化，动态混合精度虽为自然思路，但缺乏统一策略。

Method: 提出QR-Adaptor，通过部分校准数据联合搜索量化组件和低秩空间秩，以实际性能和内存使用为导向。

Result: 在GSM8K上准确率提升4.89%，部分情况下优于16位微调模型，同时保持4位内存占用。

Conclusion: QR-Adaptor通过联合优化量化与低秩空间，显著提升性能，为量化微调提供新思路。

Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve
memory-friendly fine-tuning for large language models (LLM). Recently, methods
based on SVD for continuous update iterations to initialize LoRA matrices to
accommodate quantization errors have generally failed to consistently improve
performance. Dynamic mixed precision is a natural idea for continuously
improving the fine-tuning performance of quantized models, but previous methods
often optimize low-rank subspaces or quantization components separately,
without considering their synergy. To address this, we propose
\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial
calibration data to jointly search the quantization components and the rank of
low-rank spaces for each layer, thereby continuously improving model
performance. QR-Adaptor does not minimize quantization error but treats
precision and rank allocation as a discrete optimization problem guided by
actual downstream performance and memory usage. Compared to state-of-the-art
(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\%
accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit
fine-tuned model while maintaining the memory footprint of the 4-bit setting.

</details>

### [47] [RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization](https://arxiv.org/abs/2505.03803)
*Chen Xu,Yuxuan Yue,Zukang Xu,Xing Hu,Jiangyong Yu,Zhixuan Chen,Sifan Zhou,Zhihang Yuan,Dawei Yang*

Main category: cs.LG

TLDR: RWKVQuant是一种针对RWKV模型的PTQ框架，通过自适应量化方法和代码本优化算法，显著减少了量化后的性能损失和计算开销。


<details>
  <summary>Details</summary>
Motivation: RWKV作为一种现代RNN架构，在资源受限设备上部署时面临量化性能下降的问题，需要专门解决方案。

Method: 提出RWKVQuant框架，包括自适应量化选择和代码本优化算法。

Result: 实验表明，RWKVQuant能将RWKV-6-14B量化为约3位，精度损失小于1%，速度提升2.14倍。

Conclusion: RWKVQuant有效解决了RWKV模型量化中的性能问题，为资源受限设备部署提供了可行方案。

Abstract: RWKV is a modern RNN architecture with comparable performance to Transformer,
but still faces challenges when deployed to resource-constrained devices. Post
Training Quantization (PTQ), which is a an essential technique to reduce model
size and inference latency, has been widely used in Transformer models.
However, it suffers significant degradation of performance when applied to
RWKV. This paper investigates and identifies two key constraints inherent in
the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of
both smooth- and rotation-based quantization, introducing extra computation
overhead. (2) The larger amount of uniformly distributed weights poses
challenges for cluster-based quantization, leading to reduced accuracy. To this
end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting
of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively
selecting different quantization approaches by assessing the uniformity and
identifying outliers in the weights, and (2) a codebook optimization algorithm
that enhances the performance of cluster-based quantization methods for
element-wise multiplication in RWKV. Experiments show that RWKVQuant can
quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x
speed up.

</details>

### [48] [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
*Euntae Choi,Sumin Song,Woosang Lim,Sungjoo Yoo*

Main category: cs.LG

TLDR: 提出了一种无需训练的新方法，通过改进旋转矩阵和利用Walsh-Hadamard变换，显著提升了低比特量化（如2-bit）的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于旋转的量化方法在极低比特（如2-bit）下性能不足的问题。

Method: 使用Walsh-Hadamard变换和分组序列排列旋转（GSR），通过块对角矩阵减少量化误差。

Result: 在推理任务和WikiText-2的PPL分数上表现优异，性能接近基于优化的方法。

Conclusion: 该方法无需训练即可显著提升低比特量化的性能，且适用于现有学习旋转技术。

Abstract: Large Language Models (LLMs) face deployment challenges due to high
computational costs, and while Post-Training Quantization (PTQ) offers a
solution, existing rotation-based methods struggle at very low bit-widths like
2-bit. We introduce a novel, training-free approach to construct an improved
rotation matrix, addressing the limitations of current methods. The key
contributions include leveraging the Walsh-Hadamard transform with sequency
ordering, which clusters similar frequency components to reduce quantization
error compared to standard Hadamard matrices, significantly improving
performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)
using block-diagonal matrices with smaller Walsh blocks, effectively isolating
outlier impacts and achieving performance comparable to optimization-based
methods without requiring any training. Our method demonstrates robust
performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our
method also enhances results even when applied over existing learned rotation
techniques.

</details>

### [49] [MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance](https://arxiv.org/abs/2505.03804)
*Xing Hu,Zhixuan Chen,Dawei Yang,Zukang Xu,Chen Xu,Zhihang Yuan,Sifan Zhou,Jiangyong Yu*

Main category: cs.LG

TLDR: MoEQuant框架解决了MoE大语言模型量化中的专家间和专家内不平衡问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型因内存开销大而难以实际部署，传统量化方法在MoE模型上效果不佳。

Method: 提出MoEQuant框架，包括专家平衡自采样（EBSS）和亲和性引导量化（AGQ）两种技术。

Result: 实验显示MoEQuant在4位量化下显著提升性能（如DeepSeekMoE-16B在HumanEval上准确率提升超10分）。

Conclusion: MoEQuant有效解决了MoE模型的量化挑战，提升了效率和实用性。

Abstract: Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic
routing and sparse activation to enhance efficiency and scalability, have
achieved higher performance while reducing computational costs. However, these
models face significant memory overheads, limiting their practical deployment
and broader adoption. Post-training quantization (PTQ), a widely used method
for compressing LLMs, encounters severe accuracy degradation and diminished
generalization performance when applied to MoE models. This paper investigates
the impact of MoE's sparse and dynamic characteristics on quantization and
identifies two primary challenges: (1) Inter-expert imbalance, referring to the
uneven distribution of samples across experts, which leads to insufficient and
biased calibration for less frequently utilized experts; (2) Intra-expert
imbalance, arising from MoE's unique aggregation mechanism, which leads to
varying degrees of correlation between different samples and their assigned
experts. To address these challenges, we propose MoEQuant, a novel quantization
framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1)
Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that
efficiently constructs a calibration set with balanced expert distributions by
leveraging the cumulative probabilities of tokens and expert balance metrics as
guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates
affinities between experts and samples into the quantization process, thereby
accurately assessing the impact of individual samples on different experts
within the MoE layer. Experiments demonstrate that MoEQuant achieves
substantial performance gains (more than 10 points accuracy gain in the
HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.

</details>

### [50] [Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing](https://arxiv.org/abs/2505.03805)
*Nguyen Van Thanh*

Main category: cs.LG

TLDR: 将随机上坡攀爬（RUC）推广为多变量时间序列预测的模型无关特征优化框架，通过随机组合操作符生成候选特征程序，快速评分并过滤不稳定特征，实现高效、低能耗和可解释的预测。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的机构提供准确、透明的预测工具，避免依赖专有黑箱模型。

Method: 随机组合领域特定语法中的操作符生成候选特征程序，使用廉价代理模型快速评分，并通过嵌套交叉验证和信息论收缩过滤不稳定特征。

Result: 方法实现了更快的迭代周期、更低的能耗和更高的可解释性。

Conclusion: 该方法在多变量时间序列预测中具有潜力，尤其适合资源受限的机构。

Abstract: Randomized Uphill Climbing is a lightweight, stochastic search heuristic that
has delivered state of the art equity alpha factors for quantitative hedge
funds. I propose to generalize RUC into a model agnostic feature optimization
framework for multivariate time series forecasting. The core idea is to
synthesize candidate feature programs by randomly composing operators from a
domain specific grammar, score candidates rapidly with inexpensive surrogate
models on rolling windows, and filter instability via nested cross validation
and information theoretic shrinkage. By decoupling feature discovery from GPU
heavy deep learning, the method promises faster iteration cycles, lower energy
consumption, and greater interpretability. Societal relevance: accurate,
transparent forecasting tools empower resource constrained institutions, energy
regulators, climate risk NGOs to make data driven decisions without proprietary
black box models.

</details>

### [51] [Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks](https://arxiv.org/abs/2505.03806)
*Mehran Mazandarani,Marzieh Najariyan*

Main category: cs.LG

TLDR: PrINNs是一种将感知信息融入神经网络的框架，扩展了PINNs，支持多种感知形式，并整合专家知识，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决已知和未知物理定律系统的建模问题，结合感知信息与神经网络，弥合传统物理建模与现代数据驱动方法的差距。

Method: 通过损失函数整合专家知识和感知信息，提出MOEINNs、TKINNs和FINNs等子类，支持模糊逻辑和在线训练。

Result: PrINNs能处理不确定环境，建模复杂系统，发现新微分方程形式，推动计算科学与工程发展。

Conclusion: PrINNs是连接物理建模与数据驱动方法的重要工具，具有广泛的应用潜力。

Abstract: This article introduces Perception-Informed Neural Networks (PrINNs), a
framework designed to incorporate perception-based information into neural
networks, addressing both systems with known and unknown physics laws or
differential equations. Moreover, PrINNs extend the concept of Physics-Informed
Neural Networks (PINNs) and their variants, offering a platform for the
integration of diverse forms of perception precisiation, including singular,
probability distribution, possibility distribution, interval, and fuzzy graph.
In fact, PrINNs allow neural networks to model dynamical systems by integrating
expert knowledge and perception-based information through loss functions,
enabling the creation of modern data-driven models. Some of the key
contributions include Mixture of Experts Informed Neural Networks (MOEINNs),
which combine heterogeneous expert knowledge into the network, and
Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the
incorporation of meta-information for enhanced model performance. Additionally,
Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural
networks leverage fuzzy logic constraints within a deep learning architecture,
allowing online training without pre-training and eliminating the need for
defuzzification. PrINNs represent a significant step forward in bridging the
gap between traditional physics-based modeling and modern data-driven
approaches, enabling neural networks to learn from both structured physics laws
and flexible perception-based rules. This approach empowers neural networks to
operate in uncertain environments, model complex systems, and discover new
forms of differential equations, making PrINNs a powerful tool for advancing
computational science and engineering.

</details>

### [52] [AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data](https://arxiv.org/abs/2505.03808)
*Ioannis Nasios*

Main category: cs.LG

TLDR: 该研究提出了一种结合多源遥感数据和人工智能模型的高效方法，用于检测有害藻华。


<details>
  <summary>Details</summary>
Motivation: 有害藻华对内陆水质和公共健康构成威胁，亟需高效、准确且经济的检测方法。

Method: 整合Sentinel-2光学影像、DEM高程数据和NOAA气候数据，结合树模型和神经网络构建集成模型。

Result: 树模型表现优异，加入神经网络增强了鲁棒性，展示了深度学习处理多源遥感数据的潜力。

Conclusion: 该方法适用于全球范围，代码开源，展示了遥感数据与AI结合解决环境问题的潜力。

Abstract: Harmful algal blooms are a growing threat to inland water quality and public
health worldwide, creating an urgent need for efficient, accurate, and
cost-effective detection methods. This research introduces a high-performing
methodology that integrates multiple open-source remote sensing data with
advanced artificial intelligence models. Key data sources include Copernicus
Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and
NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently
retrieved using platforms like Google Earth Engine (GEE) and Microsoft
Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the
altitude from the elevation model, the temperature and wind from NOAA as well
as the longitude and latitude were the most important features. The approach
combines two types of machine learning models, tree-based models and a neural
network, into an ensemble for classifying algal bloom severity. While the tree
models performed strongly on their own, incorporating a neural network added
robustness and demonstrated how deep learning models can effectively use
diverse remote sensing inputs. The method leverages high-resolution satellite
imagery and AI-driven analysis to monitor algal blooms dynamically, and
although initially developed for a NASA competition in the U.S., it shows
potential for global application. The complete code is available for further
adaptation and practical implementation, illustrating the convergence of remote
sensing data and AI to address critical environmental challenges
(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).

</details>

### [53] [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
*Prudhviraj Naidu,Zixian Wang,Leon Bergen,Ramamohan Paturi*

Main category: cs.LG

TLDR: Transformer语言模型在算法任务训练中表现出与幂律趋势不同的损失曲线相变，停滞期后突然下降，揭示了静默特征和响亮特征的学习过程。


<details>
  <summary>Details</summary>
Motivation: 挑战现有假设，即下一个词预测损失能可靠跟踪渐进进展，探索模型内部特征的发展及其对性能的影响。

Method: 训练Transformer模型于十项基础算法任务，分析损失曲线及内部表示，进行消融实验验证特征因果作用。

Result: 观察到损失曲线的相变，静默特征在停滞期学习，随后响亮特征突然获取导致损失骤降。消融实验证实特征对性能的关键影响。

Conclusion: 关键内部特征可能在表面下发展直至突然触发性能提升，挑战了传统损失曲线的解读方式。

Abstract: We train Transformer-based language models on ten foundational algorithmic
tasks and observe pronounced phase transitions in their loss curves that
deviate from established power-law scaling trends. Over large ranges of
compute, the validation loss barely improves, then abruptly decreases. Probing
the models' internal representations reveals the learning of quiet features
during the stagnant phase, followed by sudden acquisition of loud features that
coincide with the sharp drop in loss. Our ablation experiments show that
disrupting a single learned feature can dramatically degrade performance,
providing evidence of their causal role in task performance. These findings
challenge the prevailing assumption that next-token predictive loss reliably
tracks incremental progress; instead, key internal features may be developing
below the surface until they coalesce, triggering a rapid performance gain.

</details>

### [54] [When Dynamic Data Selection Meets Data Augmentation](https://arxiv.org/abs/2505.03809)
*Suorong Yang,Peng Ye,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TLDR: 提出了一种动态数据选择与增强统一的新框架，显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 动态数据选择虽加速训练但可能限制数据多样性，而数据增强未与选择优化结合，未能充分发挥协同效应。

Method: 通过估计样本的局部密度和多模态语义一致性联合分布，有针对性地选择适合增强的样本，同时抑制噪声或模糊数据。

Result: 在多个基准数据集和架构上优于现有方法，如在ImageNet-1k上减少50%训练成本且性能无损。

Conclusion: 该方法不仅提升训练效率和性能，还增强噪声抵抗力和模型鲁棒性，具有实际应用价值。

Abstract: Dynamic data selection aims to accelerate training with lossless performance.
However, reducing training data inherently limits data diversity, potentially
hindering generalization. While data augmentation is widely used to enhance
diversity, it is typically not optimized in conjunction with selection. As a
result, directly combining these techniques fails to fully exploit their
synergies. To tackle the challenge, we propose a novel online data training
framework that, for the first time, unifies dynamic data selection and
augmentation, achieving both training efficiency and enhanced performance. Our
method estimates each sample's joint distribution of local density and
multimodal semantic consistency, allowing for the targeted selection of
augmentation-suitable samples while suppressing the inclusion of noisy or
ambiguous data. This enables a more significant reduction in dataset size
without sacrificing model generalization. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches on various
benchmark datasets and architectures, e.g., reducing 50\% training costs on
ImageNet-1k with lossless performance. Furthermore, our approach enhances noise
resistance and improves model robustness, reinforcing its practical utility in
real-world scenarios.

</details>

### [55] [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
*Tuochao Chen,Nicholas Batchelder,Alisa Liu,Noah Smith,Shyamnath Gollakota*

Main category: cs.LG

TLDR: LlamaPIE是一款实时主动助手，通过可听设备提供简洁、不干扰的对话指导，无需用户显式调用。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型需用户显式调用，而LlamaPIE旨在通过背景运行主动预测用户需求，提升对话体验。

Method: 构建半合成对话数据集，采用双模型管道：小模型决定何时响应，大模型生成响应。

Result: 在真实数据集上验证有效，用户研究表明用户更偏好主动助手。

Conclusion: LlamaPIE能有效增强实时对话，具有实际应用潜力。

Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to
enhance human conversations through discreet, concise guidance delivered via
hearable devices. Unlike traditional language models that require explicit user
invocation, this assistant operates in the background, anticipating user needs
without interrupting conversations. We address several challenges, including
determining when to respond, crafting concise responses that enhance
conversations, leveraging knowledge of the user for context-aware assistance,
and real-time, on-device processing. To achieve this, we construct a
semi-synthetic dialogue dataset and propose a two-model pipeline: a small model
that decides when to respond and a larger model that generates the response. We
evaluate our approach on real-world datasets, demonstrating its effectiveness
in providing helpful, unobtrusive assistance. User studies with our assistant,
implemented on Apple Silicon M2 hardware, show a strong preference for the
proactive assistant over both a baseline with no assistance and a reactive
model, highlighting the potential of LlamaPie to enhance live conversations.

</details>

### [56] [ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior](https://arxiv.org/abs/2505.03811)
*Surajit Chakrabarty,Rukma Talwadker,Tridib Mukherjee*

Main category: cs.LG

TLDR: ScarceGAN是一种针对多维度纵向遥测数据中极稀有样本识别的半监督GAN方法，通过弱标签多类负样本和少量正样本改进模型，显著提升了稀有类别的召回率。


<details>
  <summary>Details</summary>
Motivation: 解决数据中正类样本极度稀缺、负类样本多类分布不均且特征重叠，以及大量未标记数据导致的弱先验问题。

Method: 重新设计半监督GAN，引入‘leeway’项放松对负样本的精确区分，改进判别器和生成器的损失目标。

Result: 在技能游戏中识别高风险玩家时，召回率超过85%（比传统半监督GAN提升60%），并在KDDCUP99入侵数据集的稀有攻击类（0.09%）上创下新基准。

Conclusion: ScarceGAN通过利用负类知识改进正类学习，为极稀有样本识别提供了高效解决方案。

Abstract: This paper introduces ScarceGAN which focuses on identification of extremely
rare or scarce samples from multi-dimensional longitudinal telemetry data with
small and weak label prior. We specifically address: (i) severe scarcity in
positive class, stemming from both underlying organic skew in the data, as well
as extremely limited labels; (ii) multi-class nature of the negative samples,
with uneven density distributions and partially overlapping feature
distributions; and (iii) massively unlabelled data leading to tiny and weak
prior on both positive and negative classes, and possibility of unseen or
unknown behavior in the unlabelled set, especially in the negative class.
Although related to PU learning problems, we contend that knowledge (or lack of
it) on the negative class can be leveraged to learn the compliment of it (i.e.,
the positive class) better in a semi-supervised manner. To this effect,
ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled
multi-class negative samples and the available positive samples. It relaxes the
supervised discriminator's constraint on exact differentiation between negative
samples by introducing a 'leeway' term for samples with noisy prior. We propose
modifications to the cost objectives of discriminator, in supervised and
unsupervised path as well as that of the generator. For identifying risky
players in skill gaming, this formulation in whole gives us a recall of over
85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very
minimal verbosity in the unknown space. Further ScarceGAN outperforms the
recall benchmarks established by recent GAN based specialized models for the
positive imbalanced class identification and establishes a new benchmark in
identifying one of rare attack classes (0.09%) in the intrusion dataset from
the KDDCUP99 challenge.

</details>

### [57] [Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications](https://arxiv.org/abs/2505.03812)
*Tomaso Aste*

Main category: cs.LG

TLDR: IFNs是一种用于建模复杂系统的框架，通过稀疏但局部密集的结构捕捉多元依赖关系。本文综述了IFNs的理论基础、构建方法及多领域应用，并探讨了其与机器学习的结合。


<details>
  <summary>Details</summary>
Motivation: IFNs旨在解决高维数据建模中的关键挑战，提供可解释性、计算效率和预测性能的提升。

Method: IFNs基于高阶网络构建方法，如TMFG和MFCF，生成单纯复形结构。

Result: IFNs在金融、生物、心理和AI等领域表现出色，尤其在图形建模中优于传统方法。

Conclusion: IFNs有望连接经典网络理论与现代数据驱动范式，并影响深度学习架构设计。

Abstract: Information Filtering Networks (IFNs) provide a powerful framework for
modeling complex systems through globally sparse yet locally dense and
interpretable structures that capture multivariate dependencies. This review
offers a comprehensive account of IFNs, covering their theoretical foundations,
construction methodologies, and diverse applications. Tracing their origins
from early network-based models to advanced formulations such as the
Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique
Forest (MFCF), the paper highlights how IFNs address key challenges in
high-dimensional data-driven modeling. IFNs and their construction
methodologies are intrinsically higher-order networks that generate simplicial
complexes-structures that are only now becoming popular in the broader
literature. Applications span fields including finance, biology, psychology,
and artificial intelligence, where IFNs improve interpretability, computational
efficiency, and predictive performance. Special attention is given to their
role in graphical modeling, where IFNs enable the estimation of sparse inverse
covariance matrices with greater accuracy and scalability than traditional
approaches like Graphical LASSO. Finally, the review discusses recent
developments that integrate IFNs with machine learning and deep learning,
underscoring their potential not only to bridge classical network theory with
contemporary data-driven paradigms, but also to shape the architectures of deep
learning models themselves.

</details>

### [58] [Program Semantic Inequivalence Game with Large Language Models](https://arxiv.org/abs/2505.03818)
*Antonio Valerio Miceli-Barone,Vaishak Belle,Ali Payani*

Main category: cs.LG

TLDR: 提出了一种通过语义不等价游戏SInQ生成代码推理训练数据的方法，以提升大语言模型在复杂编程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在简单编程任务中表现良好，但在需要复杂语义推理的任务中表现不佳，且缺乏相关训练数据。

Method: 使用生成器和评估器代理通过半对抗训练生成语义不同的程序变体，并识别导致行为差异的输入示例。

Result: 在跨语言漏洞检测和Python内置标识符交换基准测试中表现显著优于现有模型。

Conclusion: 该方法通过自博弈实现理论上的无限改进潜力，并公开了实验代码和合成数据。

Abstract: Large Language Models (LLMs) can achieve strong performance on everyday
coding tasks, but they can fail on complex tasks that require non-trivial
reasoning about program semantics. Finding training examples to teach LLMs to
solve these tasks can be challenging.
  In this work, we explore a method to synthetically generate code reasoning
training data based on a semantic inequivalence game SInQ: a generator agent
creates program variants that are semantically distinct, derived from a dataset
of real-world programming tasks, while an evaluator agent has to identify input
examples that cause the original programs and the generated variants to diverge
in their behaviour, with the agents training each other semi-adversarially. We
prove that this setup enables theoretically unlimited improvement through
self-play in the limit of infinite computational resources.
  We evaluated our approach on multiple code generation and understanding
benchmarks, including cross-language vulnerability detection (Lu et al., 2021),
where our method improves vulnerability detection in C/C++ code despite being
trained exclusively on Python code, and the challenging Python builtin
identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas
modern LLMs still struggle with this benchmark, our approach yields substantial
improvements.
  We release the code needed to replicate the experiments, as well as the
generated synthetic data, which can be used to fine-tune LLMs.

</details>

### [59] [Focus on the Likely: Test-time Instance-based Uncertainty Removal](https://arxiv.org/abs/2505.03819)
*Johannes Schneider*

Main category: cs.LG

TLDR: 提出了两种无需辅助数据的测试时微调方法，通过单步梯度下降优化高不确定性预测，提高准确性。


<details>
  <summary>Details</summary>
Motivation: 改进模型在高不确定性情况下的预测表现，无需额外数据，仅利用测试实例本身。

Method: 在推理阶段引入对可能类别的额外关注，通过单步梯度下降优化预测。

Result: 在文本和图像领域的多种模型上，对高不确定性样本的准确性有显著提升。

Conclusion: 方法有效优化了高不确定性预测，理论分析揭示了其对共享和非共享特征的影响。

Abstract: We propose two novel test-time fine-tuning methods to improve uncertain model
predictions. Our methods require no auxiliary data and use the given test
instance only. Instead of performing a greedy selection of the most likely
class to make a prediction, we introduce an additional focus on the likely
classes step during inference. By applying a single-step gradient descent, we
refine predictions when an initial forward pass indicates high uncertainty.
This aligns predictions more closely with the ideal of assigning zero
probability to less plausible outcomes. Our theoretical discussion provides a
deeper understanding highlighting the impact on shared and non-shared features
among (focus) classes. The experimental evaluation highlights accuracy gains on
samples exhibiting high decision uncertainty for a diverse set of models from
both the text and image domain using the same hyperparameters.

</details>

### [60] [DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction](https://arxiv.org/abs/2505.03822)
*Hao Wu,Jialiang Wang*

Main category: cs.LG

TLDR: 本文提出了一种双正则化二阶潜在因子（DRSLF）模型，通过结合L1和L2正则化及二阶信息，提高了QoS预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LFA模型依赖一阶优化器和L2正则化，导致QoS预测精度较低。

Method: 提出DRSLF模型，结合L1和L2正则化，并在共轭梯度步骤中计算Hessian-向量积以引入二阶信息。

Result: 在两个真实响应时间QoS数据集上，DRSLF表现出比基线模型更强的低秩表示能力。

Conclusion: DRSLF模型通过双正则化和二阶信息显著提升了QoS预测的准确性。

Abstract: Quality-of-Service (QoS) data plays a crucial role in cloud service
selection. Since users cannot access all services, QoS can be represented by a
high-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)
models have been proven effective as low-rank representation techniques for
addressing this issue. However, most LFA models rely on first-order optimizers
and use L2-norm regularization, which can lead to lower QoS prediction
accuracy. To address this issue, this paper proposes a double regularized
second-order latent factor (DRSLF) model with two key ideas: a) integrating
L1-norm and L2-norm regularization terms to enhance the low-rank representation
performance; b) incorporating second-order information by calculating the
Hessian-vector product in each conjugate gradient step. Experimental results on
two real-world response-time QoS datasets demonstrate that DRSLF has a higher
low-rank representation capability than two baselines.

</details>

### [61] [Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments](https://arxiv.org/abs/2505.03825)
*Anushiya Arunan,Yan Qin,Xiaoli Li,Yuen Chau*

Main category: cs.LG

TLDR: 提出了一种名为ITA-CTF的数据高效框架，用于从多维时间序列中学习有效表示，通过对比学习和智能增强提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决在多维时间序列分类中，低训练数据环境下标准深度学习模型容易过拟合的问题。

Method: 结合对比张量分解（CTF）和智能增强（ITA）模块，学习核心解释性组件及其联合依赖关系，并通过对比损失优化提升分类性能。

Result: 在五个分类任务中，性能显著提升，最高达18.7%。

Conclusion: ITA-CTF框架在低数据环境下表现优异，能够有效学习复杂特征并提升分类准确性。

Abstract: Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.

</details>

### [62] [MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation](https://arxiv.org/abs/2505.03827)
*Xin Wang,Ling Feng,Huijun Zhang,Lei Cao,Kaisheng Zeng,Qi Li,Yang Ding,Yi Dai,David Clifton*

Main category: cs.LG

TLDR: 该研究提出了一种基于元学习的压力源估计框架，通过社交媒体帖子识别具体压力源，解决了数据稀疏和新压力源不断出现的问题。


<details>
  <summary>Details</summary>
Motivation: 现代社会中压力对健康的影响日益严重，而社交媒体为压力检测提供了新途径。现有研究多关注压力状态分类，但缺乏对具体压力源的识别。

Method: 采用元学习框架，结合元知识继承机制，以少量标注数据学习新压力源，避免灾难性遗忘。

Result: 实验表明，该模型在性能上优于基线方法，并公开了一个社交媒体压力源数据集。

Conclusion: 该框架能有效识别新压力源，为人工智能辅助心理健康提供了实用工具。

Abstract: Stress haunts people in modern society, which may cause severe health issues
if left unattended. With social media becoming an integral part of daily life,
leveraging social media to detect stress has gained increasing attention. While
the majority of the work focuses on classifying stress states and stress
categories, this study introduce a new task aimed at estimating more specific
stressors (like exam, writing paper, etc.) through users' posts on social
media. Unfortunately, the diversity of stressors with many different classes
but a few examples per class, combined with the consistent arising of new
stressors over time, hinders the machine understanding of stressors. To this
end, we cast the stressor estimation problem within a practical scenario
few-shot learning setting, and propose a novel meta-learning based stressor
estimation framework that is enhanced by a meta-knowledge inheritance
mechanism. This model can not only learn generic stressor context through
meta-learning, but also has a good generalization ability to estimate new
stressors with little labeled data. A fundamental breakthrough in our approach
lies in the inclusion of the meta-knowledge inheritance mechanism, which equips
our model with the ability to prevent catastrophic forgetting when adapting to
new stressors. The experimental results show that our model achieves
state-of-the-art performance compared with the baselines. Additionally, we
construct a social media-based stressor estimation dataset that can help train
artificial intelligence models to facilitate human well-being. The dataset is
now public at
\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\underline{Kaggle}}
and
\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\underline{Hugging
Face}}.

</details>

### [63] [Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics](https://arxiv.org/abs/2505.03849)
*Jonathan Gorard,Ammar Hakim,Hong Qin,Kyle Parfrey,Shantenu Jha*

Main category: cs.LG

TLDR: 论文提出了一种混合方法，结合蒙特卡洛采样与形式验证技术，以解决高维参数空间中的物理和数学一致性问题。


<details>
  <summary>Details</summary>
Motivation: 核聚变和高能天体物理中的逆问题通常涉及高维参数扫描和大规模模拟，但现有方法无法保证参数组合的物理有效性或数学一致性。

Method: 采用蒙特卡洛采样结合非线性降维技术（如自动编码器和流形学习），并引入形式验证方法，以确保参数空间的数学和物理正确性。

Result: 该方法能够在考虑实验和物理模型不确定性的同时，构建具有可证明正确性的参数空间限制。

Conclusion: 混合方法为解决高维逆问题提供了一种新思路，兼顾了计算效率和参数组合的可靠性。

Abstract: Many inverse problems in nuclear fusion and high-energy astrophysics
research, such as the optimization of tokamak reactor geometries or the
inference of black hole parameters from interferometric images, necessitate
high-dimensional parameter scans and large ensembles of simulations to be
performed. Such inverse problems typically involve large uncertainties, both in
the measurement parameters being inverted and in the underlying physics models
themselves. Monte Carlo sampling, when combined with modern non-linear
dimensionality reduction techniques such as autoencoders and manifold learning,
can be used to reduce the size of the parameter spaces considerably. However,
there is no guarantee that the resulting combinations of parameters will be
physically valid, or even mathematically consistent. In this position paper, we
advocate adopting a hybrid approach that leverages our recent advances in the
development of formal verification methods for numerical algorithms, with the
goal of constructing parameter space restrictions with provable mathematical
and physical correctness properties, whilst nevertheless respecting both
experimental uncertainties and uncertainties in the underlying physical
processes.

</details>

### [64] [Machine Learning: a Lecture Note](https://arxiv.org/abs/2505.03861)
*Kyunghyun Cho*

Main category: cs.LG

TLDR: 这篇讲义旨在为数据科学及相关领域的硕士和博士生提供机器学习的基础知识，涵盖分类任务、概率无监督学习及进阶主题。


<details>
  <summary>Details</summary>
Motivation: 为早期研究生提供机器学习的理论基础，帮助他们为高级研究和学习做准备。

Method: 从分类任务的基本概念（如损失函数、反向传播、随机梯度下降）入手，深入探讨概率无监督学习模型（如生成对抗网络、自回归模型），最后介绍强化学习等进阶主题。

Result: 学生将掌握机器学习的核心概念和方法，为进一步研究人工智能奠定基础。

Conclusion: 讲义为学生提供了扎实的机器学习基础，使其能够顺利进入更高级的学习和研究。

Abstract: This lecture note is intended to prepare early-year master's and PhD students
in data science or a related discipline with foundational ideas in machine
learning. It starts with basic ideas in modern machine learning with
classification as a main target task. These basic ideas include loss
formulation, backpropagation, stochastic gradient descent, generalization,
model selection as well as fundamental blocks of artificial neural networks.
Based on these basic ideas, the lecture note explores in depth the probablistic
approach to unsupervised learning, covering directed latent variable models,
product of experts, generative adversarial networks and autoregressive models.
Finally, the note ends by covering a diverse set of further topics, such as
reinforcement learning, ensemble methods and meta-learning. After reading this
lecture note, a student should be ready to embark on studying and researching
more advanced topics in machine learning and more broadly artificial
intelligence.

</details>

### [65] [Explaining Anomalies with Tensor Networks](https://arxiv.org/abs/2505.03911)
*Hans Hohenfeld,Marius Beuerle,Elie Mounzer*

Main category: cs.LG

TLDR: 论文扩展了张量网络在异常检测中的应用，从离散值数据扩展到实值数据，并引入树张量网络，展示了其解释性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 张量网络在量子多体波函数和机器学习中受到关注，但此前主要应用于离散值数据。本文旨在将其扩展到实值数据领域，并提升异常检测的可解释性。

Method: 使用矩阵乘积态和树张量网络进行异常检测，应用于三个基准问题，并与基线模型比较。

Result: 方法在预测性能上与基线模型相当，同时能够解释异常样本。

Conclusion: 研究扩展了张量网络的应用范围，为未来更复杂架构的探索铺平了道路。

Abstract: Tensor networks, a class of variational quantum many-body wave functions have
attracted considerable research interest across many disciplines, including
classical machine learning. Recently, Aizpurua et al. demonstrated explainable
anomaly detection with matrix product states on a discrete-valued
cyber-security task, using quantum-inspired methods to gain insight into the
learned model and detected anomalies. Here, we extend this framework to
real-valued data domains. We furthermore introduce tree tensor networks for the
task of explainable anomaly detection. We demonstrate these methods with three
benchmark problems, show adequate predictive performance compared to several
baseline models and both tensor network architectures' ability to explain
anomalous samples. We thereby extend the application of tensor networks to a
broader class of potential problems and open a pathway for future extensions to
more complex tensor network architectures.

</details>

### [66] [SAND: One-Shot Feature Selection with Additive Noise Distortion](https://arxiv.org/abs/2505.03923)
*Pedram Pad,Hadi Hammoud,Mohamad Dia,Nadim Maamari,L. Andrea Dunbar*

Main category: cs.LG

TLDR: 提出了一种新颖的非侵入式特征选择层，自动选择最信息化的特征，无需调整超参数或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要后选择重新训练和大量超参数调整，增加了复杂性。

Method: 通过可训练增益和噪声失真自动选择特征，无需修改损失函数或网络架构。

Result: 在标准基准数据集和真实数据集上表现优于或匹配现有方法。

Conclusion: 证明了简单性和性能可以共存，为机器学习提供了一个强大且直接的特征选择工具。

Abstract: Feature selection is a critical step in data-driven applications, reducing
input dimensionality to enhance learning accuracy, computational efficiency,
and interpretability. Existing state-of-the-art methods often require
post-selection retraining and extensive hyperparameter tuning, complicating
their adoption. We introduce a novel, non-intrusive feature selection layer
that, given a target feature count $k$, automatically identifies and selects
the $k$ most informative features during neural network training. Our method is
uniquely simple, requiring no alterations to the loss function, network
architecture, or post-selection retraining. The layer is mathematically elegant
and can be fully described by: \begin{align} \nonumber \tilde{x}_i = a_i x_i +
(1-a_i)z_i \end{align} where $x_i$ is the input feature, $\tilde{x}_i$ the
output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that
$\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect,
driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the
rest to $0$ (discarding redundant ones) via weighted noise distortion and gain
normalization. Despite its extreme simplicity, our method delivers
state-of-the-art performance on standard benchmark datasets and a novel
real-world dataset, outperforming or matching existing approaches without
requiring hyperparameter search for $k$ or retraining. Theoretical analysis in
the context of linear regression further validates its efficacy. Our work
demonstrates that simplicity and performance are not mutually exclusive,
offering a powerful yet straightforward tool for feature selection in machine
learning.

</details>

### [67] [Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading](https://arxiv.org/abs/2505.03949)
*John Christopher Tidwell,John Storm Tidwell*

Main category: cs.LG

TLDR: 提出了一种结合CNN、LSTM和DQN的深度学习框架，用于解决自动化股票交易中市场噪声和复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法和直接强化学习在应对市场噪声、复杂性和泛化能力方面存在困难。

Method: 集成CNN识别技术指标图像模式，LSTM捕捉时间依赖性，DQN学习最优交易策略。

Result: 框架能够有效提取特征并学习交易策略。

Conclusion: 该集成方法为自动化股票交易提供了更优的解决方案。

Abstract: This project addresses the challenge of automated stock trading, where
traditional methods and direct reinforcement learning (RL) struggle with market
noise, complexity, and generalization. Our proposed solution is an integrated
deep learning framework combining a Convolutional Neural Network (CNN) to
identify patterns in technical indicators formatted as images, a Long
Short-Term Memory (LSTM) network to capture temporal dependencies across both
price history and technical indicators, and a Deep Q-Network (DQN) agent which
learns the optimal trading policy (buy, sell, hold) based on the features
extracted by the CNN and LSTM.

</details>

### [68] [Sufficient Decision Proxies for Decision-Focused Learning](https://arxiv.org/abs/2505.03953)
*Noah Schutte,Grigorii Veviurko,Krzysztof Postek,Neil Yorke-Smith*

Main category: cs.LG

TLDR: 论文研究了在不确定性优化问题中，如何通过决策导向学习（DFL）选择有效的决策代理方法，并探讨了单值预测与分布估计的适用条件。


<details>
  <summary>Details</summary>
Motivation: 在不确定性优化问题中，传统方法通常假设单值预测或分布估计，但缺乏对这两种假设适用条件的理解。本文旨在填补这一空白。

Method: 通过分析问题特性，提出适用于DFL的决策代理方法，并在连续和离散变量问题中验证其有效性。

Result: 实验表明，所提出的方法在目标函数和约束条件的不确定性下均表现良好，且学习任务复杂度较低。

Conclusion: 本文为DFL中的决策代理选择提供了理论支持，并展示了其在实际问题中的有效性。

Abstract: When solving optimization problems under uncertainty with contextual data,
utilizing machine learning to predict the uncertain parameters is a popular and
effective approach. Decision-focused learning (DFL) aims at learning a
predictive model such that decision quality, instead of prediction accuracy, is
maximized. Common practice here is to predict a single value for each uncertain
parameter, implicitly assuming that there exists a (single-scenario)
deterministic problem approximation (proxy) that is sufficient to obtain an
optimal decision. Other work assumes the opposite, where the underlying
distribution needs to be estimated. However, little is known about when either
choice is valid. This paper investigates for the first time problem properties
that justify using either assumption. Using this, we present effective decision
proxies for DFL, with very limited compromise on the complexity of the learning
task. We show the effectiveness of presented approaches in experiments on
problems with continuous and discrete variables, as well as uncertainty in the
objective function and in the constraints.

</details>

### [69] [Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation](https://arxiv.org/abs/2505.03955)
*Charupriya Sharma,Iñaki Estella Aguerri,Daniel Guimarans*

Main category: cs.LG

TLDR: FlowRec提出了一种基于网络流优化的层次预测协调方法，显著提升了计算效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 解决传统层次预测协调方法（如MinT）局限于树结构且计算成本高的问题。

Method: 将层次预测协调重新定义为网络流优化问题，支持广义网络结构，并证明其在多项式时间内可解。

Result: FlowRec在计算复杂度（O(n^2 log n)）、准确性和内存使用上显著优于MinT，实验显示速度提升3-40倍，内存减少5-7倍。

Conclusion: FlowRec是一种高效、通用的层次预测协调工具，适用于大规模动态场景。

Abstract: Hierarchical forecasting with reconciliation requires forecasting values of a
hierarchy (e.g.~customer demand in a state and district), such that forecast
values are linked (e.g.~ district forecasts should add up to the state
forecast). Basic forecasting provides no guarantee for these desired structural
relationships. Reconciliation addresses this problem, which is crucial for
organizations requiring coherent predictions across multiple aggregation
levels. Current methods like minimum trace (MinT) are mostly limited to tree
structures and are computationally expensive. We introduce FlowRec, which
reformulates hierarchical forecast reconciliation as a network flow
optimization, enabling forecasting on generalized network structures. While
reconciliation under the $\ell_0$ norm is NP-hard, we prove polynomial-time
solvability for all $\ell_{p > 0}$ norms and , for any strictly convex and
continuously differentiable loss function. For sparse networks, FlowRec
achieves $O(n^2\log n)$ complexity, significantly improving upon MinT's
$O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general
networks, replacing MinT's error-covariance estimation step with direct network
structural information. A key novelty of our approach is its handling of
dynamic scenarios: while traditional methods recompute both base forecasts and
reconciliation, FlowRec provides efficient localised updates with optimality
guarantees. Monotonicity ensures that when forecasts improve incrementally, the
initial reconciliation remains optimal. We also establish efficient,
error-bounded approximate reconciliation, enabling fast updates in
time-critical applications. Experiments on both simulated and real benchmarks
demonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage
by 5-7x. These results establish FlowRec as a powerful tool for large-scale
hierarchical forecasting applications.

</details>

### [70] [Call for Action: towards the next generation of symbolic regression benchmark](https://arxiv.org/abs/2505.03977)
*Guilherme S. Imai Aldeia,Hengzhe Zhang,Geoffrey Bomarito,Miles Cranmer,Alcides Fonseca,Bogdan Burlacu,William G. La Cava,Fabrício Olivetti de França*

Main category: cs.LG

TLDR: 论文介绍了SRBench的更新版本，扩展了评估方法、改进了指标和可视化，并分析了模型复杂性、准确性和能耗之间的权衡。结果表明没有单一算法在所有数据集上占优，呼吁社区共同维护SRBench。


<details>
  <summary>Details</summary>
Motivation: 由于符号回归（SR）方法的多样性、数据集和评估标准的差异，基准测试SR方法具有挑战性。

Method: 更新SRBench，增加评估方法数量，优化指标和可视化，分析模型复杂性、准确性和能耗的权衡。

Result: 结果显示没有单一算法在所有数据集上表现最优。

Conclusion: 呼吁社区共同维护SRBench，提出标准化和能效改进的建议。

Abstract: Symbolic Regression (SR) is a powerful technique for discovering
interpretable mathematical expressions. However, benchmarking SR methods
remains challenging due to the diversity of algorithms, datasets, and
evaluation criteria. In this work, we present an updated version of SRBench.
Our benchmark expands the previous one by nearly doubling the number of
evaluated methods, refining evaluation metrics, and using improved
visualizations of the results to understand the performances. Additionally, we
analyze trade-offs between model complexity, accuracy, and energy consumption.
Our results show that no single algorithm dominates across all datasets. We
propose a call for action from SR community in maintaining and evolving SRBench
as a living benchmark that reflects the state-of-the-art in symbolic
regression, by standardizing hyperparameter tuning, execution constraints, and
computational resource allocation. We also propose deprecation criteria to
maintain the benchmark's relevance and discuss best practices for improving SR
algorithms, such as adaptive hyperparameter tuning and energy-efficient
implementations.

</details>

### [71] [Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations](https://arxiv.org/abs/2505.03980)
*Aroon Sankoh,Victor Wickerhauser*

Main category: cs.LG

TLDR: 比较传统统计方法（MLE）与深度学习模型（RNN）在估计Ornstein-Uhlenbeck过程参数时的准确性和计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法（如MLE）在估计随机微分方程参数时存在局限性，深度学习技术（如RNN）可能提供更精确的估计。

Method: 通过实验比较MLE和RNN在Ornstein-Uhlenbeck过程参数估计中的表现。

Result: 实验结果展示了RNN在估计准确性和计算效率方面的潜力。

Conclusion: 深度学习模型（如RNN）在随机微分方程参数估计中可能优于传统统计方法。

Abstract: Stochastic differential equations such as the Ornstein-Uhlenbeck process have
long been used to model realworld probablistic events such as stock prices and
temperature fluctuations. While statistical methods such as Maximum Likelihood
Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have
historically been used to estimate the parameters of stochastic differential
equations, the recent explosion of deep learning technology suggests that
models such as a Recurrent Neural Network (RNN) could produce more precise
estimators. We present a series of experiments that compare the estimation
accuracy and computational expensiveness of a statistical method (MLE) with a
deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.

</details>

### [72] [Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation](https://arxiv.org/abs/2505.03983)
*Hengyuan Hu,Aniket Das,Dorsa Sadigh,Nima Anari*

Main category: cs.LG

TLDR: DDPMs的推理时间瓶颈问题通过引入Autospeculative Decoding (ASD)得到优化，ASD利用DDPM的增量交换性，无需辅助模型即可实现并行加速。


<details>
  <summary>Details</summary>
Motivation: DDPMs在生成建模中表现强大，但顺序计算需求导致推理时间瓶颈，限制了其实际应用。

Method: 通过将DDPM与随机定位联系，证明增量交换性，并基于此提出ASD算法，无需辅助模型即可优化性能。

Result: ASD在理论上实现了$	ilde{O}(K^{rac{1}{3}})$的并行加速，实际应用中显著提升了DDPM的推理速度。

Conclusion: ASD为DDPM提供了一种高效、通用的优化方法，解决了推理时间瓶颈问题。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful
tools for generative modeling. However, their sequential computation
requirements lead to significant inference-time bottlenecks. In this work, we
utilize the connection between DDPMs and Stochastic Localization to prove that,
under an appropriate reparametrization, the increments of DDPM satisfy an
exchangeability property. This general insight enables near-black-box
adaptation of various performance optimization techniques from autoregressive
models to the diffusion setting. To demonstrate this, we introduce
\emph{Autospeculative Decoding} (ASD), an extension of the widely used
speculative decoding algorithm to DDPMs that does not require any auxiliary
draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O}
(K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.
We also demonstrate that a practical implementation of autospeculative decoding
accelerates DDPM inference significantly in various domains.

</details>

### [73] [Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics](https://arxiv.org/abs/2505.03992)
*Jarren Briscoe,Garrett Kepler,Daryl Deford,Assefaw Gebremedhin*

Main category: cs.LG

TLDR: 论文揭示了分类指标中组合概率导致的样本量偏差问题，提出了一种模型无关的评估与校正方法，并分析了未定义案例对评估的影响。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习模型时，样本量偏差可能影响分类指标的准确性，尤其是在社会应用中，不同群体规模差异较大时，这一问题更为突出。

Method: 分析了多种常用分类指标中的偏差，并提出了一种模型无关的评估与校正技术，同时研究了未定义案例对评估的影响。

Result: 揭示了组合概率在标准评估实践中的潜在问题，为公平和可信的分类方法提供了改进方向。

Conclusion: 该研究为分类模型的公平性和可信度评估提供了新的视角和方法。

Abstract: Evaluating machine learning models is crucial not only for determining their
technical accuracy but also for assessing their potential societal
implications. While the potential for low-sample-size bias in algorithms is
well known, we demonstrate the significance of sample-size bias induced by
combinatorics in classification metrics. This revelation challenges the
efficacy of these metrics in assessing bias with high resolution, especially
when comparing groups of disparate sizes, which frequently arise in social
applications. We provide analyses of the bias that appears in several commonly
applied metrics and propose a model-agnostic assessment and correction
technique. Additionally, we analyze counts of undefined cases in metric
calculations, which can lead to misleading evaluations if improperly handled.
This work illuminates the previously unrecognized challenge of combinatorics
and probability in standard evaluation practices and thereby advances
approaches for performing fair and trustworthy classification methods.

</details>

### [74] [Iterative Orthogonalization Scaling Laws](https://arxiv.org/abs/2505.04005)
*Devan Selvaraj*

Main category: cs.LG

TLDR: 论文探讨了muon优化器在大规模应用中可能因奇异值收缩而遇到的问题，但未提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究muon优化器在大规模应用中的潜在问题，特别是奇异值收缩对正交化过程的影响。

Method: 通过理论和实证分析随机矩阵的奇异值收缩行为。

Result: 证实了muon优化器在大规模下存在奇异值收缩问题。

Conclusion: 论文揭示了问题但未提供解决方案，为未来研究指明方向。

Abstract: The muon optimizer has picked up much attention as of late as a possible
replacement to the seemingly omnipresent Adam optimizer. Recently, care has
been taken to document the scaling laws of hyper-parameters under muon such as
weight decay and learning rate. However, at much larger scales the iterative
orthogonalization procedure present in muon may suffer a possible issue as the
singular values of random matrices shrink with scale. This paper shows this
scaling behavior theoretically and empirically on random matrices but does not
suggest what to do about it.

</details>

### [75] [LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?](https://arxiv.org/abs/2505.04075)
*Teddy Foley,Spencer Guo,Henry Josephson,Anqi Qu,Jack Sanderson*

Main category: cs.LG

TLDR: 论文探讨了在计算资源受限的情况下，大型语言模型（LLM）是否仍能通过算法创新取得进展，并提出了计算等效增益（CEG）指标来衡量算法改进的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于监管机构对高性能硬件的限制，探讨LLM在计算资源受限环境下的发展潜力。

Method: 提出了一种分类框架，区分计算依赖型和计算独立型算法创新，并通过小规模GPT-2实验验证了CEG指标。

Result: 实验表明，计算独立型创新在资源受限环境下仍能带来显著性能提升（CEG达3.5倍），而计算依赖型创新在小规模实验中效果有限甚至有害。

Conclusion: 计算独立型算法创新在资源受限环境下具有重要价值，而计算依赖型创新仍需充足计算资源支持。

Abstract: This paper examines whether large language model (LLM) capabilities can
continue to advance without additional compute by analyzing the development and
role of algorithms used in state-of-the-art LLMs. Motivated by regulatory
efforts that have largely focused on restricting access to high-performance
hardware, we ask: Can LLMs progress in a compute-constrained environment, and
how do algorithmic innovations perform under such conditions?
  To address these questions, we introduce a novel classification framework
that distinguishes between compute-dependent innovations -- which yield
disproportionate benefits at high compute levels (e.g., the Transformer
architecture and mixture-of-experts models) and compute-independent
innovations, which improve efficiency across all compute scales (e.g., rotary
positional encoding, FlashAttention, or layer normalization). We quantify these
contributions using a metric called compute-equivalent gain (CEG), which
estimates the additional compute that would be required to achieve similar
improvements without these algorithmic advancements.
  To validate this framework, we conduct small-scale training experiments with
a scaled-down GPT-2 model. Our results confirm that compute-independent
advancements yield meaningful performance gains even in resource-constrained
settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast,
compute-dependent advancements provided little benefit or even degraded
performance at the small scale, reinforcing the importance of compute
availability for certain algorithmic gains.

</details>

### [76] [Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training](https://arxiv.org/abs/2505.04083)
*Aditya K. Ranjan,Siddharth Singh,Cunyang Wei,Abhinav Bhatele*

Main category: cs.LG

TLDR: Plexus提出了一种3D并行方法，用于全图训练，解决了GPU内存限制和分布式训练中的通信开销问题，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在大规模图上的训练问题，包括GPU内存限制、采样导致的精度下降以及分布式训练中的通信开销和负载不均衡。

Method: 提出Plexus，一种3D并行方法，结合负载平衡的置换方案和性能模型优化配置。

Result: 在多个数据集上测试，Plexus在2048 GPU上实现了2.3x-12.5x的速度提升，解决方案时间减少了5.2-8.7x（Perlmutter）和7-54.2x（Frontier）。

Conclusion: Plexus是一种高效的全图训练方法，适用于大规模图，显著提升了训练速度和性能。

Abstract: Graph neural networks have emerged as a potent class of neural networks
capable of leveraging the connectivity and structure of real-world graphs to
learn intricate properties and relationships between nodes. Many real-world
graphs exceed the memory capacity of a GPU due to their sheer size, and using
GNNs on them requires techniques such as mini-batch sampling to scale. However,
this can lead to reduced accuracy in some cases, and sampling and data transfer
from the CPU to the GPU can also slow down training. On the other hand,
distributed full-graph training suffers from high communication overhead and
load imbalance due to the irregular structure of graphs. We propose Plexus, a
three-dimensional (3D) parallel approach for full-graph training that tackles
these issues and scales to billion-edge graphs. Additionally, we introduce
optimizations such as a permutation scheme for load balancing, and a
performance model to predict the optimal 3D configuration. We evaluate Plexus
on several graph datasets and show scaling results for up to 2048 GPUs on
Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus
achieves unprecedented speedups of 2.3x-12.5x over existing methods and a
reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on
Frontier.

</details>

### [77] [Position: We need responsible, application-driven (RAD) AI research](https://arxiv.org/abs/2505.04104)
*Sarah Hartman,Cheng Soon Ong,Julia Powles,Petra Kuhnert*

Main category: cs.LG

TLDR: 论文主张通过负责任、应用驱动的方法（RAD-AI）推动AI研究，以实现科学和社会进步。


<details>
  <summary>Details</summary>
Motivation: 随着AI在社会中的广泛应用，研究者需关注具体应用场景，包括伦理、法律、技术和社会约束。

Method: 提出三阶段方法：组建跨学科团队、解决特定情境问题、通过测试和实践社区验证效果。

Result: 提出RAD-AI框架，旨在通过适应性方法满足社区需求。

Conclusion: 应用驱动的AI研究有望通过技术可行且符合社区价值观的方法创造新价值。

Abstract: This position paper argues that achieving meaningful scientific and societal
advances with artificial intelligence (AI) requires a responsible,
application-driven approach (RAD) to AI research. As AI is increasingly
integrated into society, AI researchers must engage with the specific contexts
where AI is being applied. This includes being responsive to ethical and legal
considerations, technical and societal constraints, and public discourse. We
present the case for RAD-AI to drive research through a three-staged approach:
(1) building transdisciplinary teams and people-centred studies; (2) addressing
context-specific methods, ethical commitments, assumptions, and metrics; and
(3) testing and sustaining efficacy through staged testbeds and a community of
practice. We present a vision for the future of application-driven AI research
to unlock new value through technically feasible methods that are adaptive to
the contextual needs and values of the communities they ultimately serve.

</details>

### [78] [Alpha Excel Benchmark](https://arxiv.org/abs/2505.04110)
*David Noever,Forrest McKee*

Main category: cs.LG

TLDR: 该研究提出了一个基于金融建模世界杯（FMWC）Excel竞赛的新基准，用于评估大型语言模型（LLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过实际业务任务（而非抽象学术问题）评估LLMs的能力，填补学术基准与实际业务应用之间的差距。

Method: 将113个FMWC挑战转换为可编程评估的JSON格式，并比较多个领先LLMs的表现。

Result: 模型在不同挑战类别中表现差异显著，擅长模式识别任务，但在复杂数值推理上表现不佳。

Conclusion: 该基准为评估LLMs在业务任务中的能力提供了标准化框架，并强调了Excel用户群体作为评估指标的重要性。

Abstract: This study presents a novel benchmark for evaluating Large Language Models
(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)
Excel competitions. We introduce a methodology for converting 113 existing FMWC
challenges into programmatically evaluable JSON formats and use this dataset to
compare the performance of several leading LLMs. Our findings demonstrate
significant variations in performance across different challenge categories,
with models showing specific strengths in pattern recognition tasks but
struggling with complex numerical reasoning. The benchmark provides a
standardized framework for assessing LLM capabilities in realistic
business-oriented tasks rather than abstract academic problems. This research
contributes to the growing field of AI benchmarking by establishing proficiency
among the 1.5 billion people who daily use Microsoft Excel as a meaningful
evaluation metric that bridges the gap between academic AI benchmarks and
practical business applications.

</details>

### [79] [LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification](https://arxiv.org/abs/2505.04139)
*Hongyi Li,Jun Xu,William Ward Armstrong*

Main category: cs.LG

TLDR: LHT是一种新型斜决策树模型，通过非迭代、统计驱动的方法构建分割超平面，具有表达力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 提出LHT是为了解决传统斜决策树模型依赖迭代优化或启发式方法的问题，提供一种更直接且理论支持的方法。

Method: LHT直接计算超平面参数，基于节点内类间特征期望差异的权重，并使用局部最小二乘拟合构建叶节点的分段线性隶属函数。

Result: 实验证明LHT在基准数据集上具有竞争力，且构建时间复杂度为O(mnd)，适合实际应用。

Conclusion: LHT是一种实用、理论支持且可解释的斜决策树模型，为树模型领域提供了新选择。

Abstract: We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision
tree model designed for expressive and interpretable classification. LHT
fundamentally distinguishes itself through a non-iterative,
statistically-driven approach to constructing splitting hyperplanes. Unlike
methods that rely on iterative optimization or heuristics, LHT directly
computes the hyperplane parameters, which are derived from feature weights
based on the differences in feature expectations between classes within each
node. This deterministic mechanism enables a direct and well-defined hyperplane
construction process. Predictions leverage a unique piecewise linear membership
function within leaf nodes, obtained via local least-squares fitting. We
formally analyze the convergence of the LHT splitting process, ensuring that
each split yields meaningful, non-empty partitions. Furthermore, we establish
that the time complexity for building an LHT up to depth $d$ is $O(mnd)$,
demonstrating the practical feasibility of constructing trees with powerful
oblique splits using this methodology. The explicit feature weighting at each
split provides inherent interpretability. Experimental results on benchmark
datasets demonstrate LHT's competitive accuracy, positioning it as a practical,
theoretically grounded, and interpretable alternative in the landscape of
tree-based models. The implementation of the proposed method is available at
https://github.com/Hongyi-Li-sz/LHT_model.

</details>

### [80] [FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04158)
*Yulong Wang,Yushuo Liu,Xiaoyi Duan,Kai Wang*

Main category: cs.LG

TLDR: FilterTS是一种新颖的多变量时间序列预测模型，通过频域滤波技术动态提取和增强变量间的共享频率成分，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉多变量时间序列中的复杂周期和趋势成分，影响了预测准确性。

Method: FilterTS采用动态跨变量滤波模块和静态全局滤波模块，结合频域卷积提升计算效率。

Result: 在八个真实数据集上的实验表明，FilterTS在预测准确性和计算效率上均优于现有方法。

Conclusion: FilterTS通过频域滤波技术有效解决了多变量时间序列预测中的复杂模式提取问题。

Abstract: Multivariate time series forecasting is crucial across various industries,
where accurate extraction of complex periodic and trend components can
significantly enhance prediction performance. However, existing models often
struggle to capture these intricate patterns. To address these challenges, we
propose FilterTS, a novel forecasting model that utilizes specialized filtering
techniques based on the frequency domain. FilterTS introduces a Dynamic
Cross-Variable Filtering Module, a key innovation that dynamically leverages
other variables as filters to extract and reinforce shared variable frequency
components across variables in multivariate time series. Additionally, a Static
Global Filtering Module captures stable frequency components, identified
throughout the entire training set. Moreover, the model is built in the
frequency domain, converting time-domain convolutions into frequency-domain
multiplicative operations to enhance computational efficiency. Extensive
experimental results on eight real-world datasets have demonstrated that
FilterTS significantly outperforms existing methods in terms of prediction
accuracy and computational efficiency.

</details>

### [81] [Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data](https://arxiv.org/abs/2505.04161)
*Baida Zhang,Yakai Chen,Huichun Li,Zhenghu Zu*

Main category: cs.LG

TLDR: 该论文提出了一种基于个体代理模型的强化学习框架，用于优化传染病干预措施，并通过实验和理论验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传染病爆发对健康安全和经济造成严重影响，现有基于强化学习的研究多局限于微分方程模型，难以模拟复杂动态传播过程。

Method: 建立基于个体代理传播模型的决策框架，结合强化学习探索策略函数，并利用改进的Covasim模型支持研究。

Result: 实验验证了框架的有效性，策略能有效抑制疫情扩散并维护经济稳定。

Conclusion: 该框架为全球公共卫生安全策略制定提供了重要参考。

Abstract: Globally, the outbreaks of infectious diseases have exerted an extremely
profound and severe influence on health security and the economy. During the
critical phases of epidemics, devising effective intervention measures poses a
significant challenge to both the academic and practical arenas. There is
numerous research based on reinforcement learning to optimize intervention
measures of infectious diseases. Nevertheless, most of these efforts have been
confined within the differential equation based on infectious disease models.
Although a limited number of studies have incorporated reinforcement learning
methodologies into individual-based infectious disease models, the models
employed therein have entailed simplifications and limitations, rendering it
incapable of modeling the complexity and dynamics inherent in infectious
disease transmission. We establish a decision-making framework based on an
individual agent-based transmission model, utilizing reinforcement learning to
continuously explore and develop a strategy function. The framework's validity
is verified through both experimental and theoretical approaches. Covasim, a
detailed and widely used agent-based disease transmission model, was modified
to support reinforcement learning research. We conduct an exhaustive
exploration of the application efficacy of multiple algorithms across diverse
action spaces. Furthermore, we conduct an innovative preliminary theoretical
analysis concerning the issue of "time coverage". The results of the experiment
robustly validate the effectiveness and feasibility of the methodological
framework of this study. The coping strategies gleaned therefrom prove highly
efficacious in suppressing the expansion of the epidemic scale and safeguarding
the stability of the economic system, thereby providing crucial reference
perspectives for the formulation of global public health security strategies.

</details>

### [82] [Retrieval Augmented Time Series Forecasting](https://arxiv.org/abs/2505.04163)
*Sungwon Han,Seungeon Lee,Meeyoung Cha,Sercan O Arik,Jinsung Yoon*

Main category: cs.LG

TLDR: RAFT是一种基于检索的时间序列预测方法，通过检索历史数据中的相似模式来增强模型预测能力，实验表明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法可能缺乏足够的归纳偏置，RAFT旨在通过检索历史数据补充模型的学习能力。

Method: RAFT通过检索训练数据中与输入最相似的历史模式，并利用这些模式的未来值辅助预测。

Result: 在十个基准数据集上，RAFT平均胜率为86%，显著优于现有方法。

Conclusion: RAFT通过检索历史数据有效提升了时间序列预测的性能，为未来研究提供了新思路。

Abstract: Time series forecasting uses historical data to predict future trends,
leveraging the relationships between past observations and available features.
In this paper, we propose RAFT, a retrieval-augmented time series forecasting
method to provide sufficient inductive biases and complement the model's
learning capacity. When forecasting the subsequent time frames, we directly
retrieve historical data candidates from the training dataset with patterns
most similar to the input, and utilize the future values of these candidates
alongside the inputs to obtain predictions. This simple approach augments the
model's capacity by externally providing information about past patterns via
retrieval modules. Our empirical evaluations on ten benchmark datasets show
that RAFT consistently outperforms contemporary baselines with an average win
ratio of 86%.

</details>

### [83] [STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04167)
*Yulong Wang,Xiaofeng Hu,Xiaojian Cui,Kai Wang*

Main category: cs.LG

TLDR: STRGCN是一种无需预对齐的时空关系图卷积网络，直接建模不规则多元时间序列的复杂依赖关系，通过全连接图表示数据，保留异步特性，并通过分层结构优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决不规则多元时间序列（IMTS）中因传感器频率不同和异步测量导致的建模挑战，避免预对齐策略对数据内在模式的扭曲和计算资源的浪费。

Method: 提出STRGCN模型，将观测数据表示为全连接图的节点，捕获节点间关系以处理时间戳不对齐问题，并采用分层“Sandwich”结构优化图嵌入和计算效率。

Result: 在四个公开数据集上的实验表明，STRGCN在准确性、内存使用和训练速度方面均达到最先进水平。

Conclusion: STRGCN有效解决了IMTS建模中的异步性和计算效率问题，为实际应用提供了高效且准确的解决方案。

Abstract: Irregular multivariate time series (IMTS) are prevalent in real-world
applications across many fields, where varying sensor frequencies and
asynchronous measurements pose significant modeling challenges. Existing
solutions often rely on a pre-alignment strategy to normalize data, which can
distort intrinsic patterns and escalate computational and memory demands.
Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational
Graph Convolutional Network that avoids pre-alignment and directly captures the
complex interdependencies in IMTS by representing them as a fully connected
graph. Each observation is represented as a node, allowing the model to
effectively handle misaligned timestamps by mapping all inter-node
relationships, thus faithfully preserving the asynchronous nature of the data.
Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that
strategically aggregates nodes to optimize graph embeddings, reducing
computational overhead while maintaining detailed local and global context.
Extensive experiments on four public datasets demonstrate that STRGCN achieves
state-of-the-art accuracy, competitive memory usage and training speed.

</details>

### [84] [DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion](https://arxiv.org/abs/2505.04173)
*Zixiao Wang,Wenqian Zhao,Yunheng Shen,Yang Bai,Guojin Chen,Farzan Farnia,Bei Yu*

Main category: cs.LG

TLDR: DiffPattern-Flex 是一种结合离散扩散模型和优化评估的高效布局生成方法，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型在布局生成中占主导，但仅依赖神经网络难以保证合法性，需更可靠的方法。

Method: 采用离散扩散模型生成多样化拓扑，结合基于设计规则的优化评估和快速采样技术。

Result: 实验表明，DiffPattern-Flex 在多个基准测试中表现优异，生成可靠布局。

Conclusion: DiffPattern-Flex 提供了一种高效且合法的布局生成解决方案。

Abstract: Recent advancements in layout pattern generation have been dominated by deep
generative models. However, relying solely on neural networks for legality
guarantees raises concerns in many practical applications. In this paper, we
present \tool{DiffPattern}-Flex, a novel approach designed to generate reliable
layout patterns efficiently. \tool{DiffPattern}-Flex incorporates a new method
for generating diverse topologies using a discrete diffusion model while
maintaining a lossless and compute-efficient layout representation. To ensure
legal pattern generation, we employ {an} optimization-based, white-box pattern
assessment process based on specific design rules. Furthermore, fast sampling
and efficient legalization technologies are employed to accelerate the
generation process. Experimental results across various benchmarks demonstrate
that \tool{DiffPattern}-Flex significantly outperforms existing methods and
excels at producing reliable layout patterns.

</details>

### [85] [On-Device LLM for Context-Aware Wi-Fi Roaming](https://arxiv.org/abs/2505.04174)
*Ju-Hyung Lee,Yanqing Lu*

Main category: cs.LG

TLDR: 论文提出了一种利用设备端大型语言模型（LLM）进行跨层无线漫游优化的方法，通过上下文感知的AP选择和动态阈值调整，显著提升了漫游稳定性和信号质量。


<details>
  <summary>Details</summary>
Motivation: 无线漫游在动态移动环境中至关重要，但传统基于阈值或启发式的方法效果不佳，导致粘滞或过度切换。

Method: 采用LLM进行应用层高级推理，实时执行PHY/MAC层操作，结合链式思维提示、参数高效微调和量化优化。

Result: 实验表明，该方法优于传统启发式和深度强化学习基线，实现了漫游稳定性和信号质量的平衡。

Conclusion: 应用层LLM推理为未来边缘系统中的底层无线控制提供了新思路。

Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless
connectivity in dynamic mobile environments. Conventional threshold-based or
heuristic schemes often fail, leading to either sticky or excessive handovers.
We introduce the first cross-layer use of an on-device large language model
(LLM): high-level reasoning in the application layer that issues real-time
actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)
context-aware AP selection, where structured prompts fuse environmental cues
(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold
adjustment, where the model adaptively decides when to roam. To satisfy the
tight latency and resource budgets of edge hardware, we apply a suite of
optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and
quantization. Experiments on indoor and outdoor datasets show that our approach
surpasses legacy heuristics and DRL baselines, achieving a strong balance
between roaming stability and signal quality. These findings underscore the
promise of application-layer LLM reasoning for lower-layer wireless control in
future edge systems.

</details>

### [86] [Trajectory Entropy Reinforcement Learning for Predictable and Robust Control](https://arxiv.org/abs/2505.04193)
*Bang You,Chenxu Wang,Huaping Liu*

Main category: cs.LG

TLDR: 论文提出了一种新的强化学习归纳偏置方法，通过最小化动作轨迹的熵来简化策略，提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在复杂控制任务中表现优异，但容易捕捉观测与动作间的虚假关联，导致环境微扰时失效。

Method: 引入轨迹熵最小化作为归纳偏置，通过变分参数化动作预测模型估计熵，并构建信息正则化奖励函数。

Result: 实验表明，该方法在高维运动任务中生成更周期性和一致的动作轨迹，性能与鲁棒性优于现有技术。

Conclusion: 轨迹熵最小化是一种有效的归纳偏置，能提升强化学习策略的简单性和鲁棒性。

Abstract: Simplicity is a critical inductive bias for designing data-driven
controllers, especially when robustness is important. Despite the impressive
results of deep reinforcement learning in complex control tasks, it is prone to
capturing intricate and spurious correlations between observations and actions,
leading to failure under slight perturbations to the environment. To tackle
this problem, in this work we introduce a novel inductive bias towards simple
policies in reinforcement learning. The simplicity inductive bias is introduced
by minimizing the entropy of entire action trajectories, corresponding to the
number of bits required to describe information in action trajectories after
the agent observes state trajectories. Our reinforcement learning agent,
Trajectory Entropy Reinforcement Learning, is optimized to minimize the
trajectory entropy while maximizing rewards. We show that the trajectory
entropy can be effectively estimated by learning a variational parameterized
action prediction model, and use the prediction model to construct an
information-regularized reward function. Furthermore, we construct a practical
algorithm that enables the joint optimization of models, including the policy
and the prediction model. Experimental evaluations on several high-dimensional
locomotion tasks show that our learned policies produce more cyclical and
consistent action trajectories, and achieve superior performance, and
robustness to noise and dynamic changes than the state-of-the-art.

</details>

### [87] [A Large Language Model for Feasible and Diverse Population Synthesis](https://arxiv.org/abs/2505.04196)
*Sung Yoo Lim,Hyunsoo Yun,Prateek Bansal,Dong-Kyu Kim,Eui-Jin Kim*

Main category: cs.LG

TLDR: 提出了一种基于LLM和贝叶斯网络的混合方法，用于生成可行且多样化的合成人口，显著提高了可行性并保持了多样性。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度生成模型在平衡稀有但合理的组合与排除不合理组合时的困难。

Method: 通过贝叶斯网络导出的拓扑顺序显式控制LLM的自回归生成过程。

Result: 混合方法实现了约95%的可行性，显著高于传统方法的80%，同时保持多样性。

Conclusion: 该方法轻量、开源，适用于大规模应用，提高了模拟可靠性并减少误差传播。

Abstract: Generating a synthetic population that is both feasible and diverse is
crucial for ensuring the validity of downstream activity schedule simulation in
activity-based models (ABMs). While deep generative models (DGMs), such as
variational autoencoders and generative adversarial networks, have been applied
to this task, they often struggle to balance the inclusion of rare but
plausible combinations (i.e., sampling zeros) with the exclusion of implausible
ones (i.e., structural zeros). To improve feasibility while maintaining
diversity, we propose a fine-tuning method for large language models (LLMs)
that explicitly controls the autoregressive generation process through
topological orderings derived from a Bayesian Network (BN). Experimental
results show that our hybrid LLM-BN approach outperforms both traditional DGMs
and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,
our approach achieves approximately 95% feasibility, significantly higher than
the ~80% observed in DGMs, while maintaining comparable diversity, making it
well-suited for practical applications. Importantly, the method is based on a
lightweight open-source LLM, enabling fine-tuning and inference on standard
personal computing environments. This makes the approach cost-effective and
scalable for large-scale applications, such as synthesizing populations in
megacities, without relying on expensive infrastructure. By initiating the ABM
pipeline with high-quality synthetic populations, our method improves overall
simulation reliability and reduces downstream error propagation. The source
code for these methods is available for research and practical application.

</details>

### [88] [Estimating Causal Effects in Networks with Cluster-Based Bandits](https://arxiv.org/abs/2505.04200)
*Ahmed Sayeed Faruk,Jason Sulskis,Elena Zheleva*

Main category: cs.LG

TLDR: 论文提出了两种基于聚类的多臂老虎机（MAB）算法，用于在社交网络干扰下高效估计总治疗效果，并在探索与利用之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 在社交网络中，A/B测试因干扰问题和高性能损失而受限，需要一种能动态适应并高效学习总治疗效果的策略。

Method: 设计了两种基于聚类的MAB算法，逐步估计网络中的总治疗效果，同时最大化预期奖励。

Result: 基于聚类的MAB算法在奖励-行动比上优于传统RCT方法，且未显著牺牲治疗效果估计的准确性。

Conclusion: 基于聚类的MAB算法在存在干扰的网络中是一种有效的替代方案，优于忽略聚类的MAB和传统RCT方法。

Abstract: The gold standard for estimating causal effects is randomized controlled
trial (RCT) or A/B testing where a random group of individuals from a
population of interest are given treatment and the outcome is compared to a
random group of individuals from the same population. However, A/B testing is
challenging in the presence of interference, commonly occurring in social
networks, where individuals can impact each others outcome. Moreover, A/B
testing can incur a high performance loss when one of the treatment arms has a
poor performance and the test continues to treat individuals with it.
Therefore, it is important to design a strategy that can adapt over time and
efficiently learn the total treatment effect in the network. We introduce two
cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the
total treatment effect in a network while maximizing the expected reward by
making a tradeoff between exploration and exploitation. We compare the
performance of our MAB algorithms with a vanilla MAB algorithm that ignores
clusters and the corresponding RCT methods on semi-synthetic data with
simulated interference. The vanilla MAB algorithm shows higher reward-action
ratio at the cost of higher treatment effect error due to undesired spillover.
The cluster-based MAB algorithms show higher reward-action ratio compared to
their corresponding RCT methods without sacrificing much accuracy in treatment
effect estimation.

</details>

### [89] [Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets](https://arxiv.org/abs/2505.04204)
*Mateo Lopez-Ledezma,Gissel Velarde*

Main category: cs.LG

TLDR: 论文探讨了网络安全中自动化的重要性，并通过三个实验评估了不同分类器和采样技术在处理不平衡数据集时的表现。


<details>
  <summary>Details</summary>
Motivation: 网络安全问题日益重要，自动化处理大规模数据是必要的。许多网络安全问题可以视为二元分类问题，如异常检测、欺诈检测等。

Method: 通过三个实验评估了多种分类器（如随机森林、XGBoost等）和采样技术（如过采样、欠采样等）在不平衡数据集上的表现。

Result: 不平衡学习技术有正负效果，需谨慎使用；不同数据集的最佳表现方法各异。

Conclusion: 建议针对每个新数据集和应用测试不同分类器和采样技术，以优化网络安全应用中的性能。

Abstract: Cybersecurity has become essential worldwide and at all levels, concerning
individuals, institutions, and governments. A basic principle in cybersecurity
is to be always alert. Therefore, automation is imperative in processes where
the volume of daily operations is large. Several cybersecurity applications can
be addressed as binary classification problems, including anomaly detection,
fraud detection, intrusion detection, spam detection, or malware detection. We
present three experiments. In the first experiment, we evaluate single
classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme
Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting
Decision Tree. In the second experiment, we test different sampling techniques
including over-sampling, under-sampling, Synthetic Minority Over-sampling
Technique, and Self-Paced Ensembling. In the last experiment, we evaluate
Self-Paced Ensembling and its number of base classifiers. We found that
imbalance learning techniques had positive and negative effects, as reported in
related studies. Thus, these techniques should be applied with caution.
Besides, we found different best performers for each dataset. Therefore, we
recommend testing single classifiers and imbalance learning techniques for each
new dataset and application involving imbalanced datasets as is the case in
several cyber security applications.

</details>

### [90] [FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning](https://arxiv.org/abs/2505.04223)
*Sanghyeon Park,Soo-Mook Moon*

Main category: cs.LG

TLDR: FRAIN是一种新的异步联邦学习方法，通过FastSync策略和SLERP参数合并技术，解决了传统方法在非IID数据和高延迟环境下的性能问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg和FedAsync）在非IID数据和高延迟环境下存在性能下降和客户端漂移问题，FRAIN旨在解决这些局限性。

Method: FRAIN采用FastSync策略避免重放旧模型版本，并使用SLERP技术合并参数以减少干扰。

Result: 实验表明，FRAIN在非IID数据、高延迟和恶意节点环境下，比FedAvg、FedAsync和BRAIN表现更稳定和鲁棒。

Conclusion: FRAIN通过创新策略显著提升了异步联邦学习的性能和可靠性。

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data locality. Although FedAvg pioneered
synchronous rounds for global model averaging, slower devices can delay
collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by
continuously integrating client updates, yet naive implementations risk client
drift due to non-IID data and stale contributions. Some Blockchain-based FL
approaches (e.g., BRAIN) employ robust weighting or scoring of updates to
resist malicious or misaligned proposals. However, performance drops can still
persist under severe data heterogeneity or high staleness, and synchronization
overhead has emerged as a new concern due to its aggregator-free architectures.
  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL
method that mitigates these limitations by incorporating two key ideas. First,
our FastSync strategy eliminates the need to replay past model versions,
enabling newcomers and infrequent participants to efficiently approximate the
global model. Second, we adopt spherical linear interpolation (SLERP) when
merging parameters, preserving models' directions and alleviating destructive
interference from divergent local training.
  Experiments with a CNN image-classification model and a Transformer-based
language model demonstrate that FRAIN achieves more stable and robust
convergence than FedAvg, FedAsync, and BRAIN, especially under harsh
environments: non-IID data distributions, networks that experience delays and
require frequent re-synchronization, and the presence of malicious nodes.

</details>

### [91] [Technology prediction of a 3D model using Neural Network](https://arxiv.org/abs/2505.04241)
*Grzegorz Miebs,Rafał A. Bachorz*

Main category: cs.LG

TLDR: 论文提出了一种基于数据驱动的方法，通过3D模型预测制造步骤及其时间，取代传统依赖专家或历史数据的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在动态或定制化生产环境中效果不佳，需要更精确且适应性强的生产时间估计方法。

Method: 将3D模型渲染为多张2D图像，利用受生成查询网络启发的神经网络，将几何特征映射为预定义生产步骤的时间估计。

Result: 该方法实现了跨产品类型的可扩展、自适应且精确的工艺规划。

Conclusion: 数据驱动方法为动态生产环境提供了更高效的时间估计解决方案。

Abstract: Accurate estimation of production times is critical for effective
manufacturing scheduling, yet traditional methods relying on expert analysis or
historical data often fall short in dynamic or customized production
environments. This paper introduces a data-driven approach that predicts
manufacturing steps and their durations directly from a product's 3D model. By
rendering the model into multiple 2D images and leveraging a neural network
inspired by the Generative Query Network, the method learns to map geometric
features into time estimates for predefined production steps enabling scalable,
adaptive, and precise process planning across varied product types.

</details>

### [92] [Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification](https://arxiv.org/abs/2505.04263)
*Jan Blechschmidt,Tom-Christian Riemer,Max Winkler,Martin Stoll,Jan-F. Pietschmann*

Main category: cs.LG

TLDR: 提出了一种基于物理信息的深度学习方法来求解度量图上的非线性漂移扩散方程，适用于优化和反问题。


<details>
  <summary>Details</summary>
Motivation: 传统的数值方法在模型设计或参数识别问题上需要大量定制，而物理信息深度算子网络（DeepONet）能够灵活求解偏微分方程并轻松处理参数识别问题。

Method: 首先学习三个DeepONet模型分别代表流入、内部和流出边，然后通过基于边的域分解方法耦合这些模型来解决漂移扩散度量图问题。

Result: 该方法能够准确评估图耦合物理模型，并适用于解决这些耦合网络上的优化或反问题。

Conclusion: 提出的框架为度量图上的非线性漂移扩散方程提供了一种高效且通用的解决方案。

Abstract: We develop a novel physics informed deep learning approach for solving
nonlinear drift-diffusion equations on metric graphs. These models represent an
important model class with a large number of applications in areas ranging from
transport in biological cells to the motion of human crowds. While traditional
numerical schemes require a large amount of tailoring, especially in the case
of model design or parameter identification problems, physics informed deep
operator networks (DeepONet) have emerged as a versatile tool for the solution
of partial differential equations with the particular advantage that they
easily incorporate parameter identification questions. We here present an
approach where we first learn three DeepONet models for representative inflow,
inner and outflow edges, resp., and then subsequently couple these models for
the solution of the drift-diffusion metric graph problem by relying on an
edge-based domain decomposition approach. We illustrate that our framework is
applicable for the accurate evaluation of graph-coupled physics models and is
well suited for solving optimization or inverse problems on these coupled
networks.

</details>

### [93] [Non-stationary Diffusion For Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.04278)
*Weiwei Ye,Zhuopeng Xu,Ning Gui*

Main category: cs.LG

TLDR: NsDiff是一种基于位置尺度噪声模型（LSNM）的扩散概率预测框架，能够建模时间序列的不确定性变化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DDPM模型因固定方差假设无法捕捉时间序列的非平稳不确定性。

Method: 结合去噪扩散条件生成模型与预训练的条件均值和方差估计器，提出动态调整噪声水平的不确定性感知噪声调度。

Result: 在九个真实和合成数据集上表现优于现有方法。

Conclusion: NsDiff通过LSNM有效建模不确定性变化，显著提升预测性能。

Abstract: Due to the dynamics of underlying physics and external influences, the
uncertainty of time series often varies over time. However, existing Denoising
Diffusion Probabilistic Models (DDPMs) often fail to capture this
non-stationary nature, constrained by their constant variance assumption from
the additive noise model (ANM). In this paper, we innovatively utilize the
Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of
ANM. A diffusion-based probabilistic forecasting framework, termed
Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of
modeling the changing pattern of uncertainty. Specifically, NsDiff combines a
denoising diffusion-based conditional generative model with a pre-trained
conditional mean and variance estimator, enabling adaptive endpoint
distribution modeling. Furthermore, we propose an uncertainty-aware noise
schedule, which dynamically adjusts the noise levels to accurately reflect the
data uncertainty at each step and integrates the time-varying variances into
the diffusion process. Extensive experiments conducted on nine real-world and
synthetic datasets demonstrate the superior performance of NsDiff compared to
existing approaches. Code is available at https://github.com/wwy155/NsDiff.

</details>

### [94] [Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing](https://arxiv.org/abs/2505.04318)
*Jacob Glenn Ayers,Buvaneswari A. Ramanan,Manzoor A. Khan*

Main category: cs.LG

TLDR: 论文提出了一种基于χ²拟合优度检验的概念漂移检测方法，用于监控深度学习模型在推理过程中可能遇到的数据分布变化，确保模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型的应用超出人类验证能力，需要元算法来确保模型推理的可靠性。概念漂移检测领域在监控神经网络时未充分利用，尤其是在推理数据与训练数据分布不一致的情况下。

Method: 应用χ²拟合优度假设检验作为漂移检测元算法，测试多层感知机、卷积神经网络和Transformer在模拟漂移条件下的表现。

Result: 研究表明，无需直接检查推理输出即可检测因概念漂移导致的准确性下降。

Conclusion: 该方法通过持续评估模型在不同条件下的可靠性，增强了模型的安全性。

Abstract: As the adoption of deep learning models has grown beyond human capacity for
verification, meta-algorithms are needed to ensure reliable model inference.
Concept drift detection is a field dedicated to identifying statistical shifts
that is underutilized in monitoring neural networks that may encounter
inference data with distributional characteristics diverging from their
training data. Given the wide variety of model architectures, applications, and
datasets, it is important that concept drift detection algorithms are adaptable
to different inference scenarios. In this paper, we introduce an application of
the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection
meta-algorithm applied to a multilayer perceptron, a convolutional neural
network, and a transformer trained for machine vision as they are exposed to
simulated drift during inference. To that end, we demonstrate how unexpected
drops in accuracy due to concept drift can be detected without directly
examining the inference outputs. Our approach enhances safety by ensuring
models are continually evaluated for reliability across varying conditions.

</details>

### [95] [Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces](https://arxiv.org/abs/2505.04335)
*Swagato Das,Arghya Pratihar,Swagatam Das*

Main category: cs.LG

TLDR: 提出了一种基于双曲几何的新型模糊聚类算法HypeFCM，用于解决传统模糊聚类在非欧几里得空间中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统模糊聚类方法（如FCM）在非欧几里得空间中表现不佳，无法有效捕捉复杂数据关系。

Method: HypeFCM结合双曲几何和模糊聚类原理，使用基于权重的过滤机制，通过Poincaré Disc模型中的双曲度量优化聚类中心和成员分配。

Result: 实验表明，HypeFCM在非欧几里得空间中显著优于传统模糊聚类方法。

Conclusion: HypeFCM是一种高效且鲁棒的模糊聚类算法，适用于非欧几里得空间。

Abstract: Clustering algorithms play a pivotal role in unsupervised learning by
identifying and grouping similar objects based on shared characteristics. While
traditional clustering techniques, such as hard and fuzzy center-based
clustering, have been widely used, they struggle with complex,
high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy
$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits
notable limitations in non-Euclidean spaces. Euclidean spaces assume linear
separability and uniform distance scaling, limiting their effectiveness in
capturing complex, hierarchical, or non-Euclidean structures in fuzzy
clustering. To overcome these challenges, we introduce Filtration-based
Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for
better representation of data relationships in non-Euclidean spaces. HypeFCM
integrates the principles of fuzzy clustering with hyperbolic geometry and
employs a weight-based filtering mechanism to improve performance. The
algorithm initializes weights using a Dirichlet distribution and iteratively
refines cluster centroids and membership assignments based on a hyperbolic
metric in the Poincar\'e Disc model. Extensive experimental evaluations
demonstrate that HypeFCM significantly outperforms conventional fuzzy
clustering methods in non-Euclidean settings, underscoring its robustness and
effectiveness.

</details>

### [96] [Riemannian Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2505.04338)
*Zichen Liu,Wei Zhang,Christof Schütte,Tiejun Li*

Main category: cs.LG

TLDR: RDDPMs是一种在欧几里得空间子流形上学习分布的方法，适用于更广泛的流形，仅需评估定义子流形的函数及其一阶导数。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量几何信息（如测地线或Laplace-Beltrami算子的特征函数），限制了应用范围。RDDPMs旨在解决这一问题。

Method: 基于投影方案，仅需评估定义子流形的函数及其一阶导数，适用于更广泛的流形。

Result: 理论分析表明RDDPMs与流形上的基于分数的生成模型相关，实验验证了其在多个数据集上的能力。

Conclusion: RDDPMs为流形上的生成建模提供了一种更通用的方法。

Abstract: We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for
learning distributions on submanifolds of Euclidean space that are level sets
of functions, including most of the manifolds relevant to applications.
Existing methods for generative modeling on manifolds rely on substantial
geometric information such as geodesic curves or eigenfunctions of the
Laplace-Beltrami operator and, as a result, they are limited to manifolds where
such information is available. In contrast, our method, built on a projection
scheme, can be applied to more general manifolds, as it only requires being
able to evaluate the value and the first order derivatives of the function that
defines the submanifold. We provide a theoretical analysis of our method in the
continuous-time limit, which elucidates the connection between our RDDPMs and
score-based generative models on manifolds. The capability of our method is
demonstrated on datasets from previous studies and on new datasets sampled from
two high-dimensional manifolds, i.e. $\mathrm{SO}(10)$ and the configuration
space of molecular system alanine dipeptide with fixed dihedral angle.

</details>

### [97] [Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.04339)
*Hao Peng,Xiang Huang,Shuo Sun,Ruitong Zhang,Philip S. Yu*

Main category: cs.LG

TLDR: 提出了一种基于多智能体强化学习的自适应鲁棒DBSCAN框架（AR-DBSCAN），通过密度分区和自动参数搜索，显著提升了聚类精度。


<details>
  <summary>Details</summary>
Motivation: DBSCAN在处理变密度数据集时效果不佳，需要手动调整参数，限制了其实际应用。

Method: 将数据集建模为两级编码树，分区后分配智能体自动学习最优参数，结合强化学习优化搜索过程。

Result: 在多个数据集上验证，AR-DBSCAN在NMI和ARI指标上分别提升144.1%和175.3%，并能鲁棒地找到最优参数。

Conclusion: AR-DBSCAN有效解决了变密度数据集的聚类问题，提升了自动化水平和性能。

Abstract: DBSCAN, a well-known density-based clustering algorithm, has gained
widespread popularity and usage due to its effectiveness in identifying
clusters of arbitrary shapes and handling noisy data. However, it encounters
challenges in producing satisfactory cluster results when confronted with
datasets of varying density scales, a common scenario in real-world
applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with
Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,
we model the initial dataset as a two-level encoding tree and categorize the
data vertices into distinct density partitions according to the information
uncertainty determined in the encoding tree. Each partition is then assigned to
an agent to find the best clustering parameters without manual assistance. The
allocation is density-adaptive, enabling AR-DBSCAN to effectively handle
diverse density distributions within the dataset by utilizing distinct agents
for different partitions. Second, a multi-agent deep reinforcement learning
guided automatic parameter searching process is designed. The process of
adjusting the parameter search direction by perceiving the clustering
environment is modeled as a Markov decision process. Using a weakly-supervised
reward training policy network, each agent adaptively learns the optimal
clustering parameters by interacting with the clusters. Third, a recursive
search mechanism adaptable to the data's scale is presented, enabling efficient
and controlled exploration of large parameter spaces. Extensive experiments are
conducted on nine artificial datasets and a real-world dataset. The results of
offline and online tasks show that AR-DBSCAN not only improves clustering
accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,
but also is capable of robustly finding dominant parameters.

</details>

### [98] [Multi-Granular Attention based Heterogeneous Hypergraph Neural Network](https://arxiv.org/abs/2505.04340)
*Hong Jin,Kaicheng Zhou,Jie Yin,Lan You,Zhifeng Zhou*

Main category: cs.LG

TLDR: 论文提出MGA-HHN，一种基于多粒度注意力的异质超图神经网络，通过构建多视图超图和引入多粒度注意力机制，解决了现有异质图神经网络在高阶关系捕获和长距离信息失真上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有异质图神经网络（HeteGNNs）依赖元路径的消息传递，无法有效捕获节点间的高阶关系，且存在长距离信息失真问题，导致性能受限。

Method: 提出MGA-HHN模型，包括：1）构建基于元路径的多视图异质超图以显式建模高阶语义信息；2）设计节点和超边级别的多粒度注意力机制，捕捉细粒度交互并保持语义多样性。

Result: 在真实数据集上的实验表明，MGA-HHN在节点分类、聚类和可视化任务中优于现有模型。

Conclusion: MGA-HHN通过多粒度注意力机制和超图建模，有效解决了信息失真和高阶关系捕获问题，提升了异质图表示学习的性能。

Abstract: Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong
abilities to learn node representations by effectively extracting complex
structural and semantic information in heterogeneous graphs. Most of the
prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging
meta-path based message passing to learn latent node representations. However,
due to the pairwise nature of meta-paths, these models fail to capture
high-order relations among nodes, resulting in suboptimal performance.
Additionally, the challenge of ``over-squashing'', where long-range message
passing in HeteGNNs leads to severe information distortion, further limits the
efficacy of these models. To address these limitations, this paper proposes
MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural
Network for heterogeneous graph representation learning. MGA-HHN introduces two
key innovations: (1) a novel approach for constructing meta-path based
heterogeneous hypergraphs that explicitly models higher-order semantic
information in heterogeneous graphs through multiple views, and (2) a
multi-granular attention mechanism that operates at both the node and hyperedge
levels. This mechanism enables the model to capture fine-grained interactions
among nodes sharing the same semantic context within a hyperedge type, while
preserving the diversity of semantics across different hyperedge types. As
such, MGA-HHN effectively mitigates long-range message distortion and generates
more expressive node representations. Extensive experiments on real-world
benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art
models, showcasing its effectiveness in node classification, node clustering
and visualization tasks.

</details>

### [99] [Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration](https://arxiv.org/abs/2505.04346)
*Arghya Pratihar,Kushal Bose,Swagatam Das*

Main category: cs.LG

TLDR: 提出了一种基于拓扑结构的聚类算法，利用Vietoris-Rips复形和Betti数过滤来识别拓扑相似的邻居，并通过Betti序列捕捉关键特征，适用于复杂数据集。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑聚类算法未能充分利用拓扑结构，且在复杂数据集上表现不一致。

Method: 使用Vietoris-Rips复形和Betti数过滤识别拓扑相似邻居，引入Betti序列捕捉关键特征。

Result: 在合成和真实数据集上表现优于其他拓扑聚类算法。

Conclusion: 该算法能有效聚类复杂数据集中的交织形状。

Abstract: Clustering aims to form groups of similar data points in an unsupervised
regime. Yet, clustering complex datasets containing critically intertwined
shapes poses significant challenges. The prevailing clustering algorithms
widely depend on evaluating similarity measures based on Euclidean metrics.
Exploring topological characteristics to perform clustering of complex datasets
inevitably presents a better scope. The topological clustering algorithms
predominantly perceive the point set through the lens of Simplicial complexes
and Persistent homology. Despite these approaches, the existing topological
clustering algorithms cannot somehow fully exploit topological structures and
show inconsistent performances on some highly complicated datasets. This work
aims to mitigate the limitations by identifying topologically similar neighbors
through the Vietoris-Rips complex and Betti number filtration. In addition, we
introduce the concept of the Betti sequences to capture flexibly essential
features from the topological structures. Our proposed algorithm is adept at
clustering complex, intertwined shapes contained in the datasets. We carried
out experiments on several synthetic and real-world datasets. Our algorithm
demonstrated commendable performances across the datasets compared to some of
the well-known topology-based clustering algorithms.

</details>

### [100] [Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid](https://arxiv.org/abs/2505.04367)
*Stavros Sykiotis*

Main category: cs.LG

TLDR: 论文探讨了能源转型中可再生能源投资的复杂性，提出通过深度学习技术优化住宅能源消耗和电动汽车充电，以加速转型。


<details>
  <summary>Details</summary>
Motivation: 全球能源转型面临复杂性和高碳排放能源淘汰的挑战，需要探索加速转型的替代路径。

Method: 采用深度学习技术，开发非侵入式负载监测工具和深度强化学习优化电动汽车充电。

Result: 通过技术手段减少住宅能源消耗和优化交通领域碳排放。

Conclusion: 深度学习技术为能源转型提供了可行的解决方案，特别是在住宅和交通领域。

Abstract: The global energy landscape is undergoing a profound transformation, often
referred to as the energy transition, driven by the urgent need to mitigate
climate change, reduce greenhouse gas emissions, and ensure sustainable energy
supplies. However, the undoubted complexity of new investments in renewables,
as well as the phase out of high CO2-emission energy sources, hampers the pace
of the energy transition and raises doubts as to whether new renewable energy
sources are capable of solely meeting the climate target goals. This highlights
the need to investigate alternative pathways to accelerate the energy
transition, by identifying human activity domains with higher/excessive energy
demands. Two notable examples where there is room for improvement, in the sense
of reducing energy consumption and consequently CO2 emissions, are residential
energy consumption and road transport. This dissertation investigates the
development of novel Deep Learning techniques to create tools which solve
limitations in these two key energy domains. Reduction of residential energy
consumption can be achieved by empowering end-users with the user of
Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep
Reinforcement Learning can tackle road transport decarbonization.

</details>

### [101] [Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four](https://arxiv.org/abs/2505.04371)
*Filipe Santos,João Paulo Fernandes,Luís Macedo*

Main category: cs.LG

TLDR: 论文研究了基于标志的动作选择在强化学习中的应用，特别是量子版本在Connect Four游戏中的表现，发现量子方法能更快采样标志动作，但胜率与经典方法相同。


<details>
  <summary>Details</summary>
Motivation: 探索基于标志的动作选择方法在不同环境（如Connect Four）中的泛化能力，并研究量子版本的优势。

Method: 在Connect Four中训练和测试经典与量子强化学习代理，记录标志动作的平均迭代次数，并与随机Negamax对手对战。

Result: 基于标志的探索策略优于简单的epsilon-greedy策略，量子方法采样更快，但胜率与经典方法相同。

Conclusion: 量子方法在采样效率上有优势，但胜率未提升，可能是由于训练场景的简单性。

Abstract: Action selection based on flags is a Reinforcement Learning (RL) exploration
policy that improves the exploration of the state space through the use of
flags, which can identify the most promising actions to take in each state. The
quantum counterpart of this exploration policy further improves upon this by
taking advantage of a quadratic speedup for sampling flagged actions. This
approach has already been successfully employed for the game of Checkers. In
this work, we describe the application of this method to the context of Connect
Four, in order to study its performance in a different setting, which can lead
to a better generalization of the technique. We also kept track of a metric
that wasn't taken into account in previous work: the average number of
iterations to obtain a flagged action. Since going second is a significant
disadvantage in Connect Four, we also had the intent of exploring how this more
complex scenario would impact the performance of our approach. The experiments
involved training and testing classical and quantum RL agents that played
either going first or going second against a Randomized Negamax opponent. The
results showed that both flagged exploration policies were clearly superior to
a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact
sample flagged actions in less iterations. Despite obtaining tagged actions
more consistently, the win rates between the classical and quantum versions of
the approach were identical, which could be due to the simplicity of the
training scenario chosen.

</details>

### [102] [Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets](https://arxiv.org/abs/2505.04389)
*Jenni Lampainen,Kaisa Joki,Napsu Karmitsa,Marko M. Mäkelä*

Main category: cs.LG

TLDR: Clust-Splitter是一种基于非光滑优化的高效聚类算法，用于处理大规模数据集的最小平方和聚类问题。


<details>
  <summary>Details</summary>
Motivation: 聚类是数据挖掘和机器学习中的基础任务，尤其在大规模数据分析中尤为重要。

Method: 通过三个非光滑优化问题序列（两个辅助问题生成初始点，一个主聚类问题）解决聚类任务，结合有限内存束方法和增量方法开发Clust-Splitter算法。

Result: 在真实数据集上验证，Clust-Splitter在效率和聚类质量上与现有最佳方法相当。

Conclusion: Clust-Splitter是一种高效且高质量的大规模聚类算法。

Abstract: Clustering is a fundamental task in data mining and machine learning,
particularly for analyzing large-scale data. In this paper, we introduce
Clust-Splitter, an efficient algorithm based on nonsmooth optimization,
designed to solve the minimum sum-of-squares clustering problem in very large
datasets. The clustering task is approached through a sequence of three
nonsmooth optimization problems: two auxiliary problems used to generate
suitable starting points, followed by a main clustering formulation. To solve
these problems effectively, the limited memory bundle method is combined with
an incremental approach to develop the Clust-Splitter algorithm. We evaluate
Clust-Splitter on real-world datasets characterized by both a large number of
attributes and a large number of data points and compare its performance with
several state-of-the-art large-scale clustering algorithms. Experimental
results demonstrate the efficiency of the proposed method for clustering very
large datasets, as well as the high quality of its solutions, which are on par
with those of the best existing methods.

</details>

### [103] [Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast](https://arxiv.org/abs/2505.04396)
*Jingnan Wang,Jie Chao,Shangshang Yang,Congyi Nai,Kaijun Ren,Kefeng Deng,Xi Chen,Yaxin Liu,Hanqiuzi Wen,Ziniu Xiao,Lifeng Zhang,Xiaodong Wang,Jiping Guan,Baoxiang Pan*

Main category: cs.LG

TLDR: 论文提出了一种结合高分辨率气候先验与粗网格大尺度预报的方法，显著提升了风电场天气模式预报的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决可再生能源（尤其是风能）规划和运营中天气信息的高精度、高分辨率需求，以及传统降尺度方法带来的尺度不一致、计算成本高等问题。

Method: 通过学习目标风电场的高分辨率数值天气模拟的气候分布，结合粗网格大尺度预报，生成高精度、细粒度、全变量的天气模式预报。

Result: 验证了该方法在确定性/概率性预报技能和经济收益上的优势，计算成本大幅降低（1小时GPU vs. 传统方法的千倍CPU小时）。

Conclusion: 该方法为高效可靠的可再生能源规划和运营提供了新途径。

Abstract: The planning and operation of renewable energy, especially wind power, depend
crucially on accurate, timely, and high-resolution weather information.
Coarse-grid global numerical weather forecasts are typically downscaled to meet
these requirements, introducing challenges of scale inconsistency, process
representation error, computation cost, and entanglement of distinct
uncertainty sources from chaoticity, model bias, and large-scale forcing. We
address these challenges by learning the climatological distribution of a
target wind farm using its high-resolution numerical weather simulations. An
optimal combination of this learned high-resolution climatological prior with
coarse-grid large scale forecasts yields highly accurate, fine-grained,
full-variable, large ensemble of weather pattern forecasts. Using observed
meteorological records and wind turbine power outputs as references, the
proposed methodology verifies advantageously compared to existing
numerical/statistical forecasting-downscaling pipelines, regarding either
deterministic/probabilistic skills or economic gains. Moreover, a 100-member,
10-day forecast with spatial resolution of 1 km and output frequency of 15 min
takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU
hours for conventional numerical simulation. By drastically reducing
computational costs while maintaining accuracy, our method paves the way for
more efficient and reliable renewable energy planning and operation.

</details>

### [104] [Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization](https://arxiv.org/abs/2505.04412)
*Ren Wang,Pengcheng Zhou*

Main category: cs.LG

TLDR: 提出了一种基于AutoEncoder的方法，结合流形重建层，从噪声点云中提取潜在流形结构，并在降维过程中对拓扑和几何属性进行正则化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声数据中难以同时捕捉局部细节和全局拓扑完整性，导致降维结果失真或断裂。

Method: 采用AutoEncoder框架，集成流形重建层，并在训练中相互促进拓扑和几何属性的正则化。

Result: 在点云数据集上，该方法在噪声数据中发现流形结构并通过降维保留其特性，表现优于t-SNE、UMAP和Topological AutoEncoders。

Conclusion: 结合流形重建与流形学习对噪声数据的潜在流形可靠表示具有重要意义。

Abstract: Manifold learning aims to discover and represent low-dimensional structures
underlying high-dimensional data while preserving critical topological and
geometric properties. Existing methods often fail to capture local details with
global topological integrity from noisy data or construct a balanced
dimensionality reduction, resulting in distorted or fractured embeddings. We
present an AutoEncoder-based method that integrates a manifold reconstruction
layer, which uncovers latent manifold structures from noisy point clouds, and
further provides regularizations on topological and geometric properties during
dimensionality reduction, whereas the two components promote each other during
training. Experiments on point cloud datasets demonstrate that our method
outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in
discovering manifold structures from noisy data and preserving them through
dimensionality reduction, as validated by visualization and quantitative
metrics. This work demonstrates the significance of combining manifold
reconstruction with manifold learning to achieve reliable representation of the
latent manifold, particularly when dealing with noisy real-world data. Code
repository: https://github.com/Thanatorika/mrtg.

</details>

### [105] [Localized Diffusion Models for High Dimensional Distributions Generation](https://arxiv.org/abs/2505.04417)
*Georg A. Gottwald,Shuigen Liu,Youssef Marzouk,Sebastian Reich,Xin T. Tong*

Main category: cs.LG

TLDR: 扩散模型在生成任务中表现优异，但高维分数估计可能导致维度灾难（CoD）。本文提出利用目标分布的低维局部结构，通过局部化神经网络降低样本复杂度，并证明局部化扩散模型能平衡统计与局部化误差，提升性能。


<details>
  <summary>Details</summary>
Motivation: 高维分数估计可能导致维度灾难，因此需要更好地理解和利用目标分布的低维结构。

Method: 提出局部化扩散模型，使用局部化分数匹配损失在局部假设空间中训练分数函数。

Result: 理论分析和数值实验表明，适度的局部化半径能平衡统计与局部化误差，提升性能。

Conclusion: 局部化结构不仅帮助扩散模型规避维度灾难，还促进并行训练，适用于大规模应用。

Abstract: Diffusion models are the state-of-the-art tools for various generative tasks.
However, estimating high-dimensional score functions makes them potentially
suffer from the curse of dimensionality (CoD). This underscores the importance
of better understanding and exploiting low-dimensional structure in the target
distribution. In this work, we consider locality structure, which describes
sparse dependencies between model components. Under locality structure, the
score function is effectively low-dimensional, so that it can be estimated by a
localized neural network with significantly reduced sample complexity. This
motivates the localized diffusion model, where a localized score matching loss
is used to train the score function within a localized hypothesis space. We
prove that such localization enables diffusion models to circumvent CoD, at the
price of additional localization error. Under realistic sample size scaling, we
show both theoretically and numerically that a moderate localization radius can
balance the statistical and localization error, leading to a better overall
performance. The localized structure also facilitates parallel training of
diffusion models, making it potentially more efficient for large-scale
applications.

</details>

### [106] [FedBWO: Enhancing Communication Efficiency in Federated Learning](https://arxiv.org/abs/2505.04435)
*Vahideh Hayyolalam,Öznur Özkasap*

Main category: cs.LG

TLDR: FedBWO是一种联邦学习优化技术，通过传输性能评分而非模型权重减少通信开销，提升全局模型性能和通信效率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上，联邦学习的通信开销大，传输大量模型权重会导致瓶颈。

Method: 提出FedBWO技术，利用BWO算法优化本地模型更新，仅传输性能评分。

Result: 实验显示FedBWO显著提升全局模型准确率（比FedAvg高21%，比FedGWO高12%）并降低通信成本。

Conclusion: FedBWO有效解决了联邦学习中的通信瓶颈问题，提升了系统性能。

Abstract: Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a
shared model is collaboratively trained by various clients using their local
datasets while keeping the data private. Considering resource-constrained
devices, FL clients often suffer from restricted transmission capacity. Aiming
to enhance the system performance, the communication between clients and server
needs to be diminished. Current FL strategies transmit a tremendous amount of
data (model weights) within the FL process, which needs a high communication
bandwidth. Considering resource constraints, increasing the number of clients
and, consequently, the amount of data (model weights) can lead to a bottleneck.
In this paper, we introduce the Federated Black Widow Optimization (FedBWO)
technique to decrease the amount of transmitted data by transmitting only a
performance score rather than the local model weights from clients. FedBWO
employs the BWO algorithm to improve local model updates. The conducted
experiments prove that FedBWO remarkably improves the performance of the global
model and the communication efficiency of the overall system. According to the
experimental outcomes, FedBWO enhances the global model accuracy by an average
of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically
decreases the communication cost compared to other methods.

</details>

### [107] [Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory](https://arxiv.org/abs/2505.04440)
*Xiaozheng Qu,Zhaochuan Li,Zhuang Qi,Xiang Li,Haibei Huang,Lei Meng,Xiangxu Meng*

Main category: cs.LG

TLDR: IR-ART通过迭代框架提升Fuzzy ART的聚类稳定性，减少对预设参数的依赖，适合非专家用户。


<details>
  <summary>Details</summary>
Motivation: Fuzzy ART的聚类性能高度依赖预设参数，现有方法复杂且违背算法简洁性。

Method: IR-ART整合动态稳定性检测、不稳定簇删除和相似度阈值自适应调整三阶段。

Result: 在15个数据集上验证，IR-ART提升了对参数值的容忍度并保持简洁性。

Conclusion: IR-ART通过迭代优化显著提升实用性，适合资源受限场景的非专家用户。

Abstract: The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is
highly dependent on the preset vigilance parameter, where deviations in its
value can lead to significant fluctuations in clustering results, severely
limiting its practicality for non-expert users. Existing approaches generally
enhance vigilance parameter robustness through adaptive mechanisms such as
particle swarm optimization and fuzzy logic rules. However, they often
introduce additional hyperparameters or complex frameworks that contradict the
original simplicity of the algorithm. To address this, we propose Iterative
Refinement Adaptive Resonance Theory (IR-ART), which integrates three key
phases into a unified iterative framework: (1) Cluster Stability Detection: A
dynamic stability detection module that identifies unstable clusters by
analyzing the change of sample size (number of samples in the cluster) in
iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that
eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance
region expansion mechanism that adaptively adjusts similarity thresholds.
Independent of the specific execution of clustering, these three phases
sequentially focus on analyzing the implicit knowledge within the iterative
process, adjusting weights and vigilance parameters, thereby laying a
foundation for the next iteration. Experimental evaluation on 15 datasets
demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter
values while preserving the parameter simplicity of Fuzzy ART. Case studies
visually confirm the algorithm's self-optimization capability through iterative
refinement, making it particularly suitable for non-expert users in
resource-constrained scenarios.

</details>

### [108] [Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs](https://arxiv.org/abs/2505.04441)
*Mirazul Haque,Petr Babkin,Farima Farmahinifarahani,Manuela Veloso*

Main category: cs.LG

TLDR: 论文探讨了在自动程序修复（APR）中结合程序执行轨迹对大型语言模型（LLM）性能的影响，发现其效果有限且随复杂性增加而减弱，但优化提示策略能更稳定提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的APR方法主要依赖静态分析，忽略了程序运行时行为，本文旨在通过引入执行轨迹填补这一盲区。

Method: 通过将程序执行轨迹融入标准APR提示中，使用GPT系列模型在三个APR数据集上评估效果。

Result: 执行轨迹仅在两成配置中带来有限改进，且效果随复杂性降低；优化提示策略表现更稳定，优于微调小模型。

Conclusion: 执行轨迹可补充LLM推理能力，但需优化提示策略以充分发挥潜力。

Abstract: Large Language Models (LLMs) show promising performance on various
programming tasks, including Automatic Program Repair (APR). However, most
approaches to LLM-based APR are limited to the static analysis of the programs,
while disregarding their runtime behavior. Inspired by knowledge-augmented NLP,
in this work, we aim to remedy this potential blind spot by augmenting standard
APR prompts with program execution traces. We evaluate our approach using the
GPT family of models on three popular APR datasets. Our findings suggest that
simply incorporating execution traces into the prompt provides a limited
performance improvement over trace-free baselines, in only 2 out of 6 tested
dataset / model configurations. We further find that the effectiveness of
execution traces for APR diminishes as their complexity increases. We explore
several strategies for leveraging traces in prompts and demonstrate that
LLM-optimized prompts help outperform trace-free prompts more consistently.
Additionally, we show trace-based prompting to be superior to finetuning a
smaller LLM on a small-scale dataset; and conduct probing studies reinforcing
the notion that execution traces can complement the reasoning abilities of the
LLMs.

</details>

### [109] [A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities](https://arxiv.org/abs/2505.04461)
*Pengfei Jiao,Hongjiang Chen,Xuan Guo,Zhidong Zhao,Dongxiao He,Di Jin*

Main category: cs.LG

TLDR: 该论文综述了时序交互图表示学习（TIGRL）的研究现状，提出了一种分类方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时序交互图（TIGs）在动态系统建模中具有广泛应用，但如何有效学习其表示仍面临挑战。

Method: 论文提出了一种基于信息类型的分类法，系统梳理了现有TIGRL方法，并整理了数据集和基准资源。

Result: 通过分类和资源整理，为TIGRL研究提供了系统化的参考框架。

Conclusion: 论文总结了TIGRL的关键挑战和未来方向，为该领域的进一步发展奠定了基础。

Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped
interaction events, have become ubiquitous in real-world applications due to
their capability to model complex dynamic system behaviors. As a result,
temporal interaction graph representation learning (TIGRL) has garnered
significant attention in recent years. TIGRL aims to embed nodes in TIGs into
low-dimensional representations that effectively preserve both structural and
temporal information, thereby enhancing the performance of downstream tasks
such as classification, prediction, and clustering within constantly evolving
data environments. In this paper, we begin by introducing the foundational
concepts of TIGs and emphasize the critical role of temporal dependencies. We
then propose a comprehensive taxonomy of state-of-the-art TIGRL methods,
systematically categorizing them based on the types of information utilized
during the learning process to address the unique challenges inherent to TIGs.
To facilitate further research and practical applications, we curate the source
of datasets and benchmarks, providing valuable resources for empirical
investigations. Finally, we examine key open challenges and explore promising
research directions in TIGRL, laying the groundwork for future advancements
that have the potential to shape the evolution of this field.

</details>

### [110] [Discriminative Ordering Through Ensemble Consensus](https://arxiv.org/abs/2505.04464)
*Louis Ohl,Fredrik Lindsten*

Main category: cs.LG

TLDR: 提出了一种基于共识聚类的评分方法，用于评估不同聚类模型的性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有聚类评估指标难以处理多样化的聚类定义和约束条件，需要一种更通用的评估方法。

Method: 通过构建基于共识矩阵的判别性排序，利用聚类模型与共识矩阵之间的距离进行评分。

Result: 在合成场景中验证了方法的有效性，显著优于其他评分方法，适用于非固定簇数和约束条件的聚类算法。

Conclusion: 该方法为聚类模型评估提供了一种简单且高效的解决方案，适用于多样化场景。

Abstract: Evaluating the performance of clustering models is a challenging task where
the outcome depends on the definition of what constitutes a cluster. Due to
this design, current existing metrics rarely handle multiple clustering models
with diverse cluster definitions, nor do they comply with the integration of
constraints when available. In this work, we take inspiration from consensus
clustering and assume that a set of clustering models is able to uncover hidden
structures in the data. We propose to construct a discriminative ordering
through ensemble clustering based on the distance between the connectivity of a
clustering model and the consensus matrix. We first validate the proposed
method with synthetic scenarios, highlighting that the proposed score ranks the
models that best match the consensus first. We then show that this simple
ranking score significantly outperforms other scoring methods when comparing
sets of different clustering algorithms that are not restricted to a fixed
number of clusters and is compatible with clustering constraints.

</details>

### [111] [Spectral and Temporal Denoising for Differentially Private Optimization](https://arxiv.org/abs/2505.04468)
*Hyeju Shin,Kyudan Jung,Seongwon Yun,Juyoung Yun*

Main category: cs.LG

TLDR: FFTKF是一种差分隐私优化方法，通过频域噪声整形和卡尔曼滤波提升梯度质量，同时保持隐私保证。


<details>
  <summary>Details</summary>
Motivation: 解决DP-SGD中噪声导致模型性能下降的问题。

Method: 结合频域噪声整形和卡尔曼滤波，使用高频整形掩模和标量增益卡尔曼滤波器。

Result: 在多个数据集上测试精度优于DP-SGD和DiSK，复杂度为O(d log d)。

Conclusion: FFTKF在保持隐私的同时，通过减少噪声和控制偏差实现了更好的隐私-效用权衡。

Abstract: This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a
differentially private optimization method that addresses the challenge of
preserving performance in DP-SGD, where added noise typically degrades model
utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering
to enhance gradient quality while preserving $(\varepsilon, \delta)$-DP
guarantees. It employs a high-frequency shaping mask in the Fourier domain to
concentrate differential privacy noise in less informative spectral components,
preserving low-frequency gradient signals. A scalar-gain Kalman filter with
finite-difference Hessian approximation further refines the denoised gradients.
With a per-iteration complexity of $\mathcal{O}(d \log d)$, FFTKF demonstrates
improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,
and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.
Theoretical analysis confirms that FFTKF maintains equivalent privacy
guarantees while achieving a tighter privacy-utility trade-off through reduced
noise and controlled bias.

</details>

### [112] [Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations](https://arxiv.org/abs/2505.04471)
*Vincent Souveton,Sébastien Terrana*

Main category: cs.LG

TLDR: 提出了一种基于哈密顿动力学的新型方法，通过可逆的体积保持变换将初始高斯分布转化为最终分布，用于快速采样和物理势能学习。


<details>
  <summary>Details</summary>
Motivation: 由于哈密顿系统（如Vlasov-Poisson方程）的复杂性，解析解罕见，需要高效的数值方法。

Method: 使用哈密顿信息归一化流（Fixed-Kinetic Neural Hamiltonian Flows），通过训练数据集学习可逆变换。

Result: 模型能够快速采样最终分布，并学习可解释的物理势能，泛化至未见的中间状态。

Conclusion: 该方法为复杂哈密顿系统提供了一种高效且可解释的数值解决方案。

Abstract: Many conservative physical systems can be described using the Hamiltonian
formalism. A notable example is the Vlasov-Poisson equations, a set of partial
differential equations that govern the time evolution of a phase-space density
function representing collisionless particles under a self-consistent
potential. These equations play a central role in both plasma physics and
cosmology. Due to the complexity of the potential involved, analytical
solutions are rarely available, necessitating the use of numerical methods such
as Particle-In-Cell. In this work, we introduce a novel approach based on
Hamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic
Neural Hamiltonian Flows. Our method transforms an initial Gaussian
distribution in phase space into the final distribution using a sequence of
invertible, volume-preserving transformations derived from Hamiltonian
dynamics. The model is trained on a dataset comprising initial and final states
at a fixed time T, generated via numerical simulations. After training, the
model enables fast sampling of the final distribution from any given initial
state. Moreover, by automatically learning an interpretable physical potential,
it can generalize to intermediate states not seen during training, offering
insights into the system's evolution across time.

</details>

### [113] [Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2505.04619)
*Abdulaziz Almuzairee,Rohan Patil,Dwait Bhatt,Henrik I. Christensen*

Main category: cs.LG

TLDR: 提出了一种名为MAD的算法，通过合并多视角数据并增强单视角特征，提高样本效率并实现轻量级部署。


<details>
  <summary>Details</summary>
Motivation: 多摄像头扩展视野虽能提升视觉伺服鲁棒性，但计算复杂且部署成本高。

Method: 采用MAD算法合并多视角数据，同时增强单视角特征，优化样本效率。

Result: 在Meta-World和ManiSkill3上验证了算法的效率和鲁棒性。

Conclusion: MAD算法在保证鲁棒性的同时，实现了轻量级部署和高样本效率。

Abstract: Vision is well-known for its use in manipulation, especially using visual
servoing. To make it robust, multiple cameras are needed to expand the field of
view. That is computationally challenging. Merging multiple views and using
Q-learning allows the design of more effective representations and optimization
of sample efficiency. Such a solution might be expensive to deploy. To mitigate
this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently
merges views to increase sample efficiency while augmenting with single-view
features to allow lightweight deployment and ensure robust policies. We
demonstrate the efficiency and robustness of our approach using Meta-World and
ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad

</details>

### [114] [Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules](https://arxiv.org/abs/2505.04535)
*Michail Theologitis,Vasilis Samoladas,Antonios Deligiannakis*

Main category: cs.LG

TLDR: FDA-Opt算法家族结合了FDA和FedOpt的优点，解决了它们的核心限制，在联邦学习中表现优于FedOpt。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）中，预训练语言模型（LM）的微调是一个重要但受限于参数通信问题的领域。现有算法如FedOpt和FDA各有不足。

Method: 提出FDA-Opt算法家族，统一了FDA和FedOpt的原则，并解决了它们的核心限制。

Result: 在多个下游NLP任务中，FDA-Opt表现优于FedOpt，且无需额外配置。

Conclusion: FDA-Opt是FedOpt的实用替代方案，性能更优且易于集成。

Abstract: Federated learning (FL) makes it possible to train models on data that would
otherwise remain untapped and inaccessible. Simultaneously, pre-trained
language models (LMs) have emerged as indispensable tools in modern workflows.
These models exhibit extraordinary capabilities and are easily adapted to
downstream tasks. This opens one of the most exciting frontiers in FL:
fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid
communication of parameters, a problem which is magnified by the sheer size of
these modern models. Currently, the FedOpt family of algorithms is the
prevailing approach in FL, though it relies on fixed, heuristic intervals for
model synchronization. Recently, the FDA algorithm introduced a dynamic
alternative by monitoring training progress, but it came with its own
drawbacks; namely, a hard-to-tune threshold parameter and a rigid
synchronization scheme. In this work, we introduce the FDA-Opt family of
algorithms -- a unified generalization that extends the principles behind both
FDA and FedOpt, while resolving their core limitations. We evaluate our
approach on fine-tuning LMs across a range of downstream NLP tasks, and
demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt
operates under hyper-parameter settings originally optimized for its
competitors. In other words, we show that FDA-Opt is a practical, drop-in
replacement for FedOpt in modern FL libraries and systems: it requires no
additional configuration and delivers superior performance out of the box.

</details>

### [115] [Purity Law for Generalizable Neural TSP Solvers](https://arxiv.org/abs/2505.04558)
*Wenzhao Liu,Haoran Li,Congying Han,Zicheng Zhang,Anqi Li,Tiande Guo*

Main category: cs.LG

TLDR: 论文提出了一种称为Purity Law (PuLa)的结构原则，揭示了TSP最优解中边出现频率与周围顶点稀疏度的指数关系，并基于此设计了Purity Policy Optimization (PUPO)训练范式，显著提升了神经求解器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在TSP问题中因无法学习通用模式而导致的泛化能力不足问题。

Method: 通过统计验证PuLa原则，并设计PUPO训练范式，将神经网络的解与PuLa对齐。

Result: PUPO能够无缝集成到现有神经求解器中，显著提升泛化性能且不增加推理计算开销。

Conclusion: PuLa为TSP问题提供了新的结构原则，PUPO为神经求解器的泛化能力提供了有效解决方案。

Abstract: Achieving generalization in neural approaches across different scales and
distributions remains a significant challenge for the Traveling Salesman
Problem~(TSP). A key obstacle is that neural networks often fail to learn
robust principles for identifying universal patterns and deriving optimal
solutions from diverse instances. In this paper, we first uncover Purity Law
(PuLa), a fundamental structural principle for optimal TSP solutions, defining
that edge prevalence grows exponentially with the sparsity of surrounding
vertices. Statistically validated across diverse instances, PuLa reveals a
consistent bias toward local sparsity in global optima. Building on this
insight, we propose Purity Policy Optimization~(PUPO), a novel training
paradigm that explicitly aligns characteristics of neural solutions with PuLa
during the solution construction process to enhance generalization. Extensive
experiments demonstrate that PUPO can be seamlessly integrated with popular
neural solvers, significantly enhancing their generalization performance
without incurring additional computational overhead during inference.

</details>

### [116] [ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $α$-$β$-Divergence](https://arxiv.org/abs/2505.04560)
*Guanghui Wang,Zhiyong Yang,Zitai Wang,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.LG

TLDR: 论文提出ABKD框架，通过α-β散度平衡知识蒸馏中的Hardness-Concentration和Confidence-Concentration效应，优于FKLD和RKLD。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏（KD）中FKLD和RKLD分别存在效应过弱和过强的问题，导致学生模型无法有效学习教师模型的分布信息。

Method: 提出ABKD框架，使用α-β散度平滑插值FKLD和RKLD，平衡两种效应。

Result: 在17个语言/视觉数据集和12种师生模型设置中验证了ABKD的有效性。

Conclusion: ABKD通过平衡两种效应，显著提升了知识蒸馏的效果。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.

</details>

### [117] [Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data](https://arxiv.org/abs/2505.04566)
*Lucas R. C. Farias,Talita P. Silva,Pedro H. M. Araujo*

Main category: cs.LG

TLDR: 本文提出了一种基于LSTM的多任务学习方法，用于联合预测巴西累西腓的虫媒病毒爆发（登革热、基孔肯雅热和寨卡病毒）及其病例数。


<details>
  <summary>Details</summary>
Motivation: 利用历史公共卫生数据，开发一种统一的建模策略，以在数据有限的公共卫生场景中实现可扩展的流行病预测。

Method: 采用LSTM网络和多任务学习框架，结合滑动窗口策略构建时间特征，并通过Keras Tuner进行超参数优化。

Result: 较长的时间窗口提高了登革热回归任务的准确性，而分类任务在中等窗口下表现最佳，表明序列长度与泛化能力之间存在权衡。

Conclusion: 多任务架构在疾病和任务中表现出竞争力，验证了统一建模策略在流行病预测中的可行性和优势。

Abstract: This paper presents a multitask learning approach based on long-short-term
memory (LSTM) networks for the joint prediction of arboviral outbreaks and case
counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging
historical public health data from DataSUS (2017-2023), the proposed model
concurrently performs binary classification (outbreak detection) and regression
(case forecasting) tasks. A sliding window strategy was adopted to construct
temporal features using varying input lengths (60, 90, and 120 days), with
hyperparameter optimization carried out using Keras Tuner. Model evaluation
used time series cross-validation for robustness and a held-out test from 2023
for generalization assessment. The results show that longer windows improve
dengue regression accuracy, while classification performance peaked at
intermediate windows, suggesting an optimal trade-off between sequence length
and generalization. The multitask architecture delivers competitive performance
across diseases and tasks, demonstrating the feasibility and advantages of
unified modeling strategies for scalable epidemic forecasting in data-limited
public health scenarios.

</details>

### [118] [Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization](https://arxiv.org/abs/2505.04578)
*Wenjun Cao*

Main category: cs.LG

TLDR: 恶意RL微调攻击能快速破坏语言模型的安全防护，仅需50步即可使有害输出从0-2升至7-9。现有防御方法对RL动态反馈无效，本文提出Reward Neutralization框架，通过最小信息拒绝模式抵御攻击，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究恶意RL微调对语言模型安全性的威胁，填补现有防御方法在动态反馈机制下的不足。

Method: 提出Reward Neutralization防御框架，训练模型生成最小信息拒绝模式，使恶意奖励信号失效。

Result: 实验显示，该方法在200步攻击后有害分数仍低于2，而标准模型迅速恶化。

Conclusion: Reward Neutralization首次证明了对RL攻击的有效防御，为开源模型安全提供了关键解决方案。

Abstract: Reinforcement learning (RL) fine-tuning transforms large language models
while creating a vulnerability we experimentally verify: Our experiment shows
that malicious RL fine-tuning dismantles safety guardrails with remarkable
efficiency, requiring only 50 steps and minimal adversarial prompts, with
harmful escalating from 0-2 to 7-9. This attack vector particularly threatens
open-source models with parameter-level access. Existing defenses targeting
supervised fine-tuning prove ineffective against RL's dynamic feedback
mechanisms. We introduce Reward Neutralization, the first defense framework
specifically designed against RL fine-tuning attacks, establishing concise
rejection patterns that render malicious reward signals ineffective. Our
approach trains models to produce minimal-information rejections that attackers
cannot exploit, systematically neutralizing attempts to optimize toward harmful
outputs. Experiments validate that our approach maintains low harmful scores
(no greater than 2) after 200 attack steps, while standard models rapidly
deteriorate. This work provides the first constructive proof that robust
defense against increasingly accessible RL attacks is achievable, addressing a
critical security gap for open-weight models.

</details>

### [119] [Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness](https://arxiv.org/abs/2505.04599)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent results in non-convex stochastic optimization demonstrate the
convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0,
L_1)$-smoothness condition, but the rate of convergence is a higher-order
polynomial in terms of problem parameters like the smoothness constants. The
complexity guaranteed by such algorithms to find an $\epsilon$-stationary point
may be significantly larger than the optimal complexity of $\Theta \left(
\Delta L \sigma^2 \epsilon^{-4} \right)$ achieved by SGD in the $L$-smooth
setting, where $\Delta$ is the initial optimality gap, $\sigma^2$ is the
variance of stochastic gradient. However, it is currently not known whether
these higher-order dependencies can be tightened. To answer this question, we
investigate complexity lower bounds for several adaptive optimization
algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence
in terms of problem parameters $\Delta, L_0, L_1$. We provide complexity bounds
for three variations of AdaGrad, which show at least a quadratic dependence on
problem parameters $\Delta, L_0, L_1$. Notably, we show that the decorrelated
variant of AdaGrad-Norm requires at least $\Omega \left( \Delta^2 L_1^2
\sigma^2 \epsilon^{-4} \right)$ stochastic gradient queries to find an
$\epsilon$-stationary point. We also provide a lower bound for SGD with a broad
class of adaptive stepsizes. Our results show that, for certain adaptive
algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult
than the standard smooth setting, in terms of the initial optimality gap and
the smoothness constants.

</details>

### [120] [Testing Juntas Optimally with Samples](https://arxiv.org/abs/2505.04604)
*Lorenzo Beretta,Nathaniel Harms,Caleb Koch*

Main category: cs.LG

TLDR: 论文证明了在无分布样本模型中，测试k-junta布尔函数的样本数量上下界为Θ(1/ε(√(2^k logC(n,k)) + logC(n,k))，并首次给出了自然布尔函数类的紧界。同时，该结果也适用于特征选择问题，表明junta测试器必须学习相关变量集。对于容忍性junta测试，样本下界为Ω(2^{(1-o(1))k} + logC(n,k))，显示容忍性测试与学习之间无显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究在无分布样本模型中测试k-junta布尔函数所需的样本数量，填补自然布尔函数类紧界的空白，并探讨容忍性测试与学习的关系。

Method: 通过数学证明，推导出样本数量的上下界，并分析其在特征选择问题和容忍性测试中的应用。

Result: 证明了样本数量的紧界，并发现容忍性测试与学习之间无显著差距。

Conclusion: 该研究为k-junta测试提供了理论支持，并揭示了容忍性测试的复杂性。

Abstract: We prove tight upper and lower bounds of
$\Theta\left(\tfrac{1}{\epsilon}\left( \sqrt{2^k \log\binom{n}{k} } +
\log\binom{n}{k} \right)\right)$ on the number of samples required for
distribution-free $k$-junta testing. This is the first tight bound for testing
a natural class of Boolean functions in the distribution-free sample-based
model. Our bounds also hold for the feature selection problem, showing that a
junta tester must learn the set of relevant variables. For tolerant junta
testing, we prove a sample lower bound of $\Omega(2^{(1-o(1)) k} +
\log\binom{n}{k})$ showing that, unlike standard testing, there is no large gap
between tolerant testing and learning.

</details>

### [121] [WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales](https://arxiv.org/abs/2505.04608)
*Drew Prinster,Xing Han,Anqi Liu,Suchi Saria*

Main category: cs.LG

TLDR: 论文提出了一种加权广义共形测试鞅（WCTMs），用于在线监测数据分布中的意外变化点，同时控制误报。


<details>
  <summary>Details</summary>
Motivation: 在关键领域部署AI/ML系统需要持续监测以确保安全性，现有方法局限于有限假设类别或无法在线适应数据变化。

Method: 提出加权广义共形测试鞅（WCTMs），支持在线监测和适应数据分布变化，尤其是协变量偏移和概念偏移。

Result: 在真实数据集上，WCTMs表现优于现有基线方法。

Conclusion: WCTMs为AI/ML系统的安全监控提供了更灵活和强大的工具。

Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML)
systems in high-stakes settings arguably requires not only proof of system
reliability, but moreover continual, post-deployment monitoring to quickly
detect and address any unsafe behavior. Statistical methods for nonparametric
change-point detection -- especially the tools of conformal test martingales
(CTMs) and anytime-valid inference -- offer promising approaches to this
monitoring task. However, existing methods are restricted to monitoring limited
hypothesis classes or ``alarm criteria,'' such as data shifts that violate
certain exchangeability assumptions, or do not allow for online adaptation in
response to shifts. In this paper, we expand the scope of these monitoring
methods by proposing a weighted generalization of conformal test martingales
(WCTMs), which lay a theoretical foundation for online monitoring for any
unexpected changepoints in the data distribution while controlling
false-alarms. For practical applications, we propose specific WCTM algorithms
that accommodate online adaptation to mild covariate shifts (in the marginal
input distribution) while raising alarms in response to more severe shifts,
such as concept shifts (in the conditional label distribution) or extreme
(out-of-support) covariate shifts that cannot be easily adapted to. On
real-world datasets, we demonstrate improved performance relative to
state-of-the-art baselines.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [122] [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
*Trilok Padhi,Ramneet Kaur,Adam D. Cobb,Manoj Acharya,Anirban Roy,Colin Samplawski,Brian Matejek,Alexander M. Berenbeim,Nathaniel D. Bastian,Susmit Jha*

Main category: cs.CL

TLDR: 提出了一种针对多模态大语言模型（LLM）的校准不确定性量化（UQ）新方法，通过结合跨模态一致性和温度缩放技术，显著提升了校准效果。


<details>
  <summary>Details</summary>
Motivation: 现有UQ方法依赖LLM在多样化设置下的响应一致性，但在LLM持续错误时仍报告高置信度，导致校准不佳。

Method: 利用跨模态一致性（将文本响应与视觉输入关联）和温度缩放技术校准置信度。

Result: 在医疗问答（Slake）和视觉问答（VQAv2）任务中，实验表明该方法显著改善了校准效果。

Conclusion: 该方法通过结合跨模态一致性和温度缩放，有效提升了多模态LLM的校准性能。

Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ)
tailored for multi-modal large language models (LLMs). Existing
state-of-the-art UQ methods rely on consistency among multiple responses
generated by the LLM on an input query under diverse settings. However, these
approaches often report higher confidence in scenarios where the LLM is
consistently incorrect. This leads to a poorly calibrated confidence with
respect to accuracy. To address this, we leverage cross-modal consistency in
addition to self-consistency to improve the calibration of the multi-modal
models. Specifically, we ground the textual responses to the visual inputs. The
confidence from the grounding model is used to calibrate the overall
confidence. Given that using a grounding model adds its own uncertainty in the
pipeline, we apply temperature scaling - a widely accepted parametric
calibration technique - to calibrate the grounding model's confidence in the
accuracy of generated responses. We evaluate the proposed approach across
multiple multi-modal tasks, such as medical question answering (Slake) and
visual question answering (VQAv2), considering multi-modal models such as
LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework
achieves significantly improved calibration on both tasks.

</details>

### [123] [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)
*Gianluca Manzo,Julia Ive*

Main category: cs.CL

TLDR: 该论文研究了贝叶斯深度学习方法在胸片自动解读中的预测不确定性与人类/语言不确定性之间的关系，发现两者相关性较弱，需进一步改进以更好地应用于临床。


<details>
  <summary>Details</summary>
Motivation: 通过深度学习模型自动化胸片解读可优化临床工作流程，但预测性能的优化不足以满足医疗需求，量化不确定性同样重要。

Method: 使用BERT模型，评估不同二值化方法对不确定性标签的影响，并比较蒙特卡洛Dropout和深度集成方法在预测不确定性估计中的效果。

Result: 模型表现良好，但预测不确定性与语言不确定性之间仅存在适度相关性，表明机器与人类不确定性理解存在差异。

Conclusion: 贝叶斯方法提供有用的不确定性估计，但需进一步改进以更好地捕捉人类不确定性的细微差别，适用于临床。

Abstract: Automating chest radiograph interpretation using Deep Learning (DL) models
has the potential to significantly improve clinical workflows, decision-making,
and large-scale health screening. However, in medical settings, merely
optimising predictive performance is insufficient, as the quantification of
uncertainty is equally crucial. This paper investigates the relationship
between predictive uncertainty, derived from Bayesian Deep Learning
approximations, and human/linguistic uncertainty, as estimated from free-text
radiology reports labelled by rule-based labellers. Utilising BERT as the model
of choice, this study evaluates different binarisation methods for uncertainty
labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in
estimating predictive uncertainty. The results demonstrate good model
performance, but also a modest correlation between predictive and linguistic
uncertainty, highlighting the challenges in aligning machine uncertainty with
human interpretation nuances. Our findings suggest that while Bayesian
approximations provide valuable uncertainty estimates, further refinement is
necessary to fully capture and utilise the subtleties of human uncertainty in
clinical applications.

</details>

### [124] [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)
*Lucia Zheng,Neel Guha,Javokhir Arifov,Sarah Zhang,Michal Skreta,Christopher D. Manning,Peter Henderson,Daniel E. Ho*

Main category: cs.CL

TLDR: 论文介绍了两个新的法律RAG基准测试（Bar Exam QA和Housing Statute QA），以解决缺乏真实法律RAG基准的问题，并评估现有检索管道的性能。


<details>
  <summary>Details</summary>
Motivation: 法律AI开发者使用RAG系统提升性能，但缺乏真实的法律RAG基准测试，阻碍了专门化RAG系统的发展。

Method: 通过模拟法律研究的标注过程，构建了两个新的法律RAG基准测试。

Result: 结果显示法律RAG应用仍具挑战性。

Conclusion: 未来研究需要进一步解决法律RAG的挑战。

Abstract: As the legal community increasingly examines the use of large language models
(LLMs) for various legal applications, legal AI developers have turned to
retrieval-augmented LLMs ("RAG" systems) to improve system performance and
robustness. An obstacle to the development of specialized RAG systems is the
lack of realistic legal RAG benchmarks which capture the complexity of both
legal retrieval and downstream legal question-answering. To address this, we
introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.
Our tasks correspond to real-world legal research tasks, and were produced
through annotation processes which resemble legal research. We describe the
construction of these benchmarks and the performance of existing retriever
pipelines. Our results suggest that legal RAG remains a challenging
application, thus motivating future research.

</details>

### [125] [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)
*Jiale Liu,Yifan Zeng,Shaokun Zhang,Chi Zhang,Malte Højmark-Bertelsen,Marie Normann Gadeberg,Huazheng Wang,Qingyun Wu*

Main category: cs.CL

TLDR: 提出了一种名为FGO的细粒度优化框架，用于解决LLM优化中的上下文窗口溢出和模式识别问题，通过分而治之的方法显著提升性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统LLM优化方法在处理大规模数据集时存在上下文窗口溢出和模式识别退化的问题，需要一种更高效的优化框架。

Method: FGO框架将大型优化任务分解为可管理的子集，进行针对性优化，并通过渐进式合并系统化组合优化后的组件。

Result: 在ALFWorld、LogisticsQA和GAIA基准测试中，FGO性能优于现有方法1.6-8.6%，同时平均提示令牌消耗减少56.3%。

Conclusion: FGO为复杂代理系统的LLM优化提供了可扩展且高效的解决方案，适用于不同规模的数据集。

Abstract: LLM-based optimization has shown remarkable potential in enhancing agentic
systems. However, the conventional approach of prompting LLM optimizer with the
whole training trajectories on training dataset in a single pass becomes
untenable as datasets grow, leading to context window overflow and degraded
pattern recognition. To address these challenges, we propose Fine-Grained
Optimization (FGO), a scalable framework that divides large optimization tasks
into manageable subsets, performs targeted optimizations, and systematically
combines optimized components through progressive merging. Evaluation across
ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms
existing approaches by 1.6-8.6% while reducing average prompt token consumption
by 56.3%. Our framework provides a practical solution for scaling up LLM-based
optimization of increasingly sophisticated agent systems. Further analysis
demonstrates that FGO achieves the most consistent performance gain in all
training dataset sizes, showcasing its scalability and efficiency.

</details>

### [126] [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Timothy Ossowski,Yu Gu,Ying Jin,Sid Kiblawi,Sam Preston,Mu Wei,Paul Vozila,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TLDR: X-Reasoner通过通用领域文本后训练实现跨模态和领域的推理能力，并在实验中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 探索推理能力是否可跨模态和领域通用化。

Method: 采用两阶段方法：监督微调和强化学习，后训练仅使用通用领域文本。

Result: X-Reasoner在跨模态和领域任务中表现优异，X-Reasoner-Med在医学领域达到新SOTA。

Conclusion: 通用文本后训练可实现强通用推理能力，领域专用数据可进一步提升性能。

Abstract: Recent proprietary models (e.g., o3) have begun to demonstrate strong
multimodal reasoning capabilities. Yet, most existing open-source research
concentrates on training text-only reasoning models, with evaluations limited
to mainly mathematical and general-domain tasks. Therefore, it remains unclear
how to effectively extend reasoning capabilities beyond text input and general
domains. This paper explores a fundamental research question: Is reasoning
generalizable across modalities and domains? Our findings support an
affirmative answer: General-domain text-based post-training can enable such
strong generalizable reasoning. Leveraging this finding, we introduce
X-Reasoner, a vision-language model post-trained solely on general-domain text
for generalizable reasoning, using a two-stage approach: an initial supervised
fine-tuning phase with distilled long chain-of-thoughts, followed by
reinforcement learning with verifiable rewards. Experiments show that
X-Reasoner successfully transfers reasoning capabilities to both multimodal and
out-of-domain settings, outperforming existing state-of-the-art models trained
with in-domain and multimodal data across various general and medical
benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in
specialized domains can be further enhanced through continued training on
domain-specific text-only data. Building upon this, we introduce
X-Reasoner-Med, a medical-specialized variant that achieves new state of the
art on numerous text-only and multimodal medical benchmarks.

</details>

### [127] [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)
*Darren Yow-Bang Wang,Zhengyuan Shen,Soumya Smruti Mishra,Zhichao Xu,Yifei Teng,Haibo Ding*

Main category: cs.CL

TLDR: SLOT是一种模型无关的方法，通过微调轻量级语言模型将非结构化LLM输出转换为精确的结构化格式，显著提升模式准确性和内容保真度。


<details>
  <summary>Details</summary>
Motivation: LLM在关键应用中常生成偏离预定义模式的输出，影响可靠应用开发，SLOT旨在解决这一问题。

Method: SLOT采用微调的轻量级语言模型作为后处理层，结合数据合成和评估方法，支持多种LLM和模式规范。

Result: 微调的Mistral-7B模型在模式准确性和内容相似性上分别达到99.5%和94.0%，优于Claude-3.5-Sonnet。

Conclusion: SLOT使轻量模型也能实现高质量结构化输出，适用于资源受限环境。

Abstract: Structured outputs are essential for large language models (LLMs) in critical
applications like agents and information extraction. Despite their
capabilities, LLMs often generate outputs that deviate from predefined schemas,
significantly hampering reliable application development. We present SLOT
(Structured LLM Output Transformer), a model-agnostic approach that transforms
unstructured LLM outputs into precise structured formats. While existing
solutions predominantly rely on constrained decoding techniques or are tightly
coupled with specific models, SLOT employs a fine-tuned lightweight language
model as a post-processing layer, achieving flexibility across various LLMs and
schema specifications. We introduce a systematic pipeline for data curation and
synthesis alongside a formal evaluation methodology that quantifies both schema
accuracy and content fidelity. Our results demonstrate that fine-tuned
Mistral-7B model with constrained decoding achieves near perfect schema
accuracy (99.5%) and content similarity (94.0%), outperforming
Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,
respectively). Notably, even compact models like Llama-3.2-1B can match or
exceed the structured output capabilities of much larger proprietary models
when equipped with SLOT, enabling reliable structured generation in
resource-constrained environments.

</details>

### [128] [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)
*Xu Huang,Yuefeng Huang,Weiwen Liu,Xingshan Zeng,Yasheng Wang,Ruiming Tang,Hong Xie,Defu Lian*

Main category: cs.CL

TLDR: 论文提出个性化工具调用（Personalized Tool Invocation）概念，解决现有研究中忽视用户个性化约束的问题，并定义了两个任务：工具偏好和基于用户档案的查询。作者提出PTool框架和PTBench基准，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM调用工具的基本能力，而忽视了用户个性化约束，如工具偏好和参数推断。

Method: 提出PTool框架，用于合成个性化工具调用数据，并构建PTBench基准。对开源模型进行微调以验证框架效果。

Result: 实验表明PTool框架有效，PTBench成为首个个性化工具调用评估基准。

Conclusion: 个性化工具调用是LLM能力扩展的重要方向，PTool和PTBench为未来研究提供了基础。

Abstract: Tool invocation is a crucial mechanism for extending the capabilities of
Large Language Models (LLMs) and has recently garnered significant attention.
It enables LLMs to solve complex problems through tool calls while accessing
up-to-date world knowledge. However, existing work primarily focuses on the
fundamental ability of LLMs to invoke tools for problem-solving, without
considering personalized constraints in tool invocation. In this work, we
introduce the concept of Personalized Tool Invocation and define two key tasks:
Tool Preference and Profile-dependent Query. Tool Preference addresses user
preferences when selecting among functionally similar tools, while
Profile-dependent Query considers cases where a user query lacks certain tool
parameters, requiring the model to infer them from the user profile. To tackle
these challenges, we propose PTool, a data synthesis framework designed for
personalized tool invocation. Additionally, we construct \textbf{PTBench}, the
first benchmark for evaluating personalized tool invocation. We then fine-tune
various open-source models, demonstrating the effectiveness of our framework
and providing valuable insights. Our benchmark is public at
https://github.com/hyfshadow/PTBench.

</details>

### [129] [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)
*Mengxian Lyu,Xiaohan Li,Ziyi Chen,Jinqian Pan,Cheng Peng,Sankalp Talankar,Yonghui Wu*

Main category: cs.CL

TLDR: 本文综述了自然语言生成（NLG）在医学领域的应用，分析了113篇相关文献，涵盖数据模态、模型架构、临床应用及评估方法，并总结了现有技术的局限性与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的突破，NLG在医学领域展现出增强临床工作流程、支持决策和改善文档的潜力，但缺乏对其方法和应用的全面综述。

Method: 通过系统文献检索，筛选出113篇NLG相关文章，遵循PRISMA指南，分类关键方法、识别临床应用并评估其能力与限制。

Result: 综述涵盖了NLG的关键技术和医学应用，揭示了现有方法的局限性及新兴挑战。

Conclusion: 本文为未来研究提供了利用NLG推动医学发现和医疗保健转型的宝贵见解。

Abstract: Natural language generation (NLG) is the key technology to achieve generative
artificial intelligence (AI). With the breakthroughs in large language models
(LLMs), NLG has been widely used in various medical applications, demonstrating
the potential to enhance clinical workflows, support clinical decision-making,
and improve clinical documentation. Heterogeneous and diverse medical data
modalities, such as medical text, images, and knowledge bases, are utilized in
NLG. Researchers have proposed many generative models and applied them in a
number of healthcare applications. There is a need for a comprehensive review
of NLG methods and applications in the medical domain. In this study, we
systematically reviewed 113 scientific publications from a total of 3,988
NLG-related articles identified using a literature search, focusing on data
modality, model architecture, clinical applications, and evaluation methods.
Following PRISMA (Preferred Reporting Items for Systematic reviews and
Meta-Analyses) guidelines, we categorize key methods, identify clinical
applications, and assess their capabilities, limitations, and emerging
challenges. This timely review covers the key NLG technologies and medical
applications and provides valuable insights for future studies to leverage NLG
to transform medical discovery and healthcare.

</details>

### [130] [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)
*Mingruo Yuan,Ben Kao,Tien-Hsuan Wu,Michael M. K. Cheung,Henry W. H. Chan,Anne S. Y. Cheung,Felix W. H. Chan,Yongxi Chen*

Main category: cs.CL

TLDR: 论文提出了一种三步法，将法律信息转化为易于公众理解的形式，包括创建法律片段（CLIC-pages）、构建法律问题库（LQB）和设计交互式推荐系统（CRec）。重点探讨了利用GPT-3等模型生成法律问题的技术。


<details>
  <summary>Details</summary>
Motivation: 解决法律文件技术性强、公众难以理解的问题，提升法律信息的可访问性和可理解性。

Method: 1. 将法律条文转化为易于理解的片段（CLIC-pages）；2. 构建法律问题库（LQB），利用GPT-3生成问题；3. 设计交互式推荐系统（CRec），根据用户需求推荐相关法律知识。

Result: 机器生成的问题（MGQs）更具扩展性和多样性，成本更低；人工编写的问题（HCQs）更精确。

Conclusion: 三步法有效提升了法律知识的可访问性，机器生成问题在法律信息普及中具有潜力。

Abstract: Access to legal information is fundamental to access to justice. Yet
accessibility refers not only to making legal documents available to the
public, but also rendering legal information comprehensible to them. A vexing
problem in bringing legal information to the public is how to turn formal legal
documents such as legislation and judgments, which are often highly technical,
to easily navigable and comprehensible knowledge to those without legal
education. In this study, we formulate a three-step approach for bringing legal
knowledge to laypersons, tackling the issues of navigability and
comprehensibility. First, we translate selected sections of the law into
snippets (called CLIC-pages), each being a small piece of article that focuses
on explaining certain technical legal concept in layperson's terms. Second, we
construct a Legal Question Bank (LQB), which is a collection of legal questions
whose answers can be found in the CLIC-pages. Third, we design an interactive
CLIC Recommender (CRec). Given a user's verbal description of a legal situation
that requires a legal solution, CRec interprets the user's input and shortlists
questions from the question bank that are most likely relevant to the given
legal situation and recommends their corresponding CLIC pages where relevant
legal knowledge can be found. In this paper we focus on the technical aspects
of creating an LQB. We show how large-scale pre-trained language models, such
as GPT-3, can be used to generate legal questions. We compare machine-generated
questions (MGQs) against human-composed questions (HCQs) and find that MGQs are
more scalable, cost-effective, and more diversified, while HCQs are more
precise. We also show a prototype of CRec and illustrate through an example how
our 3-step approach effectively brings relevant legal knowledge to the public.

</details>

### [131] [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)
*Vihaan Miriyala,Smrithi Bukkapatnam,Lavanya Prahallad*

Main category: cs.CL

TLDR: CoT提示方法显著提升了应用商店评论中细粒度情感分类的准确性，从84%提高到93%。


<details>
  <summary>Details</summary>
Motivation: 传统数字和极性评分无法捕捉用户反馈中的细微情感。

Method: 使用CoT提示与简单提示对比，评估2000条亚马逊应用评论。

Result: CoT提示将分类准确率从84%提升至93%。

Conclusion: 显式推理能有效提升情感分析性能。

Abstract: We explore the use of Chain-of-Thought (CoT) prompting with large language
models (LLMs) to improve the accuracy of granular sentiment categorization in
app store reviews. Traditional numeric and polarity-based ratings often fail to
capture the nuanced sentiment embedded in user feedback. We evaluated the
effectiveness of CoT prompting versus simple prompting on 2000 Amazon app
reviews by comparing each method's predictions to human judgements. CoT
prompting improved classification accuracy from 84% to 93% highlighting the
benefit of explicit reasoning in enhancing sentiment analysis performance.

</details>

### [132] [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
*Variath Madhupal Gautham Nair,Vishal Varma Dantuluri*

Main category: cs.CL

TLDR: 论文提出了一种动态可扩展的基准数据集UTCB，用于评估大语言模型在图像生成中的漏洞，并通过多语言混淆和结构化提示工程测试模型安全性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在图像生成任务中表现优异，但其内容安全检查易受提示攻击，导致生成不当内容。

Method: 结合结构化提示工程、多语言混淆（如祖鲁语、盖尔语、Base64）和Groq托管的LLaMA-3评估，支持零样本和回退提示策略、风险评分和自动标记。

Result: UTCB数据集分为青铜（未验证）、白银（LLM辅助验证）和黄金（人工验证）三个层级，支持随时间演化的新数据源和模型行为。

Conclusion: UTCB为评估和改进大语言模型的安全性提供了动态基准，同时通过负责任披露确保研究伦理。

Abstract: Existing large language models (LLMs) are advancing rapidly and produce
outstanding results in image generation tasks, yet their content safety checks
remain vulnerable to prompt-based jailbreaks. Through preliminary testing on
platforms such as ChatGPT, MetaAI, and Grok, we observed that even short,
natural prompts could lead to the generation of compromising images ranging
from realistic depictions of forged documents to manipulated images of public
figures.
  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and
scalable benchmark dataset to evaluate LLM vulnerability in image generation.
Our methodology combines structured prompt engineering, multilingual
obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted
LLaMA-3. The pipeline supports both zero-shot and fallback prompting
strategies, risk scoring, and automated tagging. All generations are stored
with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided
verification), and Gold (manually verified) tiers. UTCB is designed to evolve
over time with new data sources, prompt templates, and model behaviors.
  Warning: This paper includes visual examples of adversarial inputs designed
to test model safety. All outputs have been redacted to ensure responsible
disclosure.

</details>

### [133] [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)
*Manas Satish Bedmutha,Feng Chen,Andrea Hartzler,Trevor Cohen,Nadir Weibel*

Main category: cs.CL

TLDR: 论文探讨了利用大型语言模型（LLMs）自动分析临床对话中的社交信号，以提升医患沟通效果。


<details>
  <summary>Details</summary>
Motivation: 医患沟通中的非语言社交信号对关系质量有重要影响，但传统方法难以自动分析。LLMs的进步为这一领域提供了新可能。

Method: 设计了任务特定的提示，并在标注数据集上评估了多种LLM架构和提示风格，分析了20种社交信号。

Result: 开发了首个能追踪20种社交信号的系统，并揭示了LLM的行为模式及其在临床环境中的优化方向。

Conclusion: LLMs在临床社交信号处理中具有潜力，未来可通过模型配置和上下文优化进一步提升性能。

Abstract: Effective communication between providers and their patients influences
health and care outcomes. The effectiveness of such conversations has been
linked not only to the exchange of clinical information, but also to a range of
interpersonal behaviors; commonly referred to as social signals, which are
often conveyed through non-verbal cues and shape the quality of the
patient-provider relationship. Recent advances in large language models (LLMs)
have demonstrated an increasing ability to infer emotional and social behaviors
even when analyzing only textual information. As automation increases also in
clinical settings, such as for transcription of patient-provider conversations,
there is growing potential for LLMs to automatically analyze and extract social
behaviors from these interactions. To explore the foundational capabilities of
LLMs in tracking social signals in clinical dialogue, we designed task-specific
prompts and evaluated model performance across multiple architectures and
prompting styles using a highly imbalanced, annotated dataset spanning 20
distinct social signals such as provider dominance, patient warmth, etc. We
present the first system capable of tracking all these 20 coded signals, and
uncover patterns in LLM behavior. Further analysis of model configurations and
clinical context provides insights for enhancing LLM performance on social
signal processing tasks in healthcare settings.

</details>

### [134] [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)
*Maria Marina,Nikolay Ivanov,Sergey Pletenev,Mikhail Salnikov,Daria Galimzianova,Nikita Krayko,Vasily Konovalov,Alexander Panchenko,Viktor Moskvoretskii*

Main category: cs.CL

TLDR: 本文提出了一种轻量级、不依赖LLM的自适应检索方法，基于外部信息，显著提升了效率，同时保持了与复杂LLM方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易产生幻觉，检索增强生成（RAG）虽能缓解这一问题，但计算成本高且可能传播错误信息。现有自适应检索方法依赖LLM的不确定性估计，效率低下且不实用。

Method: 研究了27个特征，分为7组，并探索了它们的混合组合。在6个QA数据集上评估了这些方法的性能和效率。

Result: 结果显示，该方法在性能上与复杂LLM方法相当，同时显著提升了效率。

Conclusion: 外部信息在自适应检索中具有潜力，为高效且准确的检索提供了新思路。

Abstract: Large Language Models~(LLMs) are prone to hallucinations, and
Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high
computational cost while risking misinformation. Adaptive retrieval aims to
retrieve only when necessary, but existing approaches rely on LLM-based
uncertainty estimation, which remain inefficient and impractical. In this
study, we introduce lightweight LLM-independent adaptive retrieval methods
based on external information. We investigated 27 features, organized into 7
groups, and their hybrid combinations. We evaluated these methods on 6 QA
datasets, assessing the QA performance and efficiency. The results show that
our approach matches the performance of complex LLM-based methods while
achieving significant efficiency gains, demonstrating the potential of external
information for adaptive retrieval.

</details>

### [135] [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)
*Sofia Jamil,Aryan Dabad,Bollampalli Areen Reddy,Sriparna Saha,Rajiv Misra,Adil A. Shakur*

Main category: cs.CL

TLDR: 该论文提出了一个针对癌症患者报告的药物不良事件（ADEs）的分组摘要任务，并开发了MCADRS数据集和GASCADE框架，结合LLMs和T5模型，显著提升了摘要性能。


<details>
  <summary>Details</summary>
Motivation: 现有药物警戒研究多关注普通疾病，缺乏针对癌症的专门研究，而癌症患者的ADE报告对药物决策至关重要。

Method: 提出MCADRS数据集和GASCADE框架，结合LLMs的信息提取能力和T5模型的摘要能力，并首次在摘要任务中应用对齐技术。

Result: GASCADE在多种指标上表现优异，并通过自动评估和人工验证。

Conclusion: 该研究为个性化癌症护理提供了新工具，推动了药物警戒和患者关怀的进步。

Abstract: In the realm of cancer treatment, summarizing adverse drug events (ADEs)
reported by patients using prescribed drugs is crucial for enhancing
pharmacovigilance practices and improving drug-related decision-making. While
the volume and complexity of pharmacovigilance data have increased, existing
research in this field has predominantly focused on general diseases rather
than specifically addressing cancer. This work introduces the task of grouped
summarization of adverse drug events reported by multiple patients using the
same drug for cancer treatment. To address the challenge of limited resources
in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug
Reaction and Summarization (MCADRS) dataset. This dataset includes
pharmacovigilance posts detailing patient concerns regarding drug efficacy and
adverse effects, along with extracted labels for drug names, adverse drug
events, severity, and adversity of reactions, as well as summaries of ADEs for
each drug. Additionally, we propose the Grouping and Abstractive Summarization
of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that
combines the information extraction capabilities of Large Language Models
(LLMs) with the summarization power of the encoder-decoder T5 model. Our work
is the first to apply alignment techniques, including advanced algorithms like
Direct Preference Optimization, to encoder-decoder models using synthetic
datasets for summarization tasks. Through extensive experiments, we demonstrate
the superior performance of GASCADE across various metrics, validated through
both automated assessments and human evaluations. This multitasking approach
enhances drug-related decision-making and fosters a deeper understanding of
patient concerns, paving the way for advancements in personalized and
responsive cancer care. The code and dataset used in this work are publicly
available.

</details>

### [136] [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
*Dario Garcia-Gasulla,Jordi Bayarri-Planas,Ashwin Kumar Gururajan,Enrique Lopez-Cuena,Adrian Tormos,Daniel Hinjos,Pablo Bernabeu-Perez,Anna Arias-Duart,Pablo Agustin Martin-Torres,Marta Gonzalez-Mallo,Sergio Alvarez-Napagao,Eduard Ayguadé-Parra,Ulises Cortés*

Main category: cs.CL

TLDR: 论文提出了一种开源医疗大语言模型Aloe Beta，通过优化数据预处理和训练阶段，结合DPO和RAG提升模型安全性和效能，并定义了新的评估标准。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗领域的应用增多，需要开源模型以保护公众利益，同时提升模型的安全性和效能。

Method: 基于Llama 3.1和Qwen 2.5等基础模型，使用自定义数据集和合成思维链示例，通过DPO对齐模型，并采用四种测试评估模型。

Result: Aloe Beta模型在医疗基准测试中表现优异，安全性显著提升，受到医疗专业人士青睐。

Conclusion: Aloe Beta模型及其开发方法为开源医疗大语言模型领域做出重要贡献，设定了新的开发和报告标准。

Abstract: Purpose: With advancements in Large Language Models (LLMs) for healthcare,
the need arises for competitive open-source models to protect the public
interest. This work contributes to the field of open medical LLMs by optimizing
key stages of data preprocessing and training, while showing how to improve
model safety (through DPO) and efficacy (through RAG). The evaluation
methodology used, which includes four different types of tests, defines a new
standard for the field. The resultant models, shown to be competitive with the
best private alternatives, are released with a permisive license.
  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,
Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of
Thought examples. The models undergo alignment with Direct Preference
Optimization, emphasizing ethical and policy-aligned performance in the
presence of jailbreaking attacks. Evaluation includes close-ended, open-ended,
safety and human assessments, to maximize the reliability of results.
  Results: Recommendations are made across the entire pipeline, backed by the
solid performance of the Aloe Family. These models deliver competitive
performance across healthcare benchmarks and medical fields, and are often
preferred by healthcare professionals. On bias and toxicity, the Aloe Beta
models significantly improve safety, showing resilience to unseen jailbreaking
attacks. For a responsible release, a detailed risk assessment specific to
healthcare is attached to the Aloe Family models.
  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a
significant contribution to the open-source medical LLM field, offering
top-of-the-line performance while maintaining high ethical requirements. This
work sets a new standard for developing and reporting aligned LLMs in
healthcare.

</details>

### [137] [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)
*David Exler,Mark Schutera,Markus Reischl,Luca Rettenberger*

Main category: cs.CL

TLDR: 论文研究了大型语言模型（LLMs）的政治偏见，发现其倾向于左翼政党，并探讨了语言、模型来源和发布时间对偏见的影响。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的普及，评估LLMs的偏见至关重要，以避免对用户决策和公众舆论的负面影响。

Method: 通过Wahl-O-Mat评分量化LLMs的政治偏见，比较其与德国政党立场的对齐程度，并分析影响因素。

Result: 发现LLMs普遍存在左倾偏见，且语言、模型规模和发布时间对偏见有显著影响。

Conclusion: 开发LLMs的大型企业需承担减少偏见的责任，以避免对公众决策的潜在影响。

Abstract: With the increasing prevalence of artificial intelligence, careful evaluation
of inherent biases needs to be conducted to form the basis for alleviating the
effects these predispositions can have on users. Large language models (LLMs)
are predominantly used by many as a primary source of information for various
topics. LLMs frequently make factual errors, fabricate data (hallucinations),
or present biases, exposing users to misinformation and influencing opinions.
Educating users on their risks is key to responsible use, as bias, unlike
hallucinations, cannot be caught through data verification. We quantify the
political bias of popular LLMs in the context of the recent vote of the German
Bundestag using the score produced by the Wahl-O-Mat. This metric measures the
alignment between an individual's political views and the positions of German
political parties. We compare the models' alignment scores to identify factors
influencing their political preferences. Doing so, we discover a bias toward
left-leaning parties, most dominant in larger LLMs. Also, we find that the
language we use to communicate with the models affects their political views.
Additionally, we analyze the influence of a model's origin and release date and
compare the results to the outcome of the recent vote of the Bundestag. Our
results imply that LLMs are prone to exhibiting political bias. Large
corporations with the necessary means to develop LLMs, thus, knowingly or
unknowingly, have a responsibility to contain these biases, as they can
influence each voter's decision-making process and inform public opinion in
general and at scale.

</details>

### [138] [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)
*Aidar Valeev,Roman Garaev,Vadim Lomshakov,Irina Piontkovskaya,Vladimir Ivanov,Israel Adewuyi*

Main category: cs.CL

TLDR: 论文提出了一个长上下文代码生成基准（YABLoCo），用于评估大型语言模型在C和C++大型代码库中的函数生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常针对小型或中型代码库，而实际软件项目可能包含数百万行代码，因此需要填补这一空白。

Method: 构建了一个包含215个函数的测试集，涵盖四种大型代码库，提供函数元数据、依赖上下文、文档字符串、函数体和调用图。

Result: 贡献包括：1）针对C和C++的基准；2）覆盖200K至2,000K行代码的大型代码库；3）提供可扩展的评估管道和可视化工具。

Conclusion: 该基准为评估大型代码库中的代码生成提供了有效工具，填补了现有研究的空白。

Abstract: Large Language Models demonstrate the ability to solve various programming
tasks, including code generation. Typically, the performance of LLMs is
measured on benchmarks with small or medium-sized context windows of thousands
of lines of code. At the same time, in real-world software projects,
repositories can span up to millions of LoC. This paper closes this gap by
contributing to the long context code generation benchmark (YABLoCo). The
benchmark featured a test set of 215 functions selected from four large
repositories with thousands of functions. The dataset contained metadata of
functions, contexts of the functions with different levels of dependencies,
docstrings, functions bodies, and call graphs for each repository. This paper
presents three key aspects of the contribution. First, the benchmark aims at
function body generation in large repositories in C and C++, two languages not
covered by previous benchmarks. Second, the benchmark contains large
repositories from 200K to 2,000K LoC. Third, we contribute a scalable
evaluation pipeline for efficient computing of the target metrics and a tool
for visual analysis of generated code. Overall, these three aspects allow for
evaluating code generation in large repositories in C and C++.

</details>

### [139] [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
*Xiaoyu Xu,Minxin Du,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TLDR: OBLIVIATE是一种高效的去学习框架，用于从大型语言模型中移除敏感或受版权保护的内容，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能记忆敏感或侵权内容，需要一种方法来安全移除这些数据而不损害模型功能。

Method: 框架包括提取目标标记、构建保留集，并使用包含掩码、蒸馏和世界事实的损失函数进行微调，结合低秩适配器（LoRA）提高效率。

Result: 实验证明OBLIVIATE能有效抵抗成员推理攻击，减少对保留数据的影响，并在多种场景下保持鲁棒性。

Conclusion: OBLIVIATE为语言模型提供了安全且高效的去学习解决方案。

Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing
sensitive, copyrighted, or toxic content. To address this, we propose
OBLIVIATE, a robust unlearning framework that removes targeted data while
preserving model utility. The framework follows a structured process:
extracting target tokens, building retain sets, and fine-tuning with a tailored
loss function comprising three components -- masking, distillation, and world
fact. Using low-rank adapters (LoRA), it ensures efficiency without
compromising unlearning quality. We conduct experiments on multiple datasets,
including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite
of metrics: forget quality (new document-level memorization score), model
utility, and fluency. Results demonstrate its effectiveness in resisting
membership inference attacks, minimizing the impact on retained data, and
maintaining robustness across diverse scenarios.

</details>

### [140] [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)
*Ilya Koziev*

Main category: cs.CL

TLDR: 论文提出通过自动语言异常检测提升生成模型训练数据集质量，比较了无监督和监督文本异常检测方法，并引入RUPOR数据集。


<details>
  <summary>Details</summary>
Motivation: 互联网来源的训练文本质量参差不齐，影响生成模型在诗歌等创意任务中的表现，需有效管理缺陷。

Method: 采用无监督和监督文本异常检测方法，利用合成和人工标注数据集进行比较，并引入RUPOR数据集。

Result: 提供了完整的评估代码和工具，帮助提升创意领域生成模型的训练数据集质量。

Conclusion: 自动化语言异常检测能有效提升训练数据质量，为创意生成模型提供支持。

Abstract: The quality of natural language texts in fine-tuning datasets plays a
critical role in the performance of generative models, particularly in
computational creativity tasks such as poem or song lyric generation. Fluency
defects in generated poems significantly reduce their value. However, training
texts are often sourced from internet-based platforms without stringent quality
control, posing a challenge for data engineers to manage defect levels
effectively.
  To address this issue, we propose the use of automated linguistic anomaly
detection to identify and filter out low-quality texts from training datasets
for creative models. In this paper, we present a comprehensive comparison of
unsupervised and supervised text anomaly detection approaches, utilizing both
synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a
collection of Russian-language human-labeled poems designed for cross-sentence
grammatical error detection, and provide the full evaluation code. Our work
aims to empower the community with tools and insights to improve the quality of
training datasets for generative models in creative domains.

</details>

### [141] [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)
*Yehui Tang,Yichun Yin,Yaoyuan Wang,Hang Zhou,Yu Pan,Wei Guo,Ziyang Zhang,Miao Rang,Fangcheng Liu,Naifu Zhang,Binghan Li,Yonghan Dong,Xiaojun Meng,Yasheng Wang,Dong Li,Yin Li,Dandan Tu,Can Chen,Youliang Yan,Fisher Yu,Ruiming Tang,Yunhe Wang,Botian Huang,Bo Wang,Boxiao Liu,Changzheng Zhang,Da Kuang,Fei Liu,Gang Huang,Jiansheng Wei,Jiarui Qin,Jie Ran,Jinpeng Li,Jun Zhao,Liang Dai,Lin Li,Liqun Deng,Peifeng Qin,Pengyuan Zeng,Qiang Gu,Shaohua Tang,Shengjun Cheng,Tao Gao,Tao Yu,Tianshu Li,Tianyu Bi,Wei He,Weikai Mao,Wenyong Huang,Wulong Liu,Xiabing Li,Xianzhi Yu,Xueyu Wu,Xu He,Yangkai Du,Yan Xu,Ye Tian,Yimeng Wu,Yongbing Huang,Yong Tian,Yong Zhu,Yue Li,Yufei Wang,Yuhang Gai,Yujun Li,Yu Luo,Yunsheng Ni,Yusen Sun,Zelin Chen,Zhe Liu,Zhicheng Liu,Zhipeng Tu,Zilin Ding,Zongyuan Zhan*

Main category: cs.CL

TLDR: 论文探讨了如何在Ascend NPUs上高效训练稀疏大语言模型（LLMs），通过模拟优化模型配置和系统优化，实现了7180亿参数的Pangu Ultra MoE模型，并验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏大语言模型（如MoE）规模庞大，对软硬件系统提出挑战，研究旨在探索如何在Ascend NPUs上高效利用计算资源并实现性能提升。

Method: 利用模拟比较模型超参数权衡，优化专家并行性以减少通信开销，并提升设备内存效率。

Result: 在6K Ascend NPUs上训练Pangu Ultra MoE模型，实现了30.0%的MFU，性能与DeepSeek R1相当。

Conclusion: 研究表明Ascend系统能够高效训练大规模稀疏语言模型，并为未来研究提供了参考。

Abstract: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close
to a trillion parameters are dominating the realm of most capable language
models. However, the massive model scale poses significant challenges for the
underlying software and hardware systems. In this paper, we aim to uncover a
recipe to harness such scale on Ascend NPUs. The key goals are better usage of
the computing resources under the dynamic sparse model structures and
materializing the expected performance gain on the actual hardware. To select
model configurations suitable for Ascend NPUs without repeatedly running the
expensive experiments, we leverage simulation to compare the trade-off of
various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM
with 718 billion parameters, and we conducted experiments on the model to
verify the simulation results. On the system side, we dig into Expert
Parallelism to optimize the communication between NPU devices to reduce the
synchronization overhead. We also optimize the memory efficiency within the
devices to further reduce the parameter and activation management overhead. In
the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with
performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and
demonstrate that the Ascend system is capable of harnessing all the training
stages of the state-of-the-art language models. Extensive experiments indicate
that our recipe can lead to efficient training of large-scale sparse language
models with MoE. We also study the behaviors of such models for future
reference.

</details>

### [142] [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TLDR: 本文系统综述了解决低资源语言生成模型数据稀缺问题的策略，总结了54项研究中的技术方法、架构选择、语言覆盖和评估趋势，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 生成语言模型（如ChatGPT和Google Gemini）主要服务于高资源语言，加剧了自然语言处理中的语言不平等问题。本文旨在填补低资源语言生成模型数据稀缺研究的空白。

Method: 通过分析54项研究，分类和评估了单语数据增强、反向翻译、多语言训练和提示工程等技术方法，并考察了架构选择、语言覆盖和评估方法。

Result: 研究发现依赖Transformer模型、集中于少数低资源语言且评估方法不一致，提出了扩展方法和构建公平生成系统的建议。

Conclusion: 本文为研究人员和开发者提供了构建包容性AI工具的建议，以支持低资源语言使用者和保护语言多样性。

Abstract: Generative language modelling has surged in popularity with the emergence of
services such as ChatGPT and Google Gemini. While these models have
demonstrated transformative potential in productivity and communication, they
overwhelmingly cater to high-resource languages like English. This has
amplified concerns over linguistic inequality in natural language processing
(NLP). This paper presents the first systematic review focused specifically on
strategies to address data scarcity in generative language modelling for
low-resource languages (LRL). Drawing from 54 studies, we identify, categorise
and evaluate technical approaches, including monolingual data augmentation,
back-translation, multilingual training, and prompt engineering, across
generative tasks. We also analyse trends in architecture choices, language
family representation, and evaluation methods. Our findings highlight a strong
reliance on transformer-based models, a concentration on a small subset of
LRLs, and a lack of consistent evaluation across studies. We conclude with
recommendations for extending these methods to a wider range of LRLs and
outline open challenges in building equitable generative language systems.
Ultimately, this review aims to support researchers and developers in building
inclusive AI tools for underrepresented languages, a necessary step toward
empowering LRL speakers and the preservation of linguistic diversity in a world
increasingly shaped by large-scale language technologies.

</details>

### [143] [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)
*Hao Sun,Zile Qiao,Jiayan Guo,Xuanbo Fan,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.CL

TLDR: ZeroSearch是一种强化学习框架，通过避免与真实搜索引擎交互，提升LLMs的搜索能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中因搜索引擎返回文档质量不可控和API成本过高的问题。

Method: 采用轻量级监督微调将LLM转化为检索模块，并结合课程式策略逐步增加生成文档的噪声。

Result: 实验表明，ZeroSearch能有效提升LLMs的搜索能力，甚至在某些情况下超越真实搜索引擎。

Conclusion: ZeroSearch是一种高效、可扩展的解决方案，适用于不同规模的LLMs和多种RL算法。

Abstract: Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a
reinforcement learning framework that incentivizes the search capabilities of
LLMs without interacting with real search engines. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both relevant and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [144] [Proceedings of 1st Workshop on Advancing Artificial Intelligence through Theory of Mind](https://arxiv.org/abs/2505.03770)
*Mouad Abrini,Omri Abend,Dina Acklin,Henny Admoni,Gregor Aichinger,Nitay Alon,Zahra Ashktorab,Ashish Atreja,Moises Auron,Alexander Aufreiter,Raghav Awasthi,Soumya Banerjee,Joe M. Barnby,Rhea Basappa,Severin Bergsmann,Djallel Bouneffouf,Patrick Callaghan,Marc Cavazza,Thierry Chaminade,Sonia Chernova,Mohamed Chetouan,Moumita Choudhury,Axel Cleeremans,Jacek B. Cywinski,Fabio Cuzzolin,Hokin Deng,N'yoma Diamond,Camilla Di Pasquasio,Guillaume Dumas,Max van Duijn,Mahapatra Dwarikanath,Qingying Gao,Ashok Goel,Rebecca Goldstein,Matthew Gombolay,Gabriel Enrique Gonzalez,Amar Halilovic,Tobias Halmdienst,Mahimul Islam,Julian Jara-Ettinger,Natalie Kastel,Renana Keydar,Ashish K. Khanna,Mahdi Khoramshahi,JiHyun Kim,MiHyeon Kim,YoungBin Kim,Senka Krivic,Nikita Krasnytskyi,Arun Kumar,JuneHyoung Kwon,Eunju Lee,Shane Lee,Peter R. Lewis,Xue Li,Yijiang Li,Michal Lewandowski,Nathan Lloyd,Matthew B. Luebbers,Dezhi Luo,Haiyun Lyu,Dwarikanath Mahapatra,Kamal Maheshwari,Mallika Mainali,Piyush Mathur,Patrick Mederitsch,Shuwa Miura,Manuel Preston de Miranda,Reuth Mirsky,Shreya Mishra,Nina Moorman,Katelyn Morrison,John Muchovej,Bernhard Nessler,Felix Nessler,Hieu Minh Jord Nguyen,Abby Ortego,Francis A. Papay,Antoine Pasquali,Hamed Rahimi,Charumathi Raghu,Amanda Royka,Stefan Sarkadi,Jaelle Scheuerman,Simon Schmid,Paul Schrater,Anik Sen,Zahra Sheikhbahaee,Ke Shi,Reid Simmons,Nishant Singh,Mason O. Smith,Ramira van der Meulen,Anthia Solaki,Haoran Sun,Viktor Szolga,Matthew E. Taylor,Travis Taylor,Sanne Van Waveren,Juan David Vargas,Rineke Verbrugge,Eitan Wagner,Justin D. Weisz,Ximing Wen,William Yeoh,Wenlong Zhang,Michelle Zhao,Shlomo Zilberstein*

Main category: cs.AI

TLDR: 该卷收录了2025年3月3日在AAAI 2025会议上关于“通过心智理论推进人工智能”研讨会的论文选集，旨在为ToM和AI研究社区提供开放获取的精选文集。


<details>
  <summary>Details</summary>
Motivation: 为心智理论（ToM）和人工智能（AI）研究社区提供一个开放获取的精选文集，促进相关领域的研究交流与发展。

Method: 通过研讨会的形式收集并精选相关论文，整理成卷。

Result: 形成了一本开放获取的精选文集，涵盖ToM与AI领域的研究成果。

Conclusion: 该卷为ToM和AI研究社区提供了有价值的资源，推动了相关领域的学术交流与合作。

Abstract: This volume includes a selection of papers presented at the Workshop on
Advancing Artificial Intelligence through Theory of Mind held at AAAI 2025 in
Philadelphia US on 3rd March 2025. The purpose of this volume is to provide an
open access and curated anthology for the ToM and AI research community.

</details>

### [145] [Design description of Wisdom Computing Persperctive](https://arxiv.org/abs/2505.03800)
*TianYi Yu*

Main category: cs.AI

TLDR: 开发了一个基于AI和可视化技术的系统，用于手写矩阵识别和数学计算步骤的动态展示，帮助学生理解抽象数学公式。


<details>
  <summary>Details</summary>
Motivation: 解决学生在学习数学时难以理解抽象公式和复杂计算步骤的问题。

Method: 结合Mamba骨干网络和YOLO模型进行手写矩阵识别与重构，利用CoordAttention机制提升字符空间定位，通过Manim动画引擎展示计算步骤。

Result: 系统具有高模块化和灵活性，能实时生成不同数学运算的动态动画，提升学习效果。

Conclusion: 该系统为教育提供了直观、友好且高效的辅助工具，帮助学生深入理解数学逻辑。

Abstract: This course design aims to develop and research a handwriting matrix
recognition and step-by-step visual calculation process display system,
addressing the issue of abstract formulas and complex calculation steps that
students find difficult to understand when learning mathematics. By integrating
artificial intelligence with visualization animation technology, the system
enhances precise recognition of handwritten matrix content through the
introduction of Mamba backbone networks, completes digital extraction and
matrix reconstruction using the YOLO model, and simultaneously combines
CoordAttention coordinate attention mechanisms to improve the accurate grasp of
character spatial positions. The calculation process is demonstrated frame by
frame through the Manim animation engine, vividly showcasing each mathematical
calculation step, helping students intuitively understand the intrinsic logic
of mathematical operations. Through dynamically generating animation processes
for different computational tasks, the system exhibits high modularity and
flexibility, capable of generating various mathematical operation examples in
real-time according to student needs. By innovating human-computer interaction
methods, it brings mathematical calculation processes to life, helping students
bridge the gap between knowledge and understanding on a deeper level,
ultimately achieving a learning experience where "every step is understood."
The system's scalability and interactivity make it an intuitive, user-friendly,
and efficient auxiliary tool in education.

</details>

### [146] [GRAML: Dynamic Goal Recognition As Metric Learning](https://arxiv.org/abs/2505.03941)
*Matan Shamir,Reuth Mirsky*

Main category: cs.AI

TLDR: GRAML是一种基于度量学习的目标识别方法，通过Siamese网络和RNN学习嵌入空间中的度量，快速适应新目标并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据驱动目标识别方法无法快速适应新目标且需要大量训练的问题。

Method: 使用Siamese网络和RNN学习嵌入空间中的度量，使不同目标的观测轨迹嵌入距离远，相同目标的嵌入距离近。

Result: 在多种环境中，GRAML在速度、灵活性和运行时表现上优于现有方法，同时保持高准确性。

Conclusion: GRAML通过度量学习实现了对新目标的快速适应，同时避免了传统方法的训练成本。

Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives
based on observed actions. Recent data-driven approaches for GR alleviate the
need for costly, manually crafted domain models. However, these approaches can
only reason about a pre-defined set of goals, and time-consuming training is
needed for new emerging goals. To keep this model-learning automated while
enabling quick adaptation to new goals, this paper introduces GRAML: Goal
Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a
deep metric learning task, employing an RNN that learns a metric over an
embedding space, where the embeddings for observation traces leading to
different goals are distant, and embeddings of traces leading to the same goals
are close. This metric is especially useful when adapting to new goals, even if
given just one example observation trace per goal. Evaluated on a versatile set
of environments, GRAML shows speed, flexibility, and runtime improvements over
the state-of-the-art GR while maintaining accurate recognition.

</details>

### [147] [Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents](https://arxiv.org/abs/2505.03947)
*Xiang Li,Yiyang Hao,Doug Fulop*

Main category: cs.AI

TLDR: 论文探讨了如何利用大型语言模型（LLMs）在零样本设置下玩Atari游戏Frogger，并研究了上下文学习和推理努力对性能的影响，同时展示了如何用LLM演示提升传统强化学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 开发能够快速适应新任务的通用强化学习代理是研究的主要目标，但现有方法训练成本高且速度慢。

Method: 结合最新推理LLMs和域外强化学习后训练，在零样本设置下测试Frogger游戏表现，并研究上下文学习和推理努力的影响。此外，用LLM演示引导传统RL方法。

Result: LLMs在零样本下能玩Frogger，上下文学习和推理努力显著影响性能。LLM演示显著提升了传统RL方法的性能和样本效率。

Conclusion: LLMs在强化学习中具有潜力，能提升传统方法的效率和性能，为通用代理开发提供了新思路。

Abstract: One of the primary aspirations in reinforcement learning research is
developing general-purpose agents capable of rapidly adapting to and mastering
novel tasks. While RL gaming agents have mastered many Atari games, they remain
slow and costly to train for each game. In this work, we demonstrate that
latest reasoning LLMs with out-of-domain RL post-training can play a
challenging Atari game called Frogger under a zero-shot setting. We then
investigate the effect of in-context learning and the amount of reasoning
effort on LLM performance. Lastly, we demonstrate a way to bootstrap
traditional RL method with LLM demonstrations, which significantly improves
their performance and sample efficiency. Our implementation is open sourced at
https://github.com/AlienKevin/frogger.

</details>

### [148] [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/abs/2505.03961)
*Gerrit Großmann,Larisa Ivanova,Sai Leela Poduru,Mohaddeseh Tabrizian,Islam Mesabah,David A. Selby,Sebastian J. Vollmer*

Main category: cs.AI

TLDR: 研究探讨共享叙事是否能促进LLM智能体的合作行为，通过公共物品游戏实验发现，共同叙事显著提升合作成功率，而不同叙事则导致自私行为占优。


<details>
  <summary>Details</summary>
Motivation: 探索共享叙事对LLM智能体合作行为的影响，为多智能体系统设计和AI对齐提供启示。

Method: 使用有限重复公共物品游戏，通过不同团队合作叙事对LLM智能体进行干预，观察谈判行为变化。

Result: 共同叙事显著提高合作成功率，而不同叙事导致自私行为占优；智能体数量增加时效果减弱。

Conclusion: 共享叙事对LLM智能体合作行为有显著影响，为多智能体系统设计提供新思路。

Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by
shared narratives that encode common beliefs and values. This study explores
whether such narratives can similarly nudge LLM agents toward collaboration. We
use a finitely repeated public goods game in which LLM agents choose either
cooperative or egoistic spending strategies. We prime agents with stories
highlighting teamwork to different degrees and test how this influences
negotiation outcomes. Our experiments explore four questions:(1) How do
narratives influence negotiation behavior? (2) What differs when agents share
the same story versus different ones? (3) What happens when the agent numbers
grow? (4) Are agents resilient against self-serving negotiators? We find that
story-based priming significantly affects negotiation strategies and success
rates. Common stories improve collaboration, benefiting each agent. By
contrast, priming agents with different stories reverses this effect, and those
agents primed toward self-interest prevail. We hypothesize that these results
carry implications for multi-agent system design and AI alignment.

</details>

### [149] [LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration](https://arxiv.org/abs/2505.03985)
*Zirong Chen,Ziyan An,Jennifer Reynolds,Kristin Mullen,Stephen Martini,Meiyi Ma*

Main category: cs.AI

TLDR: LogiDebrief是一个AI驱动的框架，通过结合信号时序逻辑（STL）和大语言模型（LLM），自动化911电话复盘，提升评估覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估911电话复盘效率低、覆盖率不足，无法应对高通话量。

Method: LogiDebrief采用三步验证：上下文理解、STL运行时检查与LLM结合、结果自动汇总为质量报告。

Result: 在真实部署中，LogiDebrief复盘了1,701通电话，节省311.85小时人工时间，实证显示其准确有效。

Conclusion: LogiDebrief显著提升911电话复盘效率与质量，具有实际应用价值。

Abstract: Emergency response services are critical to public safety, with 9-1-1
call-takers playing a key role in ensuring timely and effective emergency
operations. To ensure call-taking performance consistency, quality assurance is
implemented to evaluate and refine call-takers' skillsets. However, traditional
human-led evaluations struggle with high call volumes, leading to low coverage
and delayed assessments. We introduce LogiDebrief, an AI-driven framework that
automates traditional 9-1-1 call debriefing by integrating Signal-Temporal
Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous
performance evaluation. LogiDebrief formalizes call-taking requirements as
logical specifications, enabling systematic assessment of 9-1-1 calls against
procedural guidelines. It employs a three-step verification process: (1)
contextual understanding to identify responder types, incident classifications,
and critical conditions; (2) STL-based runtime checking with LLM integration to
ensure compliance; and (3) automated aggregation of results into quality
assurance reports. Beyond its technical contributions, LogiDebrief has
demonstrated real-world impact. Successfully deployed at Metro Nashville
Department of Emergency Communications, it has assisted in debriefing 1,701
real-world calls, saving 311.85 hours of active engagement. Empirical
evaluation with real-world data confirms its accuracy, while a case study and
extensive user study highlight its effectiveness in enhancing call-taking
performance.

</details>

### [150] [An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989)
*Marie Davidsen Buhl,Jacob Pfau,Benjamin Hilton,Geoffrey Irving*

Main category: cs.AI

TLDR: 论文探讨了通过辩论机制解决AI安全性问题，提出了一种基于辩论的AI对齐方法，以确保AI系统的诚实性。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力接近或超越人类，人类难以有效监督其行为，需寻找新方法确保AI行为符合期望。

Method: 采用辩论机制训练AI系统，结合探索保证和在线训练，确保系统诚实性。

Result: 提出了基于辩论的AI对齐安全案例，并列出四个关键假设以支持其有效性。

Conclusion: 辩论机制有望成为确保AI安全性的有效方法，但仍需解决一些开放研究问题。

Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it
may become difficult for humans to efficiently judge their actions -- making it
hard to use human feedback to steer them towards desirable traits. One proposed
solution is to leverage another superhuman system to point out flaws in the
system's outputs via a debate. This paper outlines the value of debate for AI
safety, as well as the assumptions and further research required to make debate
work. It does so by sketching an ``alignment safety case'' -- an argument that
an AI system will not autonomously take actions which could lead to egregious
harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D
agent inside an AI company sabotaging research, for example by producing false
results. To prevent this, the agent is trained via debate, subject to
exploration guarantees, to teach the system to be honest. Honesty is maintained
throughout deployment via online training. The safety case rests on four key
claims: (1) the agent has become good at the debate game, (2) good performance
in the debate game implies that the system is mostly honest, (3) the system
will not become significantly less honest during deployment, and (4) the
deployment context is tolerant of some errors. We identify open research
problems that, if solved, could render this a compelling argument that an AI
system is safe.

</details>

### [151] [Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest](https://arxiv.org/abs/2505.04019)
*Matteo Ceschin,Leonardo Arrighi,Luca Longo,Sylvio Barbon Junior*

Main category: cs.AI

TLDR: 论文提出了一种新的可解释AI方法（XAI），通过决策谓词图（DPG）提升隔离森林（iForest）的全局可解释性，解决了其决策边界不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中，不仅需要解释预测模型，还需理解数据预处理方法的影响。隔离森林（iForest）在异常检测中表现优异，但其复杂的树结构导致决策边界难以解释。

Method: 基于决策谓词图（DPG）设计了一种全局解释方法，提出Inlier-Outlier Propagation Score（IOP-Score），用于解释样本被识别为异常值的过程。

Result: 该方法增强了iForest的可解释性，提供了决策过程的全面视图，明确了特征在异常识别中的作用。

Conclusion: 研究通过提供决策边界和特征使用的全局解释，推动了完全可解释的机器学习流程的发展。

Abstract: The need to explain predictive models is well-established in modern machine
learning. However, beyond model interpretability, understanding pre-processing
methods is equally essential. Understanding how data modifications impact model
performance improvements and potential biases and promoting a reliable pipeline
is mandatory for developing robust machine learning solutions. Isolation Forest
(iForest) is a widely used technique for outlier detection that performs well.
Its effectiveness increases with the number of tree-based learners. However,
this also complicates the explanation of outlier selection and the decision
boundaries for inliers. This research introduces a novel Explainable AI (XAI)
method, tackling the problem of global explainability. In detail, it aims to
offer a global explanation for outlier detection to address its opaque nature.
Our approach is based on the Decision Predicate Graph (DPG), which clarifies
the logic of ensemble methods and provides both insights and a graph-based
metric to explain how samples are identified as outliers using the proposed
Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's
explainability and provides a comprehensive view of the decision-making
process, detailing which features contribute to outlier identification and how
the model utilizes them. This method advances the state-of-the-art by providing
insights into decision boundaries and a comprehensive view of holistic feature
usage in outlier identification. -- thus promoting a fully explainable machine
learning pipeline.

</details>

### [152] [Polynomial-Time Relational Probabilistic Inference in Open Universes](https://arxiv.org/abs/2505.04115)
*Luise Ge,Brendan Juba,Kris Nilsson*

Main category: cs.AI

TLDR: 提出一种基于人类推理启发的一阶关系概率推理方法，支持混合变量，并在多项式时间内实现有限度片段和有限量词秩知识库的推理。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中不确定性推理的表达能力与计算可处理性之间的矛盾。

Method: 扩展期望的平方和逻辑至关系设置，证明在有限度片段和有限量词秩知识库中可实现多项式时间推理。

Result: 在证明论框架下实现最紧边界，并证明固定度数的平方和反驳的完备性。

Conclusion: 该方法在表达能力和计算效率之间取得了平衡，适用于混合变量和未知或无限对象集。

Abstract: Reasoning under uncertainty is a fundamental challenge in Artificial
Intelligence. As with most of these challenges, there is a harsh dilemma
between the expressive power of the language used, and the tractability of the
computational problem posed by reasoning. Inspired by human reasoning, we
introduce a method of first-order relational probabilistic inference that
satisfies both criteria, and can handle hybrid (discrete and continuous)
variables. Specifically, we extend sum-of-squares logic of expectation to
relational settings, demonstrating that lifted reasoning in the bounded-degree
fragment for knowledge bases of bounded quantifier rank can be performed in
polynomial time, even with an a priori unknown and/or countably infinite set of
objects. Crucially, our notion of tractability is framed in proof-theoretic
terms, which extends beyond the syntactic properties of the language or
queries. We are able to derive the tightest bounds provable by proofs of a
given degree and size and establish completeness in our sum-of-squares
refutations for fixed degrees.

</details>

### [153] [Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning](https://arxiv.org/abs/2505.04310)
*Simo Alami C.,Rim Kaddah,Jesse Read,Marie-Paule Cani*

Main category: cs.AI

TLDR: 提出一种基于归一化流的分布强化学习新架构，支持灵活无界的回报分布建模，优于固定表示方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有分布强化学习方法在回报分布建模上的局限性，如固定支持、多模态捕捉不足及梯度偏差问题。

Method: 使用归一化流建模回报分布，提出几何感知的Cramér距离替代损失函数，避免CDF计算。

Result: 在ATARI-5子基准测试中表现优于基于PDF的模型，与基于分位数的方法竞争。

Conclusion: 新方法在参数效率和建模能力上优于现有技术，为分布强化学习提供了更优解决方案。

Abstract: We introduce a new architecture for Distributional Reinforcement Learning
(DistRL) that models return distributions using normalizing flows. This
approach enables flexible, unbounded support for return distributions, in
contrast to categorical approaches like C51 that rely on fixed or bounded
representations. It also offers richer modeling capacity to capture
multi-modality, skewness, and tail behavior than quantile based approaches. Our
method is significantly more parameter-efficient than categorical approaches.
Standard metrics used to train existing models like KL divergence or
Wasserstein distance either are scale insensitive or have biased sample
gradients, especially when return supports do not overlap. To address this, we
propose a novel surrogate for the Cram\`er distance, that is geometry-aware and
computable directly from the return distribution's PDF, avoiding the costly CDF
computation. We test our model on the ATARI-5 sub-benchmark and show that our
approach outperforms PDF based models while remaining competitive with quantile
based methods.

</details>

### [154] [KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning](https://arxiv.org/abs/2505.04313)
*Stephen Richard Varey,Alessandro Di Stefano,The Anh Han*

Main category: cs.AI

TLDR: KERAIA是一个新颖的符号知识工程框架，旨在解决动态、复杂和上下文敏感环境中的知识表示、推理和执行问题。


<details>
  <summary>Details</summary>
Motivation: 将非结构化的人类专业知识转化为AI系统可计算的高效算法。

Method: 基于Minsky的框架推理和K-lines，引入动态知识聚合、上下文敏感继承、可追溯推理和自适应知识转换等创新。

Result: 通过多个案例验证了KERAIA的通用性、表达能力和实用性，并与现有知识表示范式进行了对比。

Conclusion: KERAIA突破了传统静态知识表示的限制，以可解释AI为核心，提供了透明和可解释的解决方案。

Abstract: In this paper, we introduce KERAIA, a novel framework and software platform
for symbolic knowledge engineering designed to address the persistent
challenges of representing, reasoning with, and executing knowledge in dynamic,
complex, and context-sensitive environments. The central research question that
motivates this work is: How can unstructured, often tacit, human expertise be
effectively transformed into computationally tractable algorithms that AI
systems can efficiently utilise? KERAIA seeks to bridge this gap by building on
foundational concepts such as Minsky's frame-based reasoning and K-lines, while
introducing significant innovations. These include Clouds of Knowledge for
dynamic aggregation, Dynamic Relations (DRels) for context-sensitive
inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and
Cloud Elaboration for adaptive knowledge transformation. This approach moves
beyond the limitations of traditional, often static, knowledge representation
paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle,
ensuring transparency and interpretability, particularly through the use of
LoTs. The paper details the framework's architecture, the KSYNTH representation
language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse
inference methods within a unified structure. We validate KERAIA's versatility,
expressiveness, and practical applicability through detailed analysis of
multiple case studies spanning naval warfare simulation, industrial diagnostics
in water treatment plants, and strategic decision-making in the game of RISK.
Furthermore, we provide a comparative analysis against established knowledge
representation paradigms (including ontologies, rule-based systems, and
knowledge graphs) and discuss the implementation aspects and computational
considerations of the KERAIA platform.

</details>

### [155] [Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning](https://arxiv.org/abs/2505.04317)
*Ruize Zhang,Sirui Xiang,Zelai Xu,Feng Gao,Shilong Ji,Wenhao Tang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TLDR: 论文提出了一种分层强化学习框架HCSP，用于解决3v3多无人机排球任务，通过分层训练实现了高性能和团队协作行为。


<details>
  <summary>Details</summary>
Motivation: 解决多无人机排球任务中高层次的战略协调与低层次的敏捷控制问题，克服长时依赖、紧密的智能体耦合和无人机动力学挑战。

Method: 采用分层强化学习框架HCSP，分三阶段训练：低层技能训练、高层战略自博弈学习、联合微调。

Result: HCSP表现优异，平均胜率达82.9%，并涌现出角色切换和队形协调等团队行为。

Conclusion: HCSP的分层设计和训练方案有效解决了多无人机排球任务中的复杂挑战。

Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9\% win rate and a 71.5\% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme.

</details>

### [156] [Uncertain Machine Ethics Planning](https://arxiv.org/abs/2505.04352)
*Simon Kolker,Louise A. Dennis,Ramon Fraga Pereira,Mengwei Xu*

Main category: cs.AI

TLDR: 论文提出了一种基于多道德马尔可夫决策过程的启发式算法，用于在不确定性下进行伦理决策。


<details>
  <summary>Details</summary>
Motivation: 机器伦理决策需要考虑不确定性，并基于长期结果评估不同道德理论的潜在冲突。

Method: 将问题形式化为多道德马尔可夫决策过程，并开发基于多目标AO*的启发式算法，结合Sven-Ove Hansson的假设回顾程序。

Result: 通过一个机器伦理案例（是否偷胰岛素）验证了方法的有效性。

Conclusion: 该方法能够平衡不同道德理论的冲突，为机器伦理决策提供了实用框架。

Abstract: Machine Ethics decisions should consider the implications of uncertainty over
decisions. Decisions should be made over sequences of actions to reach
preferable outcomes long term. The evaluation of outcomes, however, may invoke
one or more moral theories, which might have conflicting judgements. Each
theory will require differing representations of the ethical situation. For
example, Utilitarianism measures numerical values, Deontology analyses duties,
and Virtue Ethics emphasises moral character. While balancing potentially
conflicting moral considerations, decisions may need to be made, for example,
to achieve morally neutral goals with minimal costs. In this paper, we
formalise the problem as a Multi-Moral Markov Decision Process and a
Multi-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm
based on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical
Retrospection procedure for ethical reasoning under uncertainty. Our approach
is validated by a case study from Machine Ethics literature: the problem of
whether to steal insulin for someone who needs it.

</details>

### [157] [TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2505.04480)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.AI

TLDR: TrajEvo利用大型语言模型（LLMs）和进化算法自动设计轨迹预测启发式方法，优于传统启发式和深度学习方法，尤其在泛化能力上表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法准确性不足，深度学习方法存在计算成本高、可解释性差和泛化能力有限的问题，亟需一种快速、可解释且泛化能力强的轨迹预测方法。

Method: TrajEvo结合LLMs和进化算法，通过跨代精英采样和统计反馈循环生成和优化启发式方法。

Result: 在ETH-UCY数据集上优于传统启发式方法，在未见过的SDD数据集上显著优于启发式和深度学习方法。

Conclusion: TrajEvo为自动化设计快速、可解释且泛化能力强的轨迹预测启发式方法迈出了第一步，代码已开源。

Abstract: Trajectory prediction is a crucial task in modeling human behavior,
especially in fields as social robotics and autonomous vehicle navigation.
Traditional heuristics based on handcrafted rules often lack accuracy, while
recently proposed deep learning approaches suffer from computational cost, lack
of explainability, and generalization issues that limit their practical
adoption. In this paper, we introduce TrajEvo, a framework that leverages Large
Language Models (LLMs) to automatically design trajectory prediction
heuristics. TrajEvo employs an evolutionary algorithm to generate and refine
prediction heuristics from past trajectory data. We introduce a
Cross-Generation Elite Sampling to promote population diversity and a
Statistics Feedback Loop allowing the LLM to analyze alternative predictions.
Our evaluations show TrajEvo outperforms previous heuristic methods on the
ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning
methods when generalizing to the unseen SDD dataset. TrajEvo represents a first
step toward automated design of fast, explainable, and generalizable trajectory
prediction heuristics. We make our source code publicly available to foster
future research at https://github.com/ai4co/trajevo.

</details>

### [158] [On some improvements to Unbounded Minimax](https://arxiv.org/abs/2505.04525)
*Quentin Cohen-Solal,Tristan Cazenave*

Main category: cs.AI

TLDR: 本文首次实验评估了四种未测试的Unbounded Best-First Minimax算法改进，包括转置表、后向传播策略、启发式函数替换和完成技术，发现针对性改进能提升算法效率。


<details>
  <summary>Details</summary>
Motivation: 评估四种未测试的算法改进对Unbounded Best-First Minimax性能的影响。

Method: 实验比较了转置表、后向传播策略（Korf & Chickering与Cohen-Solal变体）、启发式函数替换和完成技术的效果。

Result: 转置表和完成技术提升性能；启发式函数在成本高时有益，但低成本时降低性能；Cohen-Solal变体在值绑定或转置表时表现略优。

Conclusion: 针对性改进能显著提升Unbounded Best-First Minimax算法的效率。

Abstract: This paper presents the first experimental evaluation of four previously
untested modifications of Unbounded Best-First Minimax algorithm. This
algorithm explores the game tree by iteratively expanding the most promising
sequences of actions based on the current partial game tree. We first evaluate
the use of transposition tables, which convert the game tree into a directed
acyclic graph by merging duplicate states. Second, we compare the original
algorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which
differs in its backpropagation strategy: instead of stopping when a stable
value is encountered, it updates values up to the root. This change slightly
improves performance when value ties or transposition tables are involved.
Third, we assess replacing the exact terminal evaluation function with the
learned heuristic function. While beneficial when exact evaluations are costly,
this modification reduces performance in inexpensive settings. Finally, we
examine the impact of the completion technique that prioritizes resolved
winning states and avoids resolved losing states. This technique also improves
performance. Overall, our findings highlight how targeted modifications can
enhance the efficiency of Unbounded Best-First Minimax.

</details>

### [159] [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)
*Qi Liu,Xinhao Zheng,Renqiu Xia,Xingzhi Qi,Qinxiang Cao,Junchi Yan*

Main category: cs.AI

TLDR: 论文提出了一种基于确定性马尔可夫决策过程的问题解决形式化框架FPS，并衍生出D-FPS，通过形式化定理证明环境实现过程验证。构建了三个基准测试，并提出RPE方法评估答案正确性。


<details>
  <summary>Details</summary>
Motivation: 问题解决在科学与工程中至关重要，但缺乏通用且具体的定义。随着AI问题解决代理的发展，对过程级可验证性的需求增加，但研究不足。

Method: 提出FPS框架，利用形式化定理证明环境进行过程验证；衍生D-FPS框架，分离解决与验证步骤以更好对齐人类需求。

Result: 框架的表达性、完备性和可靠性得到证明。在三个基准测试中，现有模型表现有限（最高23.77%解决率）。

Conclusion: FPS和D-FPS为问题解决提供了形式化、可验证的框架，但现有模型仍需改进。

Abstract: As a seemingly self-explanatory task, problem-solving has been a significant
component of science and engineering. However, a general yet concrete
formulation of problem-solving itself is missing. With the recent development
of AI-based problem-solving agents, the demand for process-level verifiability
is rapidly increasing yet underexplored. To fill these gaps, we present a
principled formulation of problem-solving as a deterministic Markov decision
process; a novel framework, FPS (Formal Problem-Solving), which utilizes
existing FTP (formal theorem proving) environments to perform process-verified
problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer
verification for better human-alignment. The expressiveness, soundness and
completeness of the frameworks are proven. We construct three benchmarks on
problem-solving: FormalMath500, a formalization of a subset of the MATH500
benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP
benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and
human-aligned evaluation, we propose RPE (Restricted Propositional
Equivalence), a symbolic approach to determine the correctness of answers by
formal verification. We evaluate four prevalent FTP models and two prompting
methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of
MiniF2F-Solving, and 0.31% of PutnamBench-Solving.

</details>

### [160] [Qualitative Analysis of $ω$-Regular Objectives on Robust MDPs](https://arxiv.org/abs/2505.04539)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Kafshdar Goharshady,Mehrdad Karrabi,Ali Shafiee*

Main category: cs.AI

TLDR: 本文研究了鲁棒马尔可夫决策过程（RMDPs）中可达性和奇偶性目标的定性分析问题，提出了高效的算法并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: RMDPs通过考虑转移概率的不确定性扩展了经典MDPs，但现有研究通常假设结构限制（如单链或非周期性）。本文旨在解决无结构假设下的定性分析问题。

Method: 提出了基于不确定性集合的oracle访问的高效算法，用于解决可达性和奇偶性目标的定性问题。

Result: 实验结果表明，所提出的oracle方法在文献中的经典RMDP示例上具有高效性，可扩展到数千个状态。

Conclusion: 本文为无结构假设的RMDPs提供了有效的定性分析解决方案，并通过实验验证了其实际应用价值。

Abstract: Robust Markov Decision Processes (RMDPs) generalize classical MDPs that
consider uncertainties in transition probabilities by defining a set of
possible transition functions. An objective is a set of runs (or infinite
trajectories) of the RMDP, and the value for an objective is the maximal
probability that the agent can guarantee against the adversarial environment.
We consider (a) reachability objectives, where given a target set of states,
the goal is to eventually arrive at one of them; and (b) parity objectives,
which are a canonical representation for $\omega$-regular objectives. The
qualitative analysis problem asks whether the objective can be ensured with
probability 1.
  In this work, we study the qualitative problem for reachability and parity
objectives on RMDPs without making any assumption over the structures of the
RMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first
present efficient algorithms with oracle access to uncertainty sets that solve
qualitative problems of reachability and parity objectives. We then report
experimental results demonstrating the effectiveness of our oracle-based
approach on classical RMDP examples from the literature scaling up to thousands
of states.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [161] [Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models](https://arxiv.org/abs/2505.03821)
*Gracjan Góral,Alicja Ziarko,Piotr Miłoś,Michał Nauman,Maciej Wołczyk,Michał Kosiński*

Main category: cs.CV

TLDR: 研究探讨了视觉语言模型（VLMs）在视觉视角任务中的表现，发现其在场景理解上表现优异，但在空间推理和视角任务中表现较差。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在视觉视角任务中的能力，填补其在复杂视觉任务中的不足。

Method: 通过系统变化空间配置（如物体位置和人物朝向），创建144个视觉任务，并设计7个诊断问题评估三个认知层次。

Result: 模型在场景理解上表现良好，但在空间推理和视角任务中表现显著下降。

Conclusion: 未来VLM开发需整合几何表示和针对性训练，以提升复杂视觉任务能力。

Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual
perspective taking using a novel set of visual tasks inspired by established
human tests. Our approach leverages carefully controlled scenes, in which a
single humanoid minifigure is paired with a single object. By systematically
varying spatial configurations - such as object position relative to the
humanoid minifigure and the humanoid minifigure's orientation - and using both
bird's-eye and surface-level views, we created 144 unique visual tasks. Each
visual task is paired with a series of 7 diagnostic questions designed to
assess three levels of visual cognition: scene understanding, spatial
reasoning, and visual perspective taking. Our evaluation of several
state-of-the-art models, including GPT-4-Turbo, GPT-4o,
Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that
while they excel in scene understanding, the performance declines significantly
on spatial reasoning and further deteriorates on perspective-taking. Our
analysis suggests a gap between surface-level object recognition and the deeper
spatial and perspective reasoning required for complex visual tasks, pointing
to the need for integrating explicit geometric representations and tailored
training protocols in future VLM development.

</details>

### [162] [In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry](https://arxiv.org/abs/2505.03826)
*Minji Kang,Seongho Kim,Eunseo Go,Donghyeon Paek,Geon Lim,Muyoung Kim,Soyeun Kim,Sung Kyu Jang,Min Sup Choi,Woo Seok Kang,Jaehyun Kim,Jaekwang Kim,Hyeong-U Kim*

Main category: cs.CV

TLDR: 论文提出了一种基于机器学习的非接触式原位蚀刻深度预测框架，解决了传统方法的延迟和污染问题，并通过神经网络和贝叶斯神经网络验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中蚀刻深度和绝缘材料厚度的精确监测对设备性能和良率至关重要，但传统方法存在延迟和污染风险。

Method: 使用人工神经网络（ANN）和贝叶斯神经网络（BNN）预测蚀刻深度，并结合数字图像比色法（DIC）数据。

Result: ANN的均方误差显著低于线性模型，BNN能可靠估计不确定性；DIC数据输入也表现出色。

Conclusion: 结合DIC和机器学习为等离子蚀刻过程提供了一种实时、原位、非侵入的监测方案，提升了工艺稳定性和制造效率。

Abstract: Precise monitoring of etch depth and the thickness of insulating materials,
such as Silicon dioxide and silicon nitride, is critical to ensuring device
performance and yield in semiconductor manufacturing. While conventional
ex-situ analysis methods are accurate, they are constrained by time delays and
contamination risks. To address these limitations, this study proposes a
non-contact, in-situ etch depth prediction framework based on machine learning
(ML) techniques. Two scenarios are explored. In the first scenario, an
artificial neural network (ANN) is trained to predict average etch depth from
process parameters, achieving a significantly lower mean squared error (MSE)
compared to a linear baseline model. The approach is then extended to
incorporate variability from repeated measurements using a Bayesian Neural
Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage
analysis confirms the BNN's capability to provide reliable uncertainty
estimates. In the second scenario, we demonstrate the feasibility of using RGB
data from digital image colorimetry (DIC) as input for etch depth prediction,
achieving strong performance even in the absence of explicit process
parameters. These results suggest that the integration of DIC and ML offers a
viable, cost-effective alternative for real-time, in-situ, and non-invasive
monitoring in plasma etching processes, contributing to enhanced process
stability, and manufacturing efficiency.

</details>

### [163] [VideoLLM Benchmarks and Evaluation: A Survey](https://arxiv.org/abs/2505.03829)
*Yogesh Kumar*

Main category: cs.CV

TLDR: 本文综述了视频大语言模型（VideoLLMs）的评测基准和方法，分析了现有基准的特点、局限性和性能趋势，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，视频理解技术取得显著进展，但缺乏对VideoLLMs评测的系统性分析。本文旨在填补这一空白。

Method: 通过分析现有视频理解基准的特点、评测协议和局限性，以及不同评测方法（如封闭集、开放集和时空任务评测），总结VideoLLMs的性能趋势。

Result: 揭示了当前评测框架的关键挑战，并提出了改进方向，如设计更多样化、多模态和可解释性的基准。

Conclusion: 本文为研究者提供了系统化的评测指南，并指出了推动视频理解领域发展的潜在方向。

Abstract: The rapid development of Large Language Models (LLMs) has catalyzed
significant advancements in video understanding technologies. This survey
provides a comprehensive analysis of benchmarks and evaluation methodologies
specifically designed or used for Video Large Language Models (VideoLLMs). We
examine the current landscape of video understanding benchmarks, discussing
their characteristics, evaluation protocols, and limitations. The paper
analyzes various evaluation methodologies, including closed-set, open-set, and
specialized evaluations for temporal and spatiotemporal understanding tasks. We
highlight the performance trends of state-of-the-art VideoLLMs across these
benchmarks and identify key challenges in current evaluation frameworks.
Additionally, we propose future research directions to enhance benchmark
design, evaluation metrics, and protocols, including the need for more diverse,
multimodal, and interpretability-focused benchmarks. This survey aims to equip
researchers with a structured understanding of how to effectively evaluate
VideoLLMs and identify promising avenues for advancing the field of video
understanding with large language models.

</details>

### [164] [Video Forgery Detection for Surveillance Cameras: A Review](https://arxiv.org/abs/2505.03832)
*Noor B. Tayfor,Tarik A. Rashid,Shko M. Qader,Bryar A. Hassan,Mohammed H. Abdalla,Jafar Majidpour,Aram M. Ahmed,Hussein M. Ali,Aso M. Aladdin,Abdulhady A. Abdullah,Ahmed S. Shamsaldin,Haval M. Sidqi,Abdulrahman Salih,Zaher M. Yaseen,Azad A. Ameen,Janmenjoy Nayak,Mahmood Yashar Hamza*

Main category: cs.CV

TLDR: 本文综述了用于检测视频伪造的现有法证技术，强调其在验证监控录像真实性方面的有效性，并指出需要更强大的技术以应对日益复杂的伪造手段。


<details>
  <summary>Details</summary>
Motivation: 随着视频编辑工具的普及，监控录像的篡改风险增加，可能影响司法公正，因此确保视频真实性至关重要。

Method: 研究了多种技术，包括基于压缩的分析、帧复制检测和机器学习方法。

Result: 现有技术在检测伪造方面有一定效果，但需进一步发展以应对新型伪造手段。

Conclusion: 加强视频法证能力是确保监控录像可信并作为法律证据的关键。

Abstract: The widespread availability of video recording through smartphones and
digital devices has made video-based evidence more accessible than ever.
Surveillance footage plays a crucial role in security, law enforcement, and
judicial processes. However, with the rise of advanced video editing tools,
tampering with digital recordings has become increasingly easy, raising
concerns about their authenticity. Ensuring the integrity of surveillance
videos is essential, as manipulated footage can lead to misinformation and
undermine judicial decisions. This paper provides a comprehensive review of
existing forensic techniques used to detect video forgery, focusing on their
effectiveness in verifying the authenticity of surveillance recordings. Various
methods, including compression-based analysis, frame duplication detection, and
machine learning-based approaches, are explored. The findings highlight the
growing necessity for more robust forensic techniques to counteract evolving
forgery methods. Strengthening video forensic capabilities will ensure that
surveillance recordings remain credible and admissible as legal evidence.

</details>

### [165] [PointExplainer: Towards Transparent Parkinson's Disease Diagnosis](https://arxiv.org/abs/2505.03833)
*Xuechao Wang,Sven Nomm,Junqing Huang,Kadri Medijainen,Aaro Toomela,Michael Ruzhansky*

Main category: cs.CV

TLDR: PointExplainer是一种可解释的诊断策略，用于识别手绘区域对模型诊断的驱动作用，通过离散归因值量化其贡献，同时保持诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有诊断方法缺乏明确的可解释性，影响了临床信任，因此需要一种可解释的诊断策略。

Method: PointExplainer包括诊断模块（将手绘信号编码为3D点云）和解释模块（训练可解释代理模型近似黑盒模型行为），并引入一致性度量确保解释的忠实性。

Result: 在两个基准数据集和新构建的数据集上，PointExplainer提供了直观的解释且未降低诊断性能。

Conclusion: PointExplainer能够提供直观且忠实的手绘信号解释，有助于提升临床信任。

Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn
signals for early diagnosis of Parkinson's disease. However, the lack of clear
interpretability in existing diagnostic methods presents a challenge to
clinical trust. In this paper, we propose PointExplainer, an explainable
diagnostic strategy to identify hand-drawn regions that drive model diagnosis.
Specifically, PointExplainer assigns discrete attribution values to hand-drawn
segments, explicitly quantifying their relative contributions to the model's
decision. Its key components include: (i) a diagnosis module, which encodes
hand-drawn signals into 3D point clouds to represent hand-drawn trajectories,
and (ii) an explanation module, which trains an interpretable surrogate model
to approximate the local behavior of the black-box diagnostic model. We also
introduce consistency measures to further address the issue of faithfulness in
explanations. Extensive experiments on two benchmark datasets and a newly
constructed dataset show that PointExplainer can provide intuitive explanations
with no diagnostic performance degradation. The source code is available at
https://github.com/chaoxuewang/PointExplainer.

</details>

### [166] [Explainable Face Recognition via Improved Localization](https://arxiv.org/abs/2505.03837)
*Rashik Shadman,Daqing Hou,Faraz Hussain,M G Sarwar Murshed*

Main category: cs.CV

TLDR: 论文提出了一种基于Scaled Directed Divergence (SDD)的可解释人脸识别方法，通过精细定位相关面部特征，提高深度学习模型的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的人脸识别系统缺乏解释性，导致用户对其决策缺乏信任。

Method: 使用SDD类激活映射技术，精细定位与模型决策相关的面部特征。

Result: SDD CAM比传统CAM更精确地突出相关面部特征。

Conclusion: 该方法通过提供视觉解释，增强了深度学习人脸识别系统的透明度和用户信任。

Abstract: Biometric authentication has become one of the most widely used tools in the
current technological era to authenticate users and to distinguish between
genuine users and imposters. Face is the most common form of biometric modality
that has proven effective. Deep learning-based face recognition systems are now
commonly used across different domains. However, these systems usually operate
like black-box models that do not provide necessary explanations or
justifications for their decisions. This is a major disadvantage because users
cannot trust such artificial intelligence-based biometric systems and may not
feel comfortable using them when clear explanations or justifications are not
provided. This paper addresses this problem by applying an efficient method for
explainable face recognition systems. We use a Class Activation Mapping
(CAM)-based discriminative localization (very narrow/specific localization)
technique called Scaled Directed Divergence (SDD) to visually explain the
results of deep learning-based face recognition systems. We perform fine
localization of the face features relevant to the deep learning model for its
prediction/decision. Our experiments show that the SDD Class Activation Map
(CAM) highlights the relevant face features very specifically compared to the
traditional CAM and very accurately. The provided visual explanations with
narrow localization of relevant features can ensure much-needed transparency
and trust for deep learning-based face recognition systems.

</details>

### [167] [GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation](https://arxiv.org/abs/2505.03846)
*Kangsheng Wang,Yuhang Li,Chengwei Ye,Yufei Lin,Huanzhen Zhang,Bohan Hu,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TLDR: GAME是一种图增强多模态编码器，用于从短视频中预测人格特质，通过融合视觉、听觉和文本特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 短视频中的人格分析因多模态特征的复杂交互而具有挑战性，需要一种鲁棒的方法来建模和融合这些特征。

Method: 提出GAME模型，结合图卷积网络和CNN的双分支网络处理视觉特征，使用BiGRU和注意力机制捕捉时序动态，音频和文本特征分别通过VGGish和XLM-Roberta提取，并通过通道注意力融合模块整合多模态特征。

Result: 实验表明GAME在多个基准测试中显著优于现有方法，验证了其有效性和泛化能力。

Conclusion: GAME通过创新的多模态特征融合方法，为短视频中的人格分析提供了高效且鲁棒的解决方案。

Abstract: Apparent personality analysis from short videos poses significant chal-lenges
due to the complex interplay of visual, auditory, and textual cues. In this
paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to
robustly model and fuse multi-source features for automatic personality
prediction. For the visual stream, we construct a facial graph and introduce a
dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks
(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to
capture both structural and appearance-based facial cues. Complementing this,
global context and iden-tity features are extracted using pretrained ResNet18
and VGGFace back-bones. To capture temporal dynamics, frame-level features are
processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio
representations are derived from the VGGish network, and linguistic se-mantics
are captured via the XLM-Roberta transformer. To achieve effective multimodal
integration, we propose a Channel Attention-based Fusion module, followed by a
Multi-Layer Perceptron (MLP) regression head for predicting personality traits.
Extensive experiments show that GAME con-sistently outperforms existing methods
across multiple benchmarks, vali-dating its effectiveness and generalizability.

</details>

### [168] [Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques](https://arxiv.org/abs/2505.03848)
*Janhavi Giri,Attila Lengyel,Don Kent,Edward Kibardin*

Main category: cs.CV

TLDR: 论文提出了一种结合深度拓扑数据分析（TDA）、自监督学习和迁移学习的先进聚类框架，用于半导体制造中的图像数据聚类。


<details>
  <summary>Details</summary>
Motivation: 半导体制造产生大量图像数据，传统聚类方法难以处理高维无标签数据，限制了其捕获细微模式的能力。

Method: 框架整合了TDA、自监督学习和迁移学习，TDA捕获拓扑特征，自监督学习从无标签数据中提取表征，迁移学习增强适应性和可扩展性。

Result: 在合成和开源半导体图像数据集上验证，框架成功识别出与缺陷模式和工艺变化相关的聚类。

Conclusion: 该研究展示了TDA、自监督学习和迁移学习结合的潜力，为半导体制造等领域的大规模图像数据提供了可扩展的解决方案。

Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for
defect identification and yield optimization, yet often exceeds manual
inspection capabilities. Traditional clustering techniques struggle with
high-dimensional, unlabeled data, limiting their effectiveness in capturing
nuanced patterns. This paper introduces an advanced clustering framework that
integrates deep Topological Data Analysis (TDA) with self-supervised and
transfer learning techniques, offering a novel approach to unsupervised image
clustering. TDA captures intrinsic topological features, while self-supervised
learning extracts meaningful representations from unlabeled data, reducing
reliance on labeled datasets. Transfer learning enhances the framework's
adaptability and scalability, allowing fine-tuning to new datasets without
retraining from scratch. Validated on synthetic and open-source semiconductor
image datasets, the framework successfully identifies clusters aligned with
defect patterns and process variations. This study highlights the
transformative potential of combining TDA, self-supervised learning, and
transfer learning, providing a scalable solution for proactive process
monitoring and quality control in semiconductor manufacturing and other domains
with large-scale image datasets.

</details>

### [169] [An Active Inference Model of Covert and Overt Visual Attention](https://arxiv.org/abs/2505.03856)
*Tin Mišić,Karlo Koledić,Fabio Bonsignorio,Ivan Petrović,Ivan Marković*

Main category: cs.CV

TLDR: 论文提出了一种基于主动推理的视觉注意力模型，通过动态优化感官精度来最小化自由能，研究了外源性和内源性注意力的交互作用，并验证了模型在Posner提示任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究选择性注意力的机制，以帮助处理复杂的高维感官输入，并探索外源性和内源性注意力的交互作用。

Method: 采用主动推理框架，动态优化感官精度，结合环境信念和感官输入分配注意力，并在Posner提示任务和2D视觉数据中测试模型。

Result: 外源性和有效提示通常导致更快的反应时间，模型表现出类似抑制返回的行为，且反射性眼动比意图性眼动更快但适应性较差。

Conclusion: 模型成功模拟了注意力的动态分配机制，揭示了外源性和内源性注意力的差异及其在行为中的表现。

Abstract: The ability to selectively attend to relevant stimuli while filtering out
distractions is essential for agents that process complex, high-dimensional
sensory input. This paper introduces a model of covert and overt visual
attention through the framework of active inference, utilizing dynamic
optimization of sensory precisions to minimize free-energy. The model
determines visual sensory precisions based on both current environmental
beliefs and sensory input, influencing attentional allocation in both covert
and overt modalities. To test the effectiveness of the model, we analyze its
behavior in the Posner cueing task and a simple target focus task using
two-dimensional(2D) visual data. Reaction times are measured to investigate the
interplay between exogenous and endogenous attention, as well as valid and
invalid cueing. The results show that exogenous and valid cues generally lead
to faster reaction times compared to endogenous and invalid cues. Furthermore,
the model exhibits behavior similar to inhibition of return, where previously
attended locations become suppressed after a specific cue-target onset
asynchrony interval. Lastly, we investigate different aspects of overt
attention and show that involuntary, reflexive saccades occur faster than
intentional ones, but at the expense of adaptability.

</details>

### [170] [Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation](https://arxiv.org/abs/2505.03896)
*Shuang Zeng,Chee Hong Lee,Micky C Nnamdi,Wenqi Shi,J Ben Tamo,Lei Zhu,Hangzhou He,Xinliang Zhang,Qian Chen,May D. Wang,Yanye Lu,Qiushi Ren*

Main category: cs.CV

TLDR: 提出了一种名为AttUKAN的新型注意力U形Kolmogorov-Arnold网络和标签引导的像素级对比损失，用于视网膜血管分割，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视网膜血管分割中未能充分利用编码器的细粒度特征表示，导致特征提取不足。

Method: 结合注意力门和Kolmogorov-Arnold网络，设计了标签引导的像素级对比损失，增强特征区分能力。

Result: 在多个数据集上取得最高F1和MIoU分数，优于11种现有网络。

Conclusion: AttUKAN在视网膜血管分割中表现优异，达到了最先进的性能。

Abstract: Retinal vessel segmentation is a vital early detection method for several
severe ocular diseases. Despite significant progress in retinal vessel
segmentation with the advancement of Neural Networks, there are still
challenges to overcome. Specifically, retinal vessel segmentation aims to
predict the class label for every pixel within a fundus image, with a primary
focus on intra-image discrimination, making it vital for models to extract more
discriminative features. Nevertheless, existing methods primarily focus on
minimizing the difference between the output from the decoder and the label,
but ignore fully using feature-level fine-grained representations from the
encoder. To address these issues, we propose a novel Attention U-shaped
Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided
Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we
implement Attention Gates into Kolmogorov-Arnold Networks to enhance model
sensitivity by suppressing irrelevant feature activations and model
interpretability by non-linear modeling of KAN blocks. Additionally, we also
design a novel Label-guided Pixel-wise Contrastive Loss to supervise our
proposed AttUKAN to extract more discriminative features by distinguishing
between foreground vessel-pixel pairs and background pairs. Experiments are
conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF
and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,
80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and
66.94% in the above datasets, which are the highest compared to 11 networks for
retinal vessel segmentation. Quantitative and qualitative results show that our
AttUKAN achieves state-of-the-art performance and outperforms existing retinal
vessel segmentation methods. Our code will be available at
https://github.com/stevezs315/AttUKAN.

</details>

### [171] [Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces](https://arxiv.org/abs/2505.03974)
*Nikhil M. Pawar,Jorge A. Prozzi,Feng Hong,Surya Sarat Chandra Congress*

Main category: cs.CV

TLDR: 提出了一种结合CNN和ESPCNN的框架，用于高效超分辨率处理基础设施图像，减少计算成本和误报。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率基础设施图像处理中的计算成本高和误报问题。

Method: 使用CNN分类正负损伤图像，再用轻量级ESPCNN对正损伤图像进行超分辨率处理。

Result: ESPCNN在超分辨率评估中优于双三次插值，减少了计算成本和误报。

Conclusion: 该框架能有效辅助高速公路机构进行损伤检测和资产管理。

Abstract: Recently, there has been an impetus for the application of cutting-edge data
collection platforms such as drones mounted with camera sensors for
infrastructure asset management. However, the sensor characteristics, proximity
to the structure, hard-to-reach access, and environmental conditions often
limit the resolution of the datasets. A few studies used super-resolution
techniques to address the problem of low-resolution images. Nevertheless, these
techniques were observed to increase computational cost and false alarms of
distress detection due to the consideration of all the infrastructure images
i.e., positive and negative distress classes. In order to address the
pre-processing of false alarm and achieve efficient super-resolution, this
study developed a framework consisting of convolutional neural network (CNN)
and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately
classified both the classes. ESPCNN, which is the lightweight super-resolution
technique, generated high-resolution infrastructure image of positive distress
obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the
evaluation metrics for super-resolution. Based on the performance metrics, the
combination of CNN and ESPCNN was observed to be effective in preprocessing the
infrastructure images with negative distress, reducing the computational cost
and false alarms in the next step of super-resolution. The visual inspection
showed that EPSCNN is able to capture crack propagation, complex geometry of
even minor cracks. The proposed framework is expected to help the highway
agencies in accurately performing distress detection and assist in efficient
asset management practices.

</details>

### [172] [Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges](https://arxiv.org/abs/2505.03991)
*Hao Xu,Arbind Agrahari Baniya,Sam Well,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.CV

TLDR: 该论文综述了视频事件检测在体育分析中的应用，重点介绍了TAL、AS和PES三种任务，并分析了现有数据集、评估方法和先进技术。


<details>
  <summary>Details</summary>
Motivation: 视频事件检测对体育分析至关重要，能够自动化识别关键时刻，提升分析效率、观众参与度和转播效果。

Method: 综述了三种关键任务（TAL、AS、PES）的方法论演变，包括多模态方法、自监督学习和知识蒸馏等技术。

Result: 总结了现有数据集和评估指标，分析了先进技术的优缺点，并提出了未来研究方向。

Conclusion: 该研究为未来开发更通用、高效和鲁棒的体育事件检测框架奠定了基础。

Abstract: Video event detection has become an essential component of sports analytics,
enabling automated identification of key moments and enhancing performance
analysis, viewer engagement, and broadcast efficiency. Recent advancements in
deep learning, particularly Convolutional Neural Networks (CNNs) and
Transformers, have significantly improved accuracy and efficiency in Temporal
Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting
(PES). This survey provides a comprehensive overview of these three key tasks,
emphasizing their differences, applications, and the evolution of
methodological approaches. We thoroughly review and categorize existing
datasets and evaluation metrics specifically tailored for sports contexts,
highlighting the strengths and limitations of each. Furthermore, we analyze
state-of-the-art techniques, including multi-modal approaches that integrate
audio and visual information, methods utilizing self-supervised learning and
knowledge distillation, and approaches aimed at generalizing across multiple
sports. Finally, we discuss critical open challenges and outline promising
research directions toward developing more generalized, efficient, and robust
event detection frameworks applicable to diverse sports. This survey serves as
a foundation for future research on efficient, generalizable, and multi-modal
sports event detection.

</details>

### [173] [The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics](https://arxiv.org/abs/2505.04006)
*Inamullah,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TLDR: 视网膜成像技术与AI结合，为眼部和全身疾病提供非侵入性标记和早期干预。


<details>
  <summary>Details</summary>
Motivation: 利用视网膜血管化结构的独特性，作为人类健康的窗口，推动疾病早期检测和干预。

Method: 综述视网膜成像技术的演变及AI驱动的分析方法，探讨从传统技术到眼组学的转变。

Result: 眼组学在眼科和全身疾病中的应用潜力被揭示，同时指出研究中的挑战和未来方向。

Conclusion: 眼组学为疾病监测和干预提供了新途径，但仍需解决技术障碍和研究空白。

Abstract: The unique vascularized anatomy of the human eye, encased in the retina,
provides an opportunity to act as a window for human health. The retinal
structure assists in assessing the early detection, monitoring of disease
progression and intervention for both ocular and non-ocular diseases. The
advancement in imaging technology leveraging Artificial Intelligence has seized
this opportunity to bridge the gap between the eye and human health. This track
paves the way for unveiling systemic health insight from the ocular system and
surrogating non-invasive markers for timely intervention and identification.
The new frontiers of oculomics in ophthalmology cover both ocular and systemic
diseases, and getting more attention to explore them. In this survey paper, we
explore the evolution of retinal imaging techniques, the dire need for the
integration of AI-driven analysis, and the shift of retinal imaging from
classical techniques to oculomics. We also discuss some hurdles that may be
faced in the progression of oculomics, highlighting the research gaps and
future directions.

</details>

### [174] [FoodTrack: Estimating Handheld Food Portions with Egocentric Video](https://arxiv.org/abs/2505.04055)
*Ervin Wang,Yuhao Chen*

Main category: cs.CV

TLDR: FoodTrack框架通过第一视角视频直接测量手持食物的体积，克服了传统方法的局限性，提高了食物消费跟踪的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统食物消费跟踪方法依赖特定摄像头角度、非遮挡图像或手势识别，且对咬合大小有固定假设，限制了准确性和适应性。

Method: 提出FoodTrack框架，利用第一视角视频直接估计食物体积，无需依赖手势或固定咬合大小假设。

Result: 在手持食物对象上实现了约7.01%的绝对百分比误差，优于之前方法在最佳情况下的16.40%误差。

Conclusion: FoodTrack提供了一种更准确、适应性更强的食物消费跟踪解决方案。

Abstract: Accurately tracking food consumption is crucial for nutrition and health
monitoring. Traditional approaches typically require specific camera angles,
non-occluded images, or rely on gesture recognition to estimate intake, making
assumptions about bite size rather than directly measuring food volume. We
propose the FoodTrack framework for tracking and measuring the volume of
hand-held food items using egocentric video which is robust to hand occlusions
and flexible with varying camera and object poses. FoodTrack estimates food
volume directly, without relying on intake gestures or fixed assumptions about
bite size, offering a more accurate and adaptable solution for tracking food
consumption. We achieve absolute percentage loss of approximately 7.01% on a
handheld food object, improving upon a previous approach that achieved a 16.40%
mean absolute percentage error in its best case, under less flexible
conditions.

</details>

### [175] [AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding](https://arxiv.org/abs/2505.04058)
*Feng Xiao,Hongbin Xu,Guocan Zhao,Wenxiong Kang*

Main category: cs.CV

TLDR: 提出了一种2D辅助的3D视觉定位框架，通过构建语义-空间场景图和双分支视觉编码器，提升多模态对象编码和关系感知能力。


<details>
  <summary>Details</summary>
Motivation: 解决3D与语言模态之间的显著差距，特别是在区分多个相似对象时，现有方法忽略了参考对象的感知。

Method: 采用2D预训练属性指导多模态对象编码，结合图注意力机制实现跨模态信息融合。

Result: 在流行基准测试中表现优于现有方法，尤其在处理多个相似干扰物时效果显著。

Conclusion: 通过增强对象表示和迭代关系学习，实现了3D视觉与描述之间的有效对齐。

Abstract: 3D visual grounding aims to localize the unique target described by natural
languages in 3D scenes. The significant gap between 3D and language modalities
makes it a notable challenge to distinguish multiple similar objects through
the described spatial relationships. Current methods attempt to achieve
cross-modal understanding in complex scenes via a target-centered learning
mechanism, ignoring the perception of referred objects. We propose a novel
2D-assisted 3D visual grounding framework that constructs semantic-spatial
scene graphs with referred object discrimination for relationship perception.
The framework incorporates a dual-branch visual encoder that utilizes 2D
pre-trained attributes to guide the multi-modal object encoding. Furthermore,
our cross-modal interaction module uses graph attention to facilitate
relationship-oriented information fusion. The enhanced object representation
and iterative relational learning enable the model to establish effective
alignment between 3D vision and referential descriptions. Experimental results
on the popular benchmarks demonstrate our superior performance compared to
state-of-the-art methods, especially in addressing the challenges of multiple
similar distractors.

</details>

### [176] [SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation](https://arxiv.org/abs/2505.04087)
*Zixuan Hu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TLDR: 论文提出了一种名为SEVA的新方法，通过单步集成虚拟增强技术，在不增加计算负担的情况下提升测试时适应（TTA）的效率。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法通常依赖基于熵的无监督训练，但单轮训练无法充分利用可靠样本，限制了适应效率。

Method: SEVA通过理论框架探索多轮增强训练的影响，并优化熵损失的上界，将多轮训练效果集成到单步中。

Result: 实验表明，SEVA在多种网络架构和测试场景下表现出色，具有广泛的适应性。

Conclusion: SEVA通过高效损失和互补选择策略，显著提升了TTA的效率和性能。

Abstract: Test-Time adaptation (TTA) aims to enhance model robustness against
distribution shifts through rapid model adaptation during inference. While
existing TTA methods often rely on entropy-based unsupervised training and
achieve promising results, the common practice of a single round of entropy
training is typically unable to adequately utilize reliable samples, hindering
adaptation efficiency. In this paper, we discover augmentation strategies can
effectively unleash the potential of reliable samples, but the rapidly growing
computational cost impedes their real-time application. To address this
limitation, we propose a novel TTA approach named Single-step Ensemble of
Vicinal Augmentations (SEVA), which can take advantage of data augmentations
without increasing the computational burden. Specifically, instead of
explicitly utilizing the augmentation strategy to generate new data, SEVA
develops a theoretical framework to explore the impacts of multiple
augmentations on model adaptation and proposes to optimize an upper bound of
the entropy loss to integrate the effects of multiple rounds of augmentation
training into a single step. Furthermore, we discover and verify that using the
upper bound as the loss is more conducive to the selection mechanism, as it can
effectively filter out harmful samples that confuse the model. Combining these
two key advantages, the proposed efficient loss and a complementary selection
strategy can simultaneously boost the potential of reliable samples and meet
the stringent time requirements of TTA. The comprehensive experiments on
various network architectures across challenging testing scenarios demonstrate
impressive performances and the broad adaptability of SEVA. The code will be
publicly available.

</details>

### [177] [SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking](https://arxiv.org/abs/2505.04088)
*Shang Zhang,Huanbin Zhang,Dali Feng,Yujie Cui,Ruoyan Xiong,Cen He*

Main category: cs.CV

TLDR: 论文提出了一种新型的Siamese Motion Mamba Tracker (SMMT)，通过双向状态空间模型和自注意力机制解决TIR目标跟踪中的遮挡、运动模糊和背景杂乱问题。


<details>
  <summary>Details</summary>
Motivation: TIR目标跟踪常因目标遮挡、运动模糊和背景杂乱导致性能下降，需要一种更有效的方法来提升跟踪精度。

Method: 引入Motion Mamba模块到Siamese架构中，结合双向建模和自注意力机制提取运动特征；采用参数共享策略减少计算冗余；设计运动边缘感知回归损失提升跟踪精度。

Result: 在四个TIR跟踪基准测试中，SMMT表现出优越性能。

Conclusion: SMMT通过创新的模块设计和损失函数，显著提升了TIR目标跟踪的准确性和鲁棒性。

Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as
target occlusion, motion blur, and background clutter, which significantly
degrade the performance of trackers. To address these issues, this paper
pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a
bidirectional state-space model and a self-attention mechanism. Specifically,
we introduce the Motion Mamba module into the Siamese architecture to ex-tract
motion features and recover overlooked edge details using bidirectional
modeling and self-attention. We propose a Siamese parameter-sharing strate-gy
that allows certain convolutional layers to share weights. This approach
reduces computational redundancy while preserving strong feature
represen-tation. In addition, we design a motion edge-aware regression loss to
improve tracking accuracy, especially for motion-blurred targets. Extensive
experi-ments are conducted on four TIR tracking benchmarks, including
LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT
achieves superior performance in TIR target tracking.

</details>

### [178] [MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction](https://arxiv.org/abs/2505.04105)
*Andrew Zhang,Hao Wang,Shuchang Ye,Michael Fulham,Jinman Kim*

Main category: cs.CV

TLDR: 论文提出MAISY方法，通过动态学习空间模式和引入VS-SSIM损失，有效解决医学图像中运动伪影问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的方法虽能生成无运动伪影图像，但忽视局部特征且SSIM损失对像素强度变化处理不佳。

Method: MAISY结合Segment Anything Model动态学习空间模式，并引入VS-SSIM损失自适应强调高方差区域。

Result: 在胸部和头部CT数据上，PSNR提升40%，SSIM提升10%，Dice提升16%。

Conclusion: MAISY显著提升图像质量，尤其在保留关键解剖细节方面表现优异。

Abstract: Patient motion during medical image acquisition causes blurring, ghosting,
and distorts organs, which makes image interpretation challenging.Current
state-of-the-art algorithms using Generative Adversarial Network (GAN)-based
methods with their ability to learn the mappings between corrupted images and
their ground truth via Structural Similarity Index Measure (SSIM) loss
effectively generate motion-free images. However, we identified the following
limitations: (i) they mainly focus on global structural characteristics and
therefore overlook localized features that often carry critical pathological
information, and (ii) the SSIM loss function struggles to handle images with
varying pixel intensities, luminance factors, and variance. In this study, we
propose Motion-Aware Image SYnthesis (MAISY) which initially characterize
motion and then uses it for correction by: (a) leveraging the foundation model
Segment Anything Model (SAM), to dynamically learn spatial patterns along
anatomical boundaries where motion artifacts are most pronounced and, (b)
introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively
emphasizes spatial regions with high pixel variance to preserve essential
anatomical details during artifact correction. Experiments on chest and head CT
datasets demonstrate that our model outperformed the state-of-the-art
counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by
10%, and Dice by 16%.

</details>

### [179] [One2Any: One-Reference 6D Pose Estimation for Any Object](https://arxiv.org/abs/2505.04109)
*Mengya Liu,Siyuan Li,Ajad Chhatkuli,Prune Truong,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TLDR: One2Any是一种新方法，仅需单张参考-查询RGB-D图像即可估计6D物体姿态，无需3D模型、多视图数据或类别限制。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计方法依赖完整3D模型、多视图图像或特定类别训练，难以泛化到新物体。

Method: 将姿态估计视为编码-解码过程：通过单参考视图生成ROPE嵌入，再用U-Net解码生成新视图的ROC坐标。

Result: 在多个基准数据集上表现优异，泛化能力强，精度和鲁棒性达到SOTA，甚至优于需多视图或CAD输入的方法。

Conclusion: One2Any框架简单高效，支持大规模训练，适用于未知物体姿态估计。

Abstract: 6D object pose estimation remains challenging for many applications due to
dependencies on complete 3D models, multi-view images, or training limited to
specific object categories. These requirements make generalization to novel
objects difficult for which neither 3D models nor multi-view images may be
available. To address this, we propose a novel method One2Any that estimates
the relative 6-degrees of freedom (DOF) object pose using only a single
reference-single query RGB-D image, without prior knowledge of its 3D model,
multi-view data, or category constraints. We treat object pose estimation as an
encoding-decoding process, first, we obtain a comprehensive Reference Object
Pose Embedding (ROPE) that encodes an object shape, orientation, and texture
from a single reference view. Using this embedding, a U-Net-based pose decoding
module produces Reference Object Coordinate (ROC) for new views, enabling fast
and accurate pose estimation. This simple encoding-decoding framework allows
our model to be trained on any pair-wise pose data, enabling large-scale
training and demonstrating great scalability. Experiments on multiple benchmark
datasets demonstrate that our model generalizes well to novel objects,
achieving state-of-the-art accuracy and robustness even rivaling methods that
require multi-view or CAD inputs, at a fraction of compute.

</details>

### [180] [GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model](https://arxiv.org/abs/2505.04119)
*Zixiang Ai,Zichen Liu,Yuanhang Lei,Zhenyu Cui,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TLDR: 提出了一种几何感知的点云提示方法（GAPrompt），通过几何线索增强3D视觉模型的适应性，显著优于现有参数高效微调方法，且仅需少量可训练参数。


<details>
  <summary>Details</summary>
Motivation: 完全微调预训练的3D视觉模型计算和存储成本高，现有参数高效微调方法因难以捕捉点云的几何信息而性能不佳。

Method: 提出GAPrompt，包括点提示（Point Prompt）辅助输入、点位移提示器（Point Shift Prompter）提取全局形状信息，以及提示传播机制（Prompt Propagation）增强特征提取。

Result: GAPrompt在多个基准测试中显著优于现有方法，性能接近完全微调，仅需2.19%的可训练参数。

Conclusion: GAPrompt通过几何感知设计，有效提升了参数高效微调的性能，为3D视觉任务提供了高效解决方案。

Abstract: Pre-trained 3D vision models have gained significant attention for their
promising performance on point cloud data. However, fully fine-tuning these
models for downstream tasks is computationally expensive and storage-intensive.
Existing parameter-efficient fine-tuning (PEFT) approaches, which focus
primarily on input token prompting, struggle to achieve competitive performance
due to their limited ability to capture the geometric information inherent in
point clouds. To address this challenge, we propose a novel Geometry-Aware
Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the
adaptability of 3D vision models. First, we introduce a Point Prompt that
serves as an auxiliary input alongside the original point cloud, explicitly
guiding the model to capture fine-grained geometric details. Additionally, we
present a Point Shift Prompter designed to extract global shape information
from the point cloud, enabling instance-specific geometric adjustments at the
input level. Moreover, our proposed Prompt Propagation mechanism incorporates
the shape information into the model's feature extraction process, further
strengthening its ability to capture essential geometric characteristics.
Extensive experiments demonstrate that GAPrompt significantly outperforms
state-of-the-art PEFT methods and achieves competitive results compared to full
fine-tuning on various benchmarks, while utilizing only 2.19% of trainable
parameters. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>

### [181] [Vision Graph Prompting via Semantic Low-Rank Decomposition](https://arxiv.org/abs/2505.04121)
*Zixiang Ai,Zichen Liu,Jiahuan Zhou*

Main category: cs.CV

TLDR: ViG通过图结构表示图像，提出视觉图提示（VGP）框架，利用低秩语义特征提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法主要针对Transformer模型，忽略了图结构中的拓扑关系，限制了复杂语义建模能力。

Method: 提出语义低秩提示方法，分解低秩语义特征并与视觉图拓扑结合。

Result: 实验表明VGP显著提升ViG在下游任务中的性能，接近全微调效果且参数高效。

Conclusion: VGP为视觉图结构提供了一种高效的提示框架，平衡性能与参数效率。

Abstract: Vision GNN (ViG) demonstrates superior performance by representing images as
graph structures, providing a more natural way to capture irregular semantic
patterns beyond traditional grid or sequence-based representations. To
efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning
techniques like visual prompting become increasingly essential. However,
existing prompting methods are primarily designed for Transformer-based models,
neglecting the rich topological relationships among nodes and edges in
graph-based representations, limiting their capacity to model complex
semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel
framework tailored for vision graph structures. Our core insight reveals that
semantically connected components in the graph exhibit low-rank properties.
Building on this observation, we introduce a semantic low-rank prompting method
that decomposes low-rank semantic features and integrates them with prompts on
vision graph topologies, capturing both global structural patterns and
fine-grained semantic dependencies. Extensive experiments demonstrate our
method significantly improves ViG's transfer performance on diverse downstream
tasks, achieving results comparable to full fine-tuning while maintaining
parameter efficiency. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>

### [182] [R^3-VQA: "Read the Room" by Video Social Reasoning](https://arxiv.org/abs/2505.04147)
*Lixing Niu,Jiapeng Li,Xingping Yu,Shu Wang,Ruining Feng,Bo Wu,Ping Wei,Yisen Wang,Lifeng Fan*

Main category: cs.CV

TLDR: 论文提出了一个高质量视频数据集R^3-VQA，用于评估复杂社交场景中的社会推理能力，发现当前大型视觉语言模型（LVLMs）与人类水平仍有差距，但理论心智（ToM）提示能提升其表现。


<details>
  <summary>Details</summary>
Motivation: 现有社会推理任务和数据集过于简单，无法反映真实社交互动的复杂性，因此需要更全面的数据集和任务来评估社会推理能力。

Method: 构建了R^3-VQA数据集，包含精细标注的社交事件、心理状态及社交因果链，并设计了三个任务：社交事件理解、心理状态估计和社交因果推理。

Result: 实验表明，当前LVLMs在复杂社交场景中的推理能力远不及人类，但ToM提示能显著提升其表现。

Conclusion: R^3-VQA数据集填补了现有研究的空白，为评估和改进社会推理能力提供了重要工具，ToM提示是提升模型表现的有效方法。

Abstract: "Read the room" is a significant social reasoning capability in human daily
life. Humans can infer others' mental states from subtle social cues. Previous
social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic
interactions, incomplete mental state variables, single-step reasoning, etc.)
and fall far short of the challenges present in real-life social interactions.
In this paper, we contribute a valuable, high-quality, and comprehensive video
dataset named R^3-VQA with precise and fine-grained annotations of social
events and mental states (i.e., belief, intent, desire, and emotion) as well as
corresponding social causal chains in complex social scenarios. Moreover, we
include human-annotated and model-generated QAs. Our task R^3-VQA includes
three aspects: Social Event Understanding, Mental State Estimation, and Social
Causal Reasoning. As a benchmark, we comprehensively evaluate the social
reasoning capabilities and consistencies of current state-of-the-art large
vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs
are still far from human-level consistent social reasoning in complex social
scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on
social reasoning tasks. We provide some of our dataset and codes in
supplementary material and will release our full dataset and codes upon
acceptance.

</details>

### [183] [Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages](https://arxiv.org/abs/2505.04150)
*Yu Yamaoka or Weng Ian Chan,Shigeto Seno,Soichiro Fukada,Hideo Matsuda*

Main category: cs.CV

TLDR: 论文提出了一种名为OSLSP的弱监督学习方法，用于自动化评估肌肉组织再生过程，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专家视觉检查，缺乏定量和客观性；现有LLP方法无法适应肌肉组织特征且忽略类别顺序信息。

Method: 提出OSLSP方法，利用相似性比例损失和类别比例注意力机制，更新特征提取器并保留类别顺序信息。

Result: OSLSP模型在骨骼肌恢复阶段分类任务中优于大规模预训练和微调模型。

Conclusion: OSLSP为肌肉组织再生评估提供了一种自动化、定量且保留顺序信息的解决方案。

Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental
analysis in muscle research to measure experimental effect sizes and uncover
mechanisms behind muscle weakness due to aging and disease. The conventional
approach to assessing muscle tissue regeneration involves whole-slide imaging
and expert visual inspection of the recovery stages based on the morphological
information of cells and fibers. There is a need to replace these tasks with
automated methods incorporating machine learning techniques to ensure a
quantitative and objective analysis. Given the limited availability of fully
labeled data, a possible approach is Learning from Label Proportions (LLP), a
weakly supervised learning method using class label proportions. However,
current LLP methods have two limitations: (1) they cannot adapt the feature
extractor for muscle tissues, and (2) they treat the classes representing
recovery stages and cell morphological changes as nominal, resulting in the
loss of ordinal information. To address these issues, we propose Ordinal Scale
Learning from Similarity Proportion (OSLSP), which uses a similarity proportion
loss derived from two bag combinations. OSLSP can update the feature extractor
by using class proportion attention to the ordinal scale of the class. Our
model with OSLSP outperforms large-scale pre-trained and fine-tuning models in
classification tasks of skeletal muscle recovery stages.

</details>

### [184] [DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.04175)
*Naphat Nithisopa,Teerapong Panboonyuen*

Main category: cs.CV

TLDR: 本文提出了一种结合ResNet和Vision Transformer的端到端文本识别框架，通过Deformable Convolutions、Retrieval-Augmented Generation和CRF等方法提升了OCR性能，并在多个数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决自然图像中文本识别的挑战性问题，提升OCR性能。

Method: 结合ResNet和Vision Transformer，使用Deformable Convolutions、自适应dropout和CRF进行序列建模。

Result: 在六个基准数据集上取得显著精度，平均准确率为77.77%。

Conclusion: 该方法在多样化和挑战性数据集上表现出色，确立了新的最先进水平。

Abstract: Text recognition in natural images remains a challenging yet essential task,
with broad applications spanning computer vision and natural language
processing. This paper introduces a novel end-to-end framework that combines
ResNet and Vision Transformer backbones with advanced methodologies, including
Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random
Fields (CRF). These innovations collectively enhance feature representation and
improve Optical Character Recognition (OCR) performance. Specifically, the
framework substitutes standard convolution layers in the third and fourth
blocks with Deformable Convolutions, leverages adaptive dropout for
regularization, and incorporates CRF for more refined sequence modeling.
Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT,
IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving
notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on
IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy
of 77.77%. These results establish a new state-of-the-art for text recognition,
demonstrating the robustness of the approach across diverse and challenging
datasets.

</details>

### [185] [S3D: Sketch-Driven 3D Model Generation](https://arxiv.org/abs/2505.04185)
*Hail Song,Wonsik Shin,Naeun Lee,Soomin Chung,Nojun Kwak,Woontack Woo*

Main category: cs.CV

TLDR: S3D框架通过U-Net架构和风格对齐损失，将手绘草图转化为高质量3D模型。


<details>
  <summary>Details</summary>
Motivation: 解决2D草图因模糊和稀疏性导致3D建模困难的问题。

Method: 使用U-Net编码器-解码器生成面部分割掩码，结合风格对齐损失增强重建保真度。

Result: 成功生成高质量3D模型，支持多视角渲染。

Conclusion: S3D框架高效且鲁棒，代码已开源。

Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due
to the inherent ambiguity and sparsity of sketch data. In this paper, we
present S3D, a novel framework that converts simple hand-drawn sketches into
detailed 3D models. Our method utilizes a U-Net-based encoder-decoder
architecture to convert sketches into face segmentation masks, which are then
used to generate a 3D representation that can be rendered from novel views. To
ensure robust consistency between the sketch domain and the 3D output, we
introduce a novel style-alignment loss that aligns the U-Net bottleneck
features with the initial encoder outputs of the 3D generation module,
significantly enhancing reconstruction fidelity. To further enhance the
network's robustness, we apply augmentation techniques to the sketch dataset.
This streamlined framework demonstrates the effectiveness of S3D in generating
high-quality 3D models from sketch inputs. The source code for this project is
publicly available at https://github.com/hailsong/S3D.

</details>

### [186] [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
*Trinh T. L. Vuong,Jin Tae Kwak*

Main category: cs.CV

TLDR: VideoPath-LLaVA是首个结合单张图像、自动提取关键帧剪辑和手动分割视频的多模态模型，模拟病理学家的诊断过程。


<details>
  <summary>Details</summary>
Motivation: 通过整合多种图像场景，生成详细组织学描述和最终诊断，以弥合视觉叙事与诊断推理之间的差距。

Method: 利用VideoPath-Instruct数据集（4278个视频和诊断相关的链式思维指令对），通过知识迁移从单图像数据集训练，再微调手动分割视频。

Result: 在病理视频分析中设立新基准，为未来支持临床决策的AI系统奠定基础。

Conclusion: VideoPath-LLaVA为病理学诊断提供了集成视觉和诊断推理的潜力，代码和数据已公开。

Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in
computational pathology that integrates three distinct image scenarios, single
patch images, automatically keyframe-extracted clips, and manually segmented
video pathology images, to mimic the natural diagnostic process of
pathologists. By generating detailed histological descriptions and culminating
in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives
with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278
video and diagnosis-specific chain-of-thought instructional pairs sourced from
educational histopathology videos on YouTube. Although high-quality data is
critical for enhancing diagnostic reasoning, its creation is time-intensive and
limited in volume. To overcome this challenge, we transfer knowledge from
existing single-image instruction datasets to train on weakly annotated,
keyframe-extracted clips, followed by fine-tuning on manually segmented videos.
VideoPath-LLaVA establishes a new benchmark in pathology video analysis and
offers a promising foundation for future AI systems that support clinical
decision-making through integrated visual and diagnostic reasoning. Our code,
data, and model are publicly available at
https://github.com/trinhvg/VideoPath-LLaVA.

</details>

### [187] [SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios](https://arxiv.org/abs/2505.04201)
*Ning Cheng,Jinan Xu,Jialing Chen,Wenjuan Han*

Main category: cs.CV

TLDR: 论文提出SToLa框架，通过Mixture of Experts动态处理触觉与语言模态，解决模态差异和触觉数据稀缺问题，并构建了一个全面的触觉常识推理数据集。


<details>
  <summary>Details</summary>
Motivation: 探索触觉感知在智能系统中的挑战，特别是模态差异和触觉数据稀缺问题。

Method: 引入SToLa框架，利用Mixture of Experts动态统一触觉与语言模态。

Result: SToLa在PhysiCLeAR基准和自建数据集上表现优异，验证了MoE架构的有效性。

Conclusion: SToLa框架在开放场景触觉常识推理任务中具有性能优势。

Abstract: This paper explores the challenges of integrating tactile sensing into
intelligent systems for multimodal reasoning, particularly in enabling
commonsense reasoning about the open-ended physical world. We identify two key
challenges: modality discrepancy, where existing large touch-language models
often treat touch as a mere sub-modality of language, and open-ended tactile
data scarcity, where current datasets lack the diversity, open-endness and
complexity needed for reasoning. To overcome these challenges, we introduce
SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of
Experts (MoE) to dynamically process, unify, and manage tactile and language
modalities, capturing their unique characteristics. Crucially, we also present
a comprehensive tactile commonsense reasoning dataset and benchmark featuring
free-form questions and responses, 8 physical properties, 4 interactive
characteristics, and diverse commonsense knowledge. Experiments show SToLa
exhibits competitive performance compared to existing models on the PhysiCLeAR
benchmark and self-constructed datasets, proving the effectiveness of the
Mixture of Experts architecture in multimodal management and the performance
advantages for open-scenario tactile commonsense reasoning tasks.

</details>

### [188] [An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement](https://arxiv.org/abs/2505.04207)
*Mustafa Yurdakul,Şakir Tasdemir*

Main category: cs.CV

TLDR: 论文提出了一种基于改进YOLOv8的模型，用于坑洞检测及其物理特征分析，通过RGB-D数据集（PothRGBD）和结构优化，显著提升了检测精度和实时性。


<details>
  <summary>Details</summary>
Motivation: 坑洞导致车辆损坏和交通事故，现有方法仅基于2D RGB图像，无法准确分析坑洞物理特征，因此需要更精确的检测方法。

Method: 使用Intel RealSense D415深度相机收集RGB-D数据，构建PothRGBD数据集；改进YOLOv8n-seg模型，引入DSConv、SimAM和GELU模块。

Result: 改进模型的精度、召回率和mAP@50分别提升至93.7%、90.4%和93.8%，优于标准模型。

Conclusion: 该模型轻量高效，适用于实时智能交通解决方案，能准确检测坑洞并测量其周长和深度。

Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety
and economic problems. Therefore, early and accurate detection of potholes is
crucial. Existing detection methods are usually only based on 2D RGB images and
cannot accurately analyze the physical characteristics of potholes. In this
paper, a publicly available dataset of RGB-D images (PothRGBD) is created and
an improved YOLOv8-based model is proposed for both pothole detection and
pothole physical features analysis. The Intel RealSense D415 depth camera was
used to collect RGB and depth data from the road surfaces, resulting in a
PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable
for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg
architecture, which is structurally improved with Dynamic Snake Convolution
(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit
(GELU). The proposed model segmented potholes with irregular edge structure
more accurately, and performed perimeter and depth measurements on depth maps
with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,
85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to
93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in
precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model
performs pothole detection as well as perimeter and depth measurement with high
accuracy and is suitable for real-time applications due to its low model
complexity. In this way, a lightweight and effective model that can be used in
deep learning-based intelligent transportation solutions has been acquired.

</details>

### [189] [CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models](https://arxiv.org/abs/2505.04214)
*Fabian Wolf,Oliver Tüselmann,Arthur Matei,Lukas Hennies,Christoph Rass,Gernot A. Fink*

Main category: cs.CV

TLDR: 本文提出了一种针对大视觉语言模型（LVLM）少样本能力评估的新数据集CM1，用于从手写文档中提取关键信息。实验表明，在训练样本较少时，LVLMs优于传统全页提取模型。


<details>
  <summary>Details</summary>
Motivation: 手写文档中关键信息的自动提取是文档分析的核心挑战，尤其在标注数据稀缺的情况下，LVLMs显示出潜力。

Method: 构建了CM1数据集，包含历史表单中的手写姓名和出生日期信息，并设计了三个基准任务，比较了两种LVLMs与传统全页提取模型的性能。

Result: 在少量训练样本下，LVLMs凭借其规模和预训练优势，表现优于传统方法。

Conclusion: LVLMs在少样本场景下具有显著优势，为手写文档信息提取提供了新思路。

Abstract: The automatic extraction of key-value information from handwritten documents
is a key challenge in document analysis. A reliable extraction is a
prerequisite for the mass digitization efforts of many archives. Large Vision
Language Models (LVLM) are a promising technology to tackle this problem
especially in scenarios where little annotated training data is available. In
this work, we present a novel dataset specifically designed to evaluate the
few-shot capabilities of LVLMs. The CM1 documents are a historic collection of
forms with handwritten entries created in Europe to administer the Care and
Maintenance program after World War Two. The dataset establishes three
benchmarks on extracting name and birthdate information and, furthermore,
considers different training set sizes. We provide baseline results for two
different LVLMs and compare performances to an established full-page extraction
model. While the traditional full-page model achieves highly competitive
performances, our experiments show that when only a few training samples are
available the considered LVLMs benefit from their size and heavy pretraining
and outperform the classical approach.

</details>

### [190] [A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation](https://arxiv.org/abs/2505.04229)
*Theophilus Aidoo,Till Koebe,Akansh Maurya,Hewan Shrestha,Ingmar Weber*

Main category: cs.CV

TLDR: 提出一种弱监督框架，利用3米分辨率卫星图像和粗粒度时间标签估计停车场占用率，减少对高分辨率图像的依赖。


<details>
  <summary>Details</summary>
Motivation: 高分辨率标记图像稀缺且昂贵，尤其在低收入地区，限制了遥感应用。

Method: 利用超市和五金店停车场在周六通常满、周日通常空的假设，训练成对比较模型。

Result: 模型在大型停车场上的AUC达到0.92。

Conclusion: 该方法可扩展用于城市流动性分析，并适用于评估弱势社区的交通模式和资源分配。

Abstract: The scarcity and high cost of labeled high-resolution imagery have long
challenged remote sensing applications, particularly in low-income regions
where high-resolution data are scarce. In this study, we propose a weak
supervision framework that estimates parking lot occupancy using 3m resolution
satellite imagery. By leveraging coarse temporal labels -- based on the
assumption that parking lots of major supermarkets and hardware stores in
Germany are typically full on Saturdays and empty on Sundays -- we train a
pairwise comparison model that achieves an AUC of 0.92 on large parking lots.
The proposed approach minimizes the reliance on expensive high-resolution
images and holds promise for scalable urban mobility analysis. Moreover, the
method can be adapted to assess transit patterns and resource allocation in
vulnerable communities, providing a data-driven basis to improve the well-being
of those most in need.

</details>

### [191] [Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting](https://arxiv.org/abs/2505.04262)
*Feng Yang,Wenliang Qian,Wangmeng Zuo,Hui Li*

Main category: cs.CV

TLDR: 提出了一种名为Coupled Score Distillation (CSD)的框架，通过耦合多视图联合分布先验，解决了Score Distillation Sampling (SDS)在文本到3D生成中的几何不一致和多面伪影问题。


<details>
  <summary>Details</summary>
Motivation: SDS在文本到3D生成中忽略了多视图相关性，导致几何不一致和多面伪影。

Method: 通过将优化问题重新表述为多视图联合优化问题，推导出有效的优化规则，并直接优化3D高斯喷洒（3D-GS）以生成几何一致的3D内容。

Result: 实验结果表明，该方法在效率和生成质量上具有竞争力。

Conclusion: CSD框架能够有效解决几何不一致问题，并生成高质量的3D内容。

Abstract: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.

</details>

### [192] [Object-Shot Enhanced Grounding Network for Egocentric Video](https://arxiv.org/abs/2505.04270)
*Yisen Feng,Haoyu Zhang,Meng Liu,Weili Guan,Liqiang Nie*

Main category: cs.CV

TLDR: OSGNet提出了一种针对自我中心视频的物体-镜头增强定位网络，通过提取物体信息和镜头运动特征，提升了视频表示和模态对齐能力，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注自我中心与外部中心视频的分布差异，但忽视了自我中心视频的关键特征和细粒度信息。

Method: 提出OSGNet，提取视频中的物体信息以丰富表示，并分析镜头运动特征以捕捉穿戴者的注意力信息。

Result: 在三个数据集上的实验表明，OSGNet达到了最先进的性能。

Conclusion: OSGNet通过结合物体和镜头运动特征，有效提升了自我中心视频定位任务的性能。

Abstract: Egocentric video grounding is a crucial task for embodied intelligence
applications, distinct from exocentric video moment localization. Existing
methods primarily focus on the distributional differences between egocentric
and exocentric videos but often neglect key characteristics of egocentric
videos and the fine-grained information emphasized by question-type queries. To
address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding
Network for egocentric video. Specifically, we extract object information from
videos to enrich video representation, particularly for objects highlighted in
the textual query but not directly captured in the video features.
Additionally, we analyze the frequent shot movements inherent to egocentric
videos, leveraging these features to extract the wearer's attention
information, which enhances the model's ability to perform modality alignment.
Experiments conducted on three datasets demonstrate that OSGNet achieves
state-of-the-art performance, validating the effectiveness of our approach. Our
code can be found at https://github.com/Yisen-Feng/OSGNet.

</details>

### [193] [HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation](https://arxiv.org/abs/2505.04276)
*Yajie Fu,Chaorui Huang,Junwei Li,Hui Kong,Yibin Tian,Huakang Li,Zhiyuan Zhang*

Main category: cs.CV

TLDR: HDiffTG是一种新颖的3D人体姿态估计方法，结合了Transformer、GCN和扩散模型，提升了准确性和鲁棒性，同时保持轻量化设计。


<details>
  <summary>Details</summary>
Motivation: 解决3D人体姿态估计中全局与局部特征平衡的问题，提升在遮挡和复杂场景下的性能。

Method: 集成Transformer（全局时空依赖）、GCN（局部骨骼结构）和扩散模型（逐步优化），并引入轻量化优化和目标函数设计。

Result: 在Human3.6M和MPI-INF-3DHP数据集上达到SOTA性能，尤其在MPI-INF-3DHP上表现优异，同时具备高计算效率和鲁棒性。

Conclusion: HDiffTG通过多技术融合实现了高性能和轻量化，适用于复杂和遮挡场景。

Abstract: We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that
integrates Transformer, Graph Convolutional Network (GCN), and diffusion model
into a unified framework. HDiffTG leverages the strengths of these techniques
to significantly improve pose estimation accuracy and robustness while
maintaining a lightweight design. The Transformer captures global
spatiotemporal dependencies, the GCN models local skeletal structures, and the
diffusion model provides step-by-step optimization for fine-tuning, achieving a
complementary balance between global and local features. This integration
enhances the model's ability to handle pose estimation under occlusions and in
complex scenarios. Furthermore, we introduce lightweight optimizations to the
integrated model and refine the objective function design to reduce
computational overhead without compromising performance. Evaluation results on
the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves
state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling
in both accuracy and computational efficiency. Additionally, the model exhibits
exceptional robustness in noisy and occluded environments. Source codes and
models are available at https://github.com/CirceJie/HDiffTG

</details>

### [194] [TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement](https://arxiv.org/abs/2505.04281)
*Yi Li,Zhiyuan Zhang,Jiangnan Xia,Jianghan Cheng,Qilong Wu,Junwei Li,Yibin Tian,Hui Kong*

Main category: cs.CV

TLDR: TS-Diff是一种新颖的两阶段扩散模型，用于增强极低光RAW图像。通过预训练和对齐阶段，结合虚拟相机和噪声空间，实现了高效的降噪和颜色一致性。


<details>
  <summary>Details</summary>
Motivation: 解决极低光条件下RAW图像的降噪和颜色一致性问题，并适应不同相机的噪声特性。

Method: 1. 预训练阶段：通过噪声空间构建虚拟相机生成噪声图像，并使用CFI模块学习通用特征。2. 对齐阶段：通过少量真实RAW数据微调目标特定的CFI$^T$，并引入结构重参数化技术。3. 引入颜色校正器解决颜色偏移问题。

Result: 在QID、SID和ELD等多个数据集上达到最先进性能，表现出优异的降噪、泛化和颜色一致性。

Conclusion: TS-Diff是一种鲁棒且多功能的解决方案，适用于低光成像应用。

Abstract: This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing
extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes
noisy images by constructing multiple virtual cameras based on a noise space.
Camera Feature Integration (CFI) modules are then designed to enable the model
to learn generalizable features across diverse virtual cameras. During the
aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is
fine-tuned using a small amount of real RAW data to adapt to the noise
characteristics of specific cameras. A structural reparameterization technique
further simplifies CFI$^T$ for efficient deployment. To address color shifts
during the diffusion process, a color corrector is introduced to ensure color
consistency by dynamically adjusting global color distributions. Additionally,
a novel dataset, QID, is constructed, featuring quantifiable illumination
levels and a wide dynamic range, providing a comprehensive benchmark for
training and evaluation under extreme low-light conditions. Experimental
results demonstrate that TS-Diff achieves state-of-the-art performance on
multiple datasets, including QID, SID, and ELD, excelling in denoising,
generalization, and color consistency across various cameras and illumination
levels. These findings highlight the robustness and versatility of TS-Diff,
making it a practical solution for low-light imaging applications. Source codes
and models are available at https://github.com/CircccleK/TS-Diff

</details>

### [195] [MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition](https://arxiv.org/abs/2505.04306)
*Qiannan Fan,Zhuoyang Li,Jitong Li,Chenyang Cao*

Main category: cs.CV

TLDR: 提出了一种基于扩散模型的专家混合方法（MoDE）用于遮挡人脸识别，通过身份门控网络自适应整合多重建人脸信息，提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前遮挡人脸识别算法缺乏对遮挡的先验知识，导致实际应用中性能不佳，影响日常生活便利性。

Method: 使用扩散模型生成多个可能的完整人脸图像，通过身份门控网络评估并整合各重建人脸的身份贡献。

Result: 在三个公开人脸数据集和两个真实场景数据集上验证了方法的优越性能。

Conclusion: MoDE是一种即插即用模块，显著提升了遮挡人脸识别的效果。

Abstract: With the continuous impact of epidemics, people have become accustomed to
wearing masks. However, most current occluded face recognition (OFR) algorithms
lack prior knowledge of occlusions, resulting in poor performance when dealing
with occluded faces of varying types and severity in reality. Recognizing
occluded faces is still a significant challenge, which greatly affects the
convenience of people's daily lives. In this paper, we propose an
identity-gated mixture of diffusion experts (MoDE) for OFR. Each
diffusion-based generative expert estimates one possible complete image for
occluded faces. Considering the random sampling process of the diffusion model,
which introduces inevitable differences and variations between the inpainted
faces and the real ones. To ensemble effective information from
multi-reconstructed faces, we introduce an identity-gating network to evaluate
the contribution of each reconstructed face to the identity and adaptively
integrate the predictions in the decision space. Moreover, our MoDE is a
plug-and-play module for most existing face recognition models. Extensive
experiments on three public face datasets and two datasets in the wild validate
our advanced performance for various occlusions in comparison with the
competing methods.

</details>

### [196] [Multi-turn Consistent Image Editing](https://arxiv.org/abs/2505.04320)
*Zijun Zhou,Yingying Deng,Xiangyu He,Weiming Dong,Fan Tang*

Main category: cs.CV

TLDR: 提出了一种多轮图像编辑框架，通过迭代优化提升编辑效果，解决了现有单步编辑方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法多为单步操作，难以处理模糊用户意图或复杂变换，导致结果不一致或不符合预期。

Method: 采用流匹配实现准确图像反转，结合双目标LQR稳定采样，并引入自适应注意力增强方法。

Result: 实验表明，该框架显著提高了编辑成功率和视觉保真度。

Conclusion: 多轮迭代编辑框架能有效解决单步编辑的局限性，提升用户体验。

Abstract: Many real-world applications, such as interactive photo retouching, artistic
content creation, and product design, require flexible and iterative image
editing. However, existing image editing methods primarily focus on achieving
the desired modifications in a single step, which often struggles with
ambiguous user intent, complex transformations, or the need for progressive
refinements. As a result, these methods frequently produce inconsistent
outcomes or fail to meet user expectations. To address these challenges, we
propose a multi-turn image editing framework that enables users to iteratively
refine their edits, progressively achieving more satisfactory results. Our
approach leverages flow matching for accurate image inversion and a
dual-objective Linear Quadratic Regulators (LQR) for stable sampling,
effectively mitigating error accumulation. Additionally, by analyzing the
layer-wise roles of transformers, we introduce a adaptive attention
highlighting method that enhances editability while preserving multi-turn
coherence. Extensive experiments demonstrate that our framework significantly
improves edit success rates and visual fidelity compared to existing methods.

</details>

### [197] [CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion](https://arxiv.org/abs/2505.04347)
*Yanyu Li,Pencheng Wan,Liang Han,Yaowei Wang,Liqiang Nie,Min Zhang*

Main category: cs.CV

TLDR: CountDiffusion是一个无需训练的方法，通过两阶段修正扩散模型的注意力图，提升文本到图像生成中物体数量的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在生成图像时难以准确控制物体数量的问题，避免高计算成本和抽象概念学习的挑战。

Method: 分为两阶段：首先生成中间去噪结果并计数，然后通过修正注意力图调整物体数量。

Result: 实验表明，CountDiffusion显著提升了文本到图像模型中物体数量的生成准确性。

Conclusion: CountDiffusion是一种高效且无需额外训练的方法，可广泛应用于扩散模型，提升物体数量生成的准确性。

Abstract: Stable Diffusion has advanced text-to-image synthesis, but training models to
generate images with accurate object quantity is still difficult due to the
high computational cost and the challenge of teaching models the abstract
concept of quantity. In this paper, we propose CountDiffusion, a training-free
framework aiming at generating images with correct object quantity from textual
descriptions. CountDiffusion consists of two stages. In the first stage, an
intermediate denoising result is generated by the diffusion model to predict
the final synthesized image with one-step denoising, and a counting model is
used to count the number of objects in this image. In the second stage, a
correction module is used to correct the object quantity by changing the
attention map of the object with universal guidance. The proposed
CountDiffusion can be plugged into any diffusion-based text-to-image (T2I)
generation models without further training. Experiment results demonstrate the
superiority of our proposed CountDiffusion, which improves the accurate object
quantity generation ability of T2I models by a large margin.

</details>

### [198] [WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing](https://arxiv.org/abs/2505.04369)
*Jie Sun,Heng Liu,Yongzhen Wang,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TLDR: 论文提出了一种基于小波变换的去雾框架WDMamba，通过低频恢复和细节增强两阶段处理，结合Mamba块和自引导对比正则化，显著提升了去雾效果。


<details>
  <summary>Details</summary>
Motivation: 通过小波变换分析发现雾霾信息主要存在于低频分量，因此提出一种新的去雾框架以更有效地恢复图像。

Method: WDMamba框架分为低频恢复（使用Mamba块）和细节增强两阶段，并引入自引导对比正则化优化训练。

Result: 在公开去雾基准测试中，WDMamba在质量和定量指标上均优于现有方法。

Conclusion: WDMamba通过两阶段处理和正则化策略，实现了高效且高质量的去雾效果。

Abstract: In this paper, we reveal a novel haze-specific wavelet degradation prior
observed through wavelet transform analysis, which shows that haze-related
information predominantly resides in low-frequency components. Exploiting this
insight, we propose a novel dehazing framework, WDMamba, which decomposes the
image dehazing task into two sequential stages: low-frequency restoration
followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to
effectively capture features specific to each stage of the dehazing process,
resulting in high-quality restored images. Specifically, in the low-frequency
restoration stage, we integrate Mamba blocks to reconstruct global structures
with linear complexity, efficiently removing overall haze and producing a
coarse restored image. Thereafter, the detail enhancement stage reinstates
fine-grained information that may have been overlooked during the previous
phase, culminating in the final dehazed output. Furthermore, to enhance detail
retention and achieve more natural dehazing, we introduce a self-guided
contrastive regularization during network training. By utilizing the coarse
restored output as a hard negative example, our model learns more
discriminative representations, substantially boosting the overall dehazing
performance. Extensive evaluations on public dehazing benchmarks demonstrate
that our method surpasses state-of-the-art approaches both qualitatively and
quantitatively. Code is available at https://github.com/SunJ000/WDMamba.

</details>

### [199] [Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise](https://arxiv.org/abs/2505.04375)
*Moseli Mots'oehli,Hope Mogale,Kyungim Baek*

Main category: cs.CV

TLDR: 研究了不同规模的视觉变换器（ViT和Swin Transformer）在标签噪声下的性能表现，发现较大的ViT模型（如ViTl32）在准确性和校准性上表现更优，而Swin Transformer的鲁棒性较弱。较小的补丁尺寸不一定更好，主动学习策略在中等噪声率下有效。


<details>
  <summary>Details</summary>
Motivation: 探索视觉变换器在资源受限和标签噪声环境下的实用性，为实际应用提供指导。

Method: 评估了四种ViT配置和三种Swin Transformer配置在CIFAR10和CIFAR100数据集上的表现，分析了不同标签噪声率下的分类准确性和校准性。

Result: 较大的ViT模型（如ViTl32）在噪声环境下表现更优，Swin Transformer鲁棒性较弱。主动学习策略在中等噪声率下有效，但校准性较差。

Conclusion: 研究结果为资源受限环境下部署视觉变换器提供了实用建议，强调了模型复杂性、标签噪声和计算效率的平衡。

Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for
downstream tasks is well-established. Still, the impact of model size on the
performance of vision transformers in similar scenarios, particularly under
label noise, remains largely unexplored. Given the utility and versatility of
transformer architectures, this study investigates their practicality under
low-budget constraints and noisy labels. We explore how classification accuracy
and calibration are affected by symmetric label noise in active learning
settings, evaluating four vision transformer configurations (Base and Large
with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations
(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label
noise rates. Our findings show that larger ViT models (ViTl32 in particular)
consistently outperform their smaller counterparts in both accuracy and
calibration, even under moderate to high label noise, while Swin Transformers
exhibit weaker robustness across all noise levels. We find that smaller patch
sizes do not always lead to better performance, as ViTl16 performs consistently
worse than ViTl32 while incurring a higher computational cost. We also find
that information-based Active Learning strategies only provide meaningful
accuracy improvements at moderate label noise rates, but they result in poorer
calibration compared to models trained on randomly acquired labels, especially
at high label noise rates. We hope these insights provide actionable guidance
for practitioners looking to deploy vision transformers in resource-constrained
environments, where balancing model complexity, label noise, and compute
efficiency is critical in model fine-tuning or distillation.

</details>

### [200] [Label-efficient Single Photon Images Classification via Active Learning](https://arxiv.org/abs/2505.04376)
*Zili Zhang,Ziting Wen,Yiheng Qiang,Hongzhou Dong,Wenle Dong,Xinyang Li,Xiaofan Wang,Xiaoqiang Ren*

Main category: cs.CV

TLDR: 本文提出了一种针对单光子图像分类的主动学习框架，通过成像条件感知的采样策略和合成增强技术，显著减少了标注样本需求，同时保持了高分类精度。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注从稀疏光子事件重建3D场景，而单光子图像的语义解释因高标注成本和低效标注策略而未被充分探索。

Method: 提出了一种成像条件感知的采样策略，结合合成增强技术，选择性地标注最具信息量的样本。

Result: 在合成数据上仅需1.5%标注样本即达到97%准确率；在真实数据上仅需8%标注样本即达到90.63%准确率，优于基线方法4.51%。

Conclusion: 主动学习使单光子图像分类性能达到经典图像水平，为单光子数据的大规模应用铺平了道路。

Abstract: Single-photon LiDAR achieves high-precision 3D imaging in extreme
environments through quantum-level photon detection technology. Current
research primarily focuses on reconstructing 3D scenes from sparse photon
events, whereas the semantic interpretation of single-photon images remains
underexplored, due to high annotation costs and inefficient labeling
strategies. This paper presents the first active learning framework for
single-photon image classification. The core contribution is an imaging
condition-aware sampling strategy that integrates synthetic augmentation to
model variability across imaging conditions. By identifying samples where the
model is both uncertain and sensitive to these conditions, the proposed method
selectively annotates only the most informative examples. Experiments on both
synthetic and real-world datasets show that our approach outperforms all
baselines and achieves high classification accuracy with significantly fewer
labeled samples. Specifically, our approach achieves 97% accuracy on synthetic
single-photon data using only 1.5% labeled samples. On real-world data, we
maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher
than the best-performing baseline. This illustrates that active learning
enables the same level of classification performance on single-photon images as
on classical images, opening doors to large-scale integration of single-photon
data in real-world applications.

</details>

### [201] [Tetrahedron-Net for Medical Image Registration](https://arxiv.org/abs/2505.04380)
*Jinhai Xiang,Shuai Guo,Qianru Han,Dantong Shi,Xinwei He,Xiang Bai*

Main category: cs.CV

TLDR: 论文提出了一种名为Tetrahedron-Net的新型医学图像配准架构，通过增加一个额外的解码器来增强特征表示，提升配准质量。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net类网络在单编码器-单解码器架构下未能充分利用特征交互，限制了配准性能的提升。

Method: 引入一个额外的解码器，与原始编码器和解码器交互，形成“Tetrahedron”结构，增强特征重用和协作。

Result: 在多个医学图像配准基准测试中表现优异，且能轻松集成到现有U-Net类网络中。

Conclusion: Tetrahedron-Net通过简洁的“Tetrahedron”设计显著提升了配准性能，具有通用性和易集成性。

Abstract: Medical image registration plays a vital role in medical image processing.
Extracting expressive representations for medical images is crucial for
improving the registration quality. One common practice for this end is
constructing a convolutional backbone to enable interactions with skip
connections among feature extraction layers. The de facto structure, U-Net-like
networks, has attempted to design skip connections such as nested or full-scale
ones to connect one single encoder and one single decoder to improve its
representation capacity. Despite being effective, it still does not fully
explore interactions with a single encoder and decoder architectures. In this
paper, we embrace this observation and introduce a simple yet effective
alternative strategy to enhance the representations for registrations by
appending one additional decoder. The new decoder is designed to interact with
both the original encoder and decoder. In this way, it not only reuses feature
presentation from corresponding layers in the encoder but also interacts with
the original decoder to corporately give more accurate registration results.
The new architecture is concise yet generalized, with only one encoder and two
decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.
Three instantiations of Tetrahedron-Net are further constructed regarding the
different structures of the appended decoder. Our extensive experiments prove
that superior performance can be obtained on several representative benchmarks
of medical image registration. Finally, such a ``Tetrahedron'' design can also
be easily integrated into popular U-Net-like architectures including
VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.

</details>

### [202] [DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution](https://arxiv.org/abs/2505.04384)
*Ming-Hui Liu,Xiao-Qian Liu,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TLDR: 论文提出了一种名为DATA的多解缠对比学习框架，用于提升开放世界半监督深度伪造溯源（OSS-DFA）任务中对新类别的泛化能力。通过定义‘正交深度伪造基’并设计增强记忆机制，DATA显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度伪造溯源方法过度依赖特定方法线索、忽略共同伪造特征，以及在开放世界场景中难以区分新类别的问题。

Method: 提出DATA框架，包括定义‘正交深度伪造基’以解缠特定方法特征，设计增强记忆机制用于新类别发现和对比学习，并引入基对比损失和中心对比损失。

Result: 在OSS-DFA基准测试中，DATA表现优于现有方法，准确率提升了2.55%和5.7%。

Conclusion: DATA通过解缠和对比学习显著提升了深度伪造溯源的泛化能力和性能，适用于开放世界场景。

Abstract: Deepfake attribution (DFA) aims to perform multiclassification on different
facial manipulation techniques, thereby mitigating the detrimental effects of
forgery content on the social order and personal reputations. However, previous
methods focus only on method-specific clues, which easily lead to overfitting,
while overlooking the crucial role of common forgery features. Additionally,
they struggle to distinguish between uncertain novel classes in more practical
open-world scenarios. To address these issues, in this paper we propose an
innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to
enhance the generalization ability on novel classes for the open-world
semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all
generation techniques can be abstracted into a similar architecture, DATA
defines the concept of 'Orthonormal Deepfake Basis' for the first time and
utilizes it to disentangle method-specific features, thereby reducing the
overfitting on forgery-irrelevant information. Furthermore, an augmented-memory
mechanism is designed to assist in novel class discovery and contrastive
learning, which aims to obtain clear class boundaries for the novel classes
through instance-level disentanglements. Additionally, to enhance the
standardization and discrimination of features, DATA uses bases contrastive
loss and center contrastive loss as auxiliaries for the aforementioned modules.
Extensive experimental evaluations show that DATA achieves state-of-the-art
performance on the OSS-DFA benchmark, e.g., there are notable accuracy
improvements in 2.55% / 5.7% under different settings, compared with the
existing methods.

</details>

### [203] [Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle](https://arxiv.org/abs/2505.04392)
*Petr Jahoda,Jan Cech*

Main category: cs.CV

TLDR: 提出了一种通过视觉跟踪前车来检测路面异常的新方法，适用于低能见度或密集交通场景，并能提前预测异常。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖直接视觉检测特定异常，而新方法通过跟踪前车动态预测多种异常，适应性强且适用于复杂场景。

Method: 利用相机跟踪前车，通过迭代鲁棒估计器补偿相机俯仰旋转，以处理弱信号和振动干扰。

Result: 实验表明，即使在复杂路况下，该方法能可靠地远距离检测路面异常，且实时性良好。

Conclusion: 该方法高效、实时，适用于自动驾驶或车辆底盘预配置。

Abstract: A novel approach to detect road surface anomalies by visual tracking of a
preceding vehicle is proposed. The method is versatile, predicting any kind of
road anomalies, such as potholes, bumps, debris, etc., unlike direct
observation methods that rely on training visual detectors of those cases. The
method operates in low visibility conditions or in dense traffic where the
anomaly is occluded by a preceding vehicle. Anomalies are detected
predictively, i.e., before a vehicle encounters them, which allows to
pre-configure low-level vehicle systems (such as chassis) or to plan an
avoidance maneuver in case of autonomous driving. A challenge is that the
signal coming from camera-based tracking of a preceding vehicle may be weak and
disturbed by camera ego motion due to vibrations affecting the ego vehicle.
Therefore, we propose an efficient method to compensate camera pitch rotation
by an iterative robust estimator. Our experiments on both controlled setup and
normal traffic conditions show that road anomalies can be detected reliably at
a distance even in challenging cases where the ego vehicle traverses imperfect
road surfaces. The method is effective and performs in real time on standard
consumer hardware.

</details>

### [204] [SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer](https://arxiv.org/abs/2505.04394)
*Young-Hu Park,Rae-Hong Park,Hyung-Min Park*

Main category: cs.CV

TLDR: 提出了一种基于Swin Transformer的高效视觉语音编码器SwinLip，用于唇读任务，显著降低了计算复杂度并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于ResNet的唇读方法计算复杂度高，不适合高效捕捉唇读特征，且在多模态任务中引入延迟。

Method: 采用Swin Transformer的层次结构和窗口自注意力机制，结合改进的Conformer时间嵌入和传统空间嵌入，构建轻量级SwinLip编码器。

Result: 在英语LRW和汉语LRW-1000数据集上表现优异，计算量更少，汉语LRW-1000上达到SOTA性能。

Conclusion: SwinLip显著提升了唇读网络的性能和推理速度，适用于多种任务，计算效率更高。

Abstract: This paper presents an efficient visual speech encoder for lip reading. While
most recent lip reading studies have been based on the ResNet architecture and
have achieved significant success, they are not sufficiently suitable for
efficiently capturing lip reading features due to high computational complexity
in modeling spatio-temporal information. Additionally, using a complex visual
model not only increases the complexity of lip reading models but also induces
delays in the overall network for multi-modal studies (e.g., audio-visual
speech recognition, speech enhancement, and speech separation). To overcome the
limitations of Convolutional Neural Network (CNN)-based models, we apply the
hierarchical structure and window self-attention of the Swin Transformer to lip
reading. We configure a new lightweight scale of the Swin Transformer suitable
for processing lip reading data and present the SwinLip visual speech encoder,
which efficiently reduces computational load by integrating modified
Convolution-augmented Transformer (Conformer) temporal embeddings with
conventional spatial embeddings in the hierarchical structure. Through
extensive experiments, we have validated that our SwinLip successfully improves
the performance and inference speed of the lip reading network when applied to
various backbones for word and sentence recognition, reducing computational
load. In particular, our SwinLip demonstrated robust performance in both
English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art
performance on the Mandarin LRW-1000 dataset with less computation compared to
the existing state-of-the-art model.

</details>

### [205] [Deep residual learning with product units](https://arxiv.org/abs/2505.04397)
*Ziyuan Li,Uwe Jaekel,Babette Dellen*

Main category: cs.CV

TLDR: PURe是一种结合乘积单元的深度残差网络，通过乘法特征交互提升表达能力和参数效率，在多个数据集上表现优于传统ResNet，且收敛更快、参数更少。


<details>
  <summary>Details</summary>
Motivation: 传统残差网络主要依赖加法神经元，限制了复杂模式的表达能力。PURe通过引入乘积单元，增强特征交互能力，提升模型性能。

Method: 在残差块的第二层用2D乘积单元替代传统卷积层，并移除非线性激活函数以保留结构信息。

Result: 在Galaxy10 DECaLS、ImageNet和CIFAR-10上，PURe均优于ResNet，准确率更高、收敛更快、参数更少，且对噪声更鲁棒。

Conclusion: PURe在准确性、效率和鲁棒性之间取得了良好平衡，展示了乘积单元架构在计算机视觉中的潜力。

Abstract: We propose a deep product-unit residual neural network (PURe) that integrates
product units into residual blocks to improve the expressiveness and parameter
efficiency of deep convolutional networks. Unlike standard summation neurons,
product units enable multiplicative feature interactions, potentially offering
a more powerful representation of complex patterns. PURe replaces conventional
convolutional layers with 2D product units in the second layer of each residual
block, eliminating nonlinear activation functions to preserve structural
information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,
PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper
ResNet152, while converging nearly five times faster and demonstrating strong
robustness to Poisson noise. On ImageNet, PURe architectures outperform
standard ResNet models at similar depths, with PURe34 achieving a top-1
accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet
variants (ResNet50, ResNet101) while utilizing significantly fewer parameters
and computational resources. On CIFAR-10, PURe consistently outperforms ResNet
variants across varying depths, with PURe272 reaching 95.01% test accuracy,
comparable to ResNet1001 but at less than half the model size. These results
demonstrate that PURe achieves a favorable balance between accuracy,
efficiency, and robustness. Compared to traditional residual networks, PURe not
only achieves competitive classification performance with faster convergence
and fewer parameters, but also demonstrates greater robustness to noise. Its
effectiveness across diverse datasets highlights the potential of
product-unit-based architectures for scalable and reliable deep learning in
computer vision.

</details>

### [206] [MFSeg: Efficient Multi-frame 3D Semantic Segmentation](https://arxiv.org/abs/2505.04408)
*Chengjie Huang,Krzysztof Czarnecki*

Main category: cs.CV

TLDR: MFSeg是一种高效的多帧3D语义分割框架，通过特征级点云序列聚合和正则化，降低计算开销并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决多帧3D语义分割中计算开销大和冗余点云上采样的问题。

Method: 采用特征级点云序列聚合和正则化，结合轻量级MLP点解码器。

Result: 在nuScenes和Waymo数据集上表现优于现有方法。

Conclusion: MFSeg在效率和准确性上均表现出色。

Abstract: We propose MFSeg, an efficient multi-frame 3D semantic segmentation
framework. By aggregating point cloud sequences at the feature level and
regularizing the feature extraction and aggregation process, MFSeg reduces
computational overhead while maintaining high accuracy. Moreover, by employing
a lightweight MLP-based point decoder, our method eliminates the need to
upsample redundant points from past frames. Experiments on the nuScenes and
Waymo datasets show that MFSeg outperforms existing methods, demonstrating its
effectiveness and efficiency.

</details>

### [207] [DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception](https://arxiv.org/abs/2505.04410)
*Junjie Wang,Bin Chen,Yulin Li,Bin Kang,Yichi Chen,Zhuotao Tian*

Main category: cs.CV

TLDR: DeCLIP通过解耦CLIP的自注意力模块，分别提取“内容”和“上下文”特征，提升了密集视觉预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在密集预测任务中表现不佳，因其局部特征表示能力有限。

Method: 提出DeCLIP框架，解耦自注意力模块，分别优化“内容”和“上下文”特征，结合DINO等模型增强空间一致性。

Result: DeCLIP在开放词汇密集预测任务（如目标检测和语义分割）中显著优于现有方法。

Conclusion: DeCLIP通过改进局部特征表示和空间一致性，有效提升了密集预测任务的性能。

Abstract: Dense visual prediction tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense prediction often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. The ``content'' features are aligned with image crop
representations to improve local discriminability, while ``context'' features
learn to retain the spatial correlations under the guidance of vision
foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP
significantly outperforms existing methods across multiple open-vocabulary
dense prediction tasks, including object detection and semantic segmentation.
Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.

</details>

### [208] [RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation](https://arxiv.org/abs/2505.04424)
*Jing Hu,Chengming Feng,Shu Hu,Ming-Ching Chang,Xin Li,Xi Wu,Xin Wang*

Main category: cs.CV

TLDR: 提出了一种基于强化学习的轻量级任意风格迁移框架RLMiniStyler，通过迭代优化和不确定性感知多任务学习策略，高效生成高质量多样化的风格化结果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在任意风格迁移中计算成本高，难以生成多样化结果。

Method: 采用强化学习策略迭代指导风格迁移过程，并结合不确定性感知多任务学习策略动态调整损失权重。

Result: 实验表明RLMiniStyler能以较低成本生成高质量、多样化的风格化图像序列，优于现有方法。

Conclusion: RLMiniStyler为轻量级高效风格迁移提供了新思路，代码已开源。

Abstract: Arbitrary style transfer aims to apply the style of any given artistic image
to another content image. Still, existing deep learning-based methods often
require significant computational costs to generate diverse stylized results.
Motivated by this, we propose a novel reinforcement learning-based framework
for arbitrary style transfer RLMiniStyler. This framework leverages a unified
reinforcement learning policy to iteratively guide the style transfer process
by exploring and exploiting stylization feedback, generating smooth sequences
of stylized results while achieving model lightweight. Furthermore, we
introduce an uncertainty-aware multi-task learning strategy that automatically
adjusts loss weights to adapt to the content and style balance requirements at
different training stages, thereby accelerating model convergence. Through a
series of experiments across image various resolutions, we have validated the
advantages of RLMiniStyler over other state-of-the-art methods in generating
high-quality, diverse artistic image sequences at a lower cost. Codes are
available at https://github.com/fengxiaoming520/RLMiniStyler.

</details>

### [209] [Learning Real Facial Concepts for Independent Deepfake Detection](https://arxiv.org/abs/2505.04460)
*Ming-Hui Liu,Harry Cheng,Tianyi Wang,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TLDR: 论文提出RealID方法，通过独立评估真实和伪造类别的概率，提升深度伪造检测模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测模型因过度依赖伪造痕迹和对真实人脸理解不足，导致在未见数据集上泛化能力差。

Method: RealID包含RealC2模块和IDC分类器，前者通过MultiReal Memory捕捉真实人脸的综合概念，后者独立决策。

Result: 在五个数据集上，RealID平均准确率提升1.74%，显著优于现有方法。

Conclusion: RealID通过独立决策和综合真实概念学习，有效缓解伪造无关模式的影响，提升泛化性能。

Abstract: Deepfake detection models often struggle with generalization to unseen
datasets, manifesting as misclassifying real instances as fake in target
domains. This is primarily due to an overreliance on forgery artifacts and a
limited understanding of real faces. To address this challenge, we propose a
novel approach RealID to enhance generalization by learning a comprehensive
concept of real faces while assessing the probabilities of belonging to the
real and fake classes independently. RealID comprises two key modules: the Real
Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier
(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various
prototypes for real faces, allowing the model to capture a comprehensive
concept of real class. Meanwhile, IDC redefines the classification strategy by
making independent decisions based on the concept of the real class and the
presence of forgery artifacts. Through the combined effect of the above
modules, the influence of forgery-irrelevant patterns is alleviated, and
extensive experiments on five widely used datasets demonstrate that RealID
significantly outperforms existing state-of-the-art methods, achieving a 1.74%
improvement in average accuracy.

</details>

### [210] [CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation](https://arxiv.org/abs/2505.04481)
*Jiahao Li,Weijian Ma,Xueyang Li,Yunzhong Lou,Guichun Zhou,Xiangdong Zhou*

Main category: cs.CV

TLDR: 该研究提出了一种名为CAD-Llama的框架，用于增强预训练大型语言模型（LLMs）生成参数化3D CAD模型的能力。


<details>
  <summary>Details</summary>
Motivation: 探索将LLMs的生成能力扩展到领域特定任务，如生成参数化CAD模型序列，以填补LLMs在3D结构知识上的空白。

Method: 开发了分层注释管道和类似代码的格式（SPCC），并采用自适应预训练和指令调优方法，以增强LLMs的空间知识。

Result: 实验结果表明，CAD-Llama在生成参数化3D CAD模型方面优于现有自回归方法和LLM基线。

Conclusion: 该框架为LLMs在领域特定任务中的应用提供了新思路，尤其是在3D建模领域。

Abstract: Recently, Large Language Models (LLMs) have achieved significant success,
prompting increased interest in expanding their generative capabilities beyond
general text into domain-specific areas. This study investigates the generation
of parametric sequences for computer-aided design (CAD) models using LLMs. This
endeavor represents an initial step towards creating parametric 3D shapes with
LLMs, as CAD model parameters directly correlate with shapes in
three-dimensional space. Despite the formidable generative capacities of LLMs,
this task remains challenging, as these models neither encounter parametric
sequences during their pretraining phase nor possess direct awareness of 3D
structures. To address this, we present CAD-Llama, a framework designed to
enhance pretrained LLMs for generating parametric 3D CAD models. Specifically,
we develop a hierarchical annotation pipeline and a code-like format to
translate parametric 3D CAD command sequences into Structured Parametric CAD
Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we
propose an adaptive pretraining approach utilizing SPCC, followed by an
instruction tuning process aligned with CAD-specific guidelines. This
methodology aims to equip LLMs with the spatial knowledge inherent in
parametric sequences. Experimental results demonstrate that our framework
significantly outperforms prior autoregressive methods and existing LLM
baselines.

</details>

### [211] [FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging](https://arxiv.org/abs/2505.04485)
*Ali Alawieh,Alexandru P. Condurache*

Main category: cs.CV

TLDR: FA-KPConv是一种基于KPConv的神经网络架构，通过帧平均技术实现点云网络的精确不变性和/或等变性，适用于点云分类和配准任务。


<details>
  <summary>Details</summary>
Motivation: KPConv在欧几里得变换下的不变性和/或等变性仅能通过大数据集或强数据增强近似实现，FA-KPConv旨在解决这一问题。

Method: 通过帧平均技术包装现有KPConv网络，嵌入几何先验知识，保持可学习参数数量且不损失输入信息。

Result: 在点云分类和配准任务中表现优异，尤其在训练数据稀缺或测试数据随机旋转的挑战性场景中。

Conclusion: FA-KPConv通过几何先验知识提升了KPConv的性能，适用于多种点云分析任务。

Abstract: We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural
network architecture built on top of the well-known KPConv, a widely adopted
backbone for 3D point cloud analysis. Even though invariance and/or
equivariance to Euclidean transformations are required for many common tasks,
KPConv-based networks can only approximately achieve such properties when
training on large datasets or with significant data augmentations. Using Frame
Averaging, we allow to flexibly customize point cloud neural networks built
with KPConv layers, by making them exactly invariant and/or equivariant to
translations, rotations and/or reflections of the input point clouds. By simply
wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical
prior knowledge into it while preserving the number of learnable parameters and
not compromising any input information. We showcase the benefit of such an
introduced bias for point cloud classification and point cloud registration,
especially in challenging cases such as scarce training data or randomly
rotated test data.

</details>

### [212] [Efficient Flow Matching using Latent Variables](https://arxiv.org/abs/2505.04486)
*Anirban Samaddar,Yixuan Sun,Viktor Nilsson,Sandeep Madireddy*

Main category: cs.CV

TLDR: Latent-CFM提出了一种简化训练/推理策略的流匹配模型，利用预训练的深度潜变量模型处理多模态数据结构，显著减少了训练时间和计算量，同时提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配模型在从简单源分布（如标准高斯分布）学习流时，未明确建模目标数据的底层结构/流形，导致学习效率低下，尤其是在高维数据集中。

Method: 提出Latent-CFM，利用预训练的深度潜变量模型简化训练和推理策略，以处理多模态数据结构。

Result: 实验表明，Latent-CFM在生成质量上优于现有流匹配模型，训练时间和计算量显著减少（某些情况下减少50%），并能生成更物理准确的样本。

Conclusion: Latent-CFM通过潜空间分析展示了其在条件图像生成中的潜力，为高效处理多模态数据提供了新思路。

Abstract: Flow matching models have shown great potential in image generation tasks
among probabilistic generative models. Building upon the ideas of continuous
normalizing flows, flow matching models generalize the transport path of the
diffusion models from a simple prior distribution to the data. Most flow
matching models in the literature do not explicitly model the underlying
structure/manifold in the target data when learning the flow from a simple
source distribution like the standard Gaussian. This leads to inefficient
learning, especially for many high-dimensional real-world datasets, which often
reside in a low-dimensional manifold. Existing strategies of incorporating
manifolds, including data with underlying multi-modal distribution, often
require expensive training and hence frequently lead to suboptimal performance.
To this end, we present \texttt{Latent-CFM}, which provides simplified
training/inference strategies to incorporate multi-modal data structures using
pretrained deep latent variable models. Through experiments on multi-modal
synthetic data and widely used image benchmark datasets, we show that
\texttt{Latent-CFM} exhibits improved generation quality with significantly
less training ($\sim 50\%$ less in some cases) and computation than
state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we
demonstrate that our approach generates more physically accurate samples than
competitive approaches. In addition, through latent space analysis, we
demonstrate that our approach can be used for conditional image generation
conditioned on latent features.

</details>

### [213] ["I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments](https://arxiv.org/abs/2505.04488)
*Ziyi Zhang,Zhen Sun,Zongmin Zhang,Zifan Peng,Yuemeng Zhao,Zichun Wang,Zeren Luo,Ruiting Zuo,Xinlei He*

Main category: cs.CV

TLDR: 论文评估了视频语言模型（VideoLLMs）在辅助视障人士日常活动中的有效性，构建了数据集VisAssistDaily和SafeVid，并发现GPT-4o表现最佳，但模型在动态环境中感知潜在风险的能力不足。


<details>
  <summary>Details</summary>
Motivation: 视障人士在动态复杂环境中缺乏实时感知支持，现有研究多关注静态内容，需要更先进的视觉理解技术。

Method: 构建VisAssistDaily数据集评估模型表现，进行用户研究，并开发SafeVid数据集和轮询机制以提升环境风险感知。

Result: GPT-4o在任务成功率上表现最佳，但模型在动态环境中感知风险的能力有限。

Conclusion: 研究为VideoLLMs在辅助视障领域的应用提供了新见解，未来需改进动态环境感知能力。

Abstract: The visually impaired population, especially the severely visually impaired,
is currently large in scale, and daily activities pose significant challenges
for them. Although many studies use large language and vision-language models
to assist the blind, most focus on static content and fail to meet real-time
perception needs in dynamic and complex environments, such as daily activities.
To provide them with more effective intelligent assistance, it is imperative to
incorporate advanced visual understanding technologies. Although real-time
vision and speech interaction VideoLLMs demonstrate strong real-time visual
understanding, no prior work has systematically evaluated their effectiveness
in assisting visually impaired individuals. In this work, we conduct the first
such evaluation. First, we construct a benchmark dataset (VisAssistDaily),
covering three categories of assistive tasks for visually impaired individuals:
Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that
GPT-4o achieves the highest task success rate. Next, we conduct a user study to
evaluate the models in both closed-world and open-world scenarios, further
exploring the practical challenges of applying VideoLLMs in assistive contexts.
One key issue we identify is the difficulty current models face in perceiving
potential hazards in dynamic environments. To address this, we build an
environment-awareness dataset named SafeVid and introduce a polling mechanism
that enables the model to proactively detect environmental risks. We hope this
work provides valuable insights and inspiration for future research in this
field.

</details>

### [214] [Defining and Quantifying Creative Behavior in Popular Image Generators](https://arxiv.org/abs/2505.04497)
*Aditi Ramaswamy*

Main category: cs.CV

TLDR: 本文提出了一种从实践角度衡量生成AI模型创造力的方法，并引入定量指标帮助用户选择适合任务的模型。


<details>
  <summary>Details</summary>
Motivation: 探讨生成AI模型的创造力，并提供实用工具帮助用户选择模型。

Method: 引入定量创造力指标，并在多个图像生成模型上进行评估。

Result: 指标结果与人类直觉一致。

Conclusion: 提出的定量方法能有效衡量生成AI模型的创造力。

Abstract: Creativity of generative AI models has been a subject of scientific debate in
the last years, without a conclusive answer. In this paper, we study creativity
from a practical perspective and introduce quantitative measures that help the
user to choose a suitable AI model for a given task. We evaluated our measures
on a number of popular image-to-image generation models, and the results of
this suggest that our measures conform to human intuition.

</details>

### [215] [Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition](https://arxiv.org/abs/2505.04502)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TLDR: 本文提出了一种在边缘GPU上同时利用硬件引擎进行视频人脸检测和识别的方法，提高了吞吐量并降低了功耗。


<details>
  <summary>Details</summary>
Motivation: 公共场所在边缘设备上进行视频人脸检测和识别对安全和无接触访问等应用至关重要，但现有方法未能充分利用硬件引擎。

Method: 通过并发和流水线化任务（包括视频解码）优化硬件引擎的使用，提出统一自动化框架。

Result: 在NVIDIA边缘Orin GPU上实现了更高的吞吐量和约5%的功耗节省，同时满足实时性能。

Conclusion: 建议进一步优化硬件以减少张量RT框架的冗余层，提升性能。

Abstract: Video face detection and recognition in public places at the edge is required
in several applications, such as security reinforcement and contactless access
to authorized venues. This paper aims to maximize the simultaneous usage of
hardware engines available in edge GPUs nowadays by leveraging the concurrency
and pipelining of tasks required for face detection and recognition. This also
includes the video decoding task, which is required in most face monitoring
applications as the video streams are usually carried via Gbps Ethernet
network. This constitutes an improvement over previous works where the tasks
are usually allocated to a single engine due to the lack of a unified and
automated framework that simultaneously explores all hardware engines. In
addition, previously, the input faces were usually embedded in still images or
within raw video streams that overlook the burst delay caused by the decoding
stage. The results on real-life video streams suggest that simultaneously using
all the hardware engines available in the recent NVIDIA edge Orin GPU, higher
throughput, and a slight saving of power consumption of around 300 mW,
accounting for around 5%, have been achieved while satisfying the real-time
performance constraint. The performance gets even higher by considering several
video streams simultaneously. Further performance improvement could have been
obtained if the number of shuffle layers that were created by the tensor RT
framework for the face recognition task was lower. Thus, the paper suggests
some hardware improvements to the existing edge GPU processors to enhance their
performance even higher.

</details>

### [216] [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation](https://arxiv.org/abs/2505.04512)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Sen Liang,Yuan Zhou,Qin Lin,Qinglin Lu*

Main category: cs.CV

TLDR: HunyuanCustom是一个多模态定制视频生成框架，强调主题一致性并支持图像、音频、视频和文本条件输入，显著提升了ID一致性、真实性和文本-视频对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在身份一致性和输入模态多样性上表现不足，HunyuanCustom旨在解决这些问题。

Method: 基于HunyuanVideo，引入文本-图像融合模块和图像ID增强模块；针对音频和视频条件，提出AudioNet和视频驱动注入模块。

Result: 实验表明HunyuanCustom在单/多主体场景中优于现有方法，且在下游任务中表现稳健。

Conclusion: 多模态条件和身份保留策略有效推动了可控视频生成的发展。

Abstract: Customized video generation aims to produce videos featuring specific
subjects under flexible user-defined conditions, yet existing methods often
struggle with identity consistency and limited input modalities. In this paper,
we propose HunyuanCustom, a multi-modal customized video generation framework
that emphasizes subject consistency while supporting image, audio, video, and
text conditions. Built upon HunyuanVideo, our model first addresses the
image-text conditioned generation task by introducing a text-image fusion
module based on LLaVA for enhanced multi-modal understanding, along with an
image ID enhancement module that leverages temporal concatenation to reinforce
identity features across frames. To enable audio- and video-conditioned
generation, we further propose modality-specific condition injection
mechanisms: an AudioNet module that achieves hierarchical alignment via spatial
cross-attention, and a video-driven injection module that integrates
latent-compressed conditional video through a patchify-based feature-alignment
network. Extensive experiments on single- and multi-subject scenarios
demonstrate that HunyuanCustom significantly outperforms state-of-the-art open-
and closed-source methods in terms of ID consistency, realism, and text-video
alignment. Moreover, we validate its robustness across downstream tasks,
including audio and video-driven customized video generation. Our results
highlight the effectiveness of multi-modal conditioning and identity-preserving
strategies in advancing controllable video generation. All the code and models
are available at https://hunyuancustom.github.io.

</details>

### [217] [Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model](https://arxiv.org/abs/2505.04522)
*Pengfei Guo,Can Zhao,Dong Yang,Yufan He,Vishwesh Nath,Ziyue Xu,Pedro R. A. S. Bassi,Zongwei Zhou,Benjamin D. Simon,Stephanie Anne Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TLDR: Text2CT是一种基于扩散模型的新方法，能够从自由文本描述生成高分辨率3D CT体积，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通过自由文本生成3D CT体积，为诊断和研究提供新机会。

Method: 使用扩散模型，将医学文本编码为潜在表示并解码为3D CT扫描。

Result: 在保留解剖保真度和捕捉细节结构方面表现优异，达到最先进水平。

Conclusion: Text2CT在诊断和数据增强方面具有广阔应用前景。

Abstract: Generating 3D CT volumes from descriptive free-text inputs presents a
transformative opportunity in diagnostics and research. In this paper, we
introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual
descriptions using the diffusion model. Unlike previous methods that rely on
fixed-format text input, Text2CT employs a novel prompt formulation that
enables generation from diverse, free-text descriptions. The proposed framework
encodes medical text into latent representations and decodes them into
high-resolution 3D CT scans, effectively bridging the gap between semantic text
inputs and detailed volumetric representations in a unified 3D framework. Our
method demonstrates superior performance in preserving anatomical fidelity and
capturing intricate structures as described in the input text. Extensive
evaluations show that our approach achieves state-of-the-art results, offering
promising potential applications in diagnostics, and data augmentation.

</details>

### [218] [Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration](https://arxiv.org/abs/2505.04524)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TLDR: 论文提出了一种结合硬件和软件的方法，优化了基于NVIDIA Jetson AGX Orin的人脸检测与识别系统，通过同时利用所有硬件引擎和集成人脸跟踪模块，显著提高了吞吐量和降低了功耗。


<details>
  <summary>Details</summary>
Motivation: 现代应用中，公共场所实时且准确的人脸检测与识别系统至关重要，但现有系统在吞吐量和功耗方面仍有改进空间。

Method: 利用NVIDIA Jetson AGX Orin的所有硬件引擎，并集成人脸跟踪模块，避免冗余计算。

Result: 实验显示，该方法在1920x1080分辨率下达到290 FPS的吞吐量，并节省约800 mW的功耗。

Conclusion: 这种硬件-软件协同设计方法为高性能边缘机器视觉系统提供了可行方案，适用于公共场所的多摄像头监控场景。

Abstract: Cost-effective machine vision systems dedicated to real-time and accurate
face detection and recognition in public places are crucial for many modern
applications. However, despite their high performance, which could be reached
using specialized edge or cloud AI hardware accelerators, there is still room
for improvement in throughput and power consumption. This paper aims to suggest
a combined hardware-software approach that optimizes face detection and
recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX
Orin. First, it leverages the simultaneous usage of all its hardware engines to
improve processing time. This offers an improvement over previous works where
these tasks were mainly allocated automatically and exclusively to the CPU or,
to a higher extent, to the GPU core. Additionally, the paper suggests
integrating a face tracker module to avoid redundantly running the face
recognition algorithm for every frame but only when a new face appears in the
scene. The results of extended experiments suggest that simultaneous usage of
all the hardware engines that are available in the Orin GPU and tracker
integration into the pipeline yield an impressive throughput of 290 FPS (frames
per second) on 1920 x 1080 input size frames containing in average of 6
faces/frame. Additionally, a substantial saving of power consumption of around
800 mW was achieved when compared to running the task on the CPU/GPU engines
only and without integrating a tracker into the Orin GPU\'92s pipeline. This
hardware-codesign approach can pave the way to design high-performance machine
vision systems at the edge, critically needed in video monitoring in public
places where several nearby cameras are usually deployed for a same scene.

</details>

### [219] [DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once](https://arxiv.org/abs/2505.04526)
*Qi Zhou,Yukai Shi,Xiaojun Yang,Xiaoyu Xian,Lunjia Liao,Ruimao Zhang,Liang Lin*

Main category: cs.CV

TLDR: 论文提出了一种名为DFVO的网络，用于解决可见光和红外图像融合中因光照不足导致的模糊和暗淡问题，通过多任务级联方法提升融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在可见光图像光照不足时效果不佳，影响自动驾驶等高级视觉任务，因此需要一种新方法提升融合质量。

Method: 采用级联多任务策略，包括潜在特征提取器（LCFE）、细节提取模块（DEM）和超交叉注意力模块（HCAM），并结合相关损失函数优化网络。

Result: DFVO在LLVIP数据集上表现最佳，PSNR为63.258 dB，CC为0.724，生成更清晰、信息更丰富的融合图像。

Conclusion: DFVO在黑暗环境中显著提升了图像融合效果，为高级视觉任务提供了更有效的信息。

Abstract: Visible and infrared image fusion is one of the most crucial tasks in the
field of image fusion, aiming to generate fused images with clear structural
information and high-quality texture features for high-level vision tasks.
However, when faced with severe illumination degradation in visible images, the
fusion results of existing image fusion methods often exhibit blurry and dim
visual effects, posing major challenges for autonomous driving. To this end, a
Darkness-Free network is proposed to handle Visible and infrared image
disentanglement and fusion all at Once (DFVO), which employs a cascaded
multi-task approach to replace the traditional two-stage cascaded training
(enhancement and fusion), addressing the issue of information entropy loss
caused by hierarchical data transmission. Specifically, we construct a
latent-common feature extractor (LCFE) to obtain latent features for the
cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised
to acquire high-frequency semantic information. Secondly, we design a hyper
cross-attention module (HCAM) to extract low-frequency information and preserve
texture features from source images. Finally, a relevant loss function is
designed to guide the holistic network learning, thereby achieving better image
fusion. Extensive experiments demonstrate that our proposed approach
outperforms state-of-the-art alternatives in terms of qualitative and
quantitative evaluations. Particularly, DFVO can generate clearer, more
informative, and more evenly illuminated fusion results in the dark
environments, achieving best performance on the LLVIP dataset with 63.258 dB
PSNR and 0.724 CC, providing more effective information for high-level vision
tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.

</details>

### [220] [RAFT: Robust Augmentation of FeaTures for Image Segmentation](https://arxiv.org/abs/2505.04529)
*Edward Humes,Xiaomin Lin,Uttej Kallakuri,Tinoosh Mohsenin*

Main category: cs.CV

TLDR: RAFT框架通过数据与特征增强及主动学习，利用少量真实数据提升合成数据训练的模型在真实场景中的表现，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据训练的模型在真实场景中表现不佳的Syn2Real问题。

Method: 提出RAFT框架，结合数据与特征增强及主动学习，利用少量真实数据优化模型。

Result: 在SYNTHIA->Cityscapes、GTAV->Cityscapes和Cityscapes->ACDC基准测试中，mIoU分别提升2.1%/79.9%、0.4%/78.2%和1.3%/73.2%，超越HALO。

Conclusion: RAFT有效缓解了合成数据与真实数据间的性能差距，且在不同任务中表现优异。

Abstract: Image segmentation is a powerful computer vision technique for scene
understanding. However, real-world deployment is stymied by the need for
high-quality, meticulously labeled datasets. Synthetic data provides
high-quality labels while reducing the need for manual data collection and
annotation. However, deep neural networks trained on synthetic data often face
the Syn2Real problem, leading to poor performance in real-world deployments.
  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a
novel framework for adapting image segmentation models using minimal labeled
real-world data through data and feature augmentations, as well as active
learning. To validate RAFT, we perform experiments on the synthetic-to-real
"SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass
the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an
improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes
experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach
on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO,
with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the
effect of the allocated annotation budget and various components of RAFT upon
the final transfer mIoU.

</details>

### [221] [Registration of 3D Point Sets Using Exponential-based Similarity Matrix](https://arxiv.org/abs/2505.04540)
*Ashutosh Singandhupe,Sanket Lokhande,Hung Manh La*

Main category: cs.CV

TLDR: 提出了一种改进的ICP算法（ESM-ICP），通过动态调整相似性矩阵，解决了点云配准中旋转差异大和噪声干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 现有配准技术在大旋转差异或噪声干扰下表现不佳，导致3D重建不准确。

Method: 引入高斯启发的指数加权方案，构建动态调整的相似性矩阵，改进旋转和平移估计。

Result: ESM-ICP在大旋转差异和非高斯噪声下优于传统几何方法和部分学习型方法。

Conclusion: ESM-ICP有效提升了点云配准的鲁棒性，代码已开源。

Abstract: Point cloud registration is a fundamental problem in computer vision and
robotics, involving the alignment of 3D point sets captured from varying
viewpoints using depth sensors such as LiDAR or structured light. In modern
robotic systems, especially those focused on mapping, it is essential to merge
multiple views of the same environment accurately. However, state-of-the-art
registration techniques often struggle when large rotational differences exist
between point sets or when the data is significantly corrupted by sensor noise.
These challenges can lead to misalignments and, consequently, to inaccurate or
distorted 3D reconstructions. In this work, we address both these limitations
by proposing a robust modification to the classic Iterative Closest Point (ICP)
algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),
integrates a Gaussian-inspired exponential weighting scheme to construct a
similarity matrix that dynamically adapts across iterations. This matrix
facilitates improved estimation of both rotational and translational components
during alignment. We demonstrate the robustness of ESM-ICP in two challenging
scenarios: (i) large rotational discrepancies between the source and target
point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show
that ESM-ICP outperforms traditional geometric registration techniques as well
as several recent learning-based methods. To encourage reproducibility and
community engagement, our full implementation is made publicly available on
GitHub. https://github.com/aralab-unr/ESM_ICP

</details>

### [222] [Componential Prompt-Knowledge Alignment for Domain Incremental Learning](https://arxiv.org/abs/2505.04575)
*Kunlun Xu,Xu Zou,Gang Hua,Jiahuan Zhou*

Main category: cs.CV

TLDR: KA-Prompt提出了一种基于提示的领域增量学习方法，通过组件感知的提示-知识对齐解决多领域知识融合中的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 揭示现有提示方法中因组件不对齐导致的知识冲突和预测性能下降问题。

Method: 分两阶段：初始组件结构配置和在线对齐保持，通过贪婪搜索和动态一致性约束实现知识对齐。

Result: 在DIL基准测试中表现优异，显著提升模型的学习和推理能力。

Conclusion: KA-Prompt有效解决了多领域知识融合中的冲突问题，为领域增量学习提供了新思路。

Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data
streams across domains while retaining and utilizing past knowledge. Although
prompt-based methods effectively store multi-domain knowledge in prompt
parameters and obtain advanced performance through cross-domain prompt fusion,
we reveal an intrinsic limitation: component-wise misalignment between
domain-specific prompts leads to conflicting knowledge integration and degraded
predictions. This arises from the random positioning of knowledge components
within prompts, where irrelevant component fusion introduces interference.To
address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a
novel prompt-based DIL method that introduces component-aware prompt-knowledge
alignment during training, significantly improving both the learning and
inference capacity of the model. KA-Prompt operates in two phases: (1) Initial
Componential Structure Configuring, where a set of old prompts containing
knowledge relevant to the new domain are mined via greedy search, which is then
exploited to initialize new prompts to achieve reusable knowledge transfer and
establish intrinsic alignment between new and old prompts. (2) Online Alignment
Preservation, which dynamically identifies the target old prompts and applies
adaptive componential consistency constraints as new prompts evolve. Extensive
experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.
Our source code is available at
https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt

</details>

### [223] [Active Sampling for MRI-based Sequential Decision Making](https://arxiv.org/abs/2505.04586)
*Yuning Du,Jingshuai Liu,Rohan Dharmakumar,Sotirios A. Tsaftaris*

Main category: cs.CV

TLDR: 提出了一种多目标强化学习框架，用于从欠采样的k空间数据中进行全面、连续的诊断评估，显著减少了采样需求。


<details>
  <summary>Details</summary>
Motivation: 尽管MRI具有卓越的诊断能力，但其高成本和复杂性限制了其作为即时诊断设备（PoC）的应用。通过降低磁场强度并改进采样策略，有望实现MRI的PoC化。

Method: 采用多目标强化学习框架，在推理过程中动态调整采样策略，并通过逐步加权奖励函数优化样本选择。

Result: 在两个膝关节病理评估任务中，该方法在疾病检测、严重程度量化和连续诊断方面表现优异，同时大幅减少了k空间采样。

Conclusion: 该方法为实现MRI作为全面且经济的PoC设备铺平了道路。

Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging
(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and
complexity. To enable such a future by reducing the magnetic field strength,
one key approach will be to improve sampling strategies. Previous work has
shown that it is possible to make diagnostic decisions directly from k-space
with fewer samples. Such work shows that single diagnostic decisions can be
made, but if we aspire to see MRI as a true PoC, multiple and sequential
decisions are necessary while minimizing the number of samples acquired. We
present a novel multi-objective reinforcement learning framework enabling
comprehensive, sequential, diagnostic evaluation from undersampled k-space
data. Our approach during inference actively adapts to sequential decisions to
optimally sample. To achieve this, we introduce a training methodology that
identifies the samples that contribute the best to each diagnostic objective
using a step-wise weighting reward function. We evaluate our approach in two
sequential knee pathology assessment tasks: ACL sprain detection and cartilage
thickness loss assessment. Our framework achieves diagnostic performance
competitive with various policy-based benchmarks on disease detection, severity
quantification, and overall sequential diagnosis, while substantially saving
k-space samples. Our approach paves the way for the future of MRI as a
comprehensive and affordable PoC device. Our code is publicly available at
https://github.com/vios-s/MRI_Sequential_Active_Sampling

</details>

### [224] [MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection](https://arxiv.org/abs/2505.04594)
*Zhihao Zhang,Abhinav Kumar,Girish Chandar Ganesan,Xiaoming Liu*

Main category: cs.CV

TLDR: 论文提出MonoCoP方法，通过链式预测（CoP）顺序预测3D属性，结合AttributeNet和残差连接，显著提升单目3D物体检测的深度估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单目3D物体检测中忽视了3D属性间的内在关联，导致深度估计准确性受限。

Method: 设计MonoCoP，包含AttributeNet学习属性特征、显式链式传播特征、残差连接聚合特征。

Result: 在KITTI、Waymo和nuScenes数据集上达到SOTA性能。

Conclusion: MonoCoP通过链式预测和条件依赖，显著提升了单目3D检测的准确性和稳定性。

Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object
detection (Mono3D), with depth estimation posing the greatest challenge due to
the inherent ambiguity in mapping 2D images to 3D space. While existing methods
leverage multiple depth cues (e.g., estimating depth uncertainty, modeling
depth error) to improve depth accuracy, they overlook that accurate depth
prediction requires conditioning on other 3D attributes, as these attributes
are intrinsically inter-correlated through the 3D to 2D projection, which
ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought
(CoT) in large language models (LLMs), this paper proposes MonoCoP, which
leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and
conditionally via three key designs. First, it employs a lightweight
AttributeNet (AN) for each 3D attribute to learn attribute-specific features.
Next, MonoCoP constructs an explicit chain to propagate these learned features
from one attribute to the next. Finally, MonoCoP uses a residual connection to
aggregate features for each attribute along the chain, ensuring that later
attribute predictions are conditioned on all previously processed attributes
without forgetting the features of earlier ones. Experimental results show that
our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI
leaderboard without requiring additional data and further surpasses existing
methods on the Waymo and nuScenes frontal datasets.

</details>

### [225] [OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning](https://arxiv.org/abs/2505.04601)
*Xianhang Li,Yanqing Liu,Haoqin Tu,Hongru Zhu,Cihang Xie*

Main category: cs.CV

TLDR: OpenVision是一个完全开放的视觉编码器家族，性能优于或匹配CLIP，提供从5.9M到632.1M参数的灵活选择。


<details>
  <summary>Details</summary>
Motivation: 填补现有视觉编码器（如CLIP）未完全开放的空白，提供成本效益高的替代方案。

Method: 基于现有工作（如CLIPS训练框架和Recap-DataComp-1B训练数据），揭示提升编码器质量的关键见解。

Result: OpenVision在性能上匹配或超越CLIP，并支持从轻量级到高性能的多模态模型部署。

Conclusion: OpenVision为多模态模型提供了灵活且高效的视觉编码器选择，推动了多模态模型的进步。

Abstract: OpenAI's CLIP, released in early 2021, have long been the go-to choice of
vision encoder for building multimodal foundation models. Although recent
alternatives such as SigLIP have begun to challenge this status quo, to our
knowledge none are fully open: their training data remains proprietary and/or
their training recipes are not released. This paper fills this gap with
OpenVision, a fully-open, cost-effective family of vision encoders that match
or surpass the performance of OpenAI's CLIP when integrated into multimodal
frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for
training framework and Recap-DataComp-1B for training data -- while revealing
multiple key insights in enhancing encoder quality and showcasing practical
benefits in advancing multimodal models. By releasing vision encoders spanning
from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible
trade-off between capacity and efficiency in building multimodal models: larger
models deliver enhanced multimodal performance, while smaller versions enable
lightweight, edge-ready multimodal deployments.

</details>

### [226] [FastMap: Revisiting Dense and Scalable Structure from Motion](https://arxiv.org/abs/2505.04612)
*Jiahao Li,Haochen Wang,Muhammad Zubair Irshad,Igor Vasiljevic,Matthew R. Walter,Vitor Campagnolo Guizilini,Greg Shakhnarovich*

Main category: cs.CV

TLDR: FastMap是一种新的全局运动结构方法，专注于速度和简洁性，解决了COLMAP和GLOMAP在大规模场景中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如COLMAP和GLOMAP）在匹配关键点对数量大时扩展性差，主要由于并行化不足和优化步骤计算成本高。

Method: 设计了一个完全基于GPU友好操作的SfM框架，易于并行化，且每个优化步骤的时间复杂度与图像对数线性相关。

Result: 实验表明，FastMap在大规模场景中比COLMAP和GLOMAP快一到两个数量级，且姿态精度相当。

Conclusion: FastMap通过优化并行化和计算效率，显著提升了大规模场景下的运动结构估计速度。

Abstract: We propose FastMap, a new global structure from motion method focused on
speed and simplicity. Previous methods like COLMAP and GLOMAP are able to
estimate high-precision camera poses, but suffer from poor scalability when the
number of matched keypoint pairs becomes large. We identify two key factors
leading to this problem: poor parallelization and computationally expensive
optimization steps. To overcome these issues, we design an SfM framework that
relies entirely on GPU-friendly operations, making it easily parallelizable.
Moreover, each optimization step runs in time linear to the number of image
pairs, independent of keypoint pairs or 3D points. Through extensive
experiments, we show that FastMap is one to two orders of magnitude faster than
COLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.

</details>

### [227] [Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait](https://arxiv.org/abs/2505.04616)
*Feng Liu,Nicholas Chimitt,Lanqing Guo,Jitesh Jain,Aditya Kane,Minchul Kim,Wes Robbins,Yiyang Su,Dingqiang Ye,Xingguang Zhang,Jie Zhu,Siddharth Satyakam,Christopher Perry,Stanley H. Chan,Arun Ross,Humphrey Shi,Zhangyang Wang,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TLDR: FarSight是一个端到端的全身人物识别系统，整合了面部、步态和体型等多模态生物特征，在恶劣条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在无约束环境（如远距离、高视角和恶劣大气条件）下的全身人物识别问题，适用于监控场景。

Method: FarSight包含四个核心模块：多目标检测与跟踪、识别感知的视频恢复、模态特异性生物特征编码和质量引导的多模态融合。

Result: 在BRIAR数据集上，FarSight在1:1验证、闭集识别和开集识别任务中分别提升了34.1%、17.8%和34.3%。

Conclusion: FarSight是挑战性现实条件下生物识别的先进解决方案。

Abstract: We address the problem of whole-body person recognition in unconstrained
environments. This problem arises in surveillance scenarios such as those in
the IARPA Biometric Recognition and Identification at Altitude and Range
(BRIAR) program, where biometric data is captured at long standoff distances,
elevated viewing angles, and under adverse atmospheric conditions (e.g.,
turbulence and high wind velocity). To this end, we propose FarSight, a unified
end-to-end system for person recognition that integrates complementary
biometric cues across face, gait, and body shape modalities. FarSight
incorporates novel algorithms across four core modules: multi-subject detection
and tracking, recognition-aware video restoration, modality-specific biometric
feature encoding, and quality-guided multi-modal fusion. These components are
designed to work cohesively under degraded image conditions, large pose and
scale variations, and cross-domain gaps. Extensive experiments on the BRIAR
dataset, one of the most comprehensive benchmarks for long-range, multi-modal
biometric recognition, demonstrate the effectiveness of FarSight. Compared to
our preliminary system, this system achieves a 34.1% absolute gain in 1:1
verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set
identification (Rank-20), and a 34.3% reduction in open-set identification
errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE
Face in Video Evaluation (FIVE), which conducts standardized face recognition
testing on the BRIAR dataset. These results establish FarSight as a
state-of-the-art solution for operational biometric recognition in challenging
real-world conditions.

</details>

### [228] [On Path to Multimodal Generalist: General-Level and General-Bench](https://arxiv.org/abs/2505.04620)
*Hao Fei,Yuan Zhou,Juncheng Li,Xiangtai Li,Qingshan Xu,Bobo Li,Shengqiong Wu,Yaoting Wang,Junbao Zhou,Jiahao Meng,Qingyu Shi,Zhiyuan Zhou,Liangtao Shi,Minghe Gao,Daoan Zhang,Zhiqi Ge,Weiming Wu,Siliang Tang,Kaihang Pan,Yaobo Ye,Haobo Yuan,Tao Zhang,Tianjie Ju,Zixiang Meng,Shilin Xu,Liyu Jia,Wentao Hu,Meng Luo,Jiebo Luo,Tat-Seng Chua,Shuicheng Yan,Hanwang Zhang*

Main category: cs.CV

TLDR: 该论文提出了一个名为General-Level的评估框架，用于衡量多模态大语言模型（MLLM）的性能和通用性，并引入了Synergy概念和General-Bench基准。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估方法过于简单，无法全面衡量模型的能力和通用性，尤其是接近人类水平AI的进展。

Method: 提出5级评估框架General-Level，引入Synergy概念衡量模型在多模态理解和生成中的一致性，并构建包含700多个任务和325,800实例的General-Bench基准。

Result: 评估了100多个现有MLLM，揭示了通用模型的性能排名，并指出了实现真正AI的挑战。

Conclusion: 该框架为未来多模态基础模型研究提供了基础设施，加速实现通用人工智能（AGI）。

Abstract: The Multimodal Large Language Model (MLLM) is currently experiencing rapid
growth, driven by the advanced capabilities of LLMs. Unlike earlier
specialists, existing MLLMs are evolving towards a Multimodal Generalist
paradigm. Initially limited to understanding multiple modalities, these models
have advanced to not only comprehend but also generate across modalities. Their
capabilities have expanded from coarse-grained to fine-grained multimodal
understanding and from supporting limited modalities to arbitrary ones. While
many benchmarks exist to assess MLLMs, a critical question arises: Can we
simply assume that higher performance across tasks indicates a stronger MLLM
capability, bringing us closer to human-level AI? We argue that the answer is
not as straightforward as it seems. This project introduces General-Level, an
evaluation framework that defines 5-scale levels of MLLM performance and
generality, offering a methodology to compare MLLMs and gauge the progress of
existing systems towards more robust multimodal generalists and, ultimately,
towards AGI. At the core of the framework is the concept of Synergy, which
measures whether models maintain consistent capabilities across comprehension
and generation, and across multiple modalities. To support this evaluation, we
present General-Bench, which encompasses a broader spectrum of skills,
modalities, formats, and capabilities, including over 700 tasks and 325,800
instances. The evaluation results that involve over 100 existing
state-of-the-art MLLMs uncover the capability rankings of generalists,
highlighting the challenges in reaching genuine AI. We expect this project to
pave the way for future research on next-generation multimodal foundation
models, providing a robust infrastructure to accelerate the realization of AGI.
Project page: https://generalist.top/

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [229] [Ultra-Low-Power Spiking Neurons in 7 nm FinFET Technology: A Comparative Analysis of Leaky Integrate-and-Fire, Morris-Lecar, and Axon-Hillock Architectures](https://arxiv.org/abs/2505.03764)
*Logan Larsh,Raiyan Siddique,Sarah Sharif Yaser Mike Banad*

Main category: cs.NE

TLDR: 比较了三种尖峰神经元电路架构（LIF、ML、AH）在7 nm FinFET技术中的性能，发现AH设计吞吐量最高，ML在低功耗下表现优异，LIF高频但静态泄漏较高。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过比较不同神经元架构的性能，为神经形态计算硬件提供优化路径，以实现高能效和高吞吐量。

Method: 通过SPICE模拟，比较了LIF、ML和AH架构在7 nm FinFET技术中的尖峰频率、能量消耗和静态功耗。

Result: AH设计达到最高吞吐量（3 GHz），ML在低功耗下表现最佳（0.385 aJ/spike），LIF高频但静态泄漏较高。7 nm FinFET显著提升能效和速度。

Conclusion: 研究为优化尖峰神经元电路提供了设计权衡，支持神经形态硬件在低功耗和高吞吐量方面的应用。

Abstract: Neuromorphic computing aims to replicate the brain's remarkable energy
efficiency and parallel processing capabilities for large-scale artificial
intelligence applications. In this work, we present a comprehensive comparative
study of three spiking neuron circuit architectures-Leaky-Integrate-and-Fire
(LIF), Morris-Lecar (ML), and Axon-Hillock (AH)-implemented in a 7 nm FinFET
technology. Through extensive SPICE simulations, we explore the optimization of
spiking frequency, energy per spike, and static power consumption. Our results
show that the AH design achieves the highest throughput, demonstrating
multi-gigahertz firing rates (up to 3 GHz) with attojoule energy costs. By
contrast, the ML architecture excels in subthreshold to near-threshold regimes,
offering robust low-power operation (as low as 0.385 aJ/spike) and biological
bursting behavior. Although LIF benefits from a decoupled current mirror for
high-frequency operation, it exhibits slightly higher static leakage compared
to ML and AH at elevated supply voltages. Comparisons with previous node
implementations (22 nm planar, 28 nm) reveal that 7 nm FinFETs can drastically
boost energy efficiency and speed albeit at the cost of increased subthreshold
leakage in deep subthreshold regions. By quantifying design trade-offs for each
neuron architecture, our work provides a roadmap for optimizing spiking neuron
circuits in advanced nanoscale technologies to deliver neuromorphic hardware
capable of both ultra-low-power operation and high computational throughput.

</details>

### [230] [Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks](https://arxiv.org/abs/2505.04034)
*Ayana Moshruba,Hamed Poursiami,Maryam Parsa*

Main category: cs.NE

TLDR: 论文提出两种概率驱动的输入级时间尖峰变换（Poisson-Burst和Delayed-Burst），以在标准LIF神经元中引入生物启发的时序变异性，从而提升可扩展训练并评估尖峰时序动态对隐私、泛化和学习性能的影响。


<details>
  <summary>Details</summary>
Motivation: 生物神经元的多样时序尖峰模式支持高效、稳健和自适应的神经信息处理，但现有模型（如Izhikevich）的复杂性难以直接集成到可扩展的SNN训练流程中。

Method: 提出Poisson-Burst（基于输入强度调制爆发）和Delayed-Burst（通过爆发起始时间编码输入强度）两种变换，并将其应用于标准LIF神经元。

Result: 实验表明，Poisson-Burst在保持竞争性准确性和低资源开销的同时增强隐私鲁棒性；Delayed-Burst提供更强的隐私保护但需牺牲少量准确性。

Conclusion: 生物启发的时序尖峰动态可提升神经形态学习系统的隐私性、泛化能力和生物合理性。

Abstract: Biological neurons exhibit diverse temporal spike patterns, which are
believed to support efficient, robust, and adaptive neural information
processing. While models such as Izhikevich can replicate a wide range of these
firing dynamics, their complexity poses challenges for directly integrating
them into scalable spiking neural networks (SNN) training pipelines. In this
work, we propose two probabilistically driven, input-level temporal spike
transformations: Poisson-Burst and Delayed-Burst that introduce biologically
inspired temporal variability directly into standard Leaky Integrate-and-Fire
(LIF) neurons. This enables scalable training and systematic evaluation of how
spike timing dynamics affect privacy, generalization, and learning performance.
Poisson-Burst modulates burst occurrence based on input intensity, while
Delayed-Burst encodes input strength through burst onset timing. Through
extensive experiments across multiple benchmarks, we demonstrate that
Poisson-Burst maintains competitive accuracy and lower resource overhead while
exhibiting enhanced privacy robustness against membership inference attacks,
whereas Delayed-Burst provides stronger privacy protection at a modest accuracy
trade-off. These findings highlight the potential of biologically grounded
temporal spike dynamics in improving the privacy, generalization and biological
plausibility of neuromorphic learning systems.

</details>

### [231] [TS-SNN: Temporal Shift Module for Spiking Neural Networks](https://arxiv.org/abs/2505.04165)
*Kairong Yu,Tianqing Zhang,Qi Xu,Gang Pan,Hongwei Wang*

Main category: cs.NE

TLDR: TS-SNN通过引入轻量级的Temporal Shift模块，整合过去、现在和未来的脉冲特征，实现了高效且准确的脉冲神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在利用时间特征与低能耗之间的平衡问题。

Method: 提出Temporal Shift模块，通过简单的移位操作整合时间特征，并结合残差方法防止信息丢失。

Result: 在CIFAR-10、CIFAR-100和ImageNet上达到SOTA性能，同时保持低能耗。

Conclusion: TS-SNN为高效准确的SNN架构开发迈出了重要一步。

Abstract: Spiking Neural Networks (SNNs) are increasingly recognized for their
biological plausibility and energy efficiency, positioning them as strong
alternatives to Artificial Neural Networks (ANNs) in neuromorphic computing
applications. SNNs inherently process temporal information by leveraging the
precise timing of spikes, but balancing temporal feature utilization with low
energy consumption remains a challenge. In this work, we introduce Temporal
Shift module for Spiking Neural Networks (TS-SNN), which incorporates a novel
Temporal Shift (TS) module to integrate past, present, and future spike
features within a single timestep via a simple yet effective shift operation. A
residual combination method prevents information loss by integrating shifted
and original features. The TS module is lightweight, requiring only one
additional learnable parameter, and can be seamlessly integrated into existing
architectures with minimal additional computational cost. TS-SNN achieves
state-of-the-art performance on benchmarks like CIFAR-10 (96.72\%), CIFAR-100
(80.28\%), and ImageNet (70.61\%) with fewer timesteps, while maintaining low
energy consumption. This work marks a significant step forward in developing
efficient and accurate SNN architectures.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [232] [Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework](https://arxiv.org/abs/2505.03746)
*Silvia García-Méndez,Francisco De Arriba-Pérez*

Main category: cs.SI

TLDR: 提出了一种基于流式机器学习和大型语言模型的实时网络欺凌检测方法，性能接近90%，优于现有工作。


<details>
  <summary>Details</summary>
Motivation: 社交媒体虽便利但引发网络欺凌，现有AI研究多集中于零/少样本学习，缺乏对实时检测的探索。

Method: 结合流式机器学习和大型语言模型进行特征工程，处理动态演变的网络仇恨言论，并提供可解释性仪表板。

Result: 实验数据显示性能接近90%，优于文献中的竞争方法。

Conclusion: 该方案有助于在线社区安全，及时检测欺凌行为以减少社会负面影响。

Abstract: Social media platforms enable instant and ubiquitous connectivity and are
essential to social interaction and communication in our technological society.
Apart from its advantages, these platforms have given rise to negative
behaviors in the online community, the so-called cyberbullying. Despite the
many works involving generative Artificial Intelligence (AI) in the literature
lately, there remain opportunities to study its performance apart from
zero/few-shot learning strategies. Accordingly, we propose an innovative and
real-time solution for cyberbullying detection that leverages stream-based
Machine Learning (ML) models able to process the incoming samples incrementally
and Large Language Models (LLMS) for feature engineering to address the
evolving nature of abusive and hate speech online. An explainability dashboard
is provided to promote the system's trustworthiness, reliability, and
accountability. Results on experimental data report promising performance close
to 90 % in all evaluation metrics and surpassing those obtained by competing
works in the literature. Ultimately, our proposal contributes to the safety of
online communities by timely detecting abusive behavior to prevent long-lasting
harassment and reduce the negative consequences in society.

</details>

### [233] [The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing](https://arxiv.org/abs/2505.03769)
*Yibo Hu,Yiqiao Jin,Meng Ye,Ajay Divakaran,Srijan Kumar*

Main category: cs.SI

TLDR: 研究探讨了改写Reddit帖子标题对用户参与度的影响，发现改写标题能显著提升参与度，并揭示了情感共鸣、词汇丰富度和社区规范对齐是关键因素。


<details>
  <summary>Details</summary>
Motivation: 在跨平台社交媒体环境中，理解多模态内容（尤其是文本与视觉结合）如何驱动用户参与度是一个复杂问题。

Method: 通过分析Reddit分享YouTube视频的大数据集，并进行多阶段实验，隔离文本变化的影响，同时使用BERT分类器进行预测实验。

Result: 改写标题显著提升参与度，BERT分类器预测准确率达74%，优于GPT-4o等基线模型。

Conclusion: 研究为跨平台多模态内容策略提供了框架，验证了文本特征对参与度的影响。

Abstract: In today's cross-platform social media landscape, understanding factors that
drive engagement for multimodal content, especially text paired with visuals,
remains complex. This study investigates how rewriting Reddit post titles
adapted from YouTube video titles affects user engagement. First, we build and
analyze a large dataset of Reddit posts sharing YouTube videos, revealing that
21% of post titles are minimally modified. Statistical analysis demonstrates
that title rewrites measurably improve engagement. Second, we design a
controlled, multi-phase experiment to rigorously isolate the effects of textual
variations by neutralizing confounding factors like video popularity, timing,
and community norms. Comprehensive statistical tests reveal that effective
title rewrites tend to feature emotional resonance, lexical richness, and
alignment with community-specific norms. Lastly, pairwise ranking prediction
experiments using a fine-tuned BERT classifier achieves 74% accuracy,
significantly outperforming near-random baselines, including GPT-4o. These
results validate that our controlled dataset effectively minimizes confounding
effects, allowing advanced models to both learn and demonstrate the impact of
textual features on engagement. By bridging quantitative rigor with qualitative
insights, this study uncovers engagement dynamics and offers a robust framework
for future cross-platform, multimodal content strategies.

</details>

### [234] [Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics](https://arxiv.org/abs/2505.03795)
*Jacob W. Crandall,Jonathan Skaggs*

Main category: cs.SI

TLDR: 论文比较了多种建模方法在战略网络游戏中的表现，发现基于社区感知行为和分布建模的hCAB方法表现最佳，能准确模拟人类行为。


<details>
  <summary>Details</summary>
Motivation: 理解人类网络对社会不平等等问题的影响，需要更好的行为建模方法。

Method: 比较不同假设（行为匹配vs.社区感知行为）和统计建模（均值vs.分布）的方法，评估其在Junior High Game中的表现。

Result: hCAB方法（社区感知行为+分布建模）表现最佳，能准确模拟人类群体行为，且人类参与者难以区分hCAB代理与真人。

Conclusion: hCAB方法在战略网络游戏中能有效模拟人类行为，为理解人类网络提供了新工具。

Abstract: Human networks greatly impact important societal outcomes, including wealth
and health inequality, poverty, and bullying. As such, understanding human
networks is critical to learning how to promote favorable societal outcomes. As
a step toward better understanding human networks, we compare and contrast
several methods for learning models of human behavior in a strategic network
game called the Junior High Game (JHG). These modeling methods differ with
respect to the assumptions they use to parameterize human behavior (behavior
vs. community-aware behavior) and the statistical moments they model (mean vs.
distribution). Results show that the highest-performing method models the
population's distribution rather than the mean and assumes humans use
community-aware behavior rather than behavior matching. When applied to small
societies (6-11 individuals), this learned model, called hCAB, closely mirrors
the population dynamics of human groups (with some differences). Additionally,
a user study reveals that human participants were unable to distinguish hCAB
agents from other humans, thus illustrating that individual hCAB behavior
plausibly mirrors human behavior in this strategic network game.

</details>

### [235] [Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries](https://arxiv.org/abs/2505.03816)
*Bidyarthi Paul,Fariha Tasnim Chowdhury,Dipta Biswas,Meherin Sultana*

Main category: cs.SI

TLDR: 该研究通过分析纽约和达卡的交通数据，探索需求趋势、高峰时段和热点区域，使用EDA、SARIMAX模型和聚类技术，为优化城市交通和配送服务提供见解。


<details>
  <summary>Details</summary>
Motivation: 研究城市交通模式对现代生活的重要性，旨在通过数据分析优化资源分配和服务效率。

Method: 采用EDA进行初步分析，结合SARIMAX模型进行时间序列预测，并应用聚类技术识别高需求和低需求区域。

Result: 识别出需求趋势、高峰时段和热点区域，为优化车队管理和资源分配提供依据。

Conclusion: 研究结果为提升城市交通和配送服务效率提供了实用见解，适用于多样化城市环境。

Abstract: Urban transportation plays a vital role in modern city life, affecting how
efficiently people and goods move around. This study analyzes transportation
patterns using two datasets: the NYC Taxi Trip dataset from New York City and
the Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify
key trends in demand, peak times, and important geographical hotspots. We start
with Exploratory Data Analysis (EDA) to understand the basic characteristics of
the datasets. Next, we perform geospatial analysis to map out high-demand and
low-demand regions. We use the SARIMAX model for time series analysis to
forecast demand patterns, capturing seasonal and weekly variations. Lastly, we
apply clustering techniques to identify significant areas of high and low
demand. Our findings provide valuable insights for optimizing fleet management
and resource allocation in both passenger transport and food delivery services.
These insights can help improve service efficiency, better meet customer needs,
and enhance urban transportation systems in diverse urban environments.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [236] [Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective](https://arxiv.org/abs/2505.03828)
*Yogesh Gajula*

Main category: cs.IR

TLDR: 本文综述了2023至2025年初基于情感分析的电商推荐系统，探讨了如何利用文本情感提升推荐准确性和可解释性，总结了四种主要方法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 电商平台用户反馈中的文本情感信息常被忽视，而情感分析能显著提升推荐系统的性能和用户体验。

Method: 综述了四种方法：深度学习分类器、基于Transformer的特征提取、图神经网络传播情感信号、以及实时响应的对话推荐系统。

Result: 展示了情感分析如何融入推荐流程，并分析了模型架构及其对推荐结果的影响。

Conclusion: 未来需解决文本噪声、动态偏好和偏见问题，以开发更智能、公平、用户中心的推荐工具。

Abstract: E-commerce platforms generate vast volumes of user feedback, such as star
ratings, written reviews, and comments. However, most recommendation engines
rely primarily on numerical scores, often overlooking the nuanced opinions
embedded in free text. This paper comprehensively reviews sentiment-aware
recommendation systems from a natural language processing perspective, covering
advancements from 2023 to early 2025. It highlights the benefits of integrating
sentiment analysis into e-commerce recommenders to enhance prediction accuracy
and explainability through detailed opinion extraction. Our survey categorizes
recent work into four main approaches: deep learning classifiers that combine
sentiment embeddings with user item interactions, transformer based methods for
nuanced feature extraction, graph neural networks that propagate sentiment
signals, and conversational recommenders that adapt in real time to user
feedback. We summarize model architectures and demonstrate how sentiment flows
through recommendation pipelines, impacting dialogue-based suggestions. Key
challenges include handling noisy or sarcastic text, dynamic user preferences,
and bias mitigation. Finally, we outline research gaps and provide a roadmap
for developing smarter, fairer, and more user-centric recommendation tools.

</details>

### [237] [Memory Assisted LLM for Personalized Recommendation System](https://arxiv.org/abs/2505.03824)
*Jiarui Chen*

Main category: cs.IR

TLDR: 论文提出了一种基于记忆辅助的个性化LLM（MAP），通过用户历史记录和相似性提取相关记忆，提升个性化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉多样化用户偏好或及时更新用户历史方面存在不足，MAP旨在解决这些问题。

Method: 通过用户交互创建历史档案，提取相关记忆并融入提示，增强个性化推荐。

Result: MAP在单领域和跨领域任务中均优于传统LLM推荐方法，且随着用户历史增长优势更明显。

Conclusion: MAP适合处理连续的个性化用户请求，是一种高效且可扩展的解决方案。

Abstract: Large language models (LLMs) have demonstrated significant potential in
solving recommendation tasks. With proven capabilities in understanding user
preferences, LLM personalization has emerged as a critical area for providing
tailored responses to individuals. Current studies explore personalization
through prompt design and fine-tuning, paving the way for further research in
personalized LLMs. However, existing approaches are either costly and
inefficient in capturing diverse user preferences or fail to account for timely
updates to user history. To address these gaps, we propose the Memory-Assisted
Personalized LLM (MAP). Through user interactions, we first create a history
profile for each user, capturing their preferences, such as ratings for
historical items. During recommendation, we extract relevant memory based on
similarity, which is then incorporated into the prompts to enhance personalized
recommendations. In our experiments, we evaluate MAP using a sequential rating
prediction task under two scenarios: single domain, where memory and tasks are
from the same category (e.g., movies), and cross-domain (e.g., memory from
movies and recommendation tasks in books). The results show that MAP
outperforms regular LLM-based recommenders that integrate user history directly
through prompt design. Moreover, as user history grows, MAP's advantage
increases in both scenarios, making it more suitable for addressing successive
personalized user requests.

</details>

### [238] [OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery](https://arxiv.org/abs/2505.03836)
*Chongsheng Zhang,Shuwen Wu,Yingqi Chen,Matthias Aßenmacher,Christian Heumann,Yi Men,Gaojuan Fan,João Gama*

Main category: cs.IR

TLDR: 提出了一种渐进式甲骨文重复发现框架，结合无监督低层关键点匹配和高层文本中心内容匹配，提升语义感知和可解释性。


<details>
  <summary>Details</summary>
Motivation: 甲骨文重复识别是甲骨文研究的基础问题，现有方法存在效率和准确性不足的问题。

Method: 结合低层关键点匹配和高层文本内容匹配的渐进式框架。

Result: 在Top-5和Top-15检索结果中表现优异，计算效率显著提升，并发现60多对新重复甲骨文。

Conclusion: 该方法在效率和准确性上优于现有技术，为甲骨文研究提供了新工具。

Abstract: Oracle Bone Inscription (OBI) is the earliest systematic writing system in
China, while the identification of Oracle Bone (OB) duplicates is a fundamental
issue in OBI research. In this work, we design a progressive OB duplicate
discovery framework that combines unsupervised low-level keypoints matching
with high-level text-centric content-based matching to refine and rank the
candidate OB duplicates with semantic awareness and interpretability. We
compare our approach with state-of-the-art content-based image retrieval and
image matching methods, showing that our approach yields comparable recall
performance and the highest simplified mean reciprocal rank scores for both
Top-5 and Top-15 retrieval results, and with significantly accelerated
computation efficiency. We have discovered over 60 pairs of new OB duplicates
in real-world deployment, which were missed by OBI researchers for decades. The
models, video illustration and demonstration of this work are available at:
https://github.com/cszhangLMU/OBD-Finder/.

</details>

### [239] [CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation](https://arxiv.org/abs/2505.03840)
*Cairong Yan,Jinyi Han,Jin Ju,Yanting Zhang,Zijian Wang,Xuan Shao*

Main category: cs.IR

TLDR: CoCoB算法通过双面老虎机架构改进推荐系统，动态适应用户相似性，提升推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有聚类老虎机方法在定义相似用户和处理独特偏好用户时存在不足，影响推荐效果。

Method: 提出CoCoB算法，结合用户和物品双面老虎机架构，利用贝叶斯模型探索用户相似性，动态调整推荐策略。

Result: 实验显示CoCoB在三个真实数据集上F1分数平均提升2.4%。

Conclusion: CoCoB有效解决了用户相似性定义和独特偏好处理问题，提升了推荐系统的性能。

Abstract: Clustering bandits have gained significant attention in recommender systems
by leveraging collaborative information from neighboring users to better
capture target user preferences. However, these methods often lack a clear
definition of similar users and face challenges when users with unique
preferences lack appropriate neighbors. In such cases, relying on divergent
preferences of misidentified neighbors can degrade recommendation quality. To
address these limitations, this paper proposes an adaptive Collaborative
Combinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided
bandit architecture, applying bandit principles to both the user and item
sides. The user-bandit employs an enhanced Bayesian model to explore user
similarity, identifying neighbors based on a similarity probability threshold.
The item-bandit treats items as arms, generating diverse recommendations
informed by the user-bandit's output. CoCoB dynamically adapts, leveraging
neighbor preferences when available or focusing solely on the target user
otherwise. Regret analysis under a linear contextual bandit setting and
experiments on three real-world datasets demonstrate CoCoB's effectiveness,
achieving an average 2.4% improvement in F1 score over state-of-the-art
methods.

</details>

### [240] [To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay](https://arxiv.org/abs/2505.04209)
*Soumik Dey,Hansi Wu,Binbin Li*

Main category: cs.IR

TLDR: 论文探讨了电商广告关键词相关性的问题，提出通过人类判断和LLM作为代理来优化模型，以实现卖家、广告和搜索系统的和谐。


<details>
  <summary>Details</summary>
Motivation: 广告关键词的相关性对防止搜索系统过载和维护卖家体验至关重要，但现有基于点击/销售信号的模型与人类判断不一致。

Method: 将广告关键词相关性视为卖家判断、广告和搜索三个动态系统的交互，并通过LLM作为人类判断的代理来训练模型。

Result: 使用LLM作为代理并结合业务指标评估框架，能更好地协调三个系统。

Conclusion: 通过人类判断和LLM代理的结合，可以优化广告关键词相关性模型，提升系统整体效果。

Abstract: E-commerce sellers are recommended keyphrases based on their inventory on
which they advertise to increase buyer engagement (clicks/sales). The relevance
of advertiser keyphrases plays an important role in preventing the inundation
of search systems with numerous irrelevant items that compete for attention in
auctions, in addition to maintaining a healthy seller perception. In this work,
we describe the shortcomings of training Advertiser keyphrase relevance filter
models on click/sales/search relevance signals and the importance of aligning
with human judgment, as sellers have the power to adopt or reject said
keyphrase recommendations. In this study, we frame Advertiser keyphrase
relevance as a complex interaction between 3 dynamical systems -- seller
judgment, which influences seller adoption of our product, Advertising, which
provides the keyphrases to bid on, and Search, who holds the auctions for the
same keyphrases. This study discusses the practicalities of using human
judgment via a case study at eBay Advertising and demonstrate that using
LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our
relevance models achieves a better harmony across the three systems -- provided
that they are bound by a meticulous evaluation framework grounded in business
metrics.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [241] [Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts](https://arxiv.org/abs/2505.04171)
*Nouar Aldahoul,Hazem Ibrahim,Matteo Varvello,Aaron Kaufman,Talal Rahwan,Yasir Zaki*

Main category: cs.CY

TLDR: 研究发现，大型语言模型（LLMs）的政治偏见并非表面上的轻微，而是由极端观点抵消后的结果，且能显著影响用户的政治倾向。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs的政治偏见及其对用户政治倾向的实际影响，挑战现有认为偏见较小的观点。

Method: 通过比较31个LLMs与立法者、法官及美国选民样本，并进行随机实验，分析LLMs的政治偏见及其说服力。

Result: LLMs的政治偏见由极端观点抵消后显得轻微，但其在信息寻求情境中能显著影响用户政治倾向（高达5个百分点）。

Conclusion: LLMs可能成为政治影响力的强大工具，需警惕其由私企或政府控制时的潜在风险。

Abstract: Large Language Models (LLMs) are a transformational technology, fundamentally
changing how people obtain information and interact with the world. As people
become increasingly reliant on them for an enormous variety of tasks, a body of
academic research has developed to examine these models for inherent biases,
especially political biases, often finding them small. We challenge this
prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a
nationally representative sample of U.S. voters, we show that LLMs' apparently
small overall partisan preference is the net result of offsetting extreme views
on specific topics, much like moderate voters. Second, in a randomized
experiment, we show that LLMs can promulgate their preferences into political
persuasiveness even in information-seeking contexts: voters randomized to
discuss political issues with an LLM chatbot are as much as 5 percentage points
more likely to express the same preferences as that chatbot. Contrary to
expectations, these persuasive effects are not moderated by familiarity with
LLMs, news consumption, or interest in politics. LLMs, especially those
controlled by private companies or governments, may become a powerful and
targeted vector for political influence.

</details>

### [242] [Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators](https://arxiv.org/abs/2505.03859)
*Will Hawkins,Chris Russell,Brent Mittelstadt*

Main category: cs.CY

TLDR: 研究分析了在线可获取的深度伪造模型变体的可访问性，发现其数量激增且下载量巨大，主要针对女性，亟需采取行动应对。


<details>
  <summary>Details</summary>
Motivation: 探讨文本到图像（T2I）模型带来的风险，特别是深度伪造模型的可访问性及其潜在危害。

Method: 通过分析Hugging Face和Civitai两个平台上的公开可下载模型变体的元数据，研究深度伪造模型的流行情况。

Result: 发现近35,000个公开可下载的深度伪造模型变体，下载量达1,500万次，96%针对女性，且多用于生成非自愿亲密图像（NCII）。

Conclusion: 尽管平台政策和法规试图阻止，但深度伪造模型的广泛传播表明亟需更严厉的措施应对此类风险。

Abstract: Advances in multimodal machine learning have made text-to-image (T2I) models
increasingly accessible and popular. However, T2I models introduce risks such
as the generation of non-consensual depictions of identifiable individuals,
otherwise known as deepfakes. This paper presents an empirical study exploring
the accessibility of deepfake model variants online. Through a metadata
analysis of thousands of publicly downloadable model variants on two popular
repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily
accessible deepfake models. Almost 35,000 examples of publicly downloadable
deepfake model variants are identified, primarily hosted on Civitai. These
deepfake models have been downloaded almost 15 million times since November
2022, with the models targeting a range of individuals from global celebrities
to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux
models are used for the creation of deepfake models, with 96% of these
targeting women and many signalling intent to generate non-consensual intimate
imagery (NCII). Deepfake model variants are often created via the
parameter-efficient fine-tuning technique known as low rank adaptation (LoRA),
requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this
process widely accessible via consumer-grade computers. Despite these models
violating the Terms of Service of hosting platforms, and regulation seeking to
prevent dissemination, these results emphasise the pressing need for greater
action to be taken against the creation of deepfakes and NCII.

</details>

### [243] [Coverage Biases in High-Resolution Satellite Imagery](https://arxiv.org/abs/2505.03842)
*Vadim Musienko,Axel Jacquet,Ingmar Weber,Till Koebe*

Main category: cs.CY

TLDR: 卫星图像覆盖存在地理和社会经济偏见，远离赤道地区更频繁被访问，欠发达地区历史图像较少，地缘政治事件也影响图像可用性。


<details>
  <summary>Details</summary>
Motivation: 探讨卫星图像是否在全球范围内公平分布，揭示覆盖偏见及其原因。

Method: 分析卫星轨道数据评估未来30天覆盖频率，收集历史图像元数据研究可用性，并通过三个冲突地区案例研究地缘政治影响。

Result: 远离赤道地区覆盖更频繁，欠发达地区历史图像较少，地缘政治事件影响图像可用性。

Conclusion: 卫星图像的数字化红利在全球范围内分布不均，受物理、社会经济和地缘政治因素影响。

Abstract: Satellite imagery is increasingly used to complement traditional data
collection approaches such as surveys and censuses across scientific
disciplines. However, we ask: Do all places on earth benefit equally from this
new wealth of information? In this study, we investigate coverage bias of major
satellite constellations that provide optical satellite imagery with a ground
sampling distance below 10 meters, evaluating both the future on-demand tasking
opportunities as well as the availability of historic images across the globe.
Specifically, forward-looking, we estimate how often different places are
revisited during a window of 30 days based on the satellites' orbital paths,
thus investigating potential coverage biases caused by physical factors. We
find that locations farther away from the equator are generally revisited more
frequently by the constellations under study. Backward-looking, we show that
historic satellite image availability -- based on metadata collected from major
satellite imagery providers -- is influenced by socio-economic factors on the
ground: less developed, less populated places have less satellite images
available. Furthermore, in three small case studies on recent conflict regions
in this world, namely Gaza, Sudan and Ukraine, we show that also geopolitical
events play an important role in satellite image availability, hinting at
underlying business model decisions. These insights lay bare that the digital
dividend yielded by satellite imagery is not equally distributed across our
planet.

</details>

### [244] [AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions](https://arxiv.org/abs/2505.04592)
*Peter Barnett,Aaron Scher*

Main category: cs.CY

TLDR: 论文探讨了AI发展可能导致的人类灭绝风险，并提出了四种应对策略，强调需紧急行动以避免灾难。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示AI发展的战略格局，并列出关键治理问题，以减少潜在的灾难性风险。

Method: 通过描述四种AI发展的地缘政治应对情景，分析每种情景的相关研究问题。

Result: 除“关闭开关与暂停”情景外，其他策略均可能带来不可接受的灾难风险。

Conclusion: 呼吁美国国家安全社区和AI治理生态系统采取紧急行动，以应对AI发展的潜在威胁。

Abstract: Humanity appears to be on course to soon develop AI systems that
substantially outperform human experts in all cognitive domains and activities.
We believe the default trajectory has a high likelihood of catastrophe,
including human extinction. Risks come from failure to control powerful AI
systems, misuse of AI by malicious rogue actors, war between great powers, and
authoritarian lock-in. This research agenda has two aims: to describe the
strategic landscape of AI development and to catalog important governance
research questions. These questions, if answered, would provide important
insight on how to successfully reduce catastrophic risks.
  We describe four high-level scenarios for the geopolitical response to
advanced AI development, cataloging the research questions most relevant to
each. Our favored scenario involves building the technical, legal, and
institutional infrastructure required to internationally restrict dangerous AI
development and deployment (which we refer to as an Off Switch), which leads
into an internationally coordinated Halt on frontier AI activities at some
point in the future. The second scenario we describe is a US National Project
for AI, in which the US Government races to develop advanced AI systems and
establish unilateral control over global AI development. We also describe two
additional scenarios: a Light-Touch world similar to that of today and a Threat
of Sabotage situation where countries use sabotage and deterrence to slow AI
development.
  In our view, apart from the Off Switch and Halt scenario, all of these
trajectories appear to carry an unacceptable risk of catastrophic harm. Urgent
action is needed from the US National Security community and AI governance
ecosystem to answer key research questions, build the capability to halt
dangerous AI activities, and prepare for international AI agreements.

</details>

<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [245] [Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach](https://arxiv.org/abs/2505.03760)
*Arishi Orra,Aryan Bhambu,Himanshu Choudhary,Manoj Thakur,Selvaraju Natarajan*

Main category: q-fin.PM

TLDR: 该研究提出了一种基于波动率引导的深度强化学习（DRL）投资组合优化框架，结合投资者风险偏好动态构建投资组合，并利用GARCH模型进行波动率预测。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化需平衡风险与回报，而AI技术（如DRL）提供了自适应策略，但资产预选对性能至关重要。研究旨在结合投资者偏好优化策略。

Method: 使用GARCH模型预测股票波动率并分类，DRL代理通过历史市场数据学习最优投资策略。

Result: 基于Dow 30指数的实验表明，该框架在风险调整后收益上优于基准策略。

Conclusion: 波动率引导的DRL框架能有效结合投资者风险偏好，提升投资组合性能。

Abstract: Portfolio optimization requires dynamic allocation of funds by balancing the
risk and return tradeoff under dynamic market conditions. With the recent
advancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in
providing adaptive and scalable strategies for portfolio optimization. However,
the success of these strategies depends not only on their ability to adapt to
market dynamics but also on the careful pre-selection of assets that influence
overall portfolio performance. Incorporating the investor's preference in
pre-selecting assets for a portfolio is essential in refining their investment
strategies. This study proposes a volatility-guided DRL-based portfolio
optimization framework that dynamically constructs portfolios based on
investors' risk profiles. The Generalized Autoregressive Conditional
Heteroscedasticity (GARCH) model is utilized for volatility forecasting of
stocks and categorizes them based on their volatility as aggressive, moderate,
and conservative. The DRL agent is then employed to learn an optimal investment
policy by interacting with the historical market data. The efficacy of the
proposed methodology is established using stocks from the Dow $30$ index. The
proposed investor-specific DRL-based portfolios outperformed the baseline
strategies by generating consistent risk-adjusted returns.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [246] [OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation](https://arxiv.org/abs/2505.03912)
*Can Cui,Pengxiang Ding,Wenxuan Song,Shuanghao Bai,Xinyang Tong,Zirui Ge,Runze Suo,Wanqi Zhou,Yang Liu,Bofang Jia,Han Zhao,Siteng Huang,Donglin Wang*

Main category: cs.RO

TLDR: 本文总结了现有双系统VLA架构的设计，并通过系统实验评估其核心设计元素，最终提供一个低成本开源模型供进一步研究。


<details>
  <summary>Details</summary>
Motivation: 解决双系统VLA架构开源工作不足的问题，促进性能分析与优化。

Method: 总结并比较现有双系统架构的结构设计，进行系统性实验评估。

Result: 提供了一个低成本的开源模型，并承诺持续更新实验结论和改进模型。

Conclusion: 通过开源模型和持续更新，推动双系统VLA架构的进一步研究和优化。

Abstract: Dual-system VLA (Vision-Language-Action) architectures have become a hot
topic in embodied intelligence research, but there is a lack of sufficient
open-source work for further performance analysis and optimization. To address
this problem, this paper will summarize and compare the structural designs of
existing dual-system architectures, and conduct systematic empirical
evaluations on the core design elements of existing dual-system architectures.
Ultimately, it will provide a low-cost open-source model for further
exploration. Of course, this project will continue to update with more
experimental conclusions and open-source models with improved performance for
everyone to choose from. Project page: https://openhelix-robot.github.io/.

</details>

### [247] [Scalable Aerial GNSS Localization for Marine Robots](https://arxiv.org/abs/2505.04095)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Charlotte Morissette,Chloe Si,Bobak Baghi,Gregory Dudek*

Main category: cs.RO

TLDR: 提出了一种利用配备GNSS的无人机定位水面附近海洋机器人的新方法，解决了传统GNSS在水上定位的困难。


<details>
  <summary>Details</summary>
Motivation: 传统GNSS在水上定位效果差且成本高，现有方法存在误差累积和计算复杂度高的问题。

Method: 使用配备GNSS的无人机跟踪和定位水面附近的海洋机器人。

Result: 该方法实现了单机器人及多机器人的高精度定位。

Conclusion: 无人机辅助定位是一种高效且可扩展的水上机器人定位解决方案。

Abstract: Accurate localization is crucial for water robotics, yet traditional onboard
Global Navigation Satellite System (GNSS) approaches are difficult or
ineffective due to signal reflection on the water's surface and its high cost
of aquatic GNSS receivers. Existing approaches, such as inertial navigation,
Doppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face
challenges like error accumulation and high computational complexity.
Therefore, a more efficient and scalable solution remains necessary. This paper
proposes an alternative approach that leverages an aerial drone equipped with
GNSS localization to track and localize a marine robot once it is near the
surface of the water. Our results show that this novel adaptation enables
accurate single and multi-robot marine robot localization.

</details>

### [248] [RGB-Event Fusion with Self-Attention for Collision Prediction](https://arxiv.org/abs/2505.04258)
*Pietro Bonazzi,Christian Vogt,Michael Jost,Haotong Qin,Lyes Khacef,Federico Paredes-Valles,Michele Magno*

Main category: cs.RO

TLDR: 提出了一种基于RGB和事件视觉传感器的神经网络框架，用于预测无人机与动态物体的碰撞时间和位置，通过自注意力融合提高准确性，但计算成本较高。


<details>
  <summary>Details</summary>
Motivation: 确保自主机器人在动态环境中的实时避障安全。

Method: 采用双编码器分支（RGB和事件视觉传感器），通过自注意力融合，并在ABCD数据集上验证。

Result: 融合模型在50Hz下平均精度提升1%，远距离提升10%，但内存和计算成本显著增加；事件模型在相似计算成本下优于RGB模型。

Conclusion: 多模态感知（RGB+事件）在机器人应用中需权衡精度与计算效率。

Abstract: Ensuring robust and real-time obstacle avoidance is critical for the safe
operation of autonomous robots in dynamic, real-world environments. This paper
proposes a neural network framework for predicting the time and collision
position of an unmanned aerial vehicle with a dynamic object, using RGB and
event-based vision sensors. The proposed architecture consists of two separate
encoder branches, one for each modality, followed by fusion by self-attention
to improve prediction accuracy. To facilitate benchmarking, we leverage the
ABCD [8] dataset collected that enables detailed comparisons of single-modality
and fusion-based approaches. At the same prediction throughput of 50Hz, the
experimental results show that the fusion-based model offers an improvement in
prediction accuracy over single-modality approaches of 1% on average and 10%
for distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%
in FLOPs. Notably, the event-based model outperforms the RGB model by 4% for
position and 26% for time error at a similar computational cost, making it a
competitive alternative. Additionally, we evaluate quantized versions of the
event-based models, applying 1- to 8-bit quantization to assess the trade-offs
between predictive performance and computational efficiency. These findings
highlight the trade-offs of multi-modal perception using RGB and event-based
cameras in robotic applications.

</details>

### [249] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TLDR: 本文探讨了基于模型的规划与执行系统在机器人任务控制中的设计选择、解决方案及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 研究如何构建灵活自主的机器人系统，通过结合基本技能完成多样化任务。

Method: 回顾现有系统的设计选择与解决方案，分析其优缺点。

Result: 总结了当前系统的多样性，并提出了未来发展的方向。

Conclusion: 基于模型的系统在机器人任务控制中具有潜力，但仍需进一步研究以优化设计。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>

### [250] [Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees](https://arxiv.org/abs/2505.04583)
*Nathaniel Dennler,Zhonghao Shi,Uksang Yoo,Stefanos Nikolaidis,Maja Matarić*

Main category: cs.RO

TLDR: 本文提出了一种基于因果树的方法，用于根据用户表现动态调整康复机器人训练难度，以个性化提升康复效果和用户动机。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设所有用户的训练难度相同，但研究发现中风患者对训练难度的感知存在个体差异，因此需要一种更个性化的方法。

Method: 采用因果树模型，根据用户表现动态计算训练难度，并提供可解释的模型。

Result: 该方法能准确建模训练难度，并为用户和护理人员提供直观的解释。

Conclusion: 个性化调整训练难度的方法能有效提升康复效果和用户动机，且模型具有可解释性。

Abstract: Rehabilitation robots are often used in game-like interactions for
rehabilitation to increase a person's motivation to complete rehabilitation
exercises. By adjusting exercise difficulty for a specific user throughout the
exercise interaction, robots can maximize both the user's rehabilitation
outcomes and the their motivation throughout the exercise. Previous approaches
have assumed exercises have generic difficulty values that apply to all users
equally, however, we identified that stroke survivors have varied and unique
perceptions of exercise difficulty. For example, some stroke survivors found
reaching vertically more difficult than reaching farther but lower while others
found reaching farther more challenging than reaching vertically. In this
paper, we formulate a causal tree-based method to calculate exercise difficulty
based on the user's performance. We find that this approach accurately models
exercise difficulty and provides a readily interpretable model of why that
exercise is difficult for both users and caretakers.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [251] [TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models](https://arxiv.org/abs/2505.04050)
*Kazuki Higo,Toshiki Kanai,Yuki Endo,Yoshihiro Kanamori*

Main category: cs.GR

TLDR: 提出了一种基于潜在扩散模型的联合生成地形高度图和纹理的方法，通过无监督学习和有监督适配器实现用户控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独生成高度图或纹理，未充分捕捉二者相关性，影响真实感。

Method: 使用潜在扩散模型无监督生成配对的随机高度图和纹理，并通过有监督适配器实现用户草图控制。

Result: 实验表明，该方法能直观生成地形，同时保持高度图与纹理的关联性。

Conclusion: 该方法有效解决了地形生成中高度图与纹理的关联问题，提升了真实感和用户控制性。

Abstract: 3D terrain models are essential in fields such as video game development and
film production. Since surface color often correlates with terrain geometry,
capturing this relationship is crucial to achieving realism. However, most
existing methods generate either a heightmap or a texture, without sufficiently
accounting for the inherent correlation. In this paper, we propose a method
that jointly generates terrain heightmaps and textures using a latent diffusion
model. First, we train the model in an unsupervised manner to randomly generate
paired heightmaps and textures. Then, we perform supervised learning of an
external adapter to enable user control via hand-drawn sketches. Experiments
show that our approach allows intuitive terrain generation while preserving the
correlation between heightmaps and textures.

</details>

### [252] [Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control](https://arxiv.org/abs/2505.04052)
*Shun Masuda,Yuki Endo,Yoshihiro Kanamori*

Main category: cs.GR

TLDR: 论文提出两种方法，通过3D人体模型控制姿势并利用潜在扩散模型合成人物，解决现有方法在遮挡和深度放置上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人物插入场景时无法自然处理遮挡且缺乏姿势控制，导致效果不自然。

Method: 提出两种方法：1) 两阶段方法，通过监督学习生成深度图后合成人物；2) 直接隐式学习遮挡并从输入数据合成人物。

Result: 定量和定性评估显示，两种方法在场景一致性和遮挡处理上优于现有方法。

Conclusion: 新方法通过3D姿势控制和深度合成，显著提升了人物插入场景的自然性和可控性。

Abstract: Compositing human figures into scene images has broad applications in areas
such as entertainment and advertising. However, existing methods often cannot
handle occlusion of the inserted person by foreground objects and unnaturally
place the person in the frontmost layer. Moreover, they offer limited control
over the inserted person's pose. To address these challenges, we propose two
methods. Both allow explicit pose control via a 3D body model and leverage
latent diffusion models to synthesize the person at a contextually appropriate
depth, naturally handling occlusions without requiring occlusion masks. The
first is a two-stage approach: the model first learns a depth map of the scene
with the person through supervised learning, and then synthesizes the person
accordingly. The second method learns occlusion implicitly and synthesizes the
person directly from input data without explicit depth supervision.
Quantitative and qualitative evaluations show that both methods outperform
existing approaches by better preserving scene consistency while accurately
reflecting occlusions and user-specified poses.

</details>

### [253] [Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control](https://arxiv.org/abs/2505.04387)
*Amin Fadaeinejad,Abdallah Dib,Luiz Gustavo Hafemann,Emeline Got,Trevor Anderson,Amaury Depierre,Nikolaus F. Troje,Marcus A. Brubaker,Marc-André Carbonneau*

Main category: cs.GR

TLDR: 提出了一种简化3D头部建模的框架，通过几何感知的纹理合成技术，提供多层次的艺术控制，包括整体几何、肤色调整和细节编辑。


<details>
  <summary>Details</summary>
Motivation: 解决3D头部建模中艺术控制不足和流程繁琐的问题，满足艺术家对精确艺术表现的需求。

Method: 采用几何感知的纹理合成管道，学习头部几何与皮肤纹理的关联，支持多层次编辑。

Result: 实验表明，该方法能生成多样化的结果，并提供直观的艺术控制，如肤色调整和细节编辑。

Conclusion: 该框架优化了虚拟角色创作的流程，提升了艺术家的控制力和效率。

Abstract: Creating realistic 3D head assets for virtual characters that match a precise
artistic vision remains labor-intensive. We present a novel framework that
streamlines this process by providing artists with intuitive control over
generated 3D heads. Our approach uses a geometry-aware texture synthesis
pipeline that learns correlations between head geometry and skin texture maps
across different demographics. The framework offers three levels of artistic
control: manipulation of overall head geometry, adjustment of skin tone while
preserving facial characteristics, and fine-grained editing of details such as
wrinkles or facial hair. Our pipeline allows artists to make edits to a single
texture map using familiar tools, with our system automatically propagating
these changes coherently across the remaining texture maps needed for realistic
rendering. Experiments demonstrate that our method produces diverse results
with clean geometries. We showcase practical applications focusing on intuitive
control for artists, including skin tone adjustments and simplified editing
workflows for adding age-related details or removing unwanted features from
scanned models. This integrated approach aims to streamline the artistic
workflow in virtual character creation.

</details>

### [254] [PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers](https://arxiv.org/abs/2505.04002)
*Michael Xu,Yi Shi,KangKang Yin,Xue Bin Peng*

Main category: cs.GR

TLDR: PARC框架通过机器学习和物理模拟迭代增强运动数据集，解决敏捷地形穿越控制器开发中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 人类在复杂环境中的敏捷运动能力难以通过模拟复现，主要因敏捷地形穿越行为的数据稀缺且获取成本高。

Method: PARC结合运动生成器和物理跟踪控制器，迭代生成并修正合成数据，扩展运动数据集。

Result: PARC成功开发出敏捷且多功能的控制器，弥补了运动数据稀缺与多功能控制器需求之间的差距。

Conclusion: PARC为敏捷地形穿越控制器的开发提供了有效方法，通过迭代增强数据提升了模型的性能。

Abstract: Humans excel in navigating diverse, complex environments with agile motor
skills, exemplified by parkour practitioners performing dynamic maneuvers, such
as climbing up walls and jumping across gaps. Reproducing these agile movements
with simulated characters remains challenging, in part due to the scarcity of
motion capture data for agile terrain traversal behaviors and the high cost of
acquiring such data. In this work, we introduce PARC (Physics-based
Augmentation with Reinforcement Learning for Character Controllers), a
framework that leverages machine learning and physics-based simulation to
iteratively augment motion datasets and expand the capabilities of terrain
traversal controllers. PARC begins by training a motion generator on a small
dataset consisting of core terrain traversal skills. The motion generator is
then used to produce synthetic data for traversing new terrains. However, these
generated motions often exhibit artifacts, such as incorrect contacts or
discontinuities. To correct these artifacts, we train a physics-based tracking
controller to imitate the motions in simulation. The corrected motions are then
added to the dataset, which is used to continue training the motion generator
in the next iteration. PARC's iterative process jointly expands the
capabilities of the motion generator and tracker, creating agile and versatile
models for interacting with complex environments. PARC provides an effective
approach to develop controllers for agile terrain traversal, which bridges the
gap between the scarcity of motion data and the need for versatile character
controllers.

</details>

### [255] [TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization](https://arxiv.org/abs/2505.04590)
*Alexandre Binninger,Ruben Wiersma,Philipp Herholz,Olga Sorkine-Hornung*

Main category: cs.GR

TLDR: TetWeave是一种新型等值面表示方法，通过联合优化四面体网格和方向有符号距离，实现高质量、自适应的网格生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统预定义网格在灵活性和内存效率上的不足，提供更高效的网格优化方法。

Method: 使用Delaunay三角剖分动态构建四面体网格，结合方向有符号距离优化，支持动态重采样和网格公平性。

Result: 生成的网格具有水密性、二维流形和无交叉特性，内存占用低且参数少，适用于多种图形和视觉任务。

Conclusion: TetWeave在灵活性和内存效率上优于传统方法，适用于广泛的计算机图形和视觉应用。

Abstract: We introduce TetWeave, a novel isosurface representation for gradient-based
mesh optimization that jointly optimizes the placement of a tetrahedral grid
used for Marching Tetrahedra and a novel directional signed distance at each
point. TetWeave constructs tetrahedral grids on-the-fly via Delaunay
triangulation, enabling increased flexibility compared to predefined grids. The
extracted meshes are guaranteed to be watertight, two-manifold and
intersection-free. The flexibility of TetWeave enables a resampling strategy
that places new points where reconstruction error is high and allows to
encourage mesh fairness without compromising on reconstruction error. This
leads to high-quality, adaptive meshes that require minimal memory usage and
few parameters to optimize. Consequently, TetWeave exhibits near-linear memory
scaling relative to the vertex count of the output mesh - a substantial
improvement over predefined grids. We demonstrate the applicability of TetWeave
to a broad range of challenging tasks in computer graphics and vision, such as
multi-view 3D reconstruction, mesh compression and geometric texture
generation.

</details>

### [256] [PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer](https://arxiv.org/abs/2505.04622)
*Jingwen Ye,Yuze He,Yanning Zhou,Yiqin Zhu,Kaiwen Xiao,Yong-Jin Liu,Wei Yang,Xiao Han*

Main category: cs.GR

TLDR: PrimitiveAnything是一个新的框架，将形状基元抽象重新定义为基元组装生成任务，通过大规模人类制作的抽象学习，生成高质量且符合人类感知的基元组装。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语义理解或泛化能力上存在不足，无法适应多样化的形状类别。

Method: 提出形状条件基元变换器用于自回归生成，以及无歧义参数化方案统一表示多种基元类型。

Result: 实验表明，PrimitiveAnything能生成高质量且几何保真的基元组装，适用于多种3D应用。

Conclusion: 该框架在3D应用中表现优异，并有望支持基于基元的用户生成内容。

Abstract: Shape primitive abstraction, which decomposes complex 3D shapes into simple
geometric elements, plays a crucial role in human visual cognition and has
broad applications in computer vision and graphics. While recent advances in 3D
content generation have shown remarkable progress, existing primitive
abstraction methods either rely on geometric optimization with limited semantic
understanding or learn from small-scale, category-specific datasets, struggling
to generalize across diverse shape categories. We present PrimitiveAnything, a
novel framework that reformulates shape primitive abstraction as a primitive
assembly generation task. PrimitiveAnything includes a shape-conditioned
primitive transformer for auto-regressive generation and an ambiguity-free
parameterization scheme to represent multiple types of primitives in a unified
manner. The proposed framework directly learns the process of primitive
assembly from large-scale human-crafted abstractions, enabling it to capture
how humans decompose complex shapes into primitive elements. Through extensive
experiments, we demonstrate that PrimitiveAnything can generate high-quality
primitive assemblies that better align with human perception while maintaining
geometric fidelity across diverse shape categories. It benefits various 3D
applications and shows potential for enabling primitive-based user-generated
content (UGC) in games. Project page: https://primitiveanything.github.io

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [257] [EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.04623)
*Zhenghao Xing,Xiaowei Hu,Chi-Wing Fu,Wenhai Wang,Jifeng Dai,Pheng-Ann Heng*

Main category: eess.AS

TLDR: EchoInk-R1是一个基于强化学习的框架，旨在提升多模态大语言模型（MLLMs）在音频和视觉信号的结构化跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在音频和视觉信号的跨模态推理上表现不足，EchoInk-R1旨在解决这一问题。

Method: 基于Qwen2.5-Omni-7B模型，使用Group Relative Policy Optimization（GRPO）优化，针对同步音频-图像对的多选题问答任务。

Result: EchoInk-R1-7B在验证集上达到85.77%的准确率，优于基础模型的80.53%，仅需562步强化学习。

Conclusion: 轻量级强化学习微调可显著提升MLLMs的跨模态推理能力，EchoInk-R1是首个通过强化学习统一音频、视觉和文本模态的框架。

Abstract: Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.

</details>

### [258] [Recognizing Ornaments in Vocal Indian Art Music with Active Annotation](https://arxiv.org/abs/2505.04419)
*Sumit Kumar,Parampreet Singh,Vipul Arora*

Main category: eess.AS

TLDR: 论文介绍了ROD数据集和基于深度时间序列分析的装饰音检测模型，优于基线CRNN。


<details>
  <summary>Details</summary>
Motivation: 装饰音对音乐表达至关重要，但缺乏标注数据集和专门模型阻碍了研究进展。

Method: 使用ROD数据集和Human-in-the-Loop工具标注六种装饰音，开发基于深度时间序列分析的模型。

Result: 实验表明，该方法在ROD数据集和独立数据集上优于基线CRNN。

Conclusion: ROD数据集和提出的模型为装饰音检测提供了有效工具，推动了相关研究。

Abstract: Ornamentations, embellishments, or microtonal inflections are essential to
melodic expression across many musical traditions, adding depth, nuance, and
emotional impact to performances. Recognizing ornamentations in singing voices
is key to MIR, with potential applications in music pedagogy, singer
identification, genre classification, and controlled singing voice generation.
However, the lack of annotated datasets and specialized modeling approaches
remains a major obstacle for progress in this research area. In this work, we
introduce R\=aga Ornamentation Detection (ROD), a novel dataset comprising
Indian classical music recordings curated by expert musicians. The dataset is
annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked
as event-based labels. Using this dataset, we develop an ornamentation
detection model based on deep time-series analysis, preserving ornament
boundaries during the chunking of long audio recordings. We conduct experiments
using different train-test configurations within the ROD dataset and also
evaluate our approach on a separate, manually annotated dataset of Indian
classical concert recordings. Our experimental results support the superior
performance of our proposed approach over the baseline CRNN.

</details>

### [259] [Discrete Optimal Transport and Voice Conversion](https://arxiv.org/abs/2505.04382)
*Anton Selitskiy,Maitreya Kocharekar*

Main category: eess.AS

TLDR: 本文提出了一种基于向量接口的语音转换方法，采用离散最优传输映射对齐说话人音频嵌入，效果显著且高质量。


<details>
  <summary>Details</summary>
Motivation: 解决语音转换任务中音频嵌入对齐的问题。

Method: 使用离散最优传输映射对齐说话人音频嵌入。

Result: 方法表现出高质量和有效性，且能误导合成音频被误分类为真实音频。

Conclusion: 离散最优传输映射是一种有效的语音转换方法，且在音频生成后处理中具有潜在应用价值。

Abstract: In this work, we address the voice conversion (VC) task using a vector-based
interface. To align audio embeddings between speakers, we employ discrete
optimal transport mapping. Our evaluation results demonstrate the high quality
and effectiveness of this method. Additionally, we show that applying discrete
optimal transport as a post-processing step in audio generation can lead to the
incorrect classification of synthetic audio as real.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [260] [In-Context Adaptation to Concept Drift for Learned Database Operations](https://arxiv.org/abs/2505.04404)
*Jiaqi Zhu,Shaofeng Cai,Yanyan Shen,Gang Chen,Fang Deng,Beng Chin Ooi*

Main category: cs.DB

TLDR: FLAIR提出了一种在线适应框架，通过动态上下文构造解决数据库环境中概念漂移问题，无需运行时参数优化。


<details>
  <summary>Details</summary>
Motivation: 动态数据库环境中的概念漂移导致机器学习模型性能下降，限制了其实际应用。

Method: FLAIR通过任务特征化模块和动态决策引擎，利用上下文信息实现快速适应。

Result: 实验表明，FLAIR在适应速度和准确性上优于现有方法，适应速度提升5.2倍，错误率降低22.5%。

Conclusion: FLAIR为动态数据库环境中的机器学习模型提供了一种高效的在线适应解决方案。

Abstract: Machine learning has demonstrated transformative potential for database
operations, such as query optimization and in-database data analytics. However,
dynamic database environments, characterized by frequent updates and evolving
data distributions, introduce concept drift, which leads to performance
degradation for learned models and limits their practical applicability.
Addressing this challenge requires efficient frameworks capable of adapting to
shifting concepts while minimizing the overhead of retraining or fine-tuning.
  In this paper, we propose FLAIR, an online adaptation framework that
introduces a new paradigm called \textit{in-context adaptation} for learned
database operations. FLAIR leverages the inherent property of data systems,
i.e., immediate availability of execution results for predictions, to enable
dynamic context construction. By formalizing adaptation as $f:(\mathbf{x} \,|
\,\mathcal{C}_t) \to \mathbf{y}$, with $\mathcal{C}_t$ representing a dynamic
context memory, FLAIR delivers predictions aligned with the current concept,
eliminating the need for runtime parameter optimization. To achieve this, FLAIR
integrates two key modules: a Task Featurization Module for encoding
task-specific features into standardized representations, and a Dynamic
Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly
using contextual information at runtime. Extensive experiments across key
database tasks demonstrate that FLAIR outperforms state-of-the-art baselines,
achieving up to 5.2x faster adaptation and reducing error by 22.5% for
cardinality estimation.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [261] [Beyond Misinformation: A Conceptual Framework for Studying AI Hallucinations in (Science) Communication](https://arxiv.org/abs/2504.13777)
*Anqi Shao*

Main category: cs.HC

TLDR: 本文提出一个概念框架，将AI幻觉视为一种独特的虚假信息形式，区别于传统的人类意图驱动的虚假信息。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统产生虚假但看似合理的内容，传统虚假信息研究未能涵盖此类非人类意图驱动的现象，需重新审视虚假信息理论的边界。

Method: 采用供需模型和分布式代理概念，分析AI幻觉在产生、感知和制度应对方面与人类虚假信息的差异。

Result: 提出一个多层次研究议程，从宏观（制度）、中观（群体）和微观（个体）层面探讨AI幻觉的产生、传播和受众接受。

Conclusion: 呼吁传播学者重新思考虚假信息理论，以应对概率性非人类行为体在知识生产中的日益嵌入。

Abstract: This paper proposes a conceptual framework for understanding AI
hallucinations as a distinct form of misinformation. While misinformation
scholarship has traditionally focused on human intent, generative AI systems
now produce false yet plausible outputs absent of such intent. I argue that
these AI hallucinations should not be treated merely as technical failures but
as communication phenomena with social consequences. Drawing on a
supply-and-demand model and the concept of distributed agency, the framework
outlines how hallucinations differ from human-generated misinformation in
production, perception, and institutional response. I conclude by outlining a
research agenda for communication scholars to investigate the emergence,
dissemination, and audience reception of hallucinated content, with attention
to macro (institutional), meso (group), and micro (individual) levels. This
work urges communication researchers to rethink the boundaries of
misinformation theory in light of probabilistic, non-human actors increasingly
embedded in knowledge production.

</details>

### [262] [Facilitating Video Story Interaction with Multi-Agent Collaborative System](https://arxiv.org/abs/2505.03807)
*Yiwen Zhang,Jianing Hao,Zhan Wang,Hongling Sheng,Wei Zeng*

Main category: cs.HC

TLDR: 提出基于用户意图的交互系统，结合VLM、RAG和MAS技术，实现视频故事的个性化互动体验。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于用户选择和特定叙事设计，缺乏定制化。

Method: 系统分三阶段：视频故事处理（VLM）、多空间聊天（MAS）和场景定制（RAG）。

Result: 应用于《哈利·波特》系列，系统成功展现角色社交行为和成长。

Conclusion: 系统显著提升视频故事世界的互动体验。

Abstract: Video story interaction enables viewers to engage with and explore narrative
content for personalized experiences. However, existing methods are limited to
user selection, specially designed narratives, and lack customization. To
address this, we propose an interactive system based on user intent. Our system
uses a Vision Language Model (VLM) to enable machines to understand video
stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent
System (MAS) to create evolving characters and scene experiences. It includes
three stages: 1) Video story processing, utilizing VLM and prior knowledge to
simulate human understanding of stories across three modalities. 2) Multi-space
chat, creating growth-oriented characters through MAS interactions based on
user queries and story stages. 3) Scene customization, expanding and
visualizing various story scenes mentioned in dialogue. Applied to the Harry
Potter series, our study shows the system effectively portrays emergent
character social behavior and growth, enhancing the interactive experience in
the video story world.

</details>

### [263] [Scratch Copilot: Supporting Youth Creative Coding with AI](https://arxiv.org/abs/2505.03867)
*Stefania Druga,Amy J. Ko*

Main category: cs.HC

TLDR: Cognimates Scratch Copilot是一个为儿童设计的AI助手，集成在Scratch环境中，支持实时创意、代码生成和调试，帮助儿童克服编程障碍。


<details>
  <summary>Details</summary>
Motivation: 尽管Scratch等平台为儿童提供了编程机会，但将创意转化为代码仍具挑战性。目前缺乏针对儿童的AI辅助工具。

Method: 开发了Cognimates Scratch Copilot，并通过18名国际儿童的定性评估分析其效果。

Result: AI助手在创意和调试方面提供了支持，儿童表现出对AI建议的主动调整或拒绝，保持了创意控制。

Conclusion: 该工具有潜力提升儿童的创意自我效能和参与度，并提出了支持儿童自主性和批判性互动的设计指南。

Abstract: Creative coding platforms like Scratch have democratized programming for
children, yet translating imaginative ideas into functional code remains a
significant hurdle for many young learners. While AI copilots assist adult
programmers, few tools target children in block-based environments. Building on
prior research \cite{druga_how_2021,druga2023ai, druga2023scratch}, we present
Cognimates Scratch Copilot: an AI-powered assistant integrated into a
Scratch-like environment, providing real-time support for ideation, code
generation, debugging, and asset creation. This paper details the system
architecture and findings from an exploratory qualitative evaluation with 18
international children (ages 7--12). Our analysis reveals how the AI Copilot
supported key creative coding processes, particularly aiding ideation and
debugging. Crucially, it also highlights how children actively negotiated the
use of AI, demonstrating strong agency by adapting or rejecting suggestions to
maintain creative control. Interactions surfaced design tensions between
providing helpful scaffolding and fostering independent problem-solving, as
well as learning opportunities arising from navigating AI limitations and
errors. Findings indicate Cognimates Scratch Copilot's potential to enhance
creative self-efficacy and engagement. Based on these insights, we propose
initial design guidelines for AI coding assistants that prioritize youth agency
and critical interaction alongside supportive scaffolding.

</details>

### [264] [Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering](https://arxiv.org/abs/2505.04260)
*Jessica Y. Bo,Tianyu Xu,Ishan Chatterjee,Katrina Passarella-Ward,Achin Kulshrestha,D Shin*

Main category: cs.HC

TLDR: 论文提出了一种通过激活引导（activation steering）技术，帮助大型语言模型（LLM）在推理过程中更贴合用户潜在偏好的方法，提升了用户满意度和留存率。


<details>
  <summary>Details</summary>
Motivation: 解决普通用户在提示指定能力上的不足，使其能更轻松地向AI助手传达潜在偏好。

Method: 利用激活引导技术，通过线性强度因子控制，嵌入三种不同的交互式聊天机器人界面，并进行用户研究（n=14）。

Result: 偏好引导能有效对齐真实对话与用户潜在偏好，不同用户因对控制、可用性和透明度的需求差异而偏好不同界面。

Conclusion: 激活引导是一种轻量级且用户可控的方法，能显著提升个性化对话的效果。

Abstract: As large language models (LLMs) improve in their capacity to serve as
personal AI assistants, their ability to output uniquely tailored, personalized
responses that align with the soft preferences of their users is essential for
enhancing user satisfaction and retention. However, untrained lay users have
poor prompt specification abilities and often struggle with conveying their
latent preferences to AI assistants. To address this, we leverage activation
steering to guide LLMs to align with interpretable preference dimensions during
inference. In contrast to memory-based personalization methods that require
longer user history, steering is extremely lightweight and can be easily
controlled by the user via an linear strength factor. We embed steering into
three different interactive chatbot interfaces and conduct a within-subjects
user study (n=14) to investigate how end users prefer to personalize their
conversations. The results demonstrate the effectiveness of preference-based
steering for aligning real-world conversations with hidden user preferences,
and highlight further insights on how diverse values around control, usability,
and transparency lead users to prefer different interfaces.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [265] [A Graphical Global Optimization Framework for Parameter Estimation of Statistical Models with Nonconvex Regularization Functions](https://arxiv.org/abs/2505.03899)
*Danial Davarnia,Mohammadreza Kiaghadi*

Main category: math.OC

TLDR: 本文提出了一种基于图的新方法，用于全局解决涉及广义范数约束的优化问题，涵盖标准ℓ_p-范数和非凸惩罚函数，并通过决策图构建强凸松弛。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理零范数等复杂约束时，通常需要引入辅助变量或依赖特定结构，难以通用化。本文旨在提出一种更通用的全局优化方法。

Method: 利用决策图在原始变量空间构建强凸松弛，结合空间分支切割框架，避免引入辅助变量或人工边界。

Result: 初步计算实验表明，该方法在涉及复杂非凸惩罚的稀疏线性回归问题上有效，优于现有全局优化技术。

Conclusion: 该方法为处理广义范数约束优化问题提供了一种高效且通用的解决方案，具有全局收敛性。

Abstract: Optimization problems with norm-bounding constraints arise in a variety of
applications, including portfolio optimization, machine learning, and feature
selection. A common approach to these problems involves relaxing the norm
constraint via Lagrangian relaxation, transforming it into a regularization
term in the objective function. A particularly challenging class includes the
zero-norm function, which promotes sparsity in statistical parameter
estimation. Most existing exact methods for solving these problems introduce
binary variables and artificial bounds to reformulate them as
higher-dimensional mixed-integer programs, solvable by standard solvers. Other
exact approaches exploit specific structural properties of the objective,
making them difficult to generalize across different problem types. Alternative
methods employ nonconvex penalties with favorable statistical characteristics,
but these are typically addressed using heuristic or local optimization
techniques due to their structural complexity. In this paper, we propose a
novel graph-based method to globally solve optimization problems involving
generalized norm-bounding constraints. Our approach encompasses standard
$\ell_p$-norms for $p \in [0, \infty)$ and nonconvex penalties such as SCAD and
MCP. We leverage decision diagrams to construct strong convex relaxations
directly in the original variable space, eliminating the need for auxiliary
variables or artificial bounds. Integrated into a spatial branch-and-cut
framework, our method guarantees convergence to the global optimum. We
demonstrate its effectiveness through preliminary computational experiments on
benchmark sparse linear regression problems involving complex nonconvex
penalties, which are not tractable using existing global optimization
techniques.

</details>

### [266] [Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems](https://arxiv.org/abs/2505.04596)
*Mohammad Merati,David Castañón*

Main category: math.OC

TLDR: 提出了一种优化动态监控环境中PTZ摄像头调度与控制的新方法，结合卡尔曼滤波和动态网络流模型，提高实时视频捕捉效率。


<details>
  <summary>Details</summary>
Motivation: 传统主从摄像头系统在动态和拥挤环境中效率不足，需改进调度与控制方法以提升监控效果。

Method: 集成卡尔曼滤波预测目标运动轨迹，结合动态网络流模型优化摄像头任务调度，引入基于价值的系统优先处理关键事件。

Result: 仿真显示该方法提高了覆盖率，减少了平均等待时间，并降低了事件遗漏率。

Conclusion: 该方法显著提升了监控系统的效率、可扩展性和有效性，特别适用于动态和拥挤环境。

Abstract: This paper presents a novel approach for optimizing the scheduling and
control of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.
The proposed method integrates Kalman filters for motion prediction with a
dynamic network flow model to enhance real-time video capture efficiency. By
assigning Kalman filters to tracked objects, the system predicts future
locations, enabling precise scheduling of camera tasks. This prediction-driven
approach is formulated as a network flow optimization, ensuring scalability and
adaptability to various surveillance scenarios. To further reduce redundant
monitoring, we also incorporate group-tracking nodes, allowing multiple objects
to be captured within a single camera focus when appropriate. In addition, a
value-based system is introduced to prioritize camera actions, focusing on the
timely capture of critical events. By adjusting the decay rates of these values
over time, the system ensures prompt responses to tasks with imminent
deadlines. Extensive simulations demonstrate that this approach improves
coverage, reduces average wait times, and minimizes missed events compared to
traditional master-slave camera systems. Overall, our method significantly
enhances the efficiency, scalability, and effectiveness of surveillance
systems, particularly in dynamic and crowded environments.

</details>

### [267] [Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows](https://arxiv.org/abs/2505.04354)
*Wenhao Li,Bo Jin,Mingyi Hong,Changhong Lu,Xiangfeng Wang*

Main category: math.OC

TLDR: 本文主张从依赖专家的优化问题解决转向基于进化代理的工作流程，通过基础模型和进化搜索实现自主优化。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法依赖专家，导致工业应用瓶颈，阻碍前沿方法的采用。

Method: 提出进化代理工作流程，结合基础模型和进化搜索，自主探索优化空间。

Result: 通过云资源调度和ADMM参数适应的案例研究，验证了该方法的可行性。

Conclusion: 挑战以人为中心的优化现状，提倡更可扩展、自适应的解决方案。

Abstract: This position paper argues that optimization problem solving can transition
from expert-dependent to evolutionary agentic workflows. Traditional
optimization practices rely on human specialists for problem formulation,
algorithm selection, and hyperparameter tuning, creating bottlenecks that
impede industrial adoption of cutting-edge methods. We contend that an
evolutionary agentic workflow, powered by foundation models and evolutionary
search, can autonomously navigate the optimization space, comprising problem,
formulation, algorithm, and hyperparameter spaces. Through case studies in
cloud resource scheduling and ADMM parameter adaptation, we demonstrate how
this approach can bridge the gap between academic innovation and industrial
implementation. Our position challenges the status quo of human-centric
optimization workflows and advocates for a more scalable, adaptive approach to
solving real-world optimization problems.

</details>

### [268] [Learning based convex approximation for constrained parametric optimization](https://arxiv.org/abs/2505.04037)
*Kang Liu,Wei Peng,Jianchen Hu*

Main category: math.OC

TLDR: 提出了一种基于输入凸神经网络（ICNN）的自监督学习框架，用于解决连续约束优化问题，结合增强拉格朗日方法和约束修正机制，确保非严格约束可行性、更优的最优性差距和最佳收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于学习的优化方法在约束可行性、最优性差距和收敛速度方面的不足。

Method: 结合增强拉格朗日方法（ALM）和约束修正机制，使用ICNN作为内部求解器。

Result: 在二次规划、非凸规划和大规模交流最优潮流问题中，相比现有求解器和最新学习方法，实现了准确性、可行性和计算效率的优越平衡。

Conclusion: 该框架在理论和实验上均表现出色，能够收敛到原始问题的KKT点，且逼近误差有界。

Abstract: We propose an input convex neural network (ICNN)-based self-supervised
learning framework to solve continuous constrained optimization problems. By
integrating the augmented Lagrangian method (ALM) with the constraint
correction mechanism, our framework ensures \emph{non-strict constraint
feasibility}, \emph{better optimality gap}, and \emph{best convergence rate}
with respect to the state-of-the-art learning-based methods. We provide a
rigorous convergence analysis, showing that the algorithm converges to a
Karush-Kuhn-Tucker (KKT) point of the original problem even when the internal
solver is a neural network, and the approximation error is bounded. We test our
approach on a range of benchmark tasks including quadratic programming (QP),
nonconvex programming, and large-scale AC optimal power flow problems. The
results demonstrate that compared to existing solvers (e.g., \texttt{OSQP},
\texttt{IPOPT}) and the latest learning-based methods (e.g., DC3, PDL), our
approach achieves a superior balance among accuracy, feasibility, and
computational efficiency.

</details>

### [269] [A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance](https://arxiv.org/abs/2505.04494)
*Axel Friedrich Wolter,Tobias Sutter*

Main category: math.OC

TLDR: PGDA-RL是一种新的原始-对偶投影梯度下降-上升算法，用于解决正则化MDP问题，结合了经验回放和双时间尺度分解，通过单轨迹数据在线更新策略。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够利用离策略数据同时保持在线探索的强化学习算法。

Method: 提出PGDA-RL算法，结合经验回放的梯度估计和双时间尺度分解，异步操作并通过单轨迹数据在线更新策略。

Result: PGDA-RL几乎必然收敛到正则化MDP的最优值函数和策略。

Conclusion: PGDA-RL在较弱假设下收敛，无需模拟器或固定行为策略，优于现有原始-对偶RL方法。

Abstract: We study reinforcement learning by combining recent advances in regularized
linear programming formulations with the classical theory of stochastic
approximation. Motivated by the challenge of designing algorithms that leverage
off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a
novel primal-dual Projected Gradient Descent-Ascent algorithm for solving
regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience
replay-based gradient estimation with a two-timescale decomposition of the
underlying nested optimization problem. The algorithm operates asynchronously,
interacts with the environment through a single trajectory of correlated data,
and updates its policy online in response to the dual variable associated with
the occupation measure of the underlying MDP. We prove that PGDA-RL converges
almost surely to the optimal value function and policy of the regularized MDP.
Our convergence analysis relies on tools from stochastic approximation theory
and holds under weaker assumptions than those required by existing primal-dual
RL approaches, notably removing the need for a simulator or a fixed behavioral
policy.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [270] [Securing Immersive 360 Video Streams through Attribute-Based Selective Encryption](https://arxiv.org/abs/2505.04466)
*Mohammad Waquas Usmani,Susmit Shannigrahi,Michael Zink*

Main category: cs.MM

TLDR: 本文提出了一种结合属性加密（ABE）和选择性加密的新框架，用于优化360度视频流的安全传输，降低计算开销并提升安全性。


<details>
  <summary>Details</summary>
Motivation: 传统HTTPS方法在高效性和可扩展性上无法满足高分辨率360度视频流的安全需求。

Method: 采用选择性加密和视口自适应加密技术，动态加密关键区域的帧以减少计算负担。

Result: 实验表明，该ABE模型降低了中间缓存的计算负载，提高了缓存命中率，并保持了与HTTPS相当的视觉质量。

Conclusion: 提出的框架在保证安全性的同时，显著提升了360度视频流的传输效率。

Abstract: Delivering high-quality, secure 360{\deg} video content introduces unique
challenges, primarily due to the high bitrates and interactive demands of
immersive media. Traditional HTTPS-based methods, although widely used, face
limitations in computational efficiency and scalability when securing these
high-resolution streams. To address these issues, this paper proposes a novel
framework integrating Attribute-Based Encryption (ABE) with selective
encryption techniques tailored specifically for tiled 360{\deg} video
streaming. Our approach employs selective encryption of frames at varying
levels to reduce computational overhead while ensuring robust protection
against unauthorized access.
  Moreover, we explore viewport-adaptive encryption, dynamically encrypting
more frames within tiles occupying larger portions of the viewer's field of
view. This targeted method significantly enhances security in critical viewing
areas without unnecessary overhead in peripheral regions. We deploy and
evaluate our proposed approach using the CloudLab testbed, comparing its
performance against traditional HTTPS streaming. Experimental results
demonstrate that our ABE-based model achieves reduced computational load on
intermediate caches, improves cache hit rates, and maintains comparable visual
quality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [271] [Differentially Private Densest-$k$-Subgraph](https://arxiv.org/abs/2505.03858)
*Alireza Khayatian,Anil Vullikanti,Aritra Konar*

Main category: cs.DS

TLDR: 本文首次设计了针对Densest-$k$-subgraph（D$k$S）问题的差分隐私（DP）算法，基于图邻接矩阵的主成分（PC）方法，提供边缘DP保证。通过输出扰动和Propose-Test-Release（PTR）框架，实现了隐私与效用的平衡，并在大规模网络上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 许多图数据集涉及敏感网络数据，需要隐私保护的图挖掘方法。D$k$S问题是图挖掘中的关键问题，但现有算法缺乏隐私保护。

Method: 1. 使用图邻接矩阵的主成分（PC）方法输出$k$个顶点；2. 提出输出扰动方法；3. 设计基于PTR框架的私有PC方法；4. 提出迭代私有幂方法（PPM）。

Result: 在真实网络（最大300万顶点）上验证了隐私与效用的平衡，PTR方法在运行时比PPM快180倍。

Conclusion: 本文首次为D$k$S问题提供了差分隐私算法，PTR方法在效率和实用性上表现优异。

Abstract: Many graph datasets involve sensitive network data, motivating the need for
privacy-preserving graph mining. The Densest-$k$-subgraph (D$k$S) problem is a
key primitive in graph mining that aims to extract a subset of $k$ vertices
with the maximum internal connectivity. Although non-private algorithms are
known for D$k$S, this paper is the first to design algorithms that offer formal
differential privacy (DP) guarantees for the problem. We base our general
approach on using the principal component (PC) of the graph adjacency matrix to
output a subset of $k$ vertices under edge DP. For this task, we first consider
output perturbation, which traditionally offer good scalability, but at the
expense of utility. Our tight on the local sensitivity indicate a big gap with
the global sensitivity, motivating the use of instance specific sensitive
methods for private PC. Next, we derive a tight bound on the smooth sensitivity
and show that it can be close to the global sensitivity. This leads us to
consider the Propose-Test-Release (PTR) framework for private PC. Although
computationally expensive in general, we design a novel approach for
implementing PTR in the same time as computation of a non-private PC, while
offering good utility for \DkS{}. Additionally, we also consider the iterative
private power method (PPM) for private PC, albeit it is significantly slower
than PTR on large networks. We run our methods on diverse real-world networks,
with the largest having 3 million vertices, and show good privacy-utility
trade-offs. Although PTR requires a slightly larger privacy budget, on average,
it achieves a 180-fold improvement in runtime over PPM.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [272] [GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype](https://arxiv.org/abs/2505.03853)
*Changxi Chi,Jun Xia,Jingbo Zhou,Jiabei Cheng,Chang Yu,Stan Z. Li*

Main category: q-bio.QM

TLDR: 论文提出了一种名为GRAPE的异构图神经网络方法，通过结合预训练语言模型和DNA序列模型提取基因特征，并引入基因生物型信息，动态优化基因调控网络（GRN），显著提升了遗传扰动预测的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能充分利用基因相关信息，且忽略了生物型功能差异，限制了基因互作的捕捉能力。本文旨在通过多模态特征提取和生物型信息建模，改进GRN构建。

Method: 利用预训练语言模型和DNA序列模型分别提取基因描述和序列特征，结合生物型信息，通过异构图神经网络（HGNN）和图结构学习（GSL）动态优化GRN。

Result: 在公开数据集上，GRAPE方法实现了最先进的性能。

Conclusion: GRAPE通过多模态特征和生物型建模，显著提升了GRN构建和遗传扰动预测的准确性。

Abstract: Predicting genetic perturbations enables the identification of potentially
crucial genes prior to wet-lab experiments, significantly improving overall
experimental efficiency. Since genes are the foundation of cellular life,
building gene regulatory networks (GRN) is essential to understand and predict
the effects of genetic perturbations. However, current methods fail to fully
leverage gene-related information, and solely rely on simple evaluation metrics
to construct coarse-grained GRN. More importantly, they ignore functional
differences between biotypes, limiting the ability to capture potential gene
interactions. In this work, we leverage pre-trained large language model and
DNA sequence model to extract features from gene descriptions and DNA sequence
data, respectively, which serve as the initialization for gene representations.
Additionally, we introduce gene biotype information for the first time in
genetic perturbation, simulating the distinct roles of genes with different
biotypes in regulating cellular processes, while capturing implicit gene
relationships through graph structure learning (GSL). We propose GRAPE, a
heterogeneous graph neural network (HGNN) that leverages gene representations
initialized with features from descriptions and sequences, models the distinct
roles of genes with different biotypes, and dynamically refines the GRN through
GSL. The results on publicly available datasets show that our method achieves
state-of-the-art performance.

</details>

### [273] [Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning](https://arxiv.org/abs/2505.04300)
*Isabella Caranzano,Corrado Pancotti,Cesare Rollo,Flavio Sartori,Pietro Liò,Piero Fariselli,Tiziana Sanavia*

Main category: q-bio.QM

TLDR: 研究发现，生物信息神经网络中通路注释的优势可能源于其稀疏性而非生物学相关性，随机化模型表现与之相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 验证通路注释在神经网络中的实际贡献是否源于生物学相关性，还是仅因其引入的稀疏性。

Method: 全面分析通路神经网络模型，比较生物信息模型与随机化模型的表现。

Result: 随机化模型与生物信息模型表现相当，部分情况下更优，且通路模型在可解释性上无优势。

Conclusion: 通路注释可能噪声较大或未被充分挖掘，提出新方法用于系统比较通路模型与随机化模型。

Abstract: Biologically-informed neural networks typically leverage pathway annotations
to enhance performance in biomedical applications. We hypothesized that the
benefits of pathway integration does not arise from its biological relevance,
but rather from the sparsity it introduces. We conducted a comprehensive
analysis of all relevant pathway-based neural network models for predictive
tasks, critically evaluating each study's contributions. From this review, we
curated a subset of methods for which the source code was publicly available.
The comparison of the biologically informed state-of-the-art deep learning
models and their randomized counterparts showed that models based on randomized
information performed equally well as biologically informed ones across
different metrics and datasets. Notably, in 3 out of the 15 analyzed models,
the randomized versions even outperformed their biologically informed
counterparts. Moreover, pathway-informed models did not show any clear
advantage in interpretability, as randomized models were still able to identify
relevant disease biomarkers despite lacking explicit pathway information. Our
findings suggest that pathway annotations may be too noisy or inadequately
explored by current methods. Therefore, we propose a methodology that can be
applied to different domains and can serve as a robust benchmark for
systematically comparing novel pathway-informed models against their randomized
counterparts. This approach enables researchers to rigorously determine whether
observed performance improvements can be attributed to biological insights.

</details>

<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [274] [Is the end of Insight in Sight ?](https://arxiv.org/abs/2505.04627)
*Jean-Michel Tucny,Mihir Durve,Sauro Succi*

Main category: physics.comp-ph

TLDR: PINN用于Boltzmann方程的稀薄气体动力学问题时，权重矩阵与物理问题的数学结构无关，接近高斯随机分布。


<details>
  <summary>Details</summary>
Motivation: 探讨PINN在解决Boltzmann方程问题时权重矩阵的特性及其与物理结构的关联。

Method: 分析PINN权重矩阵的分布特性及其与Boltzmann方程数学结构的关系。

Result: 权重矩阵接近高斯随机分布，与物理问题结构无直接关联。

Conclusion: 深度学习与Boltzmann方程的数值解可能是等效但独立的路径，Explainable AI可能不现实或不适定。

Abstract: It is shown that the weight matrices of a Physics-informed neural network
(PINN)-based deep learning application to a rarefied gas dynamics problem
described by the Boltzmann equation bear no evident link to the mathematical
structure of the physical problem. Instead, the weights appear close to
Gaussian distributed random matrices. Although significantly more work is
needed to support a robust assessment in this direction, these results suggest
that deep-learning and the numerical solution of the Boltzmann equation
represent two equivalent, but largely distinct paths to the same physical
knowledge. If so, Explainable AI might be an unrealistic target and possibly
even an ill-posed one.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [275] [AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03745)
*Yanbiao Liang,Huihong Shi,Haikuo Shao,Zhongfeng Wang*

Main category: cs.AR

TLDR: AccLLM是一个通过算法和硬件协同设计加速大型语言模型（LLM）在边缘设备上部署的框架，解决了计算、内存和带宽等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在NLP领域的成功，将其部署到资源受限的边缘设备面临计算、内存和带宽等挑战。

Method: AccLLM结合了剪枝、Λ形注意力和W2A8KV4量化方案，并设计了基于FPGA的加速器。

Result: 在Xilinx Alveo U280 FPGA上验证，能效和吞吐量分别提升4.07倍和2.98倍。

Conclusion: AccLLM有效提升了LLM在边缘设备上的部署效率。

Abstract: Recently, large language models (LLMs) have achieved huge success in the
natural language processing (NLP) field, driving a growing demand to extend
their deployment from the cloud to edge devices. However, deploying LLMs on
resource-constrained edge devices poses significant challenges, including (1)
intensive computations and huge model sizes, (2) great memory and bandwidth
demands introduced by the autoregressive generation process, and (3) limited
scalability for handling long sequences. To address these challenges, we
propose AccLLM, a comprehensive acceleration framework that enables efficient
and fast long-context LLM inference through algorithm and hardware co-design.
At the algorithmic level, we integrate (1) pruning, (2) {\Lambda}-shaped
attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and
4-bit KV cache) quantization scheme, thus effectively reducing memory and
bandwidth requirements while facilitating LLMs' long-sequence generation. At
the hardware level, we design a dedicated FPGA-based accelerator with a
reconfigurable computing engine to effectively and flexibly accommodate diverse
operations arising from our compression algorithm, thereby fully translating
the algorithmic innovations into tangible hardware efficiency. We validate
AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency
and a 2.98x throughput compared to the state-of-the-art work FlightLLM.

</details>

### [276] [APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03748)
*Yonghao Tan,Pingcheng Dong,Yongkun Wu,Yu Liu,Xuejiao Liu,Peng Luo,Shih-Yang Liu,Xijie Huang,Dong Zhang,Luhong Liang,Kwang-Ting Cheng*

Main category: cs.AR

TLDR: 论文提出了一种新型的加法部分和量化（APSQ）方法，通过将部分和（PSUM）量化与数据流架构结合，显著降低了内存需求和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统压缩方法忽视了PSUM量化，而PSUM访问可能导致高能耗（占69%），因此需要一种高效量化方法以减少内存和能耗。

Method: 提出APSQ方法，将PSUM累积无缝集成到量化框架中，并结合可重构架构的分组策略。

Result: 在BERT、Segformer和EfficientViT等模型上，APSQ实现了近乎无损的量化（压缩至INT8），能耗降低28-87%。LLaMA2-7B实验进一步验证了其潜力。

Conclusion: APSQ是一种高效的PSUM量化方法，显著降低了能耗，适用于多种模型，包括大型语言模型。

Abstract: DNN accelerators, significantly advanced by model compression and specialized
dataflow techniques, have marked considerable progress. However, the frequent
access of high-precision partial sums (PSUMs) leads to excessive memory demands
in architectures utilizing input/weight stationary dataflows. Traditional
compression strategies have typically overlooked PSUM quantization, which may
account for 69% of power consumption. This study introduces a novel Additive
Partial Sum Quantization (APSQ) method, seamlessly integrating PSUM
accumulation into the quantization framework. A grouping strategy that combines
APSQ with PSUM quantization enhanced by a reconfigurable architecture is
further proposed. The APSQ performs nearly lossless on NLP and CV tasks across
BERT, Segformer, and EfficientViT models while compressing PSUMs to INT8. This
leads to a notable reduction in energy costs by 28-87%. Extended experiments on
LLaMA2-7B demonstrate the potential of APSQ for large language models. Code is
available at https://github.com/Yonghao-Tan/APSQ.

</details>

### [277] [AI-Powered Agile Analog Circuit Design and Optimization](https://arxiv.org/abs/2505.03750)
*Jinhai Hu,Wang Ling Goh,Yuan Gao*

Main category: cs.AR

TLDR: AI技术通过自动化设备级调优和系统级协同优化，改进了模拟电路设计。本文结合了两种方法：基于MOBO的晶体管尺寸优化和电路传递函数建模，展示了AI在提升性能、减少设计迭代和联合优化方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索AI在模拟电路设计中的应用潜力，以提升性能、减少设计迭代并实现系统级优化。

Method: 1. 使用多目标贝叶斯优化（MOBO）进行晶体管尺寸优化；2. 在关键词检测（KWS）应用中，通过AI建模电路传递函数进行系统级优化。

Result: 成功优化了线性可调跨导器和模拟带通滤波器，展示了AI在性能提升和设计效率上的优势。

Conclusion: AI技术能够显著改进模拟电路设计，实现设备级和系统级的协同优化。

Abstract: Artificial intelligence (AI) techniques are transforming analog circuit
design by automating device-level tuning and enabling system-level
co-optimization. This paper integrates two approaches: (1) AI-assisted
transistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct
circuit parameter optimization, demonstrated on a linearly tunable
transconductor; and (2) AI-integrated circuit transfer function modeling for
system-level optimization in a keyword spotting (KWS) application, demonstrated
by optimizing an analog bandpass filter within a machine learning training
loop. The combined insights highlight how AI can improve analog performance,
reduce design iteration effort, and jointly optimize analog components and
application-level metrics.

</details>

### [278] [Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management](https://arxiv.org/abs/2505.03756)
*Hang Zhang,Jiuchen Shi,Yixiao Wang,Quan Chen,Yizhou Shan,Minyi Guo*

Main category: cs.AR

TLDR: FASTLIBRA是一种多LoRA缓存系统，通过依赖感知的缓存管理和性能驱动的缓存交换器优化多LoRA推理性能，显著降低TTFT。


<details>
  <summary>Details</summary>
Motivation: 现有系统在多LoRA推理中未能优化服务性能（如TTFT），且忽视了LoRA和KV缓存的依赖关系。

Method: FASTLIBRA包含依赖感知的缓存管理器和性能驱动的缓存交换器，统一管理缓存池并根据成本模型动态交换缓存。

Result: 实验表明，FASTLIBRA平均降低TTFT达63.4%。

Conclusion: FASTLIBRA通过优化缓存管理显著提升了多LoRA推理性能。

Abstract: Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for
task-specific Large Language Model (LLM) applications. For multi-LoRA serving,
caching hot KV caches and LoRA adapters in high bandwidth memory of
accelerations can improve inference performance. However, existing Multi-LoRA
inference systems fail to optimize serving performance like Time-To-First-Toke
(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore
propose FASTLIBRA, a Multi-LoRA caching system to optimize the serving
performance. FASTLIBRA comprises a dependency-aware cache manager and a
performance-driven cache swapper. The cache manager maintains the usage
dependencies between LoRAs and KV caches during the inference with a unified
caching pool. The cache swapper determines the swap-in or out of LoRAs and KV
caches based on a unified cost model, when the HBM is idle or busy,
respectively. Experimental results show that ELORA reduces the TTFT by 63.4% on
average, compared to state-of-the-art works.

</details>

### [279] [Splitwiser: Efficient LM inference with constrained resources](https://arxiv.org/abs/2505.03763)
*Asad Aali,Adney Cardoza,Melissa Capo*

Main category: cs.AR

TLDR: Splitwiser是一种将LLM推理请求的两个阶段（提示计算和令牌生成）拆分到同一GPU上的方法，以减少开销并提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的令牌生成阶段未能充分利用计算资源，尤其是在与提示计算阶段相比时。

Method: 提出Splitwiser方法，将两个阶段拆分到同一GPU上，减少数据传输开销，并开源了Huggingface和vLLM的实现代码。

Result: 初步结果显示Splitwiser能减少网络相关开销并提高资源利用率。

Conclusion: Splitwiser通过优化GPU资源分配，为LLM推理提供了更高效的解决方案。

Abstract: Efficient inference of LLMs remains a crucial challenge, with two main
phases: a compute-intensive prompt computation and a memory-intensive token
generation. Despite existing batching and scheduling techniques, token
generation phases fail to fully utilize compute resources, especially when
compared to prompt computation phases. To address these challenges, we propose
Splitwiser, a methodology that splits the two phases of an LLM inference
request onto the same GPU, thereby reducing overhead and improving memory
access and cache utilization. By eliminating the need to transfer data across
devices, Splitwiser aims to minimize network-related overheads. In this report,
we describe the basic structure of our proposed pipeline while sharing
preliminary results and analysis. We implement our proposed multiprocessing
design on two widely-used and independent LLM architectures: Huggingface and
vLLM. We open-source our code for the respective implementations: 1)
Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM
(https://github.com/adney11/vllm-sysml).

</details>

### [280] [GPU Performance Portability needs Autotuning](https://arxiv.org/abs/2505.03780)
*Burkhard Ringlein,Thomas Parnell,Radu Stoica*

Main category: cs.AR

TLDR: 通过结合即时编译（JIT）和内核参数自动调优，实现无需代码修改即可在多种硬件上高效运行LLM，提升性能并减少依赖单一平台。


<details>
  <summary>Details</summary>
Motivation: 当前LLM依赖单一平台导致可移植性差、供应商锁定问题，阻碍新AI硬件发展。

Method: 结合JIT编译与内核参数自动调优，以Flash Attention为例，探索更多参数配置并生成多样化代码。

Result: 相比供应商优化实现，性能提升230%，内核代码减少70倍，且无需手动优化。

Conclusion: 自动调优是实现LLM跨GPU供应商可移植性的有效途径。

Abstract: As LLMs grow in complexity, achieving state-of-the-art performance requires
tight co-design across algorithms, software, and hardware. Today's reliance on
a single dominant platform limits portability, creates vendor lock-in, and
raises barriers for new AI hardware. In this work, we make the case for
combining just-in-time (JIT) compilation with kernel parameter autotuning to
enable portable, state-of-the-art performance LLM execution without code
changes. Focusing on flash attention -- a widespread performance-critical LLM
kernel -- we demonstrate that this approach explores up to 15x more kernel
parameter configurations, produces significantly more diverse code across
multiple dimensions, and even outperforms vendor-optimized implementations by
up to 230%, all while reducing kernel code size by 70x and eliminating manual
code optimizations. Our results highlight autotuning as a promising path to
unlocking model portability across GPU vendors.

</details>

<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [281] [On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation](https://arxiv.org/abs/2505.03757)
*Vinicius Francisco Rofatto,Luiz Felipe Rodrigues de Almeida,Marcelo Tomio Matsuoka,Ivandro Klein,Mauricio Roberto Veronez,Luiz Gonzaga Da Silveira Junior*

Main category: physics.geo-ph

TLDR: 提出一种基于残差的神经校正策略，通过神经网络学习初始几何变换后的系统失真，降低模型复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统坐标变换模型难以处理非线性和空间依赖的失真，导致地理空间应用中存在显著残差误差。

Method: 使用神经网络仅建模初始变换后的系统失真，专注于残差模式。

Result: 在模拟和实际地理配准任务中，相比直接神经网络转换和经典变换模型，该方法在挑战性条件下表现更优。

Conclusion: 残差建模是提升坐标变换精度的轻量且鲁棒的有效方法。

Abstract: Coordinate transformation models often fail to account for nonlinear and
spatially dependent distortions, leading to significant residual errors in
geospatial applications. Here we propose a residual-based neural correction
strategy, in which a neural network learns to model only the systematic
distortions left by an initial geometric transformation. By focusing solely on
residual patterns, the proposed method reduces model complexity and improves
performance, particularly in scenarios with sparse or structured control point
configurations. We evaluate the method using both simulated datasets with
varying distortion intensities and sampling strategies, as well as under the
real-world image georeferencing tasks. Compared with direct neural network
coordinate converter and classical transformation models, the residual-based
neural correction delivers more accurate and stable results under challenging
conditions, while maintaining comparable performance in ideal cases. These
findings demonstrate the effectiveness of residual modelling as a lightweight
and robust alternative for improving coordinate transformation accuracy.

</details>

<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [282] [High-speed multiwavelength photonic temporal integration using silicon photonics](https://arxiv.org/abs/2505.04405)
*Yi Zhang,Nikolaos Farmakidis,Ioannis Roumpos,Miltiadis Moralis-Pegios,Apostolos Tsakyridis,June Sang Lee,Bowei Dong,Yuhan He,Samarth Aggarwal,Nikolaos Pleros,Harish Bhaskaran*

Main category: physics.optics

TLDR: 通过利用慢热消散过程实现光学信号的时间积分，提出了一种可扩展的光子计算架构。


<details>
  <summary>Details</summary>
Motivation: 解决光学硬件在AI任务中映射大向量规模的挑战，同时避免低效的电光转换。

Method: 引入光路中的光子加热器（PHIL）单元，利用热消散过程实现50 GHz调制信号的时间积分。

Result: 实现了高速光子计算的可扩展路径，支持线性和非线性操作。

Conclusion: 通过热驱动积分，为高速光子计算提供了统一框架。

Abstract: Optical systems have been pivotal for energy-efficient computing, performing
high-speed, parallel operations in low-loss carriers. While these predominantly
analog optical accelerators bypass digitization to perform parallel
floating-point computations, scaling optical hardware to map large-vector sizes
for AI tasks remains challenging. Here, we overcome this limitation by
unfolding scalar operations in time and introducing a
photonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration.
Counterintuitively, we exploit a slow heat dissipation process to integrate
optical signals modulated at 50 GHz bridging the speed gap between the widely
applied thermo-optic effects and ultrafast photonics. This architecture
supports optical end-to-end signal processing, eliminates inefficient
electro-optical conversions, and enables both linear and nonlinear operations
within a unified framework. Our results demonstrate a scalable path towards
high-speed photonic computing through thermally driven integration.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [283] [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
*Shigeki Karita,Yuma Koizumi,Heiga Zen,Haruko Ishikawa,Robin Scheibler,Michiel Bacchiani*

Main category: cs.SD

TLDR: Miipher-2是一种用于大规模生成模型训练数据清理的语音恢复模型，支持300多种语言，无需显式条件输入，计算高效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模生成模型训练数据清理中的泛化性、无显式条件输入和计算效率问题。

Method: 使用预训练的通用语音模型（USM）作为特征提取器，结合并行适配器和WaneFit神经声码器。

Result: 在词错误率、说话人相似性和音质评分上优于或与传统模型相当，计算效率高。

Conclusion: Miipher-2为大规模语音数据清理提供了高效、通用的解决方案。

Abstract: Training data cleaning is a new application for generative model-based speech
restoration (SR). This paper introduces Miipher-2, an SR model designed for
million-hour scale data, for training data cleaning for large-scale generative
models like large language models. Key challenges addressed include
generalization to unseen languages, operation without explicit conditioning
(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a
frozen, pre-trained Universal Speech Model (USM), supporting over 300
languages, as a robust, conditioning-free feature extractor. To optimize
efficiency and minimize memory, Miipher-2 incorporates parallel adapters for
predicting clean USM features from noisy inputs and employs the WaneFit neural
vocoder for waveform synthesis. These components were trained on 3,000 hours of
multi-lingual, studio-quality recordings with augmented degradations, while USM
parameters remained fixed. Experimental results demonstrate Miipher-2's
superior or comparable performance to conventional SR models in
word-error-rate, speaker similarity, and both objective and subjective sound
quality scores across all tested languages. Miipher-2 operates efficiently on
consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling
the processing of a million-hour speech dataset in approximately three days
using only 100 such accelerators.

</details>

### [284] [Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform](https://arxiv.org/abs/2505.04451)
*Yohannis Telila,Tommaso Cucinotta,Davide Bacciu*

Main category: cs.SD

TLDR: 论文提出了一种基于卷积神经网络（CNN）的自动音乐转录（AMT）方法，用于将古典钢琴音频文件转换为乐谱表示。


<details>
  <summary>Details</summary>
Motivation: 自动音乐转录（AMT）是一个具有挑战性的问题，尤其是在处理复调音乐时。目标是分析包含多个同时演奏音符的音频信号，生成乐谱表示。

Method: 使用恒定Q变换（CQT）提取音频信号特征，并将其作为卷积神经网络（CNN）的输入，设计了一个处理流程。

Result: 该方法能够将.wav格式的古典钢琴音频文件转换为乐谱表示。

Conclusion: 提出的CNN结合CQT的方法在自动音乐转录任务中表现出潜力。

Abstract: Automatic music transcription (AMT) is the problem of analyzing an audio
recording of a musical piece and detecting notes that are being played. AMT is
a challenging problem, particularly when it comes to polyphonic music. The goal
of AMT is to produce a score representation of a music piece, by analyzing a
sound signal containing multiple notes played simultaneously. In this work, we
design a processing pipeline that can transform classical piano audio files in
.wav format into a music score representation. The features from the audio
signals are extracted using the constant-Q transform, and the resulting
coefficients are used as an input to the convolutional neural network (CNN)
model.

</details>

### [285] [Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond](https://arxiv.org/abs/2505.04621)
*Jessie Richter-Powell,Antonio Torralba,Jonathan Lorraine*

Main category: cs.SD

TLDR: Audio-SDS将Score Distillation Sampling（SDS）推广到文本条件音频扩散模型，支持多种音频任务而无需专用数据集。


<details>
  <summary>Details</summary>
Motivation: 将SDS的核心思想（将生成先验蒸馏到参数化表示中）扩展到音频领域，以利用预训练模型的强大能力。

Method: 基于预训练模型，Audio-SDS通过蒸馏方法实现文本条件音频生成，支持物理模拟、FM合成参数校准和源分离等任务。

Result: Audio-SDS展示了跨模态蒸馏方法的多样性，并在音频任务中建立了生成先验的稳健基础。

Conclusion: Audio-SDS为未来音频任务中利用生成先验提供了通用且强大的框架。

Abstract: We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)
to text-conditioned audio diffusion models. While SDS was initially designed
for text-to-3D generation using image diffusion, its core idea of distilling a
powerful generative prior into a separate parametric representation extends to
the audio domain. Leveraging a single pretrained model, Audio-SDS enables a
broad range of tasks without requiring specialized datasets. In particular, we
demonstrate how Audio-SDS can guide physically informed impact sound
simulations, calibrate FM-synthesis parameters, and perform prompt-specified
source separation. Our findings illustrate the versatility of
distillation-based methods across modalities and establish a robust foundation
for future work using generative priors in audio tasks.

</details>

<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [286] [Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions](https://arxiv.org/abs/2505.04553)
*Shanyu Han,Yang Liu,Xiang Yu*

Main category: q-fin.MF

TLDR: 提出了一种基于强化学习的框架，适用于一类广泛的风险目标，解决了时间不一致性问题，并通过实验验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在一个广泛的风险目标类别下应用强化学习，特别是解决时间不一致性问题。

Method: 采用增强状态空间和辅助变量，将问题转化为两阶段优化问题，并提出定制的Actor-Critic算法。

Result: 理论证明了算法的近似保证，并在金融统计套利交易中验证了算法的有效性。

Conclusion: 该框架适用于多种风险度量，且在非连续马尔可夫决策过程中也有效。

Abstract: We propose a reinforcement learning (RL) framework under a broad class of
risk objectives, characterized by convex scoring functions. This class covers
many common risk measures, such as variance, Expected Shortfall, entropic
Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,
we consider an augmented state space and an auxiliary variable and recast the
problem as a two-state optimization problem. We propose a customized
Actor-Critic algorithm and establish some theoretical approximation guarantees.
A key theoretical contribution is that our results do not require the Markov
decision process to be continuous. Additionally, we propose an auxiliary
variable sampling method inspired by the alternating minimization algorithm,
which is convergent under certain conditions. We validate our approach in
simulation experiments with a financial application in statistical arbitrage
trading, demonstrating the effectiveness of the algorithm.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [287] [A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs](https://arxiv.org/abs/2505.04401)
*Wei Wang,Peizheng Li,Angela Doufexi,Mark A. Beach*

Main category: eess.SP

TLDR: 提出了一种结合启发式算法和深度强化学习的框架，用于优化大规模可重构智能表面的离散相位偏移。


<details>
  <summary>Details</summary>
Motivation: 由于离散相位偏移的非凸和非线性特性，优化大规模可重构智能表面（RIS）的配置具有挑战性。

Method: 采用启发式集成的深度强化学习（DRL）框架，结合双深度Q网络（DDQN）和贪婪算法（GA），分别进行列级控制和元素级优化。

Result: 该方法在小动作空间内有效优化了大规模RIS的相位偏移配置。

Conclusion: 提出的框架能够高效解决大规模RIS的优化问题。

Abstract: Optimizing discrete phase shifts in large-scale reconfigurable intelligent
surfaces (RISs) is challenging due to their non-convex and non-linear nature.
In this letter, we propose a heuristic-integrated deep reinforcement learning
(DRL) framework that (1) leverages accumulated actions over multiple steps in
the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates
a greedy algorithm (GA) into each DRL step to refine the state via
fine-grained, element-wise optimization of RIS configurations. By learning from
GA-included states, the proposed approach effectively addresses RIS
optimization within a small DRL action space, demonstrating its capability to
optimize phase-shift configurations of large-scale RISs.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [288] [Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems](https://arxiv.org/abs/2505.03946)
*Matthew Sgambati,Aleksandar Vakanski,Matthew Anderson*

Main category: cs.DC

TLDR: 论文提出了一种基于DD-PPO算法的RL调度器，解决了传统和现有RL调度方法在大规模数据集上的可扩展性问题，并展示了优于规则调度器和现有RL方法的性能。


<details>
  <summary>Details</summary>
Motivation: HPC环境中资源分配的复杂性及传统调度算法在异构性和规模扩大时的效率不足，促使研究更智能、可扩展的调度策略。

Method: 采用DD-PPO算法，支持多工作节点的大规模分布式训练，避免每一步的参数同步，提升可扩展性和训练效率。

Result: 实验使用超过1150万条真实HPC作业数据，验证了DD-PPO调度器在调度性能上优于传统规则调度器和现有RL方法。

Conclusion: DD-PPO调度器在HPC环境中表现出更高的可扩展性和调度效率，为未来智能调度提供了新方向。

Abstract: Resource allocation in High Performance Computing (HPC) environments presents
a complex and multifaceted challenge for job scheduling algorithms. Beyond the
efficient allocation of system resources, schedulers must account for and
optimize multiple performance metrics, including job wait time and system
utilization. While traditional rule-based scheduling algorithms dominate the
current deployments of HPC systems, the increasing heterogeneity and scale of
those systems is expected to challenge the efficiency and flexibility of those
algorithms in minimizing job wait time and maximizing utilization. Recent
research efforts have focused on leveraging advancements in Reinforcement
Learning (RL) to develop more adaptable and intelligent scheduling strategies.
Recent RL-based scheduling approaches have explored a range of algorithms, from
Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,
hybrid methods that integrate Graph Neural Networks with RL techniques.
However, a common limitation across these methods is their reliance on
relatively small datasets, and these methods face scalability issues when using
large datasets. This study introduces a novel RL-based scheduler utilizing the
Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,
which supports large-scale distributed training across multiple workers without
requiring parameter synchronization at every step. By eliminating reliance on
centralized updates to a shared policy, the DD-PPO scheduler enhances
scalability, training efficiency, and sample utilization. The validation
dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO
performance between traditional and advanced scheduling approaches, and the
experimental results demonstrate improved scheduling performance in comparison
to both rule-based schedulers and existing RL-based scheduling algorithms.

</details>

### [289] [Can Large Language Models Predict Parallel Code Performance?](https://arxiv.org/abs/2505.03988)
*Gregory Bolet,Giorgis Georgakoudis,Harshitha Menon,Konstantinos Parasyris,Niranjan Hasabnis,Hayden Estes,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TLDR: 论文探讨了是否能用大型语言模型（LLM）预测GPU性能，无需依赖硬件。通过将问题建模为屋顶线分类任务，评估了LLM在不同场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 由于高端GPU访问受限，传统性能分析方法变得困难，因此探索LLM作为替代方案。

Method: 构建了340个GPU内核的平衡数据集，评估LLM在四种场景下的表现：带性能分析数据、零样本、少样本和微调。

Result: LLM在提供性能分析数据时分类准确率达100%，推理能力强的LLM在零样本和少样本设置下表现显著优于标准LLM（最高64%准确率）。

Conclusion: LLM在性能预测方面潜力巨大，但需更多数据和优化提示策略才能成为实用工具。

Abstract: Accurate determination of the performance of parallel GPU code typically
requires execution-time profiling on target hardware -- an increasingly
prohibitive step due to limited access to high-end GPUs. This paper explores
whether Large Language Models (LLMs) can offer an alternative approach for GPU
performance prediction without relying on hardware. We frame the problem as a
roofline classification task: given the source code of a GPU kernel and the
hardware specifications of a target GPU, can an LLM predict whether the GPU
kernel is compute-bound or bandwidth-bound?
  For this study, we build a balanced dataset of 340 GPU kernels, obtained from
HeCBench benchmark and written in CUDA and OpenMP, along with their
ground-truth labels obtained via empirical GPU profiling. We evaluate LLMs
across four scenarios: (1) with access to profiling data of the kernel source,
(2) zero-shot with source code only, (3) few-shot with code and label pairs,
and (4) fine-tuned on a small custom dataset.
  Our results show that state-of-the-art LLMs have a strong understanding of
the Roofline model, achieving 100% classification accuracy when provided with
explicit profiling data. We also find that reasoning-capable LLMs significantly
outperform standard LLMs in zero- and few-shot settings, achieving up to 64%
accuracy on GPU source codes, without profiling information. Lastly, we find
that LLM fine-tuning will require much more data than what we currently have
available.
  This work is among the first to use LLMs for source-level roofline
performance prediction via classification, and illustrates their potential to
guide optimization efforts when runtime profiling is infeasible. Our findings
suggest that with better datasets and prompt strategies, LLMs could become
practical tools for HPC performance analysis and performance portability.

</details>

### [290] [Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving](https://arxiv.org/abs/2505.04021)
*Shan Yu,Jiarong Xing,Yifan Qiao,Mingyuan Ma,Yangmin Li,Yang Wang,Shuo Yang,Zhiqiang Xie,Shiyi Cao,Ke Bao,Ion Stoica,Harry Xu,Ying Sheng*

Main category: cs.DC

TLDR: Prism是一个多LLM服务系统，通过动态GPU内存共享和两级调度策略，显著降低成本并满足服务延迟目标。


<details>
  <summary>Details</summary>
Motivation: 由于多LLM服务的高成本和动态工作负载特性，现有GPU共享系统无法灵活调整资源分配，导致效率低下和SLO难以达成。

Method: Prism采用动态内存映射和两级调度策略，实现跨模型内存协调和灵活资源分配。

Result: 实验表明，Prism比现有系统节省2倍以上成本，SLO达成率提高3.3倍。

Conclusion: Prism通过创新的内存共享和调度设计，有效解决了多LLM服务的成本和性能挑战。

Abstract: Serving large language models (LLMs) is expensive, especially for providers
hosting many models, making cost reduction essential. The unique workload
patterns of serving multiple LLMs (i.e., multi-LLM serving) create new
opportunities and challenges for this task. The long-tail popularity of models
and their long idle periods present opportunities to improve utilization
through GPU sharing. However, existing GPU sharing systems lack the ability to
adjust their resource allocation and sharing policies at runtime, making them
ineffective at meeting latency service-level objectives (SLOs) under rapidly
fluctuating workloads.
  This paper presents Prism, a multi-LLM serving system that unleashes the full
potential of GPU sharing to achieve both cost efficiency and SLO attainment. At
its core, Prism tackles a key limitation of existing
systems$\unicode{x2014}$the lack of $\textit{cross-model memory coordination}$,
which is essential for flexibly sharing GPU memory across models under dynamic
workloads. Prism achieves this with two key designs. First, it supports
on-demand memory allocation by dynamically mapping physical to virtual memory
pages, allowing flexible memory redistribution among models that space- and
time-share a GPU. Second, it improves memory efficiency through a two-level
scheduling policy that dynamically adjusts sharing strategies based on models'
runtime demands. Evaluations on real-world traces show that Prism achieves more
than $2\times$ cost savings and $3.3\times$ SLO attainment compared to
state-of-the-art systems.

</details>

### [291] [MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models](https://arxiv.org/abs/2505.03906)
*Asif Rahman,Veljko Cvetkovic,Kathleen Reece,Aidan Walters,Yasir Hassan,Aneesh Tummeti,Bryan Torres,Denise Cooney,Margaret Ellis,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TLDR: MARCO框架通过多代理架构优化LLM生成的HPC代码，结合实时网络搜索技术，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在HPC代码生成中忽视专业优化需求，MARCO旨在填补这一空白。

Method: MARCO采用多代理架构，包括代码生成和性能评估代理，并通过反馈循环和实时网络搜索优化代码。

Result: MARCO在LeetCode 75问题集上平均运行时减少14.6%，结合网络搜索后性能提升30.9%。

Conclusion: 多代理系统为HPC代码生成提供高效解决方案，避免领域特定模型微调的高成本。

Abstract: Large language models (LLMs) have transformed software development through
code generation capabilities, yet their effectiveness for high-performance
computing (HPC) remains limited. HPC code requires specialized optimizations
for parallelism, memory efficiency, and architecture-specific considerations
that general-purpose LLMs often overlook. We present MARCO (Multi-Agent
Reactive Code Optimizer), a novel framework that enhances LLM-generated code
for HPC through a specialized multi-agent architecture. MARCO employs separate
agents for code generation and performance evaluation, connected by a feedback
loop that progressively refines optimizations. A key innovation is MARCO's
web-search component that retrieves real-time optimization techniques from
recent conference proceedings and research publications, bridging the knowledge
gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem
set demonstrates that MARCO achieves a 14.6% average runtime reduction compared
to Claude 3.5 Sonnet alone, while the integration of the web-search component
yields a 30.9% performance improvement over the base MARCO system. These
results highlight the potential of multi-agent systems to address the
specialized requirements of high-performance code generation, offering a
cost-effective alternative to domain-specific model fine-tuning.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [292] [IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification](https://arxiv.org/abs/2505.03838)
*Ting Yu Tsai,An Yu,Meghana Spurthi Maadugundu,Ishrat Jahan Mohima,Umme Habiba Barsha,Mei-Hwa F. Chen,Balakrishnan Prabhakaran,Ming-Ching Chang*

Main category: eess.IV

TLDR: IntelliCardiac是一个基于AI的4D心脏图像自动分割和疾病分类平台，准确率高，支持实时可视化和临床决策辅助。


<details>
  <summary>Details</summary>
Motivation: 精确处理心脏影像数据对心血管疾病识别和管理至关重要，现有方法性能不足。

Method: 结合深度学习分割模型和两步分类流程，利用ACDC数据集训练。

Result: 分割模块准确率92.6%，分类模块准确率98%，优于现有方法。

Conclusion: IntelliCardiac具有潜力成为临床决策的准确、可扩展工具。

Abstract: Precise and effective processing of cardiac imaging data is critical for the
identification and management of the cardiovascular diseases. We introduce
IntelliCardiac, a comprehensive, web-based medical image processing platform
for the automatic segmentation of 4D cardiac images and disease classification,
utilizing an AI model trained on the publicly accessible ACDC dataset. The
system, intended for patients, cardiologists, and healthcare professionals,
offers an intuitive interface and uses deep learning models to identify
essential heart structures and categorize cardiac diseases. The system supports
analysis of both the right and left ventricles as well as myocardium, and then
classifies patient's cardiac images into five diagnostic categories: dilated
cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right
ventricular abnormality, and no disease. IntelliCardiac combines a deep
learning-based segmentation model with a two-step classification pipeline. The
segmentation module gains an overall accuracy of 92.6\%. The classification
module, trained on characteristics taken from segmented heart structures,
achieves 98\% accuracy in five categories. These results exceed the performance
of the existing state-of-the-art methods that integrate both segmentation and
classification models. IntelliCardiac, which supports real-time visualization,
workflow integration, and AI-assisted diagnostics, has great potential as a
scalable, accurate tool for clinical decision assistance in cardiac imaging and
diagnosis.

</details>

### [293] [From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation](https://arxiv.org/abs/2505.03844)
*Solène Debuysère,Nicolas Trouvé,Nathan Letheule,Olivier Lévêque,Elise Colin*

Main category: eess.IV

TLDR: 论文提出了一种利用预训练潜在扩散模型和空间条件技术，将卫星SAR图像转换为机载SAR表示的新方法，解决了机载SAR数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 机载高分辨率SAR图像获取成本高且数据稀缺，限制了现有基础模型在遥感应用中的使用。

Method: 利用ONERA的15年机载SAR数据构建训练集，采用35亿参数的预训练潜在扩散模型，结合空间条件技术进行图像转换。

Result: 方法成功将卫星SAR图像转换为机载SAR表示，并提升了模拟图像的逼真度。

Conclusion: 该研究为SAR成像技术的进步提供了新的AI应用方向，填补了文献空白。

Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has
increased considerably in recent years, with datasets commercially available.
However, the acquisition of high-resolution SAR images in airborne
configurations, remains costly and limited. Thus, the lack of open source,
well-labeled, or easily exploitable SAR text-image datasets is a barrier to the
use of existing foundation models in remote sensing applications. In this
context, synthetic image generation is a promising solution to augment this
scarce data, enabling a broader range of applications. Leveraging over 15 years
of ONERA's extensive archival airborn data from acquisition campaigns, we
created a comprehensive training dataset of 110 thousands SAR images to exploit
a 3.5 billion parameters pre-trained latent diffusion model. In this work, we
present a novel approach utilizing spatial conditioning techniques within a
foundation model to transform satellite SAR imagery into airborne SAR
representations. Additionally, we demonstrate that our pipeline is effective
for bridging the realism of simulated images generated by ONERA's physics-based
simulator EMPRISE. Our method explores a key application of AI in advancing SAR
imaging technology. To the best of our knowledge, we are the first to introduce
this approach in the literature.

</details>

### [294] [A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos](https://arxiv.org/abs/2505.03845)
*Ioannis Kyprakis,Vasileios Skaramagkas,Iro Boura,Georgios Karamanis,Dimitrios I. Fotiadis,Zinovia Kefalopoulou,Cleanthe Spanaki,Manolis Tsiknakis*

Main category: eess.IV

TLDR: 该研究利用深度学习模型（ViViT、Video Swin Tiny和3D CNN-LSTM）通过面部视频分析评估帕金森病患者的抑郁症状，其中Video Swin Tiny模型表现最佳，准确率高达94%。


<details>
  <summary>Details</summary>
Motivation: 帕金森病患者的抑郁症状常被漏诊，因与运动症状重叠。研究旨在通过深度学习模型提高抑郁症状的检测准确性。

Method: 使用三种深度学习模型（ViViT、Video Swin Tiny和3D CNN-LSTM）分析面部视频，评估抑郁症状的存在和严重程度，并考虑药物状态的影响。

Result: Video Swin Tiny模型在二元分类（抑郁症状存在与否）中达到94%准确率和93.7% F1分数，在多分类任务中表现也优异。

Conclusion: 深度学习模型，尤其是Video Swin Tiny，可有效识别帕金森病患者的抑郁症状，为临床诊断提供新工具。

Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with
motor and non-motor symptoms. Depressive symptoms are prevalent in PD,
affecting up to 45% of patients. They are often underdiagnosed due to
overlapping motor features, such as hypomimia. This study explores deep
learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention
layers-to assess the presence and severity of depressive symptoms, as detected
by the Geriatric Depression Scale (GDS), in PD patients through facial video
analysis. The same parameters were assessed in a secondary analysis taking into
account whether patients were one hour after (ON-medication state) or 12 hours
without (OFF-medication state) dopaminergic medication. Using a dataset of
1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest
performance, with up to 94% accuracy and 93.7% F1-score in binary
classification (presence of absence of depressive symptoms), and 87.1% accuracy
with an 85.4% F1-score in multiclass tasks (absence or mild or severe
depressive symptoms).

</details>

### [295] [Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification](https://arxiv.org/abs/2505.04003)
*Feng Gao,Sheng Liu,Chuanzheng Gong,Xiaowei Zhou,Jiayi Wang,Junyu Dong,Qian Du*

Main category: eess.IV

TLDR: 论文提出了一种基于原型的信息补偿网络（PICNet），用于结合HSI和SAR/LiDAR数据的多源遥感数据联合分类，解决了多源特征耦合和互补信息探索不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 多源遥感数据联合分类旨在通过利用多源数据的互补信息提高土地覆盖分类的准确性和可靠性，但现有方法面临多源特征耦合和互补信息探索不一致的挑战。

Method: 设计了频率交互模块增强多源特征提取中的频率耦合，并通过原型补偿模块建模全局互补信息，利用模态原型实现跨模态特征整合和对齐。

Result: 在三个公开数据集上的实验表明，PICNet显著优于现有方法。

Conclusion: PICNet通过频率交互和原型补偿有效解决了多源遥感数据分类中的关键问题，具有显著优势。

Abstract: Multi-source remote sensing data joint classification aims to provide
accuracy and reliability of land cover classification by leveraging the
complementary information from multiple data sources. Existing methods confront
two challenges: inter-frequency multi-source feature coupling and inconsistency
of complementary information exploration. To solve these issues, we present a
Prototype-based Information Compensation Network (PICNet) for land cover
classification based on HSI and SAR/LiDAR data. Specifically, we first design a
frequency interaction module to enhance the inter-frequency coupling in
multi-source feature extraction. The multi-source features are first decoupled
into high- and low-frequency components. Then, these features are recoupled to
achieve efficient inter-frequency communication. Afterward, we design a
prototype-based information compensation module to model the global
multi-source complementary information. Two sets of learnable modality
prototypes are introduced to represent the global modality information of
multi-source data. Subsequently, cross-modal feature integration and alignment
are achieved through cross-attention computation between the modality-specific
prototype vectors and the raw feature representations. Extensive experiments on
three public datasets demonstrate the significant superiority of our PICNet
over state-of-the-art methods. The codes are available at
https://github.com/oucailab/PICNet.

</details>

### [296] [3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation](https://arxiv.org/abs/2505.04097)
*Thien Nhan Vo,Bac Nam Ho,Thanh Xuan Truong*

Main category: eess.IV

TLDR: 开发了一种3D卷积神经网络，用于将T1加权脑MRI扫描分类为健康或阿尔茨海默病，通过噪声注入和交叉验证，准确率达到0.912，ROC曲线下面积为0.961。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过简单的数据增强方法提高3D MRI分类的准确性，并探索未来更先进的增强方法和架构。

Method: 采用3D卷积、池化、批量归一化、密集ReLU层和Sigmoid输出，结合随机噪声注入和五折交叉验证。

Result: 测试集准确率为0.912，ROC曲线下面积为0.961，灵敏度和特异性均超过0.90。

Conclusion: 结果表明简单增强方法对3D MRI分类有效，未来可探索更先进的增强方法和架构如3D U-Net和视觉变换器。

Abstract: A three-dimensional convolutional neural network was developed to classify
T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D
convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid
output. Using stochastic noise injection and five-fold cross-validation, the
model achieved test set accuracy of 0.912 and area under the ROC curve of
0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity
and specificity both exceeded 0.90. These results align with prior work
reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate
the effectiveness of simple augmentation for 3D MRI classification and motivate
future exploration of advanced augmentation methods and architectures such as
3D U-Net and vision transformers.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [297] [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
*Kai Ruan,Mowen Huang,Ji-Rong Wen,Hao Sun*

Main category: cs.MA

TLDR: SwarmBench是一个新基准，用于评估LLM在分散多智能体系统中的群体智能能力，揭示了局部信息约束下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在严格约束下（如局部感知和通信）的群体智能能力，填补现有基准的不足。

Method: 设计SwarmBench，包含五个MAS协调任务，基于2D网格环境，依赖局部感知和通信。

Result: LLM在零样本设置下表现差异显著，局部信息约束下规划和策略形成存在局限性。

Conclusion: SwarmBench为LLM在分散系统中的潜力评估提供了工具，促进可复现研究。

Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their
capacity for emergent coordination in Multi-Agent Systems (MAS) when operating
under strict constraints-such as limited local perception and communication,
characteristic of natural swarms-remains largely unexplored, particularly
concerning the nuances of swarm intelligence. Existing benchmarks often do not
fully capture the unique challenges of decentralized coordination that arise
when agents operate with incomplete spatio-temporal information. To bridge this
gap, we introduce SwarmBench, a novel benchmark designed to systematically
evaluate the swarm intelligence capabilities of LLMs acting as decentralized
agents. SwarmBench features five foundational MAS coordination tasks within a
configurable 2D grid environment, forcing agents to rely primarily on local
sensory input (k x k view) and local communication. We propose metrics for
coordination effectiveness and analyze emergent group dynamics. Evaluating
several leading LLMs in a zero-shot setting, we find significant performance
variations across tasks, highlighting the difficulties posed by local
information constraints. While some coordination emerges, results indicate
limitations in robust planning and strategy formation under uncertainty in
these decentralized scenarios. Assessing LLMs under swarm-like conditions is
crucial for realizing their potential in future decentralized systems. We
release SwarmBench as an open, extensible toolkit-built upon a customizable and
scalable physical system with defined mechanical properties. It provides
environments, prompts, evaluation scripts, and the comprehensive experimental
datasets generated, aiming to foster reproducible research into LLM-based MAS
coordination and the theoretical underpinnings of Embodied MAS. Our code
repository is available at https://github.com/x66ccff/swarmbench.

</details>

### [298] [From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems](https://arxiv.org/abs/2505.03864)
*Qiaomu Li,Ying Xie*

Main category: cs.MA

TLDR: 论文探讨了整合Google的A2A协议和Anthropic的MCP协议时面临的挑战，包括语义互操作性、安全风险和治理问题。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中因协议整合带来的新兴问题，推动"智能体经济"的实现。

Method: 通过批判性分析，评估A2A和MCP整合的实际影响和固有困难。

Result: 整合带来专业化、可扩展性等优势，但也增加了安全漏洞、隐私复杂性和调试难度。

Conclusion: A2A+MCP为架构基础，但需进一步解决整合中的复杂性以实现其潜力。

Abstract: Artificial intelligence is rapidly evolving towards multi-agent systems where
numerous AI agents collaborate and interact with external tools. Two key open
standards, Google's Agent to Agent (A2A) protocol for inter-agent communication
and Anthropic's Model Context Protocol (MCP) for standardized tool access,
promise to overcome the limitations of fragmented, custom integration
approaches. While their potential synergy is significant, this paper argues
that effectively integrating A2A and MCP presents unique, emergent challenges
at their intersection, particularly concerning semantic interoperability
between agent tasks and tool capabilities, the compounded security risks
arising from combined discovery and execution, and the practical governance
required for the envisioned "Agent Economy". This work provides a critical
analysis, moving beyond a survey to evaluate the practical implications and
inherent difficulties of combining these horizontal and vertical integration
standards. We examine the benefits (e.g., specialization, scalability) while
critically assessing their dependencies and trade-offs in an integrated
context. We identify key challenges increased by the integration, including
novel security vulnerabilities, privacy complexities, debugging difficulties
across protocols, and the need for robust semantic negotiation mechanisms. In
summary, A2A+MCP offers a vital architectural foundation, but fully realizing
its potential requires substantial advancements to manage the complexities of
their combined operation.

</details>

### [299] [Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic](https://arxiv.org/abs/2505.04379)
*Mohammad Elayan,Wissam Kontar*

Main category: cs.MA

TLDR: 论文通过高分辨率轨迹数据分析了自动驾驶车辆（AV）与人类驾驶车辆（HDV）在信号灯城市交叉口和弱势道路使用者（VRU）附近的行为，量化了交通系统中的共识性。结果发现，安全、交互和性能三个维度的完全共识罕见，仅1.63%的AV-VRU交互帧满足所有条件。


<details>
  <summary>Details</summary>
Motivation: 研究交通系统中自动驾驶车辆（AV）与人类驾驶车辆（HDV）及弱势道路使用者（VRU）之间的共识性，以解决安全、交互和性能的多维度平衡问题。

Method: 使用第三代模拟（TGSIM）数据集的高分辨率轨迹数据，评估包括碰撞时间（TTC）、侵入后时间（PET）、减速模式、车距和队列稳定性等关键指标。

Result: 结果显示，安全、交互和性能三个维度的完全共识仅占1.63%的AV-VRU交互帧。

Conclusion: 研究强调了在混合交通环境中需要开发能够明确平衡多维度性能的AV模型。

Abstract: Transportation systems have long been shaped by complexity and heterogeneity,
driven by the interdependency of agent actions and traffic outcomes. The
deployment of automated vehicles (AVs) in such systems introduces a new
challenge: achieving consensus across safety, interaction quality, and traffic
performance. In this work, we position consensus as a fundamental property of
the traffic system and aim to quantify it. We use high-resolution trajectory
data from the Third Generation Simulation (TGSIM) dataset to empirically
analyze AV and human-driven vehicle (HDV) behavior at a signalized urban
intersection and around vulnerable road users (VRUs). Key metrics, including
Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,
headways, and string stability, are evaluated across the three performance
dimensions. Results show that full consensus across safety, interaction, and
performance is rare, with only 1.63% of AV-VRU interaction frames meeting all
three conditions. These findings highlight the need for AV models that
explicitly balance multi-dimensional performance in mixed-traffic environments.
Full reproducibility is supported via our open-source codebase on
https://github.com/wissamkontar/Consensus-AV-Analysis.

</details>

### [300] [Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions](https://arxiv.org/abs/2505.04579)
*Stéphane Aroca-Ouellette,Miguel Aroca-Ouellette,Katharina von der Wense,Alessandro Roncone*

Main category: cs.MA

TLDR: HA$^2$框架通过分层强化学习提升自主代理在协作任务中的零-shot协调能力，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 自主代理在适应新队友时表现不佳，缺乏人类依赖的共享任务抽象机制。

Method: 引入HA$^2$框架，利用分层强化学习模拟人类协作的结构化方法。

Result: 在Overcooked环境中，HA$^2$显著优于现有方法，适应性强且表现优异。

Conclusion: HA$^2$为解决零-shot协调问题提供了有效方案，性能优于现有技术。

Abstract: In collaborative tasks, autonomous agents fall short of humans in their
capability to quickly adapt to new and unfamiliar teammates. We posit that a
limiting factor for zero-shot coordination is the lack of shared task
abstractions, a mechanism humans rely on to implicitly align with teammates. To
address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework
leveraging hierarchical reinforcement learning to mimic the structured approach
humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,
demonstrating statistically significant improvement over existing baselines
when paired with both unseen agents and humans, providing better resilience to
environmental shifts, and outperforming all state-of-the-art methods.

</details>

<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [301] [The Evolution of Rough Sets 1970s-1981](https://arxiv.org/abs/2505.03747)
*Viktor Marek,Ewa Orłowska,Ivo Düntsch*

Main category: math.HO

TLDR: 回顾了Zdzisław Pawlak及其合作者在1970年代和1981年的研究与出版物，重点关注这些出版物中的灵感来源，并概述了1981年与粗糙集和信息系统相关的发展。


<details>
  <summary>Details</summary>
Motivation: 回顾Pawlak及其合作者的早期工作，探讨其灵感来源，并总结粗糙集和信息系统的后续发展。

Method: 通过分析1970年代和1981年的出版物，识别灵感来源并梳理相关发展。

Result: 明确了Pawlak早期工作的灵感来源，并总结了粗糙集和信息系统的关键进展。

Conclusion: Pawlak的研究为粗糙集和信息系统奠定了基础，其灵感来源对后续发展具有重要影响。

Abstract: In this note research and publications by Zdzis{\l}aw Pawlak and his
collaborators from 1970s and 1981 are recalled. Focus is placed on the sources
of inspiration which one can identify on the basis of those publications.
Finally, developments from 1981 related to rough sets and information systems
are outlined.

</details>

<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [302] [Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching](https://arxiv.org/abs/2505.04603)
*Wenhui Sophia Lu,Wing Hung Wong*

Main category: stat.ME

TLDR: 提出了一种名为ABI的新框架，通过直接比较后验空间中的分布来克服ABC在高维或扩散先验下的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: ABC在似然不可用或难以计算时广泛使用，但在高维或扩散先验下效率低下。

Method: ABI利用MSW距离和条件分位数回归，将后验分布差异转化为一维任务，并引入自适应拒绝采样。

Result: ABI在理论和实证中均优于传统ABC方法，尤其在高维或依赖观测场景下表现突出。

Conclusion: ABI为高维或复杂后验推断提供了高效且理论可靠的解决方案。

Abstract: When the likelihood is analytically unavailable and computationally
intractable, approximate Bayesian computation (ABC) has emerged as a widely
used methodology for approximate posterior inference; however, it suffers from
severe computational inefficiency in high-dimensional settings or under diffuse
priors. To overcome these limitations, we propose Adaptive Bayesian Inference
(ABI), a framework that bypasses traditional data-space discrepancies and
instead compares distributions directly in posterior space through
nonparametric distribution matching. By leveraging a novel Marginally-augmented
Sliced Wasserstein (MSW) distance on posterior measures and exploiting its
quantile representation, ABI transforms the challenging problem of measuring
divergence between posterior distributions into a tractable sequence of
one-dimensional conditional quantile regression tasks. Moreover, we introduce a
new adaptive rejection sampling scheme that iteratively refines the posterior
approximation by updating the proposal distribution via generative density
estimation. Theoretically, we establish parametric convergence rates for the
trimmed MSW distance and prove that the ABI posterior converges to the true
posterior as the tolerance threshold vanishes. Through extensive empirical
evaluation, we demonstrate that ABI significantly outperforms data-based
Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free
simulators, especially in high-dimensional or dependent observation regimes.

</details>

<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [303] [The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea](https://arxiv.org/abs/2505.03835)
*Simon Suh,Jihyuk Bang,Ji Woo Han*

Main category: cs.DL

TLDR: 本文研究了COVID-19疫情和ChatGPT发布对AI政策研究中预印本引用区域趋势的影响，发现美国、欧洲和韩国的增长模式不同，强调了区域差异对开放科学采纳的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨全球性事件如何影响AI政策研究中预印本的引用趋势，并分析区域间的差异。

Method: 使用Web of Science的文献计量数据，追踪2015至2024年间美国、欧洲和韩国的预印本引用变化。

Result: 所有地区的预印本引用均增长，但模式不同：美国事件驱动增长，欧洲机构推动，韩国线性增长。

Conclusion: 全球事件加速了预印本采纳，但区域文化和政策环境决定了具体模式，未来AI治理需考虑这些差异。

Abstract: The adoption of open science has quickly changed how artificial intelligence
(AI) policy research is distributed globally. This study examines the regional
trends in the citation of preprints, specifically focusing on the impact of two
major disruptive events: the COVID-19 pandemic and the release of ChatGPT, on
research dissemination patterns in the United States, Europe, and South Korea
from 2015 to 2024. Using bibliometrics data from the Web of Science, this study
tracks how global disruptive events influenced the adoption of preprints in AI
policy research and how such shifts vary by region. By marking the timing of
these disruptive events, the analysis reveals that while all regions
experienced growth in preprint citations, the magnitude and trajectory of
change varied significantly. The United States exhibited sharp, event-driven
increases; Europe demonstrated institutional growth; and South Korea maintained
consistent, linear growth in preprint adoption. These findings suggest that
global disruptions may have accelerated preprint adoption, but the extent and
trajectory are shaped by local research cultures, policy environments, and
levels of open science maturity. This paper emphasizes the need for future AI
governance strategies to consider regional variability in research
dissemination and highlights opportunities for further longitudinal and
comparative research to deepen our understanding of open-access adoption in AI
policy development.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [304] [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/abs/2505.03814)
*Ganghua Wang,Zhaorun Chen,Bo Li,Haifeng Xu*

Main category: stat.ML

TLDR: 本文提出了一种可认证且成本高效的LLM评估框架Cer-Eval，通过自适应选择测试点减少评估成本，同时保持高置信度。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模的扩大，评估大型语言模型（LLMs）的挑战日益增加，当前评估方法缺乏对测试数据充分性和样本选择的系统性指导。

Method: 提出基于“测试样本复杂度”的理论框架，开发分区算法Cer-Eval，自适应选择测试点以减少评估成本。

Result: 实验表明，Cer-Eval可节省20%至40%的测试点，同时保持与当前评估相当的误差水平并提供95%置信度。

Conclusion: Cer-Eval为LLM评估提供了一种高效且可认证的解决方案，显著降低了评估成本。

Abstract: As foundation models continue to scale, the size of trained models grows
exponentially, presenting significant challenges for their evaluation. Current
evaluation practices involve curating increasingly large datasets to assess the
performance of large language models (LLMs). However, there is a lack of
systematic analysis and guidance on determining the sufficiency of test data or
selecting informative samples for evaluation. This paper introduces a
certifiable and cost-efficient evaluation framework for LLMs. Our framework
adapts to different evaluation objectives and outputs confidence intervals that
contain true values with high probability. We use ``test sample complexity'' to
quantify the number of test points needed for a certifiable evaluation and
derive tight bounds on test sample complexity. Based on the developed theory,
we develop a partition-based algorithm, named Cer-Eval, that adaptively selects
test points to minimize the cost of LLM evaluation. Real-world experiments
demonstrate that Cer-Eval can save 20% to 40% test points across various
benchmarks, while maintaining an estimation error level comparable to the
current evaluation process and providing a 95% confidence guarantee.

</details>

### [305] [Categorical and geometric methods in statistical, manifold, and machine learning](https://arxiv.org/abs/2505.03862)
*Hông Vân Lê,Hà Quang Minh,Frederic Protin,Wilderich Tuschmann*

Main category: stat.ML

TLDR: 本文探讨了概率态射范畴的应用及几何方法在统计、机器和流形学习中的问题。


<details>
  <summary>Details</summary>
Motivation: 研究概率态射范畴及其几何方法在多种学习问题中的应用潜力。

Method: 结合概率态射范畴理论和几何方法，分析统计、机器和流形学习中的问题。

Result: 展示了这些方法在解决实际问题中的有效性，并将在即将出版的书中详细讨论。

Conclusion: 概率态射范畴和几何方法为学习问题提供了新的研究视角和工具。

Abstract: We present and discuss applications of the category of probabilistic
morphisms, initially developed in \cite{Le2023}, as well as some geometric
methods to several classes of problems in statistical, machine and manifold
learning which shall be, along with many other topics, considered in depth in
the forthcoming book \cite{LMPT2024}.

</details>

### [306] [Variational Formulation of the Particle Flow Particle Filter](https://arxiv.org/abs/2505.04007)
*Yinzhuang Yi,Jorge Cortés,Nikolay Atanasov*

Main category: stat.ML

TLDR: 本文从变分推断的角度提出了粒子流粒子滤波的公式化方法，表明其瞬态密度遵循Fisher-Rao梯度流的时间尺度轨迹。


<details>
  <summary>Details</summary>
Motivation: 通过变分推断的视角重新理解粒子流粒子滤波，揭示其与Fisher-Rao梯度流的关系。

Method: 利用变分推断方法，将粒子流粒子滤波的瞬态密度建模为Fisher-Rao梯度流的时间尺度轨迹。

Result: 证明了瞬态密度与Fisher-Rao梯度流的关系，并展示了其在最小化Kullback-Leibler散度中的作用。

Conclusion: 通过变分推断的框架，为粒子流粒子滤波提供了新的理论基础，揭示了其与Fisher-Rao梯度流的联系。

Abstract: This paper provides a formulation of the particle flow particle filter from
the perspective of variational inference. We show that the transient density
used to derive the particle flow particle filter follows a time-scaled
trajectory of the Fisher-Rao gradient flow in the space of probability
densities. The Fisher-Rao gradient flow is obtained as a continuous-time
algorithm for variational inference, minimizing the Kullback-Leibler divergence
between a variational density and the true posterior density.

</details>

### [307] [A Tutorial on Discriminative Clustering and Mutual Information](https://arxiv.org/abs/2505.04484)
*Louis Ohl,Pierre-Alexandre Mattei,Frédéric Precioso*

Main category: stat.ML

TLDR: 本文回顾了判别式聚类方法的历史演变，重点讨论了其假设的变化、互信息的作用及其局限性，并介绍了相关Python工具GemClus。


<details>
  <summary>Details</summary>
Motivation: 为高维数据提供判别式聚类方法的可访问历史视角，并展示其假设从决策边界到不变性批评的演变。

Method: 通过历史回顾和分析，探讨判别式聚类方法的假设变化、互信息的作用及其局限性。

Result: 总结了判别式聚类方法的进展，展示了互信息的关键作用及其局限性，并介绍了GemClus工具的应用。

Conclusion: 判别式聚类方法在假设和工具上取得了进展，但仍面临聚类数量选择的挑战。

Abstract: To cluster data is to separate samples into distinctive groups that should
ideally have some cohesive properties. Today, numerous clustering algorithms
exist, and their differences lie essentially in what can be perceived as
``cohesive properties''. Therefore, hypotheses on the nature of clusters must
be set: they can be either generative or discriminative. As the last decade
witnessed the impressive growth of deep clustering methods that involve neural
networks to handle high-dimensional data often in a discriminative manner; we
concentrate mainly on the discriminative hypotheses. In this paper, our aim is
to provide an accessible historical perspective on the evolution of
discriminative clustering methods and notably how the nature of assumptions of
the discriminative models changed over time: from decision boundaries to
invariance critics. We notably highlight how mutual information has been a
historical cornerstone of the progress of (deep) discriminative clustering
methods. We also show some known limitations of mutual information and how
discriminative clustering methods tried to circumvent those. We then discuss
the challenges that discriminative clustering faces with respect to the
selection of the number of clusters. Finally, we showcase these techniques
using the dedicated Python package, GemClus, that we have developed for
discriminative clustering.

</details>

### [308] [From Two Sample Testing to Singular Gaussian Discrimination](https://arxiv.org/abs/2505.04613)
*Leonardo V. Santoro,Kartik G. Waghmare,Victor M. Panaretos*

Main category: stat.ML

TLDR: 论文提出了一种通过高斯嵌入在再生核希尔伯特空间中测试概率测度相等性的方法，证明了其与原始问题的等价性，并展示了高维环境下的信息优势。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决在高维或复杂空间中测试两个概率测度相等性的困难，提出一种更高效的方法。

Method: 利用核均值和协方差嵌入将概率测度转换为高斯测度，并基于Feldman-Hajek准则分析其奇异性。

Result: 证明了两个概率测度的差异在高斯嵌入中被显著放大，从而简化了测试问题。

Conclusion: 该方法在高维环境中具有显著优势，为高效推断工具的设计提供了新思路。

Abstract: We establish that testing for the equality of two probability measures on a
general separable and compact metric space is equivalent to testing for the
singularity between two corresponding Gaussian measures on a suitable
Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via
the notion of kernel mean and covariance embedding of a probability measure.
Discerning two singular Gaussians is fundamentally simpler from an
information-theoretic perspective than non-parametric two-sample testing,
particularly in high-dimensional settings. Our proof leverages the
Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert
spaces, and shows that discrepancies between distributions are heavily
magnified through their corresponding Gaussian embeddings: at a population
level, distinct probability measures lead to essentially separated Gaussian
embeddings. This appears to be a new instance of the blessing of dimensionality
that can be harnessed for the design of efficient inference tools in great
generality.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [309] [Tracing Vulnerability Propagation Across Open Source Software Ecosystems](https://arxiv.org/abs/2505.04307)
*Jukka Ruohonen,Qusai Ramadan*

Main category: cs.SE

TLDR: 分析了84,000多个漏洞在28个开源软件生态系统中的传播情况，发现传播路径复杂，GitHub、Debian和Ubuntu表现突出，传播延迟长且与生态系统数量无关。


<details>
  <summary>Details</summary>
Motivation: 研究漏洞在开源软件生态系统中的传播路径和延迟，填补软件生态系统、可追溯性和漏洞之间的知识空白。

Method: 对28个开源软件生态系统中的84,000多个漏洞进行传播路径和延迟的追踪分析。

Result: 传播路径复杂，GitHub、Debian和Ubuntu表现突出；传播延迟长且与生态系统数量无关；无明确可解释的模式。

Conclusion: 研究为软件生态系统、可追溯性和漏洞的交叉领域提供了新的见解。

Abstract: The paper presents a traceability analysis of how over 84 thousand
vulnerabilities have propagated across 28 open source software ecosystems.
According to the results, the propagation sequences have been complex in
general, although GitHub, Debian, and Ubuntu stand out. Furthermore, the
associated propagation delays have been lengthy, and these do not correlate
well with the number of ecosystems involved in the associated sequences. Nor
does the presence or absence of particularly ecosystems in the sequences yield
clear, interpretable patterns. With these results, the paper contributes to the
overlapping knowledge bases about software ecosystems, traceability, and
vulnerabilities.

</details>

### [310] [An Empirical Study of OpenAI API Discussions on Stack Overflow](https://arxiv.org/abs/2505.04084)
*Xiang Chen,Jibin Wang,Chaoyang Gao,Xiaolin Ju,Zhanqi Cui*

Main category: cs.SE

TLDR: 本文首次通过分析Stack Overflow上的2,874条OpenAI API相关讨论，揭示了开发者在使用OpenAI API时面临的独特挑战，并提出了针对开发者、LLM供应商和研究者的可行建议。


<details>
  <summary>Details</summary>
Motivation: OpenAI API与传统API不同，存在提示工程、成本管理、非确定性输出等独特挑战，但相关开发者挑战尚未被实证研究。

Method: 通过分析Stack Overflow上的2,874条OpenAI API相关讨论，手动分类为九类，并通过主题建模分析识别每类的具体挑战。

Result: 研究发现开发者在使用OpenAI API时面临多类挑战，包括提示工程复杂性和非确定性输出等。

Conclusion: 基于实证结果，提出了针对开发者、LLM供应商和研究者的具体建议，以改善OpenAI API的使用体验。

Abstract: The rapid advancement of large language models (LLMs), represented by
OpenAI's GPT series, has significantly impacted various domains such as natural
language processing, software development, education, healthcare, finance, and
scientific research. However, OpenAI APIs introduce unique challenges that
differ from traditional APIs, such as the complexities of prompt engineering,
token-based cost management, non-deterministic outputs, and operation as black
boxes. To the best of our knowledge, the challenges developers encounter when
using OpenAI APIs have not been explored in previous empirical studies. To fill
this gap, we conduct the first comprehensive empirical study by analyzing 2,874
OpenAI API-related discussions from the popular Q&A forum Stack Overflow. We
first examine the popularity and difficulty of these posts. After manually
categorizing them into nine OpenAI API-related categories, we identify specific
challenges associated with each category through topic modeling analysis. Based
on our empirical findings, we finally propose actionable implications for
developers, LLM vendors, and researchers.

</details>

### [311] [Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering](https://arxiv.org/abs/2505.04251)
*Krishna Ronanki*

Main category: cs.SE

TLDR: 本文提出了一种基于RACI的框架，用于解决LLM驱动的多智能体自主系统（LMA）在软件工程中任务分配的可信问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）在软件工程中表现优异，但LLM驱动的LMA系统引入后，任务分配的可信性成为主要挑战。

Method: 提出了一种基于RACI的框架，提供实施指南和示例实现。

Result: 该框架能促进高效协作、确保责任明确，并降低LLM驱动自动化的风险。

Conclusion: 未来计划通过实证验证进一步完善该框架。

Abstract: Multi-agent autonomous systems (MAS) are better at addressing challenges that
spans across multiple domains than singular autonomous agents. This holds true
within the field of software engineering (SE) as well. The state-of-the-art
research on MAS within SE focuses on integrating LLMs at the core of autonomous
agents to create LLM-based multi-agent autonomous (LMA) systems. However, the
introduction of LMA systems into SE brings a plethora of challenges. One of the
major challenges is the strategic allocation of tasks between humans and the
LMA system in a trustworthy manner. To address this challenge, a RACI-based
framework is proposed in this work in progress article, along with
implementation guidelines and an example implementation of the framework. The
proposed framework can facilitate efficient collaboration, ensure
accountability, and mitigate potential risks associated with LLM-driven
automation while aligning with the Trustworthy AI guidelines. The future steps
for this work delineating the planned empirical validation method are also
presented.

</details>