<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.CL](#cs.CL) [Total: 40]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.CV](#cs.CV) [Total: 64]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [stat.ME](#stat.ME) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.SE](#cs.SE) [Total: 9]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 14]
- [quant-ph](#quant-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.HC](#cs.HC) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [FLARE: Feature-based Lightweight Aggregation for Robust Evaluation of IoT Intrusion Detection](https://arxiv.org/abs/2504.15375)
*Bradley Boswell,Seth Barrett,Swarnamugi Rajaganapathy,Gokila Dorai*

Main category: cs.CR

TLDR: FLARE是一种基于特征的轻量级聚合方法，用于提升物联网（IoT）入侵检测系统的性能，通过多层数据处理和特征工程优化模型表现。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的普及，攻击面扩大，需要高效的入侵检测系统（IDS）来保护网络。FLARE旨在通过特征聚合技术解决物联网环境的安全挑战。

Method: FLARE采用多层处理方式，结合会话、流和时间滑动窗口数据聚合，分析网络行为并提取关键特征。使用四种监督学习和两种深度学习模型进行分类。

Result: 实验表明，FLARE作为特征工程的基础步骤，能够显著提升模型的准确性、精确率、召回率和F1分数，同时降低计算成本。

Conclusion: FLARE是一种有效的技术，可增强物联网入侵检测系统的性能，为端到端模型提供结构化表示，具有实际应用潜力。

Abstract: The proliferation of Internet of Things (IoT) devices has expanded the attack
surface, necessitating efficient intrusion detection systems (IDSs) for network
protection. This paper presents FLARE, a feature-based lightweight aggregation
for robust evaluation of IoT intrusion detection to address the challenges of
securing IoT environments through feature aggregation techniques. FLARE
utilizes a multilayered processing approach, incorporating session, flow, and
time-based sliding-window data aggregation to analyze network behavior and
capture vital features from IoT network traffic data. We perform extensive
evaluations on IoT data generated from our laboratory experimental setup to
assess the effectiveness of the proposed aggregation technique. To classify
attacks in IoT IDS, we employ four supervised learning models and two deep
learning models. We validate the performance of these models in terms of
accuracy, precision, recall, and F1-score. Our results reveal that
incorporating the FLARE aggregation technique as a foundational step in feature
engineering, helps lay a structured representation, and enhances the
performance of complex end-to-end models, making it a crucial step in IoT IDS
pipeline. Our findings highlight the potential of FLARE as a valuable technique
to improve performance and reduce computational costs of end-to-end IDS
implementations, thereby fostering more robust IoT intrusion detection systems.

</details>

### [2] [MST3 Encryption improvement with three-parameter group of Hermitian function field](https://arxiv.org/abs/2504.15391)
*Gennady Khalimov,Yevgen Kotukh*

Main category: cs.CR

TLDR: 本文提出了一种基于自同构群的高级加密框架，采用三参数群结构和中心外的对数签名，结合Hermitian函数域，实现了分阶段密钥解封装，增强了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在量子计算时代面临挑战，需要设计更具弹性的非传统加密方案。

Method: 利用三参数群结构和中心外的对数签名，结合Hermitian函数域，实现分阶段密钥解封装。

Result: 该加密机制在攻击复杂度和消息大小参数之间建立了直接关联，实现了安全性和效率的精确校准。

Conclusion: 该方法为非传统加密设计提供了重要进展，特别适用于后量子加密场景。

Abstract: This scholarly work presents an advanced cryptographic framework utilizing
automorphism groups as the foundational structure for encryption scheme
implementation. The proposed methodology employs a three-parameter group
construction, distinguished by its application of logarithmic signatures
positioned outside the group's center, a significant departure from
conventional approaches. A key innovation in this implementation is utilizing
the Hermitian function field as the underlying mathematical framework. This
particular function field provides enhanced structural properties that
strengthen the cryptographic protocol when integrated with the three-parameter
group architecture. The encryption mechanism features phased key
de-encapsulation from ciphertext, representing a substantial advantage over
alternative implementations. This sequential extraction process introduces
additional computational complexity for potential adversaries while maintaining
efficient legitimate decryption. A notable characteristic of this cryptosystem
is the direct correlation between the underlying group's mathematical strength
and both the attack complexity and message size parameters. This relationship
enables precise security-efficiency calibration based on specific
implementation requirements and threat models. The application of automorphism
groups with logarithmic signatures positioned outside the center represents a
significant advancement in non-traditional cryptographic designs, particularly
relevant in the context of post-quantum cryptographic resilience.

</details>

### [3] [Measuring likelihood in cybersecurity](https://arxiv.org/abs/2504.15395)
*Pablo Corona-Fraga,Vanessa Diaz-Rodriguez,Jesus Manuel Niebla-Zatarain,Gabriel Sanchez-Perez*

Main category: cs.CR

TLDR: 论文提出了一种新的数据模型和量化指标，用于间接但客观地衡量网络安全事件的发生概率，弥补了历史数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 网络安全领域缺乏公开且格式一致的历史数据，无法直接客观地衡量事件概率，需要新的方法。

Method: 通过分析现有方法、框架和事件数据，结合攻击者的TTP、IOC和防御控制，提出了一种描述网络暴露轮廓的数据模型和量化指标。

Result: 提出的数据模型和指标为组织提供了可操作的框架，能够持续优化网络安全策略。

Conclusion: 该研究为网络安全风险评估提供了不依赖历史数据的新方法，具有实际应用价值。

Abstract: In cybersecurity risk is commonly measured by impact and probability, the
former is objectively measured based on the consequences from the use of
technology to obtain business gains, or by achieving business objectives. The
latter has been measured, in sectors such as financial or insurance, based on
historical data because there is vast information, and many other fields have
applied the same approach. Although in cybersecurity, as a new discipline,
there is not always historical data to support an objective measure of
probability, the data available is not public and there is no consistent
formatting to store and share it, so a new approach is required to measure
cybersecurity events incidence. Through a comprehensive analysis of the state
of the art, including current methodologies, frameworks, and incident data,
considering tactics, techniques, and procedures (TTP) used by attackers,
indicators of compromise (IOC), and defence controls, this work proposes a data
model that describes a cyber exposure profile that provides an indirect but
objective measure for likelihood, including different sources and metrics to
update the model if needed. We further propose a set of practical, quantifiable
metrics for risk assessment, enabling cybersecurity practitioners to measure
likelihood without relying solely on historical incident data. By combining
these metrics with our data model, organizations gain an actionable framework
for continuously refining their cybersecurity strategies.

</details>

### [4] [Valkyrie: A Response Framework to Augment Runtime Detection of Time-Progressive Attacks](https://arxiv.org/abs/2504.15447)
*Nikhilesh Singh,Chester Rebeiro*

Main category: cs.CR

TLDR: 论文提出了一种名为Valkyrie的框架，旨在减少误报的影响而非消除误报，通过限制计算资源来缓解攻击，同时最小化对合法操作的干扰。


<details>
  <summary>Details</summary>
Motivation: 现有实时检测系统存在误报问题，导致合法操作中断和生产力下降。传统方法试图减少误报但无法完全消除，因此需要一种新方法以减少误报的影响。

Method: 引入Valkyrie框架，为现有运行时检测器添加后检测响应机制，通过限制计算资源来减缓攻击，直到检测器置信度足够高。

Result: 误报导致的执行时间增加平均小于1%（单线程）和6.7%（多线程）。攻击如rowhammer被阻止，其他攻击（如微架构攻击、勒索软件和挖矿程序）的效力大幅降低。

Conclusion: Valkyrie框架有效减少了误报的影响，同时显著削弱了攻击的效力，为实时检测系统提供了一种实用解决方案。

Abstract: A popular approach to detect cyberattacks is to monitor systems in real-time
to identify malicious activities as they occur. While these solutions aim to
detect threats early, minimizing damage, they suffer from a significant
challenge due to the presence of false positives. False positives have a
detrimental impact on computer systems, which can lead to interruptions of
legitimate operations and reduced productivity. Most contemporary works tend to
use advanced Machine Learning and AI solutions to address this challenge.
Unfortunately, false positives can, at best, be reduced but not eliminated.
  In this paper, we propose an alternate approach that focuses on reducing the
impact of false positives rather than eliminating them. We introduce Valkyrie,
a framework that can enhance any existing runtime detector with a
post-detection response. Valkyrie is designed for time-progressive attacks,
such as micro-architectural attacks, rowhammer, ransomware, and cryptominers,
that achieve their objectives incrementally using system resources. As soon as
an attack is detected, Valkyrie limits the allocated computing resources,
throttling the attack, until the detector's confidence is sufficiently high to
warrant a more decisive action. For a false positive, limiting the system
resources only results in a small increase in execution time. On average, the
slowdown incurred due to false positives is less than 1% for single-threaded
programs and 6.7% for multi-threaded programs. On the other hand, attacks like
rowhammer are prevented, while the potency of micro-architectural attacks,
ransomware, and cryptominers is greatly reduced.

</details>

### [5] [Scalable APT Malware Classification via Parallel Feature Extraction and GPU-Accelerated Learning](https://arxiv.org/abs/2504.15497)
*Noah Subedar,Taeui Kim,Saathwick Venkataramalingam*

Main category: cs.CR

TLDR: 提出了一种自动化加速恶意软件分类的框架，通过分析汇编指令（opcodes）将恶意可执行文件映射到已知APT组织。


<details>
  <summary>Details</summary>
Motivation: 传统方法在恶意软件分类中效率低下且依赖元数据，需要更高效且独立的方法。

Method: 利用开源逆向工程工具和并行计算脚本分析opcodes，构建一阶和二阶n-gram数据集，并应用传统机器学习模型（SVM、KNN、决策树）和CNN。

Result: 传统模型依赖元数据，效果有限；CNN结合GPU加速显著提升了分类性能。

Conclusion: CNN结合并行计算和GPU资源是高效分类恶意软件的有效方法。

Abstract: This paper presents an underlying framework for both automating and
accelerating malware classification, more specifically, mapping malicious
executables to known Advanced Persistent Threat (APT) groups. The main feature
of this analysis is the assembly-level instructions present in executables
which are also known as opcodes. The collection of such opcodes on many
malicious samples is a lengthy process; hence, open-source reverse engineering
tools are used in tandem with scripts that leverage parallel computing to
analyze multiple files at once. Traditional and deep learning models are
applied to create models capable of classifying malware samples. One-gram and
two-gram datasets are constructed and used to train models such as SVM, KNN,
and Decision Tree; however, they struggle to provide adequate results without
relying on metadata to support n-gram sequences. The computational limitations
of such models are overcome with convolutional neural networks (CNNs) and
heavily accelerated using graphical compute unit (GPU) resources.

</details>

### [6] [Guillotine: Hypervisors for Isolating Malicious AIs](https://arxiv.org/abs/2504.15499)
*James Mickens,Sarah Radway,Ravi Netravali*

Main category: cs.CR

TLDR: Guillotine是一种用于隔离高风险AI模型的超虚拟架构，通过软硬件协同设计防止AI利用侧信道漏洞，并提供物理故障保护。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在金融、医疗和军事等关键领域的广泛应用，其不可预测行为对社会构成巨大风险，需要一种机制来隔离和控制这些高风险AI。

Method: Guillotine结合了虚拟化技术和新型隔离机制，包括软硬件协同设计以防止侧信道泄漏，以及物理故障保护措施（如断电或数据中心淹没）。

Result: Guillotine能够有效隔离高风险AI，防止其通过反射漏洞或侧信道攻击突破控制平面，并提供多层防御机制。

Conclusion: Guillotine为高风险AI提供了一种全面的隔离方案，结合软硬件和物理保护，显著降低了AI失控的潜在威胁。

Abstract: As AI models become more embedded in critical sectors like finance,
healthcare, and the military, their inscrutable behavior poses ever-greater
risks to society. To mitigate this risk, we propose Guillotine, a hypervisor
architecture for sandboxing powerful AI models -- models that, by accident or
malice, can generate existential threats to humanity. Although Guillotine
borrows some well-known virtualization techniques, Guillotine must also
introduce fundamentally new isolation mechanisms to handle the unique threat
model posed by existential-risk AIs. For example, a rogue AI may try to
introspect upon hypervisor software or the underlying hardware substrate to
enable later subversion of that control plane; thus, a Guillotine hypervisor
requires careful co-design of the hypervisor software and the CPUs, RAM, NIC,
and storage devices that support the hypervisor software, to thwart side
channel leakage and more generally eliminate mechanisms for AI to exploit
reflection-based vulnerabilities. Beyond such isolation at the software,
network, and microarchitectural layers, a Guillotine hypervisor must also
provide physical fail-safes more commonly associated with nuclear power plants,
avionic platforms, and other types of mission critical systems. Physical
fail-safes, e.g., involving electromechanical disconnection of network cables,
or the flooding of a datacenter which holds a rogue AI, provide defense in
depth if software, network, and microarchitectural isolation is compromised and
a rogue AI must be temporarily shut down or permanently destroyed.

</details>

### [7] [T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models](https://arxiv.org/abs/2504.15512)
*Siyuan Liang,Jiayang Liu,Jiecheng Zhai,Tianmeng Fang,Rongcheng Tu,Aishan Liu,Xiaochun Cao,Dacheng Tao*

Main category: cs.CR

TLDR: T2VShield是一个模型无关的防御框架，用于保护文本到视频模型免受越狱攻击，通过输入改写和多范围检测模块，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展使文本到视频模型成为多模态世界模拟器的关键，但其易受越狱攻击，导致不安全内容生成，威胁模拟应用的可靠性。

Method: T2VShield通过输入阶段的提示改写（基于推理和多模态检索）和输出阶段的多范围检测（捕捉时间和模态上的不一致性）来防御攻击。

Result: 在五个平台上的实验表明，T2VShield能将越狱攻击成功率降低高达35%。

Conclusion: T2VShield通过视觉级防御提升了多模态模拟器的可信度，强调了对感知安全的人类中心评估的重要性。

Abstract: The rapid development of generative artificial intelligence has made text to
video models essential for building future multimodal world simulators.
However, these models remain vulnerable to jailbreak attacks, where specially
crafted prompts bypass safety mechanisms and lead to the generation of harmful
or unsafe content. Such vulnerabilities undermine the reliability and security
of simulation based applications. In this paper, we propose T2VShield, a
comprehensive and model agnostic defense framework designed to protect text to
video models from jailbreak threats. Our method systematically analyzes the
input, model, and output stages to identify the limitations of existing
defenses, including semantic ambiguities in prompts, difficulties in detecting
malicious content in dynamic video outputs, and inflexible model centric
mitigation strategies. T2VShield introduces a prompt rewriting mechanism based
on reasoning and multimodal retrieval to sanitize malicious inputs, along with
a multi scope detection module that captures local and global inconsistencies
across time and modalities. The framework does not require access to internal
model parameters and works with both open and closed source systems. Extensive
experiments on five platforms show that T2VShield can reduce jailbreak success
rates by up to 35 percent compared to strong baselines. We further develop a
human centered audiovisual evaluation protocol to assess perceptual safety,
emphasizing the importance of visual level defense in enhancing the
trustworthiness of next generation multimodal simulators.

</details>

### [8] [DecETT: Accurate App Fingerprinting Under Encrypted Tunnels via Dual Decouple-based Semantic Enhancement](https://arxiv.org/abs/2504.15565)
*Zheyuan Gu,Chang Liu,Xiyuan Zhang,Chen Yang,Gaopeng Gou,Gang Xiong,Zhen Li,Sijia Li*

Main category: cs.CR

TLDR: DecETT是一种基于双重解耦的语义增强方法，用于在加密隧道下实现准确的应用程序指纹识别（AF）。它通过增强应用程序特定特征和解耦无关隧道特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着加密隧道的普及，传统的AF方法在加密环境下表现不佳，主要忽略了隧道的混淆和重新封装机制。DecETT旨在解决这一问题。

Method: DecETT通过引入TLS流量作为语义锚点增强应用程序特征，并设计双重解耦模块分离隧道特征和应用程序语义特征。

Result: 在五种流行的加密隧道下评估，DecETT优于现有方法，尤其在复杂混淆隧道中表现更优。

Conclusion: DecETT通过语义增强和特征解耦，显著提升了加密隧道下的AF性能，为网络管理提供了新思路。

Abstract: Due to the growing demand for privacy protection, encrypted tunnels have
become increasingly popular among mobile app users, which brings new challenges
to app fingerprinting (AF)-based network management. Existing methods primarily
transfer traditional AF methods to encrypted tunnels directly, ignoring the
core obfuscation and re-encapsulation mechanism of encrypted tunnels, thus
resulting in unsatisfactory performance. In this paper, we propose DecETT, a
dual decouple-based semantic enhancement method for accurate AF under encrypted
tunnels. Specifically, DecETT improves AF under encrypted tunnels from two
perspectives: app-specific feature enhancement and irrelevant tunnel feature
decoupling.Considering the obfuscated app-specific information in encrypted
tunnel traffic, DecETT introduces TLS traffic with stronger app-specific
information as a semantic anchor to guide and enhance the fingerprint
generation for tunnel traffic. Furthermore, to address the app-irrelevant
tunnel feature introduced by the re-encapsulation mechanism, DecETT is designed
with a dual decouple-based fingerprint enhancement module, which decouples the
tunnel feature and app semantic feature from tunnel traffic separately, thereby
minimizing the impact of tunnel features on accurate app fingerprint
extraction. Evaluation under five prevalent encrypted tunnels indicates that
DecETT outperforms state-of-the-art methods in accurate AF under encrypted
tunnels, and further demonstrates its superiority under tunnels with more
complicated obfuscation. \textit{Project page:
\href{https://github.com/DecETT/DecETT}{https://github.com/DecETT/DecETT}}

</details>

### [9] [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
*Kun Wang,Guibin Zhang,Zhenhong Zhou,Jiahao Wu,Miao Yu,Shiqian Zhao,Chenlong Yin,Jinhu Fu,Yibo Yan,Hanjun Luo,Liang Lin,Zhihao Xu,Haolang Lu,Xinye Cao,Xinyun Zhou,Weifei Jin,Fanci Meng,Junyuan Mao,Hao Wu,Minghe Wang,Fan Zhang,Junfeng Fang,Chengwei Liu,Yifan Zhang,Qiankun Li,Chongye Guo,Yalan Qin,Yi Ding,Donghai Hong,Jiaming Ji,Xinfeng Li,Yifan Jiang,Dongxia Wang,Yihao Huang,Yufei Guo,Jen-tse Huang,Yanwei Yue,Wenke Huang,Guancheng Wan,Tianlin Li,Lei Bai,Jie Zhang,Qing Guo,Jingyi Wang,Tianlong Chen,Joey Tianyi Zhou,Xiaojun Jia,Weisong Sun,Cong Wu,Jing Chen,Xuming Hu,Yiming Li,Xiao Wang,Ningyu Zhang,Luu Anh Tuan,Guowen Xu,Tianwei Zhang,Xingjun Ma,Xiang Wang,Bo An,Jun Sun,Mohit Bansal,Shirui Pan,Yuval Elovici,Bhavya Kailkhura,Bo Li,Yaodong Yang,Hongwei Li,Wenyuan Xu,Yizhou Sun,Wei Wang,Qing Li,Ke Tang,Yu-Gang Jiang,Felix Juefei-Xu,Hui Xiong,Xiaofeng Wang,Shuicheng Yan,Dacheng Tao,Philip S. Yu,Qingsong Wen,Yang Liu*

Main category: cs.CR

TLDR: 本文提出了“全栈”安全概念，系统性地研究大语言模型（LLM）从训练到商业化的全生命周期安全问题，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLM安全的研究多集中于特定阶段，缺乏对全生命周期的系统性理解，本文旨在填补这一空白。

Method: 通过定义LLM的完整生命周期（数据准备、预训练、后训练、部署和商业化），并基于800多篇文献的系统性分析，提出全栈安全框架。

Result: 研究提供了全面的安全视角、广泛的文献支持和独特见解，包括数据生成安全、对齐技术、模型编辑和基于LLM的代理系统等研究方向。

Conclusion: 本文为LLM全生命周期的安全研究提供了系统性框架和未来研究方向，对学术界和工业界具有重要指导意义。

Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a
promising pathway toward achieving Artificial General Intelligence for both
academic and industrial communities, owing to their unprecedented performance
across various applications. As LLMs continue to gain prominence in both
research and commercial domains, their security and safety implications have
become a growing concern, not only for researchers and corporations but also
for every nation. Currently, existing surveys on LLM safety primarily focus on
specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning
phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs.
To address this gap, this paper introduces, for the first time, the concept of
"full-stack" safety to systematically consider safety issues throughout the
entire process of LLM training, deployment, and eventual commercialization.
Compared to the off-the-shelf LLM safety surveys, our work demonstrates several
distinctive advantages: (I) Comprehensive Perspective. We define the complete
LLM lifecycle as encompassing data preparation, pre-training, post-training,
deployment and final commercialization. To our knowledge, this represents the
first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive
Literature Support. Our research is grounded in an exhaustive review of over
800+ papers, ensuring comprehensive coverage and systematic organization of
security issues within a more holistic understanding. (III) Unique Insights.
Through systematic literature analysis, we have developed reliable roadmaps and
perspectives for each chapter. Our work identifies promising research
directions, including safety in data generation, alignment techniques, model
editing, and LLM-based agent systems. These insights provide valuable guidance
for researchers pursuing future work in this field.

</details>

### [10] [Yet Another Diminishing Spark: Low-level Cyberattacks in the Israel-Gaza Conflict](https://arxiv.org/abs/2504.15592)
*Anh V. Vu,Alice Hutchings,Ross Anderson*

Main category: cs.CR

TLDR: 研究发现，以色列-加沙冲突期间，低级别网络犯罪者发动了网页篡改和DDoS攻击，攻击量在冲突爆发后迅速上升，但几周后迅速下降，规模不及俄乌战争初期。


<details>
  <summary>Details</summary>
Motivation: 探讨网络攻击在地区冲突中的表现及其与俄乌战争的对比。

Method: 通过定量测量分析网络攻击的频率、规模和目标。

Result: 攻击量在冲突初期激增后迅速下降，攻击主要针对以色列，且支持巴勒斯坦的声音更多。

Conclusion: 网络攻击在地区冲突中表现出短期性，且攻击者的立场和规模因冲突性质而异。

Abstract: We report empirical evidence of web defacement and DDoS attacks carried out
by low-level cybercrime actors in the Israel-Gaza conflict. Our quantitative
measurements indicate an immediate increase in such cyberattacks following the
Hamas-led assault and the subsequent declaration of war. However, the surges
waned quickly after a few weeks, with patterns resembling those observed in the
aftermath of the Russian invasion of Ukraine. The scale of attacks and
discussions within the hacking community this time was both significantly lower
than those during the early days of the Russia-Ukraine war, and attacks have
been prominently one-sided: many pro-Palestinian supporters have targeted
Israel, while attacks on Palestine have been much less significant. Beyond
targeting these two, attackers also defaced sites of other countries to express
their war support. Their broader opinions are also largely disparate, with far
more support for Palestine and many objections expressed toward Israel.

</details>

### [11] [Exploring the Role of Large Language Models in Cybersecurity: A Systematic Survey](https://arxiv.org/abs/2504.15622)
*Shuang Tian,Tao Zhang,Jiqiang Liu,Jiacheng Wang,Xuangou Wu,Xiaoqiang Zhu,Ruichen Zhang,Weiting Zhang,Zhenhui Yuan,Shiwen Mao,Dong In Kim*

Main category: cs.CR

TLDR: 论文探讨了大型语言模型（LLM）在应对日益复杂的网络安全威胁中的应用，分析了其在网络攻击生命周期中的潜力，并总结了相关风险和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全方法难以应对快速演变的网络攻击，亟需更智能的防御策略。LLM因其分析复杂攻击模式、预测威胁和实时响应的潜力而受到关注。

Method: 从网络攻击生命周期的三个阶段（防御侦察、立足点建立和横向移动）分析LLM的应用，并探讨其在网络威胁情报（CTI）任务中的潜力。

Result: LLM在不同网络场景中的部署和应用展示了其潜力，但也面临内外风险问题。

Conclusion: 论文总结了LLM在网络安全中的风险和未来研究方向，强调了其作为智能防御工具的潜力。

Abstract: With the rapid development of technology and the acceleration of
digitalisation, the frequency and complexity of cyber security threats are
increasing. Traditional cybersecurity approaches, often based on static rules
and predefined scenarios, are struggling to adapt to the rapidly evolving
nature of modern cyberattacks. There is an urgent need for more adaptive and
intelligent defence strategies. The emergence of Large Language Model (LLM)
provides an innovative solution to cope with the increasingly severe cyber
threats, and its potential in analysing complex attack patterns, predicting
threats and assisting real-time response has attracted a lot of attention in
the field of cybersecurity, and exploring how to effectively use LLM to defend
against cyberattacks has become a hot topic in the current research field. This
survey examines the applications of LLM from the perspective of the cyber
attack lifecycle, focusing on the three phases of defense reconnaissance,
foothold establishment, and lateral movement, and it analyzes the potential of
LLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how
LLM-based security solutions are deployed and applied in different network
scenarios. It also summarizes the internal and external risk issues faced by
LLM during its application. Finally, this survey also points out the facing
risk issues and possible future research directions in this domain.

</details>

### [12] [TrojanDam: Detection-Free Backdoor Defense in Federated Learning through Proactive Model Robustification utilizing OOD Data](https://arxiv.org/abs/2504.15674)
*Yanbo Dai,Songze Li,Zihan Gan,Xueluan Gong*

Main category: cs.CR

TLDR: 论文提出了一种名为TrojanDam的新型联邦学习后门防御机制，通过激活冗余神经元来抵消后门更新的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的后门防御机制在联邦学习中效果不一致或无法检测精心设计的后门更新，导致后门攻击效果累积。

Method: 利用分布外（OOD）样本激活冗余神经元，提出TrojanDam机制，服务器持续注入OOD映射以抵消后门更新的影响。

Result: TrojanDam在多种联邦学习设置中表现优于现有先进防御方法。

Conclusion: TrojanDam通过激活冗余神经元有效防御后门攻击，为联邦学习提供了更可靠的防御范式。

Abstract: Federated learning (FL) systems allow decentralized data-owning clients to
jointly train a global model through uploading their locally trained updates to
a centralized server. The property of decentralization enables adversaries to
craft carefully designed backdoor updates to make the global model misclassify
only when encountering adversary-chosen triggers. Existing defense mechanisms
mainly rely on post-training detection after receiving updates. These methods
either fail to identify updates which are deliberately fabricated statistically
close to benign ones, or show inconsistent performance in different FL training
stages. The effect of unfiltered backdoor updates will accumulate in the global
model, and eventually become functional. Given the difficulty of ruling out
every backdoor update, we propose a backdoor defense paradigm, which focuses on
proactive robustification on the global model against potential backdoor
attacks. We first reveal that the successful launching of backdoor attacks in
FL stems from the lack of conflict between malicious and benign updates on
redundant neurons of ML models. We proceed to prove the feasibility of
activating redundant neurons utilizing out-of-distribution (OOD) samples in
centralized settings, and migrating to FL settings to propose a novel backdoor
defense mechanism, TrojanDam. The proposed mechanism has the FL server
continuously inject fresh OOD mappings into the global model to activate
redundant neurons, canceling the effect of backdoor updates during aggregation.
We conduct systematic and extensive experiments to illustrate the superior
performance of TrojanDam, over several SOTA backdoor defense methods across a
wide range of FL settings.

</details>

### [13] [A Time Series Analysis of Malware Uploads to Programming Language Ecosystems](https://arxiv.org/abs/2504.15695)
*Jukka Ruohonen,Mubashrah Saddiqa*

Main category: cs.CR

TLDR: 该论文研究了编程语言生态系统中恶意软件上传的纵向安全问题，发现恶意软件记录在OSV数据库中占比显著增加，并提出了简单的自回归模型进行预测。


<details>
  <summary>Details</summary>
Motivation: 编程语言生态系统的安全性问题日益突出，但纵向研究较少，论文旨在填补这一空白。

Method: 基于OSV数据库，分析了六个流行编程语言生态系统中的恶意软件上传数据，并采用自回归模型进行时间序列分析。

Result: 恶意软件记录在数据库中占比显著增加，2025年甚至达到80%；简单的自回归模型能有效预测恶意软件频率。

Conclusion: 论文通过纵向分析提升了人们对生态系统安全性和恶意软件的理解，为未来研究提供了基础。

Abstract: Software ecosystems built around programming languages have greatly
facilitated software development. At the same time, their security has
increasingly been acknowledged as a problem. To this end, the paper examines
the previously overlooked longitudinal aspects of software ecosystem security,
focusing on malware uploaded to six popular programming language ecosystems.
The dataset examined is based on the new Open Source Vulnerabilities (OSV)
database. According to the results, records about detected malware uploads in
the database have recently surpassed those addressing vulnerabilities in
packages distributed in the ecosystems. In the early 2025 even up to 80% of all
entries in the OSV have been about malware. Regarding time series analysis of
malware frequencies and their shares to all database entries, good predictions
are available already by relatively simple autoregressive models using the
numbers of ecosystems, security advisories, and media and other articles as
predictors. With these results and the accompanying discussion, the paper
improves and advances the understanding of the thus far overlooked longitudinal
aspects of ecosystems and malware.

</details>

### [14] [Trusted Compute Units: A Framework for Chained Verifiable Computations](https://arxiv.org/abs/2504.15717)
*Fernando Castillo,Jonathan Heiss,Sebastian Werner,Stefan Tai*

Main category: cs.CR

TLDR: 论文提出了一种名为Trusted Compute Unit (TCU)的统一框架，支持异构技术间的可验证计算，以解决区块链中复杂计算的低成本和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 区块链和分布式账本技术（DLTs）在跨信任边界进行去中心化计算时，面临复杂计算的高成本和隐私保护挑战。

Method: 引入TCU框架，结合硬件可信执行环境（TEEs）和零知识虚拟机（zkVMs），支持可组合和互操作的可验证计算。

Result: 实验证明TCU能显著提升系统性能和可扩展性，支持如联邦学习等区块链应用。

Conclusion: TCU通过统一异构技术，为区块链生态系统提供了高效、隐私保护的可验证链下服务。

Abstract: Blockchain and distributed ledger technologies (DLTs) facilitate
decentralized computations across trust boundaries. However, ensuring complex
computations with low gas fees and confidentiality remains challenging. Recent
advances in Confidential Computing -- leveraging hardware-based Trusted
Execution Environments (TEEs) -- and Proof-carrying Data -- employing
cryptographic Zero-Knowledge Virtual Machines (zkVMs) -- hold promise for
secure, privacy-preserving off-chain and layer-2 computations.On the other
side, a homogeneous reliance on a single technology, such as TEEs or zkVMs, is
impractical for decentralized environments with heterogeneous computational
requirements. This paper introduces the Trusted Compute Unit (TCU), a unifying
framework that enables composable and interoperable verifiable computations
across heterogeneous technologies. Our approach allows decentralized
applications (dApps) to flexibly offload complex computations to TCUs,
obtaining proof of correctness. These proofs can be anchored on-chain for
automated dApp interactions, while ensuring confidentiality of input data, and
integrity of output data. We demonstrate how TCUs can support a prominent
blockchain use case, such as federated learning. By enabling secure off-chain
interactions without incurring on-chain confirmation delays or gas fees, TCUs
significantly improve system performance and scalability. Experimental insights
and performance evaluations confirm the feasibility and practicality of this
unified approach, advancing the state of the art in verifiable off-chain
services for the blockchain ecosystem.

</details>

### [15] [RRC Signaling Storm Detection in O-RAN](https://arxiv.org/abs/2504.15738)
*Dang Kien Nguyen,Rim El Malki,Filippo Rebecchi*

Main category: cs.CR

TLDR: 论文提出了一种基于O-RAN的RRC信令风暴攻击检测方法，通过实验和理论模型验证其有效性，并开发了一种基于阈值的检测技术，部署为xApp，实现了快速检测和低开销。


<details>
  <summary>Details</summary>
Motivation: 解决5G网络中因RRC信令风暴攻击导致的资源耗尽和服务降级问题，利用O-RAN的开放性设计解决方案。

Method: 使用OpenAirInterface平台实现RRC信令风暴攻击，开发基于阈值的检测技术，并部署为O-RAN的xApp。

Result: 攻击可在90ms内检测到，提供60ms的缓解窗口，CPU和内存开销分别为1.2%和0%。

Conclusion: 提出的方法有效缓解了RRC信令风暴攻击，展示了O-RAN在提升5G网络安全性方面的潜力。

Abstract: The Open Radio Access Network (O-RAN) marks a significant shift in the mobile
network industry. By transforming a traditionally vertically integrated
architecture into an open, data-driven one, O-RAN promises to enhance
operational flexibility and drive innovation. In this paper, we harness O-RAN's
openness to address one critical threat to 5G availability: signaling storms
caused by abuse of the Radio Resource Control (RRC) protocol. Such attacks
occur when a flood of RRC messages from one or multiple User Equipments (UEs)
deplete resources at a 5G base station (gNB), leading to service degradation.
We provide a reference implementation of an RRC signaling storm attack, using
the OpenAirInterface (OAI) platform to evaluate its impact on a gNB. We
supplement the experimental results with a theoretical model to extend the
findings for different load conditions. To mitigate RRC signaling storms, we
develop a threshold-based detection technique that relies on RRC layer features
to distinguish between malicious activity and legitimate high network load
conditions. Leveraging O-RAN capabilities, our detection method is deployed as
an external Application (xApp). Performance evaluation shows attacks can be
detected within 90ms, providing a mitigation window of 60ms before gNB
unavailability, with an overhead of 1.2% and 0% CPU and memory consumption,
respectively.

</details>

### [16] [EFFACT: A Highly Efficient Full-Stack FHE Acceleration Platform](https://arxiv.org/abs/2504.15817)
*Yi Huang,Xinsheng Gong,Xiangyu Kong,Dibei Chen,Jianfeng Zhu,Wenping Zhu,Liangwei Li,Mingyu Gao,Shaojun Wei,Aoyang Zhang,Leibo Liu*

Main category: cs.CR

TLDR: 本文提出了一种高效的全栈FHE加速平台EFFACT，通过编译器和硬件优化解决现有FHE方案中性能低下的问题。


<details>
  <summary>Details</summary>
Motivation: FHE在隐私保护计算中具有潜力，但现有方案因数据膨胀和资源需求高导致性能低下，亟需高效、低开销的加速平台。

Method: 通过计算资源重分配、SRAM优化设计、流式内存访问、电路级功能单元复用以及新型NTT和自同构单元设计，实现高效加速。

Result: EFFACT平台在保持低面积和高效率的同时，支持多种FHE方案，显著提升了性能。

Conclusion: EFFACT为FHE的实际应用提供了高效、低开销的解决方案，具有广泛适用性。

Abstract: Fully Homomorphic Encryption (FHE) is a set of powerful cryptographic schemes
that allows computation to be performed directly on encrypted data with an
unlimited depth. Despite FHE's promising in privacy-preserving computing, yet
in most FHE schemes, ciphertext generally blows up thousands of times compared
to the original message, and the massive amount of data load from off-chip
memory for bootstrapping and privacy-preserving machine learning applications
(such as HELR, ResNet-20), both degrade the performance of FHE-based
computation. Several hardware designs have been proposed to address this issue,
however, most of them require enormous resources and power. An acceleration
platform with easy programmability, high efficiency, and low overhead is a
prerequisite for practical application.
  This paper proposes EFFACT, a highly efficient full-stack FHE acceleration
platform with a compiler that provides comprehensive optimizations and
vector-friendly hardware. We start by examining the computational overhead
across different real-world benchmarks to highlight the potential benefits of
reallocating computing resources for efficiency enhancement. Then we make a
design space exploration to find an optimal SRAM size with high utilization and
low cost. On the other hand, EFFACT features a novel optimization named
streaming memory access which is proposed to enable high throughput with
limited SRAMs. Regarding the software-side optimization, we also propose a
circuit-level function unit reuse scheme, to substantially reduce the computing
resources without performance degradation. Moreover, we design novel NTT and
automorphism units that are suitable for a cost-sensitive and highly efficient
architecture, leading to low area. For generality, EFFACT is also equipped with
an ISA and a compiler backend that can support several FHE schemes like CKKS,
BGV, and BFV.

</details>

### [17] [Cryptoanalysis of a public key exchange based on circulant matrix over digital semiring](https://arxiv.org/abs/2504.15880)
*Alvaro Otero Sanchez*

Main category: cs.CR

TLDR: 对基于数字半环的密钥交换协议进行了密码分析，通过求解线性系统的最大解并利用循环矩阵的性质，证明了该协议的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 分析基于数字半环的密钥交换协议的安全性，揭示其潜在的漏洞。

Method: 通过求解数字半环上线性系统的最大解，并利用循环矩阵的性质，设计了一种高效攻击方法。

Result: 在多项式时间内，能够从公开交换的信息中恢复共享密钥。

Conclusion: 该密钥交换协议存在安全漏洞，易受高效攻击。

Abstract: We present a cryptanalysis of a key exchange protocol based on the digital
semiring. For this purpose, we find the maximal solution of a linear system
over such semiring, and use the properties of circulant matrix to demonstrate
that the protocol is vulnerable. Specifically, we provide an efficient attack
that recovers the shared secret key from publicly exchanged information for any
instance of the digital semiring in polynomial time.

</details>

### [18] [Adversarial Observations in Weather Forecasting](https://arxiv.org/abs/2504.15942)
*Erik Imgrund,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.CR

TLDR: 论文探讨了AI天气预报系统（如GenCast）的安全漏洞，提出了一种针对自回归扩散模型的攻击方法，能够通过微小扰动操纵天气预报甚至伪造极端天气事件。


<details>
  <summary>Details</summary>
Motivation: 随着AI天气预报系统逐渐取代传统方法，其潜在的安全风险尚未被充分研究。本文旨在揭示这些系统的新漏洞及其可能带来的大规模破坏。

Method: 提出了一种针对自回归扩散模型的攻击方法，通过引入统计上无法区分的微小扰动（少于0.1%的数据变化）来操纵天气预报。

Result: 攻击能够成功伪造极端天气事件（如飓风、热浪等），并可能破坏公众对天气预报的信任。

Conclusion: 现代天气预报系统存在严重的安全风险，需加强防护措施以避免潜在的大规模破坏。

Abstract: AI-based systems, such as Google's GenCast, have recently redefined the state
of the art in weather forecasting, offering more accurate and timely
predictions of both everyday weather and extreme events. While these systems
are on the verge of replacing traditional meteorological methods, they also
introduce new vulnerabilities into the forecasting process. In this paper, we
investigate this threat and present a novel attack on autoregressive diffusion
models, such as those used in GenCast, capable of manipulating weather
forecasts and fabricating extreme events, including hurricanes, heat waves, and
intense rainfall. The attack introduces subtle perturbations into weather
observations that are statistically indistinguishable from natural noise and
change less than 0.1% of the measurements - comparable to tampering with data
from a single meteorological satellite. As modern forecasting integrates data
from nearly a hundred satellites and many other sources operated by different
countries, our findings highlight a critical security risk with the potential
to cause large-scale disruptions and undermine public trust in weather
prediction.

</details>

### [19] [Automated Static Vulnerability Detection via a Holistic Neuro-symbolic Approach](https://arxiv.org/abs/2504.16057)
*Penghui Li,Songchen Yao,Josef Sarfati Korich,Changhua Luo,Jianjia Yu,Yinzhi Cao,Junfeng Yang*

Main category: cs.CR

TLDR: MoCQ是一个结合大型语言模型（LLM）和静态分析的新型框架，用于自动化生成漏洞检测模式并提高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 静态漏洞检测需要大量人工工作，现有方法未能完全自动化生成漏洞模式且准确性不足。

Method: MoCQ利用LLM自动提取漏洞模式并转换为检测查询，再通过静态分析反馈循环优化查询并执行大规模代码分析。

Result: MoCQ在七种漏洞类型上表现优异，发现了专家遗漏的12种模式，并在真实应用中识别出7个未知漏洞。

Conclusion: MoCQ展示了结合LLM和静态分析的潜力，显著提升了漏洞检测的自动化水平和准确性。

Abstract: Static vulnerability detection is still a challenging problem and demands
excessive human efforts, e.g., manual curation of good vulnerability patterns.
None of prior works, including classic program analysis or Large Language Model
(LLM)-based approaches, have fully automated such vulnerability pattern
generations with reasonable detection accuracy. In this paper, we design and
implement, MoCQ, a novel holistic neuro-symbolic framework that combines the
complementary strengths of LLMs and classical static analysis to enable
scalable vulnerability detection. The key insight is that MoCQ leverages an LLM
to automatically extract vulnerability patterns and translate them into
detection queries, and then on static analysis to refine such queries in a
feedback loop and eventually execute them for analyzing large codebases and
mining vulnerabilities. We evaluate MoCQ on seven types of vulnerabilities
spanning two programming languages. We found MoCQ-generated queries uncovered
at least 12 patterns that were missed by experts. On a ground truth dataset,
MoCQ achieved comparable precision and recall compared to expert-crafted
queries. Moreover, MoCQ has identified seven previously unknown vulnerabilities
in real-world applications, demonstrating its practical effectiveness. We have
responsibly disclosed them to the corresponding developers.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions](https://arxiv.org/abs/2504.15300)
*Chaoyue Niu,Yucheng Ding,Junhui Lu,Zhengxiang Huang,Hang Zeng,Yutong Dai,Xuezhen Tu,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.LG

TLDR: 本文探讨了一种新兴的协作学习范式，结合设备端小模型与云端大模型，以解决传统云端大模型学习框架在延迟、成本、个性化和隐私方面的问题。


<details>
  <summary>Details</summary>
Motivation: 传统云端大模型学习框架存在延迟高、成本大、个性化不足和隐私泄露等问题，因此需要一种更高效、低成本的协作学习方案。

Method: 通过硬件、系统、算法和应用层的全面综述，总结了协作学习中的关键问题和最新进展，并将协作算法分为数据、特征和参数三类框架。

Result: 提供了公开数据集和评估指标，并展示了协作学习在推荐系统、移动直播和个人智能助手等实际应用中的部署案例。

Conclusion: 指出了未来研究方向，以推动这一快速发展领域的进一步进步。

Abstract: The conventional cloud-based large model learning framework is increasingly
constrained by latency, cost, personalization, and privacy concerns. In this
survey, we explore an emerging paradigm: collaborative learning between
on-device small model and cloud-based large model, which promises low-latency,
cost-efficient, and personalized intelligent services while preserving user
privacy. We provide a comprehensive review across hardware, system, algorithm,
and application layers. At each layer, we summarize key problems and recent
advances from both academia and industry. In particular, we categorize
collaboration algorithms into data-based, feature-based, and parameter-based
frameworks. We also review publicly available datasets and evaluation metrics
with user-level or device-level consideration tailored to collaborative
learning settings. We further highlight real-world deployments, ranging from
recommender systems and mobile livestreaming to personal intelligent
assistants. We finally point out open research directions to guide future
development in this rapidly evolving field.

</details>

### [21] [Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches](https://arxiv.org/abs/2504.15310)
*Syeda Tahreem Zahra,Syed Kashif Imdad,Sohail Khan,Sohail Khalid,Nauman Anwar Baig*

Main category: cs.LG

TLDR: 本文综述了电力变压器健康评估和寿命预测的现有技术，重点分析了传统和前沿方法的优缺点，并探讨了多种人工智能算法在变压器故障诊断中的应用。


<details>
  <summary>Details</summary>
Motivation: 电力变压器在电力系统中至关重要，其健康评估和寿命预测对确保高效运行和维护计划至关重要。

Method: 通过文献综述，分析了传统和前沿技术，并详细探讨了多种AI算法（如ANN、CNN、SVM、RF、GA和PSO）在变压器故障诊断中的应用。

Result: 研究发现，结合多种AI方法和时间序列分析可提高故障诊断的精度和早期检测能力。

Conclusion: 本文为变压器故障诊断领域的未来研究提供了基础，并推动了这一关键领域的发展。

Abstract: Power transformers play a critical role within the electrical power system,
making their health assessment and the prediction of their remaining lifespan
paramount for the purpose of ensuring efficient operation and facilitating
effective maintenance planning. This paper undertakes a comprehensive
examination of existent literature, with a primary focus on both conventional
and cutting-edge techniques employed within this domain. The merits and
demerits of recent methodologies and techniques are subjected to meticulous
scrutiny and explication. Furthermore, this paper expounds upon intelligent
fault diagnosis methodologies and delves into the most widely utilized
intelligent algorithms for the assessment of transformer conditions. Diverse
Artificial Intelligence (AI) approaches, including Artificial Neural Networks
(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),
Random Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization
(PSO), are elucidated offering pragmatic solutions for enhancing the
performance of transformer fault diagnosis. The amalgamation of multiple AI
methodologies and the exploration of timeseries analysis further contribute to
the augmentation of diagnostic precision and the early detection of faults in
transformers. By furnishing a comprehensive panorama of AI applications in the
field of transformer fault diagnosis, this study lays the groundwork for future
research endeavors and the progression of this critical area of study.

</details>

### [22] [M-TabNet: A Multi-Encoder Transformer Model for Predicting Neonatal Birth Weight from Multimodal Data](https://arxiv.org/abs/2504.15312)
*Muhammad Mursil,Hatem A. Rashwan,Luis Santos-Calderon,Pere Cavalle-Busquets,Michelle M. Murphy,Domenec Puig*

Main category: cs.LG

TLDR: 该研究提出了一种基于注意力机制的Transformer模型，用于早期（妊娠12周内）预测新生儿出生体重，整合了生理、生活方式、营养和遗传等多维数据，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如超声检查在妊娠20周前准确性较低，且现有模型常忽略营养和遗传因素。本研究旨在通过多编码器架构的Transformer模型解决这些局限性。

Method: 采用注意力机制的Transformer模型，整合多维母体数据（生理、生活方式、营养、遗传），并与TabNet等现有模型对比。

Result: 模型在内部数据集上MAE为122克，R²为0.94；在IEEE数据集上MAE为105克，R²为0.95。分类任务中敏感性和特异性分别达97.55%和94.48%。

Conclusion: 该模型为早期出生体重预测提供了高精度、可解释的工具，有助于临床风险分层和优化新生儿健康。

Abstract: Birth weight (BW) is a key indicator of neonatal health, with low birth
weight (LBW) linked to increased mortality and morbidity. Early prediction of
BW enables timely interventions; however, current methods like ultrasonography
have limitations, including reduced accuracy before 20 weeks and operator
dependent variability. Existing models often neglect nutritional and genetic
influences, focusing mainly on physiological and lifestyle factors. This study
presents an attention-based transformer model with a multi-encoder architecture
for early (less than 12 weeks of gestation) BW prediction. Our model
effectively integrates diverse maternal data such as physiological, lifestyle,
nutritional, and genetic, addressing limitations seen in prior attention-based
models such as TabNet. The model achieves a Mean Absolute Error (MAE) of 122
grams and an R-squared value of 0.94, demonstrating high predictive accuracy
and interoperability with our in-house private dataset. Independent validation
confirms generalizability (MAE: 105 grams, R-squared: 0.95) with the IEEE
children dataset. To enhance clinical utility, predicted BW is classified into
low and normal categories, achieving a sensitivity of 97.55% and a specificity
of 94.48%, facilitating early risk stratification. Model interpretability is
reinforced through feature importance and SHAP analyses, highlighting
significant influences of maternal age, tobacco exposure, and vitamin B12
status, with genetic factors playing a secondary role. Our results emphasize
the potential of advanced deep-learning models to improve early BW prediction,
offering clinicians a robust, interpretable, and personalized tool for
identifying pregnancies at risk and optimizing neonatal outcomes.

</details>

### [23] [Diffusion-Driven Inertial Generated Data for Smartphone Location Classification](https://arxiv.org/abs/2504.15315)
*Noa Cohen,Rotem Dror,Itzik Klein*

Main category: cs.LG

TLDR: 论文提出了一种基于扩散模型的智能手机位置识别方法，通过生成合成数据减少实际数据收集的负担。


<details>
  <summary>Details</summary>
Motivation: 惯性测量数据收集耗时且资源密集，限制了机器学习模型的开发。扩散模型在生成数据方面表现优异，为解决这一问题提供了新思路。

Method: 提出扩散驱动的特定力生成数据方法，并通过多指标比较合成数据与实际记录数据。

Result: 扩散模型成功捕捉了不同智能手机放置条件下特定力信号的独特特征，生成的数据质量高。

Conclusion: 通过生成多样且真实的合成数据，可以减轻数据收集负担，为机器学习模型提供高质量训练数据。

Abstract: Despite the crucial role of inertial measurements in motion tracking and
navigation systems, the time-consuming and resource-intensive nature of
collecting extensive inertial data has hindered the development of robust
machine learning models in this field. In recent years, diffusion models have
emerged as a revolutionary class of generative models, reshaping the landscape
of artificial data generation. These models surpass generative adversarial
networks and other state-of-the-art approaches to complex tasks. In this work,
we propose diffusion-driven specific force-generated data for smartphone
location recognition. We provide a comprehensive evaluation methodology by
comparing synthetic and real recorded specific force data across multiple
metrics. Our results demonstrate that our diffusion-based generative model
successfully captures the distinctive characteristics of specific force signals
across different smartphone placement conditions. Thus, by creating diverse,
realistic synthetic data, we can reduce the burden of extensive data collection
while providing high-quality training data for machine learning models.

</details>

### [24] [How to systematically develop an effective AI-based bias correction model?](https://arxiv.org/abs/2504.15322)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: cs.LG

TLDR: ReSA-ConvLSTM是一个用于数值天气预报（NWP）系统偏差校正的AI框架，通过动态气候归一化、时间因果约束的ConvLSTM和残差自注意力机制，显著降低了1-7天预报的误差。


<details>
  <summary>Details</summary>
Motivation: 数值天气预报存在系统性偏差，需要一种高效且物理感知的校正方法。

Method: 结合动态气候归一化、ConvLSTM和残差自注意力机制，建立ECMWF预报与ERA5再分析数据之间的非线性映射。

Result: 在41年全球数据上，T2m、U10/V10和SLP的RMSE降低达20%，模型轻量化且泛化能力强。

Conclusion: 该框架显著提升预报技能，为多变量校正和下游应用提供了高效解决方案。

Abstract: This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)
framework for systematic bias correction in numerical weather prediction (NWP).
We propose three innovations by integrating dynamic climatological
normalization, ConvLSTM with temporal causality constraints, and residual
self-attention mechanisms. The model establishes a physics-aware nonlinear
mapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years
(1981-2021) of global atmospheric data, the framework reduces systematic biases
in 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure
(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to
operational ECMWF outputs. The lightweight architecture (10.6M parameters)
enables efficient generalization to multiple variables and downstream
applications, reducing retraining time by 85% for cross-variable correction
while improving ocean model skill through bias-corrected boundary conditions.
The ablation experiments demonstrate that our innovations significantly improve
the model's correction performance, suggesting that incorporating variable
characteristics into the model helps enhance forecasting skills.

</details>

### [25] [HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning](https://arxiv.org/abs/2504.15323)
*Donggyun Kim,Chanwoo Kim,Seunghoon Hong*

Main category: cs.LG

TLDR: 提出一种无需计算梯度的测试时适应方法，通过模拟梯度下降实现高效少样本学习。


<details>
  <summary>Details</summary>
Motivation: 解决测试时微调在实时或低资源场景中因多次反向传播导致的高成本问题。

Method: 将梯度下降建模为ODE的欧拉离散化，训练辅助网络预测任务条件漂移，仅需少量前向传播。

Result: 在跨域少样本分类任务中显著提升性能，仅需6%内存和0.02%计算时间。

Conclusion: 该方法在直接迁移和完全微调之间提供了实用高效的中间方案。

Abstract: While test-time fine-tuning is beneficial in few-shot learning, the need for
multiple backpropagation steps can be prohibitively expensive in real-time or
low-resource scenarios. To address this limitation, we propose an approach that
emulates gradient descent without computing gradients, enabling efficient
test-time adaptation. Specifically, we formulate gradient descent as an Euler
discretization of an ordinary differential equation (ODE) and train an
auxiliary network to predict the task-conditional drift using only the few-shot
support set. The adaptation then reduces to a simple numerical integration
(e.g., via the Euler method), which requires only a few forward passes of the
auxiliary network -- no gradients or forward passes of the target model are
needed. In experiments on cross-domain few-shot classification using the
Meta-Dataset and CDFSL benchmarks, our method significantly improves
out-of-domain performance over the non-fine-tuned baseline while incurring only
6\% of the memory cost and 0.02\% of the computation time of standard
fine-tuning, thus establishing a practical middle ground between direct
transfer and fully fine-tuned approaches.

</details>

### [26] [Significativity Indices for Agreement Values](https://arxiv.org/abs/2504.15325)
*Alberto Casagrande,Francesco Fabris,Rossano Girometti,Roberto Pagliarini*

Main category: cs.LG

TLDR: 本文提出了一种评估分类器间一致性显著性的通用方法，并引入了两种显著性指数，分别处理有限数据集和分类概率分布。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性度量（如Cohen's kappa）的质量评估标准过于简单且边界任意，需要更科学的显著性评估方法。

Method: 提出通用方法评估一致性值的显著性，并设计两种显著性指数，同时解决计算效率问题。

Result: 开发了两种显著性指数，并确定了高效计算算法。

Conclusion: 该方法为分类器一致性评估提供了更科学的工具，解决了现有标准的局限性。

Abstract: Agreement measures, such as Cohen's kappa or intraclass correlation, gauge
the matching between two or more classifiers. They are used in a wide range of
contexts from medicine, where they evaluate the effectiveness of medical
treatments and clinical trials, to artificial intelligence, where they can
quantify the approximation due to the reduction of a classifier. The
consistency of different classifiers to a golden standard can be compared
simply by using the order induced by their agreement measure with respect to
the golden standard itself. Nevertheless, labelling an approach as good or bad
exclusively by using the value of an agreement measure requires a scale or a
significativity index. Some quality scales have been proposed in the literature
for Cohen's kappa, but they are mainly naive, and their boundaries are
arbitrary. This work proposes a general approach to evaluate the
significativity of any agreement value between two classifiers and introduces
two significativity indices: one dealing with finite data sets, the other one
handling classification probability distributions. Moreover, this manuscript
considers the computational issues of evaluating such indices and identifies
some efficient algorithms to evaluate them.

</details>

### [27] [Bayesian Federated Learning for Continual Training](https://arxiv.org/abs/2504.15328)
*Usevalad Milasheuski,Luca Barbieri,Sanaz Kianoush,Monica Nicoli,Stefano Savazzi*

Main category: cs.LG

TLDR: 提出了一种持续贝叶斯联邦学习框架，用于动态环境中数据分布变化的挑战，通过SGLD方法更新模型并评估其性能。


<details>
  <summary>Details</summary>
Motivation: 当前贝叶斯联邦学习方法未解决动态环境中数据分布变化的持续学习问题。

Method: 使用随机梯度朗之万动力学（SGLD）方法，逐步更新模型，并利用过去的后验构建新任务的先验。

Result: 实验表明，该方法在准确性、预期校准误差和收敛速度上优于基线方法，有效保留知识并适应数据变化。

Conclusion: 持续贝叶斯更新在动态环境中具有显著优势，能够适应数据分布的变化并保持模型性能。

Abstract: Bayesian Federated Learning (BFL) enables uncertainty quantification and
robust adaptation in distributed learning. In contrast to the frequentist
approach, it estimates the posterior distribution of a global model, offering
insights into model reliability. However, current BFL methods neglect continual
learning challenges in dynamic environments where data distributions shift over
time. We propose a continual BFL framework applied to human sensing with radar
data collected over several days. Using Stochastic Gradient Langevin Dynamics
(SGLD), our approach sequentially updates the model, leveraging past posteriors
to construct the prior for the new tasks. We assess the accuracy, the expected
calibration error (ECE) and the convergence speed of our approach against
several baselines. Results highlight the effectiveness of continual Bayesian
updates in preserving knowledge and adapting to evolving data.

</details>

### [28] [FedFetch: Faster Federated Learning with Adaptive Downstream Prefetching](https://arxiv.org/abs/2504.15366)
*Qifan Yan,Andrew Liu,Shiqi He,Mathias Lécuyer,Ivan Beschastnikh*

Main category: cs.LG

TLDR: FedFetch是一种策略，通过预取模型状态减少联邦学习中客户端采样和压缩技术结合时的下载时间开销，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）中大量异构客户端导致通信瓶颈，尤其是未选中客户端需要同步过时的本地模型状态，进一步拖慢训练速度。

Method: 提出FedFetch策略，通过高效预取计划让客户端在训练轮次前多次预取模型状态，减少下载时间。

Result: 实验表明，FedFetch将端到端训练时间减少1.26倍，下载时间减少4.49倍。

Conclusion: FedFetch有效解决了联邦学习中客户端采样和压缩技术结合的下载时间问题，显著提升了训练效率。

Abstract: Federated learning (FL) is a machine learning paradigm that facilitates
massively distributed model training with end-user data on edge devices
directed by a central server. However, the large number of heterogeneous
clients in FL deployments leads to a communication bottleneck between the
server and the clients. This bottleneck is made worse by straggling clients,
any one of which will further slow down training. To tackle these challenges,
researchers have proposed techniques like client sampling and update
compression. These techniques work well in isolation but combine poorly in the
downstream, server-to-client direction. This is because unselected clients have
outdated local model states and need to synchronize these states with the
server first.
  We introduce FedFetch, a strategy to mitigate the download time overhead
caused by combining client sampling and compression techniques. FedFetch
achieves this with an efficient prefetch schedule for clients to prefetch model
states multiple rounds before a stated training round. We empirically show that
adding FedFetch to communication efficient FL techniques reduces end-to-end
training time by 1.26$\times$ and download time by 4.49$\times$ across
compression techniques with heterogeneous client settings. Our implementation
is available at https://github.com/DistributedML/FedFetch

</details>

### [29] [Solving New Tasks by Adapting Internet Video Knowledge](https://arxiv.org/abs/2504.15369)
*Calvin Luo,Zilai Zeng,Yilun Du,Chen Sun*

Main category: cs.LG

TLDR: 论文研究了如何通过适应技术将大规模预训练视频模型与特定领域信息结合，以支持机器人任务中的文本条件泛化。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型在机器人领域有潜力，但预训练模型可能忽略特定环境细节，而领域内训练数据不足。

Method: 提出多种适应技术，特别是“逆概率适应”策略，结合小规模示例数据调整预训练模型。

Result: 实验证明，该方法能有效支持机器人任务中的文本条件泛化，即使数据质量不高。

Conclusion: 逆概率适应策略在机器人任务中表现优异，具有鲁棒性，适用于数据不足或质量不佳的情况。

Abstract: Video generative models demonstrate great promise in robotics by serving as
visual planners or as policy supervisors. When pretrained on internet-scale
data, such video models intimately understand alignment with natural language,
and can thus facilitate generalization to novel downstream behavior through
text-conditioning. However, they may not be sensitive to the specificities of
the particular environment the agent inhabits. On the other hand, training
video models on in-domain examples of robotic behavior naturally encodes
environment-specific intricacies, but the scale of available demonstrations may
not be sufficient to support generalization to unseen tasks via natural
language specification. In this work, we investigate different adaptation
techniques that integrate in-domain information with large-scale pretrained
video models, and explore the extent to which they enable novel
text-conditioned generalization for robotic tasks, while also considering their
independent data and resource considerations. We successfully demonstrate
across robotic environments that adapting powerful video models with small
scales of example data can successfully facilitate generalization to novel
behaviors. In particular, we present a novel adaptation strategy, termed
Inverse Probabilistic Adaptation, that not only consistently achieves strong
generalization performance across robotic tasks and settings, but also exhibits
robustness to the quality of adaptation data, successfully solving novel tasks
even when only suboptimal in-domain demonstrations are available.

</details>

### [30] [Improving Learning to Optimize Using Parameter Symmetries](https://arxiv.org/abs/2504.15399)
*Guy Zamir,Aryan Dokania,Bo Zhao,Rose Yu*

Main category: cs.LG

TLDR: 本文提出了一种利用参数空间对称性提升优化效率的学习优化（L2O）算法，理论分析表明其局部类似于牛顿法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用参数空间对称性改进元优化器的性能，以提升优化效率。

Method: 通过联合学习对称变换和局部更新，结合理论分析，验证算法在局部类似于牛顿法，并通过实验引入基准测试和动量增强。

Result: 算法在训练中能学习到正确的对称变换，实验表明动量等增强进一步提升了性能。

Conclusion: 利用神经网络参数空间对称性具有推动元优化的潜力。

Abstract: We analyze a learning-to-optimize (L2O) algorithm that exploits parameter
space symmetry to enhance optimization efficiency. Prior work has shown that
jointly learning symmetry transformations and local updates improves
meta-optimizer performance. Supporting this, our theoretical analysis
demonstrates that even without identifying the optimal group element, the
method locally resembles Newton's method. We further provide an example where
the algorithm provably learns the correct symmetry transformation during
training. To empirically evaluate L2O with teleportation, we introduce a
benchmark, analyze its success and failure cases, and show that enhancements
like momentum further improve performance. Our results highlight the potential
of leveraging neural network parameter space symmetry to advance
meta-optimization.

</details>

### [31] [Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering](https://arxiv.org/abs/2504.15439)
*Hao Zhuo,Yicheng Yang,Kewen Peng*

Main category: cs.LG

TLDR: 本文综述了大型语言模型（LLMs）在软件工程（SE）中广泛应用带来的毒性语言问题，总结了检测与缓解方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLMs在SE中的广泛应用可能导致毒性语言的传播，影响开发环境的包容性，因此需要系统研究其检测与缓解方法。

Method: 通过综述近期研究，分析了毒性语言的标注与预处理技术、检测方法及LLM驱动的缓解策略，并进行了消融实验验证LLM重写的效果。

Result: 研究发现LLM重写能有效减少毒性语言，同时总结了现有方法的局限性。

Conclusion: 未来研究需进一步优化毒性检测与缓解技术，以确保LLMs在SE及其他领域的负责任使用。

Abstract: Large Language Models (LLMs) have become integral to software engineering
(SE), where they are increasingly used in development workflows. However, their
widespread use raises concerns about the presence and propagation of toxic
language--harmful or offensive content that can foster exclusionary
environments. This paper provides a comprehensive review of recent research on
toxicity detection and mitigation, focusing on both SE-specific and
general-purpose datasets. We examine annotation and preprocessing techniques,
assess detection methodologies, and evaluate mitigation strategies,
particularly those leveraging LLMs. Additionally, we conduct an ablation study
demonstrating the effectiveness of LLM-based rewriting for reducing toxicity.
By synthesizing existing work and identifying open challenges, this review
highlights key areas for future research to ensure the responsible deployment
of LLMs in SE and beyond.

</details>

### [32] [Compton Form Factor Extraction using Quantum Deep Neural Networks](https://arxiv.org/abs/2504.15458)
*Brandon Le,Dustin Keller*

Main category: cs.LG

TLDR: 论文通过伪数据提取康普顿形状因子，比较了经典深度神经网络（CDNN）和量子深度神经网络（QDNN），发现QDNN表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究目的是利用实验数据提取康普顿形状因子，并探索量子算法在此类问题中的潜力。

Method: 采用标准Belitsky-Kirchner-Muller形式主义（twist-two）和减少模型依赖性的拟合方法，同时使用CDNN和QDNN进行提取。

Result: QDNN在预测准确性和精度上优于CDNN，尤其在模型复杂度有限时表现更佳。

Conclusion: QDNN展示了在量子算法优化后的未来研究中具有潜力。

Abstract: Extraction tests of Compton Form Factors are performed using pseudodata based
on experimental data from Deeply Virtual Compton Scattering experiments
conducted at Jefferson Lab. The standard Belitsky, Kirchner, and Muller
formalism at twist-two is employed, along with a fitting procedure designed to
reduce model dependency similar to traditional local fits. The extraction of
the Compton Form Factors is performed using both Classical Deep Neural Networks
(CDNNs) and Quantum Deep Neural Networks (QDNNs). Comparative studies reveal
that QDNNs outperform CDNNs for this application, demonstrating improved
predictive accuracy and precision even for limited model complexity. The
results demonstrate the potential of QDNNs for future studies in which quantum
algorithms can be fully optimized.

</details>

### [33] [In-context Ranking Preference Optimization](https://arxiv.org/abs/2504.15477)
*Junda Wu,Rohan Surana,Zhouhang Xie,Yiran Shen,Yu Xia,Tong Yu,Ryan A. Rossi,Prithviraj Ammanabrolu,Julian McAuley*

Main category: cs.LG

TLDR: IRPO框架通过优化LLM基于推理时构建的排名列表，解决了有限和稀疏的成对反馈问题，并扩展了DPO目标以支持灵活的反馈形式。


<details>
  <summary>Details</summary>
Motivation: 解决在上下文设置中有限和稀疏的成对反馈问题，并支持自然和灵活的用户反馈形式。

Method: 提出IRPO框架，通过位置聚合成对项目偏好，引入可微分目标以优化离散排名指标。

Result: IRPO在排名性能上优于标准DPO方法，有效对齐LLM与直接上下文排名偏好。

Conclusion: IRPO通过可微分目标和理论支持，显著提升了LLM在复杂信息检索任务中的排名性能。

Abstract: Recent developments in Direct Preference Optimization (DPO) allow large
language models (LLMs) to function as implicit ranking models by maximizing the
margin between preferred and non-preferred responses. In practice, user
feedback on such lists typically involves identifying a few relevant items in
context rather than providing detailed pairwise comparisons for every possible
item pair. Moreover, many complex information retrieval tasks, such as
conversational agents and summarization systems, critically depend on ranking
the highest-quality outputs at the top, emphasizing the need to support natural
and flexible forms of user feedback. To address the challenge of limited and
sparse pairwise feedback in the in-context setting, we propose an In-context
Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs
based on ranking lists constructed during inference. To further capture
flexible forms of feedback, IRPO extends the DPO objective by incorporating
both the relevance of items and their positions in the list. Modeling these
aspects jointly is non-trivial, as ranking metrics are inherently discrete and
non-differentiable, making direct optimization difficult. To overcome this,
IRPO introduces a differentiable objective based on positional aggregation of
pairwise item preferences, enabling effective gradient-based optimization of
discrete ranking metrics. We further provide theoretical insights showing that
IRPO (i) automatically emphasizes items with greater disagreement between the
model and the reference ranking, and (ii) links its gradient to an importance
sampling estimator, yielding an unbiased estimator with reduced variance.
Empirical results show IRPO outperforms standard DPO approaches in ranking
performance, highlighting its effectiveness in aligning LLMs with direct
in-context ranking preferences.

</details>

### [34] [Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks](https://arxiv.org/abs/2504.15479)
*Jeremy Goldwasser,Giles Hooker*

Main category: cs.LG

TLDR: 该论文提出了一种新的反事实图像生成框架，结合生成模型和特征归因，高效生成可解释的反事实图像。


<details>
  <summary>Details</summary>
Motivation: 解决现有反事实解释方法在计算机视觉中易产生对抗样本的问题，并提供可量化的特征变化解释。

Method: 提出Counterfactual Attacks方法，通过低维流形上的对抗攻击生成反事实图像，并结合辅助数据集进行特征归因。

Result: 在MNIST和CelebA数据集上验证了方法的有效性，能够高效生成可解释的反事实图像。

Conclusion: 该方法为计算机视觉模型提供了一种灵活且高效的反事实解释框架。

Abstract: Counterfactuals are a popular framework for interpreting machine learning
predictions. These what if explanations are notoriously challenging to create
for computer vision models: standard gradient-based methods are prone to
produce adversarial examples, in which imperceptible modifications to image
pixels provoke large changes in predictions. We introduce a new,
easy-to-implement framework for counterfactual images that can flexibly adapt
to contemporary advances in generative modeling. Our method, Counterfactual
Attacks, resembles an adversarial attack on the representation of the image
along a low-dimensional manifold. In addition, given an auxiliary dataset of
image descriptors, we show how to accompany counterfactuals with feature
attribution that quantify the changes between the original and counterfactual
images. These importance scores can be aggregated into global counterfactual
explanations that highlight the overall features driving model predictions.
While this unification is possible for any counterfactual method, it has
particular computational efficiency for ours. We demonstrate the efficacy of
our approach with the MNIST and CelebA datasets.

</details>

### [35] [Fourier analysis of the physics of transfer learning for data-driven subgrid-scale models of ocean turbulence](https://arxiv.org/abs/2504.15487)
*Moein Darman,Pedram Hassanzadeh,Laure Zanna,Ashesh Chattopadhyay*

Main category: cs.LG

TLDR: 研究探讨了迁移学习在提升神经网络性能中的作用，特别是在预测海洋准地转系统中的子网格强迫时，分析了其通用性和性能指标。


<details>
  <summary>Details</summary>
Motivation: 迁移学习能帮助神经网络在少量新系统数据下泛化到分布外数据，但具体机制尚不明确，研究旨在揭示其原理。

Method: 使用9层卷积神经网络预测子网格强迫，通过傅里叶分析核函数和激活谱，评估性能及通用性。

Result: 发现神经网络在未经迁移学习时会低估输出谱，而通过重新训练一层可纠正此问题，使预测匹配目标谱。

Conclusion: 迁移学习能有效提升神经网络在动态系统中的通用性，适用于数据驱动的参数化方法。

Abstract: Transfer learning (TL) is a powerful tool for enhancing the performance of
neural networks (NNs) in applications such as weather and climate prediction
and turbulence modeling. TL enables models to generalize to out-of-distribution
data with minimal training data from the new system. In this study, we employ a
9-layer convolutional NN to predict the subgrid forcing in a two-layer ocean
quasi-geostrophic system and examine which metrics best describe its
performance and generalizability to unseen dynamical regimes. Fourier analysis
of the NN kernels reveals that they learn low-pass, Gabor, and high-pass
filters, regardless of whether the training data are isotropic or anisotropic.
By analyzing the activation spectra, we identify why NNs fail to generalize
without TL and how TL can overcome these limitations: the learned weights and
biases from one dataset underestimate the out-of-distribution sample spectra as
they pass through the network, leading to an underestimation of output spectra.
By re-training only one layer with data from the target system, this
underestimation is corrected, enabling the NN to produce predictions that match
the target spectra. These findings are broadly applicable to data-driven
parameterization of dynamical systems.

</details>

### [36] [Application of Deep Generative Models for Anomaly Detection in Complex Financial Transactions](https://arxiv.org/abs/2504.15491)
*Tengda Tang,Jianhua Yao,Yixian Wang,Qiuwu Sha,Hanrui Feng,Zhen Xu*

Main category: cs.LG

TLDR: 提出了一种基于深度生成模型的算法，用于检测大规模支付流中的可疑行为，结合GAN和VAE，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 金融交易中的异常行为检测（如欺诈和洗钱）需要高效且准确的方法，传统算法在复杂数据中表现不足。

Method: 结合GAN生成模拟数据以近似正常支付流，利用判别器识别异常；引入VAE建模潜在分布，提升生成数据真实性。

Result: 实验表明，该方法在多种评估指标上显著优于传统机器学习和其他深度学习模型，尤其在罕见欺诈行为检测中表现突出。

Conclusion: 生成模型在复杂金融数据处理中具有优势，为异常行为检测提供了高效解决方案。

Abstract: This study proposes an algorithm for detecting suspicious behaviors in large
payment flows based on deep generative models. By combining Generative
Adversarial Networks (GAN) and Variational Autoencoders (VAE), the algorithm is
designed to detect abnormal behaviors in financial transactions. First, the GAN
is used to generate simulated data that approximates normal payment flows. The
discriminator identifies anomalous patterns in transactions, enabling the
detection of potential fraud and money laundering behaviors. Second, a VAE is
introduced to model the latent distribution of payment flows, ensuring that the
generated data more closely resembles real transaction features, thus improving
the model's detection accuracy. The method optimizes the generative
capabilities of both GAN and VAE, ensuring that the model can effectively
capture suspicious behaviors even in sparse data conditions. Experimental
results show that the proposed method significantly outperforms traditional
machine learning algorithms and other deep learning models across various
evaluation metrics, especially in detecting rare fraudulent behaviors.
Furthermore, this study provides a detailed comparison of performance in
recognizing different transaction patterns (such as normal, money laundering,
and fraud) in large payment flows, validating the advantages of generative
models in handling complex financial data.

</details>

### [37] [Federated Latent Factor Learning for Recovering Wireless Sensor Networks Signal with Privacy-Preserving](https://arxiv.org/abs/2504.15525)
*Chengjun Yu,Yixin Ran,Yangyi Xia,Jia Wu,Xiaojing Liu*

Main category: cs.LG

TLDR: 本文提出了一种基于联邦潜在因子学习（FLFL）的空间信号恢复（SSR）模型FLFL-SSR，用于解决无线传感器网络（WSNs）中数据缺失和隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 由于传感器故障和节能策略，WSNs收集的数据常存在大量缺失，影响后续分析。现有潜在因子学习方法（LFL）虽有效但未充分考虑数据隐私保护。

Method: 提出FLFL-SSR模型，包括传感器级联邦学习框架（仅上传梯度更新）和局部空间共享策略（同区域传感器共享潜在特征向量）。

Result: 在两个真实WSNs数据集上的实验表明，FLFL-SSR在恢复性能上优于现有联邦方法。

Conclusion: FLFL-SSR模型有效解决了数据缺失和隐私保护问题，并提升了恢复精度。

Abstract: Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of
intelligent sensing. Due to sensor failures and energy-saving strategies, the
collected data often have massive missing data, hindering subsequent analysis
and decision-making. Although Latent Factor Learning (LFL) has been proven
effective in recovering missing data, it fails to sufficiently consider data
privacy protection. To address this issue, this paper innovatively proposes a
federated latent factor learning (FLFL) based spatial signal recovery (SSR)
model, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level
federated learning framework, where each sensor uploads only gradient updates
instead of raw data to optimize the global model, and 2) it proposes a local
spatial sharing strategy, allowing sensors within the same spatial region to
share their latent feature vectors, capturing spatial correlations and
enhancing recovery accuracy. Experimental results on two real-world WSNs
datasets demonstrate that the proposed model outperforms existing federated
methods in terms of recovery performance.

</details>

### [38] [Interpretable Deep Learning for Polar Mechanistic Reaction Prediction](https://arxiv.org/abs/2504.15539)
*Ryan J. Miller,Alexander E. Dashuta,Brayden Rudisill,David Van Vranken,Pierre Baldi*

Main category: cs.LG

TLDR: PMechRP是一种基于深度学习的化学反应预测系统，通过结合多种机器学习模型和数据集，显著提高了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 化学反应预测对合成化学创新至关重要，但现有方法缺乏机制细节和可解释性。

Method: 使用PMechDB数据集，结合多种机器学习模型（如Transformer和图神经网络），并引入混合模型（Chemformer与Siamese框架）。

Result: 在PMechDB测试集上达到94.9%的top-10准确率，在路径数据集上达到84.9%的目标恢复率。

Conclusion: PMechRP通过结合机制细节和多样化模型，显著提升了反应预测的性能和实用性。

Abstract: Accurately predicting chemical reactions is essential for driving innovation
in synthetic chemistry, with broad applications in medicine, manufacturing, and
agriculture. At the same time, reaction prediction is a complex problem which
can be both time-consuming and resource-intensive for chemists to solve. Deep
learning methods offer an appealing solution by enabling high-throughput
reaction prediction. However, many existing models are trained on the US Patent
Office dataset and treat reactions as overall transformations: mapping
reactants directly to products with limited interpretability or mechanistic
insight. To address this, we introduce PMechRP (Polar Mechanistic Reaction
Predictor), a system that trains machine learning models on the PMechDB
dataset, which represents reactions as polar elementary steps that capture
electron flow and mechanistic detail. To further expand model coverage and
improve generalization, we augment PMechDB with a diverse set of
combinatorially generated reactions. We train and compare a range of machine
learning models, including transformer-based, graph-based, and two-step siamese
architectures. Our best-performing approach was a hybrid model, which combines
a 5-ensemble of Chemformer models with a two-step Siamese framework to leverage
the accuracy of transformer architectures, while filtering away "alchemical"
products using the two-step network predictions. For evaluation, we use a test
split of the PMechDB dataset and additionally curate a human benchmark dataset
consisting of complete mechanistic pathways extracted from an organic chemistry
textbook. Our hybrid model achieves a top-10 accuracy of 94.9% on the PMechDB
test set and a target recovery rate of 84.9% on the pathway dataset.

</details>

### [39] [Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis](https://arxiv.org/abs/2504.15562)
*Dip Roy*

Main category: cs.LG

TLDR: 本文提出了一种基于贝叶斯变分自编码器（VAE）和多头注意力机制的脑部MRI异常检测方法，通过贝叶斯推理估计不确定性和异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定性方法在捕捉异常检测任务的不确定性方面存在不足，尤其是在医疗影像中对神经系统疾病的诊断至关重要。

Method: 采用贝叶斯变分自编码器（VAE）结合多头注意力机制，通过贝叶斯推理估计认知和随机不确定性。

Result: 在BraTS2020数据集上测试，ROC AUC和PR AUC均为0.83。

Conclusion: 建模不确定性是异常检测的关键，不仅提升性能和可解释性，还为临床决策提供置信度估计和异常预测。

Abstract: In medical imaging, anomaly detection is a vital element of healthcare
diagnostics, especially for neurological conditions which can be
life-threatening. Conventional deterministic methods often fall short when it
comes to capturing the inherent uncertainty of anomaly detection tasks. This
paper introduces a Bayesian Variational Autoencoder (VAE) equipped with
multi-head attention mechanisms for detecting anomalies in brain magnetic
resonance imaging (MRI). For the purpose of improving anomaly detection
performance, we incorporate both epistemic and aleatoric uncertainty estimation
through Bayesian inference. The model was tested on the BraTS2020 dataset, and
the findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper
suggests that modeling uncertainty is an essential component of anomaly
detection, enhancing both performance and interpretability and providing
confidence estimates, as well as anomaly predictions, for clinicians to
leverage in making medical decisions.

</details>

### [40] [Smooth Calibration and Decision Making](https://arxiv.org/abs/2504.15582)
*Jason Hartline,Yifan Wu,Yunran Yang*

Main category: cs.LG

TLDR: 论文探讨了机器学习预测器的校准误差在决策中的不连续性，并提出了一种后处理方法，通过添加噪声实现差分隐私，以优化决策校准误差。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决机器学习预测器的校准误差在决策中的不连续性问题，确保预测器对决策者的可信度。

Method: 提出了一种后处理方法，通过添加噪声使预测具有差分隐私性，从而优化决策校准误差（ECE和CDL）。

Result: 后处理方法能够将距离校准误差（Distance to Calibration）为ε的预测器优化至O(√ε)的ECE和CDL，达到渐近最优。

Conclusion: 后处理方法虽能优化决策校准误差，但相比直接优化ECE和CDL的在线校准算法，其最优边界仍非最优。

Abstract: Calibration requires predictor outputs to be consistent with their Bayesian
posteriors. For machine learning predictors that do not distinguish between
small perturbations, calibration errors are continuous in predictions, e.g.,
smooth calibration error (Foster and Hart, 2018), Distance to Calibration
(Blasiok et al., 2023a). On the contrary, decision-makers who use predictions
make optimal decisions discontinuously in probabilistic space, experiencing
loss from miscalibration discontinuously. Calibration errors for
decision-making are thus discontinuous, e.g., Expected Calibration Error
(Foster and Vohra, 1997), and Calibration Decision Loss (Hu and Wu, 2024).
Thus, predictors with a low calibration error for machine learning may suffer a
high calibration error for decision-making, i.e., they may not be trustworthy
for decision-makers optimizing assuming their predictions are correct. It is
natural to ask if post-processing a predictor with a low calibration error for
machine learning is without loss to achieve a low calibration error for
decision-making. In our paper, we show that post-processing an online predictor
with $\epsilon$ distance to calibration achieves $O(\sqrt{\epsilon})$ ECE and
CDL, which is asymptotically optimal. The post-processing algorithm adds noise
to make predictions differentially private. The optimal bound from low distance
to calibration predictors from post-processing is non-optimal compared with
existing online calibration algorithms that directly optimize for ECE and CDL.

</details>

### [41] [MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design](https://arxiv.org/abs/2504.15587)
*Zimo Yan,Jie Zhang,Zheng Xie,Chang Liu,Yizhen Liu,Yiping Song*

Main category: cs.LG

TLDR: MetaMolGen是一种基于元学习的分子生成器，用于少样本和属性条件分子生成，通过标准化图基序分布和轻量级自回归序列模型生成SMILES序列，并在低数据条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统生成模型在数据稀缺场景下难以实现满意条件泛化的问题。

Method: MetaMolGen通过映射图基序到归一化潜在空间，使用轻量级自回归序列模型生成SMILES序列，并集成可学习属性投影器支持条件生成。

Result: 实验表明，MetaMolGen在低数据条件下能生成有效且多样的SMILES序列，优于传统基线方法。

Conclusion: MetaMolGen在快速适应和高效条件生成方面具有优势，适用于实际分子设计。

Abstract: Molecular generation plays an important role in drug discovery and materials
science, especially in data-scarce scenarios where traditional generative
models often struggle to achieve satisfactory conditional generalization. To
address this challenge, we propose MetaMolGen, a first-order
meta-learning-based molecular generator designed for few-shot and
property-conditioned molecular generation. MetaMolGen standardizes the
distribution of graph motifs by mapping them to a normalized latent space, and
employs a lightweight autoregressive sequence model to generate SMILES
sequences that faithfully reflect the underlying molecular structure. In
addition, it supports conditional generation of molecules with target
properties through a learnable property projector integrated into the
generative process.Experimental results demonstrate that MetaMolGen
consistently generates valid and diverse SMILES sequences under low-data
regimes, outperforming conventional baselines. This highlights its advantage in
fast adaptation and efficient conditional generation for practical molecular
design.

</details>

### [42] [Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification](https://arxiv.org/abs/2504.15594)
*Tatsuhito Hasegawa,Shunsuke Sakai*

Main category: cs.LG

TLDR: 本文提出了一种理论，证明在深度学习分类任务中，softmax函数的温度参数$T^*$由特征表示的维度唯一决定，并提出了无需训练的$T^*$确定方法。通过实验验证了$T^*$的波动性，并提出了调整系数和批量归一化层以稳定特征空间。最终推导出一个经验公式，无需额外训练即可估计$T^*$，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解softmax温度参数$T$对分类任务性能的影响，并找到一种无需训练即可确定最优$T^*$的方法。

Method: 方法包括理论推导$T^*$与特征维度的关系，提出调整系数和批量归一化层以稳定特征空间，并通过大规模实验验证经验公式。

Result: 结果表明，推导的温度$T^*$不仅符合理论预期，还能有效提升分类性能，适用于多种任务。

Conclusion: 结论是提出的方法提供了一种无需训练的实用方案，能够有效确定softmax温度参数$T^*$，并提升分类任务的性能。

Abstract: In deep learning-based classification tasks, the softmax function's
temperature parameter $T$ critically influences the output distribution and
overall performance. This study presents a novel theoretical insight that the
optimal temperature $T^*$ is uniquely determined by the dimensionality of the
feature representations, thereby enabling training-free determination of $T^*$.
Despite this theoretical grounding, empirical evidence reveals that $T^*$
fluctuates under practical conditions owing to variations in models, datasets,
and other confounding factors. To address these influences, we propose and
optimize a set of temperature determination coefficients that specify how $T^*$
should be adjusted based on the theoretical relationship to feature
dimensionality. Additionally, we insert a batch normalization layer immediately
before the output layer, effectively stabilizing the feature space. Building on
these coefficients and a suite of large-scale experiments, we develop an
empirical formula to estimate $T^*$ without additional training while also
introducing a corrective scheme to refine $T^*$ based on the number of classes
and task complexity. Our findings confirm that the derived temperature not only
aligns with the proposed theoretical perspective but also generalizes
effectively across diverse tasks, consistently enhancing classification
performance and offering a practical, training-free solution for determining
$T^*$.

</details>

### [43] [Learning Dynamic Graphs via Tensorized and Lightweight Graph Convolutional Networks](https://arxiv.org/abs/2504.15613)
*Minglian Han*

Main category: cs.LG

TLDR: 该论文提出了一种新的张量轻量图卷积网络（TLGCN），用于动态图学习，解决了传统方法中时空依赖分离的问题。


<details>
  <summary>Details</summary>
Motivation: 传统动态图卷积网络（DGCN）将静态GCN与序列神经网络（SNN）结合，导致时空依赖分离，破坏了复杂的时空关系。

Method: 设计了基于张量M乘积框架的时空信息联合传播方法，并提出了轻量化的图卷积网络，减少了内存占用。

Result: 在四个真实数据集上的实验表明，TLGCN在动态图权重估计任务中优于现有模型。

Conclusion: TLGCN通过联合建模时空依赖和轻量化设计，显著提升了动态图学习的性能。

Abstract: A dynamic graph (DG) is frequently encountered in numerous real-world
scenarios. Consequently, A dynamic graph convolutional network (DGCN) has been
successfully applied to perform precise representation learning on a DG.
However, conventional DGCNs typically consist of a static GCN coupled with a
sequence neural network (SNN) to model spatial and temporal patterns
separately. This decoupled modeling mechanism inherently disrupts the intricate
spatio-temporal dependencies. To address the issue, this study proposes a novel
Tensorized Lightweight Graph Convolutional Network (TLGCN) for accurate dynamic
graph learning. It mainly contains the following two key concepts: a) designing
a novel spatio-temporal information propagation method for joint propagation of
spatio-temporal information based on the tensor M-product framework; b)
proposing a tensorized lightweight graph convolutional network based on the
above method, which significantly reduces the memory occupation of the model by
omitting complex feature transformation and nonlinear activation. Numerical
experiments on four real-world datasets demonstrate that the proposed TLGCN
outperforms the state-of-the-art models in the weight estimation task on DGs.

</details>

### [44] [Dimension-Free Decision Calibration for Nonlinear Loss Functions](https://arxiv.org/abs/2504.15615)
*Jingwu Tang,Jiayun Wu,Zhiwei Steven Wu,Jiahao Zhang*

Main category: cs.LG

TLDR: 论文探讨了在高维预测结果空间中，如何通过决策校准（decision calibration）保证简单最佳响应规则的最优性，并提出了一种平滑版本以解决非线性损失问题。


<details>
  <summary>Details</summary>
Motivation: 研究在模型预测用于下游决策时，如何确保决策者可以像对待真实结果一样响应预测，尤其是在高维和非线性损失情况下。

Method: 提出了一种平滑决策校准方法，通过平滑最佳响应实现维度无关的决策校准算法，并给出了高效的后处理算法。

Result: 证明了标准决策校准的样本复杂度与特征维度相关，而平滑版本可以实现维度无关的校准，并保持预测准确性。

Conclusion: 平滑决策校准为解决高维和非线性损失问题提供了有效方法，扩展了决策校准的应用范围。

Abstract: When model predictions inform downstream decision making, a natural question
is under what conditions can the decision-makers simply respond to the
predictions as if they were the true outcomes. Calibration suffices to
guarantee that simple best-response to predictions is optimal. However,
calibration for high-dimensional prediction outcome spaces requires exponential
computational and statistical complexity. The recent relaxation known as
decision calibration ensures the optimality of the simple best-response rule
while requiring only polynomial sample complexity in the dimension of outcomes.
However, known results on calibration and decision calibration crucially rely
on linear loss functions for establishing best-response optimality. A natural
approach to handle nonlinear losses is to map outcomes $y$ into a feature space
$\phi(y)$ of dimension $m$, then approximate losses with linear functions of
$\phi(y)$. Unfortunately, even simple classes of nonlinear functions can demand
exponentially large or infinite feature dimensions $m$. A key open problem is
whether it is possible to achieve decision calibration with sample complexity
independent of~$m$. We begin with a negative result: even verifying decision
calibration under standard deterministic best response inherently requires
sample complexity polynomial in~$m$. Motivated by this lower bound, we
investigate a smooth version of decision calibration in which decision-makers
follow a smooth best-response. This smooth relaxation enables dimension-free
decision calibration algorithms. We introduce algorithms that, given
$\mathrm{poly}(|A|,1/\epsilon)$ samples and any initial predictor~$p$, can
efficiently post-process it to satisfy decision calibration without worsening
accuracy. Our algorithms apply broadly to function classes that can be
well-approximated by bounded-norm functions in (possibly infinite-dimensional)
separable RKHS.

</details>

### [45] [SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2504.15616)
*Kai Chen,Xiaodong Zhao,Yujie Huang,Guoyu Fang,Xiao Song,Ruiping Wang,Ziyuan Wang*

Main category: cs.LG

TLDR: SocialMOIF提出了一种多阶意图融合模型，用于更全面地理解直接和间接意图信息，以解决智能系统中轨迹预测的高不确定性和复杂高阶影响问题。


<details>
  <summary>Details</summary>
Motivation: 当前工作因代理意图的高不确定性和邻近群体间复杂的高阶影响存在局限性，需改进轨迹预测的准确性和模型解释性。

Method: 开发多阶意图融合模型，设计轨迹分布近似器和全局轨迹优化器，并引入考虑距离和方向的损失函数。

Result: 实验表明，该模型在动态和静态数据集上均优于现有基线。

Conclusion: SocialMOIF通过多阶意图融合和优化策略，显著提升了轨迹预测的准确性和效率。

Abstract: The analysis and prediction of agent trajectories are crucial for
decision-making processes in intelligent systems, with precise short-term
trajectory forecasting being highly significant across a range of applications.
Agents and their social interactions have been quantified and modeled by
researchers from various perspectives; however, substantial limitations exist
in the current work due to the inherent high uncertainty of agent intentions
and the complex higher-order influences among neighboring groups. SocialMOIF is
proposed to tackle these challenges, concentrating on the higher-order
intention interactions among neighboring groups while reinforcing the primary
role of first-order intention interactions between neighbors and the target
agent. This method develops a multi-order intention fusion model to achieve a
more comprehensive understanding of both direct and indirect intention
information. Within SocialMOIF, a trajectory distribution approximator is
designed to guide the trajectories toward values that align more closely with
the actual data, thereby enhancing model interpretability. Furthermore, a
global trajectory optimizer is introduced to enable more accurate and efficient
parallel predictions. By incorporating a novel loss function that accounts for
distance and direction during training, experimental results demonstrate that
the model outperforms previous state-of-the-art baselines across multiple
metrics in both dynamic and static datasets.

</details>

### [46] [RadioDiff-$k^2$: Helmholtz Equation Informed Generative Diffusion Model for Multi-Path Aware Radio Map Construction](https://arxiv.org/abs/2504.15623)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Ruijin Sun,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TLDR: 提出了一种基于物理信息的生成学习方法RadioDiff-$m{k^2}$，用于高效构建多路径感知的无线电地图（RM），结合数据驱动和物理建模的优势。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络对智能优化的需求增加，传统电磁方法计算量大且适应性差，现有神经网络方法缺乏对电磁波传播物理的考虑，导致建模不准确。

Method: 提出了一种受Helmholtz方程启发的RM构建方法，设计了一个双生成扩散模型框架，分别推断电磁奇点和重建完整RM。

Result: 该方法显著提高了RM的准确性，尤其在多路径效应主导的复杂传播环境中。

Conclusion: RadioDiff-$m{k^2}$结合了数据驱动的高效性和物理建模的严谨性，为6G网络中的RM构建提供了创新解决方案。

Abstract: In this paper, we propose a novel physics-informed generative learning
approach, termed RadioDiff-$\bm{k^2}$, for accurate and efficient
multipath-aware radio map (RM) construction. As wireless communication evolves
towards environment-aware paradigms, driven by the increasing demand for
intelligent and proactive optimization in sixth-generation (6G) networks,
accurate construction of RMs becomes crucial yet highly challenging.
Conventional electromagnetic (EM)-based methods, such as full-wave solvers and
ray-tracing approaches, exhibit substantial computational overhead and limited
adaptability to dynamic scenarios. Although, existing neural network (NN)
approaches have efficient inferencing speed, they lack sufficient consideration
of the underlying physics of EM wave propagation, limiting their effectiveness
in accurately modeling critical EM singularities induced by complex multipath
environments. To address these fundamental limitations, we propose a novel
physics-inspired RM construction method guided explicitly by the Helmholtz
equation, which inherently governs EM wave propagation. Specifically, we
theoretically establish a direct correspondence between EM singularities, which
correspond to the critical spatial features influencing wireless propagation,
and regions defined by negative wave numbers in the Helmholtz equation. Based
on this insight, we design an innovative dual generative diffusion model (DM)
framework comprising one DM dedicated to accurately inferring EM singularities
and another DM responsible for reconstructing the complete RM using these
singularities along with environmental contextual information. Our
physics-informed approach uniquely combines the efficiency advantages of
data-driven methods with rigorous physics-based EM modeling, significantly
enhancing RM accuracy, particularly in complex propagation environments
dominated by multipath effects.

</details>

### [47] [Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers](https://arxiv.org/abs/2504.15634)
*Peizheng Liu,Hitoshi Iba*

Main category: cs.LG

TLDR: 本文提出了一种结合Transformer和DQN的方法，用于解决3D H-P蛋白质折叠问题，通过强化学习优化折叠决策，并在实验中取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: Transformer在序列建模中表现优异，但在H-P蛋白质折叠模型中的应用较少，因此探索其潜力。

Method: 采用结合注意力机制的DQN，引入自避行走、奖励函数优化、有效性检查及多种强化学习技术（如双Q学习和优先回放）。

Result: 在标准测试序列上，方法对短序列取得已知最优解，对长序列接近最优。

Conclusion: 研究表明基于注意力的强化学习在蛋白质折叠中具有潜力，并提出了Transformer-Q网络结构的原型。

Abstract: Transformer-based architectures have recently propelled advances in sequence
modeling across domains, but their application to the hydrophobic-hydrophilic
(H-P) model for protein folding remains relatively unexplored. In this work, we
adapt a Deep Q-Network (DQN) integrated with attention mechanisms
(Transformers) to address the 3D H-P protein folding problem. Our system
formulates folding decisions as a self-avoiding walk in a reinforced
environment, and employs a specialized reward function based on favorable
hydrophobic interactions. To improve performance, the method incorporates
validity check including symmetry-breaking constraints, dueling and double
Q-learning, and prioritized replay to focus learning on critical transitions.
Experimental evaluations on standard benchmark sequences demonstrate that our
approach achieves several known best solutions for shorter sequences, and
obtains near-optimal results for longer chains. This study underscores the
promise of attention-based reinforcement learning for protein folding, and
created a prototype of Transformer-based Q-network structure for 3-dimensional
lattice models.

</details>

### [48] [An XAI-based Analysis of Shortcut Learning in Neural Networks](https://arxiv.org/abs/2504.15664)
*Phuong Quynh Le,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TLDR: 论文提出了一种名为神经元虚假分数的诊断方法，用于量化神经元对虚假特征的依赖，并分析了CNN和ViT中虚假特征的编码方式。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型倾向于学习虚假特征，现有方法在某些情况下有效，但在其他情况下失败，因此需要系统性分析。

Method: 引入神经元虚假分数，结合XAI方法，分析CNN和ViT中虚假特征的编码方式。

Result: 虚假特征在模型中部分解耦，但解耦程度因架构而异，现有缓解方法的假设不完整。

Conclusion: 研究为开发新方法缓解虚假相关性奠定了基础，使AI模型更安全。

Abstract: Machine learning models tend to learn spurious features - features that
strongly correlate with target labels but are not causal. Existing approaches
to mitigate models' dependence on spurious features work in some cases, but
fail in others. In this paper, we systematically analyze how and where neural
networks encode spurious correlations. We introduce the neuron spurious score,
an XAI-based diagnostic measure to quantify a neuron's dependence on spurious
features. We analyze both convolutional neural networks (CNNs) and vision
transformers (ViTs) using architecture-specific methods. Our results show that
spurious features are partially disentangled, but the degree of disentanglement
varies across model architectures. Furthermore, we find that the assumptions
behind existing mitigation methods are incomplete. Our results lay the
groundwork for the development of novel methods to mitigate spurious
correlations and make AI models safer to use in practice.

</details>

### [49] [Invariant Learning with Annotation-free Environments](https://arxiv.org/abs/2504.15686)
*Phuong Quynh Le,Christin Seifert,Jörg Schlötterer*

Main category: cs.LG

TLDR: 本文提出了一种无需额外标注即可推断环境的方法，通过观察ERM模型的表示空间特性，在ColoredMNIST基准上取得了与需要显式环境标签的方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统不变性学习方法依赖预划分的环境标签，而本文旨在无需额外标注的情况下推断环境，提升领域泛化能力。

Method: 通过分析训练好的ERM模型的表示空间特性，推断环境分布，无需依赖预定义的环境标签。

Result: 在ColoredMNIST基准上，性能与需要显式环境标签的方法相当，且与一种对ERM参考模型有严格限制的无标注方法表现相当。

Conclusion: 该方法展示了无需额外标注即可推断环境的潜力，为领域泛化提供了新的思路。

Abstract: Invariant learning is a promising approach to improve domain generalization
compared to Empirical Risk Minimization (ERM). However, most invariant learning
methods rely on the assumption that training examples are pre-partitioned into
different known environments. We instead infer environments without the need
for additional annotations, motivated by observations of the properties within
the representation space of a trained ERM model. We show the preliminary
effectiveness of our approach on the ColoredMNIST benchmark, achieving
performance comparable to methods requiring explicit environment labels and on
par with an annotation-free method that poses strong restrictions on the ERM
reference model.

</details>

### [50] [Riemannian Neural Geodesic Interpolant](https://arxiv.org/abs/2504.15736)
*Jiawen Wu,Bingguang Chen,Yuyi Zhou,Qi Meng,Rongchan Zhu,Zhi-Ming Ma*

Main category: cs.LG

TLDR: 论文提出了一种在黎曼流形上插值概率密度的RNGI模型，并通过E-SDE算法改进采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有随机插值模型局限于欧几里得空间，无法直接应用于黎曼流形上的分布学习问题。

Method: RNGI模型通过随机测地线插值黎曼流形上的概率密度，并利用E-SDE算法减少采样误差。

Result: 实验证明RNGI和E-SDE在S2和SO(3)流形上有效，且E-SDE显著提升了采样质量。

Conclusion: RNGI和E-SDE为黎曼流形上的生成建模提供了高效且理论支持的方法。

Abstract: Stochastic interpolants are efficient generative models that bridge two
arbitrary probability density functions in finite time, enabling flexible
generation from the source to the target distribution or vice versa. These
models are primarily developed in Euclidean space, and are therefore limited in
their application to many distribution learning problems defined on Riemannian
manifolds in real-world scenarios. In this work, we introduce the Riemannian
Neural Geodesic Interpolant (RNGI) model, which interpolates between two
probability densities on a Riemannian manifold along the stochastic geodesics,
and then samples from one endpoint as the final state using the continuous flow
originating from the other endpoint. We prove that the temporal marginal
density of RNGI solves a transport equation on the Riemannian manifold. After
training the model's the neural velocity and score fields, we propose the
Embedding Stochastic Differential Equation (E-SDE) algorithm for stochastic
sampling of RNGI. E-SDE significantly improves the sampling quality by reducing
the accumulated error caused by the excessive intrinsic discretization of
Riemannian Brownian motion in the classical Geodesic Random Walk (GRW)
algorithm. We also provide theoretical bounds on the generative bias measured
in terms of KL-divergence. Finally, we demonstrate the effectiveness of the
proposed RNGI and E-SDE through experiments conducted on both collected and
synthetic distributions on S2 and SO(3).

</details>

### [51] [Observability conditions for neural state-space models with eigenvalues and their roots of unity](https://arxiv.org/abs/2504.15758)
*Andrew Gracyk*

Main category: cs.LG

TLDR: 论文通过微分方程和控制理论研究神经状态空间模型和Mamba架构的可观测性，提出高效的计算方法，并展示了五种非平凡结果。


<details>
  <summary>Details</summary>
Motivation: 研究神经状态空间模型的可观测性，特别是在高维和可学习初始状态的情况下，为机器学习提供理论支持。

Method: 结合控制理论和傅里叶变换，提出基于特征值、单位根和Vandermonde矩阵的可观测性条件，并设计高效训练算法。

Result: 提出了五种结果，包括基于傅里叶变换的高概率可观测性、Mamba的类Hautus条件，以及共享参数的高效计算架构。

Conclusion: 论文为神经状态空间模型的可观测性提供了理论框架和高效计算方法，展示了在机器学习中的实际应用潜力。

Abstract: We operate through the lens of ordinary differential equations and control
theory to study the concept of observability in the context of neural
state-space models and the Mamba architecture. We develop strategies to enforce
observability, which are tailored to a learning context, specifically where the
hidden states are learnable at initial time, in conjunction to over its
continuum, and high-dimensional. We also highlight our methods emphasize
eigenvalues, roots of unity, or both. Our methods effectuate computational
efficiency when enforcing observability, sometimes at great scale. We formulate
observability conditions in machine learning based on classical control theory
and discuss their computational complexity. Our nontrivial results are
fivefold. We discuss observability through the use of permutations in neural
applications with learnable matrices without high precision. We present two
results built upon the Fourier transform that effect observability with high
probability up to the randomness in the learning. These results are worked with
the interplay of representations in Fourier space and their eigenstructure,
nonlinear mappings, and the observability matrix. We present a result for Mamba
that is similar to a Hautus-type condition, but instead employs an argument
using a Vandermonde matrix instead of eigenvectors. Our final result is a
shared-parameter construction of the Mamba system, which is computationally
efficient in high exponentiation. We develop a training algorithm with this
coupling, showing it satisfies a Robbins-Monro condition under certain
orthogonality, while a more classical training procedure fails to satisfy a
contraction with high Lipschitz constant.

</details>

### [52] [Grounded in Context: Retrieval-Based Method for Hallucination Detection](https://arxiv.org/abs/2504.15771)
*Assaf Gerner,Netta Madvil,Nadav Barak,Alex Zaikman,Jonatan Liberman,Liron Hamra,Rotem Brazilay,Shay Tsadok,Yaron Friedman,Neal Harow,Noam Bresler,Shir Chorev,Philip Tannor*

Main category: cs.LG

TLDR: Deepchecks提出了一种名为“Grounded in Context”的幻觉检测框架，用于生产级长上下文数据，结合检索和NLI模型，在RAGTruth任务中F1得分0.83。


<details>
  <summary>Details</summary>
Motivation: 尽管内容生成技术有所进步，但基于LLM的应用仍存在幻觉答案问题，需要一种高效的检测框架。

Method: 框架结合检索和NLI模型，通过编码器模型（512-token上下文窗口）预测前提与假设的事实一致性。

Result: 在RAGTruth任务中，框架的F1得分为0.83，表现优于同类模型。

Conclusion: 该框架能有效检测幻觉答案，适用于多种用例，如摘要、数据提取和RAG。

Abstract: Despite advancements in grounded content generation, production Large
Language Models (LLMs) based applications still suffer from hallucinated
answers. We present "Grounded in Context" - Deepchecks' hallucination detection
framework, designed for production-scale long-context data and tailored to
diverse use cases, including summarization, data extraction, and RAG. Inspired
by RAG architecture, our method integrates retrieval and Natural Language
Inference (NLI) models to predict factual consistency between premises and
hypotheses using an encoder-based model with only a 512-token context window.
Our framework identifies unsupported claims with an F1 score of 0.83 in
RAGTruth's response-level classification task, matching methods that trained on
the dataset, and outperforming all comparable frameworks using similar-sized
models.

</details>

### [53] [Clifford Group Equivariant Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2504.15773)
*Cong Liu,Sharvaree Vadgama,David Ruhe,Erik Bekkers,Patrick Forrè*

Main category: cs.LG

TLDR: 本文提出了一种基于Clifford代数的E(n)-等变扩散模型（CDMs），通过利用Clifford多向量的几何积和子空间中的几何信息，扩展了扩散过程到高阶多向量子空间。


<details>
  <summary>Details</summary>
Motivation: 探索Clifford代数的表达能力，以增强E(n)-等变扩散模型的几何信息捕捉能力。

Method: 利用Clifford多向量的几何积和子空间信息，将扩散过程扩展到所有高阶多向量子空间，并在QM9数据集上进行无条件分子生成实验。

Result: 实验结果表明，CDMs在生成建模方面具有潜力，能够捕捉不同子空间的联合分布。

Conclusion: CDMs为生成建模提供了一种新方法，能够通过高阶特征整合更丰富的几何信息。

Abstract: This paper explores leveraging the Clifford algebra's expressive power for
$\E(n)$-equivariant diffusion models. We utilize the geometric products between
Clifford multivectors and the rich geometric information encoded in Clifford
subspaces in \emph{Clifford Diffusion Models} (CDMs). We extend the diffusion
process beyond just Clifford one-vectors to incorporate all higher-grade
multivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us
to apply latent diffusion across complete multivectors. This enables CDMs to
capture the joint distribution across different subspaces of the algebra,
incorporating richer geometric information through higher-order features. We
provide empirical results for unconditional molecular generation on the QM9
dataset, showing that CDMs provide a promising avenue for generative modeling.

</details>

### [54] [DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations](https://arxiv.org/abs/2504.15806)
*Kai Luo,Juan Tang,Mingchao Cai,Xiaoqing Zeng,Manqi Xie,Ming Yan*

Main category: cs.LG

TLDR: 论文提出了一种名为DAE-KAN的新框架，结合Kolmogorov-Arnold Networks（KANs）和Physics-Informed Neural Networks（PINNs），用于解决高指数微分代数方程（DAEs）。实验表明，DAE-KAN在误差控制方面显著优于传统PINNs。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理高指数DAEs时存在性能限制，而KANs因其强大的函数拟合能力被视为潜在解决方案。

Method: 提出DAE-KAN框架，整合KANs和PINNs，利用KANs的函数拟合能力增强PINNs的性能。

Result: DAE-KAN将微分和代数变量的绝对误差降低了1到2个数量级，且在控制漂移误差方面优于传统数值方法。

Conclusion: DAE-KAN展示了神经网络方法在解决高指数DAEs中的潜力，具有高计算精度和泛化能力。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
Multi-Layer Perceptrons (MLPs) due to their superior function-fitting abilities
in data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,
for solving high-index differential-algebraic equations (DAEs) by integrating
KANs with Physics-Informed Neural Networks (PINNs). This framework not only
preserves the ability of traditional PINNs to model complex systems governed by
physical laws but also enhances their performance by leveraging the
function-fitting strengths of KANs. Numerical experiments demonstrate that for
DAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute
errors of both differential and algebraic variables by 1 to 2 orders of
magnitude compared to traditional PINNs. To assess the effectiveness of this
approach, we analyze the drift-off error and find that both PINNs and DAE-KAN
outperform classical numerical methods in controlling this phenomenon. Our
results highlight the potential of neural network methods, particularly
DAE-KAN, in solving high-index DAEs with substantial computational accuracy and
generalization, offering a promising solution for challenging partial
differential-algebraic equations.

</details>

### [55] [Fusing Reward and Dueling Feedback in Stochastic Bandits](https://arxiv.org/abs/2504.15812)
*Xuchuang Wang,Qirun Zeng,Jinhang Zuo,Xutong Liu,Mohammad Hajiesmaili,John C. S. Lui,Adam Wierman*

Main category: cs.LG

TLDR: 本文研究了在随机多臂老虎机问题中结合绝对（奖励）和相对（对决）反馈的方法，提出了两种融合算法，并证明了其理论性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何在多臂老虎机问题中同时利用绝对和相对反馈，以提高算法的效率和性能。

Method: 提出了两种融合算法：(1) 基于消除的融合算法，统一利用两种反馈；(2) 基于分解的融合算法，动态选择更有效的反馈类型。

Result: 消除融合算法因对决消除的固有次优性导致遗憾存在次优乘性项，而分解融合算法在常见假设下达到与下界匹配的遗憾。

Conclusion: 实验验证了算法的有效性，分解融合算法在理论上和实际中表现更优。

Abstract: This paper investigates the fusion of absolute (reward) and relative
(dueling) feedback in stochastic bandits, where both feedback types are
gathered in each decision round. We derive a regret lower bound, demonstrating
that an efficient algorithm may incur only the smaller among the reward and
dueling-based regret for each individual arm. We propose two fusion approaches:
(1) a simple elimination fusion algorithm that leverages both feedback types to
explore all arms and unifies collected information by sharing a common
candidate arm set, and (2) a decomposition fusion algorithm that selects the
more effective feedback to explore the corresponding arms and randomly assigns
one feedback type for exploration and the other for exploitation in each round.
The elimination fusion experiences a suboptimal multiplicative term of the
number of arms in regret due to the intrinsic suboptimality of dueling
elimination. In contrast, the decomposition fusion achieves regret matching the
lower bound up to a constant under a common assumption. Extensive experiments
confirm the efficacy of our algorithms and theoretical results.

</details>

### [56] [DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers](https://arxiv.org/abs/2504.15827)
*Xuyang Zhong,Haochen Luo,Chen Liu*

Main category: cs.LG

TLDR: 论文提出DualOptim方法，通过自适应学习率和解耦动量因子，解决了现有机器遗忘（MU）方法对超参数敏感的问题，提升了遗忘效果和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法对超参数敏感，需要精细调参，限制了实际应用。

Method: 提出DualOptim方法，结合自适应学习率和解耦动量因子。

Result: 实验表明DualOptim显著提升了机器遗忘的效果和稳定性，适用于多种任务。

Conclusion: DualOptim是一种通用方法，能够增强现有机器遗忘算法的性能。

Abstract: Existing machine unlearning (MU) approaches exhibit significant sensitivity
to hyperparameters, requiring meticulous tuning that limits practical
deployment. In this work, we first empirically demonstrate the instability and
suboptimal performance of existing popular MU methods when deployed in
different scenarios. To address this issue, we propose Dual Optimizer
(DualOptim), which incorporates adaptive learning rate and decoupled momentum
factors. Empirical and theoretical evidence demonstrates that DualOptim
contributes to effective and stable unlearning. Through extensive experiments,
we show that DualOptim can significantly boost MU efficacy and stability across
diverse tasks, including image classification, image generation, and large
language models, making it a versatile approach to empower existing MU
algorithms.

</details>

### [57] [Adaptive PCA-Based Outlier Detection for Multi-Feature Time Series in Space Missions](https://arxiv.org/abs/2504.15846)
*Jonah Ekelund,Savvas Raptis,Vicki Toy-Edens,Wenli Mo,Drew L. Turner,Ian J. Cohen,Stefano Markidis*

Main category: cs.LG

TLDR: 提出了一种基于PCA重建误差的自适应异常检测算法，用于空间任务中的实时多特征时间序列数据分析。


<details>
  <summary>Details</summary>
Motivation: 空间任务中有限的计算资源和数据下行限制需要实时识别感兴趣区域的鲁棒方法。

Method: 使用增量PCA动态适应数据分布，结合预缩放过程归一化特征幅度。

Result: 在NASA的MMS和THEMIS任务中成功检测到空间等离子体事件和日侧瞬态现象。

Conclusion: 该算法适用于空间任务中的实时异常检测，无需预定义模型。

Abstract: Analyzing multi-featured time series data is critical for space missions
making efficient event detection, potentially onboard, essential for automatic
analysis. However, limited onboard computational resources and data downlink
constraints necessitate robust methods for identifying regions of interest in
real time. This work presents an adaptive outlier detection algorithm based on
the reconstruction error of Principal Component Analysis (PCA) for feature
reduction, designed explicitly for space mission applications. The algorithm
adapts dynamically to evolving data distributions by using Incremental PCA,
enabling deployment without a predefined model for all possible conditions. A
pre-scaling process normalizes each feature's magnitude while preserving
relative variance within feature types. We demonstrate the algorithm's
effectiveness in detecting space plasma events, such as distinct space
environments, dayside and nightside transients phenomena, and transition layers
through NASA's MMS mission observations. Additionally, we apply the method to
NASA's THEMIS data, successfully identifying a dayside transient using
onboard-available measurements.

</details>

### [58] [Consistent Causal Inference of Group Effects in Non-Targeted Trials with Finitely Many Effect Levels](https://arxiv.org/abs/2504.15854)
*Georgios Mavroudeas,Malik Magdon-Ismail,Kristin P. Bennett,Jason Kuruzovich*

Main category: cs.LG

TLDR: 论文提出了一种名为PCM的非参数方法，用于估计治疗对“病态”和“健康”群体的异质性影响，解决了传统试验中效果混淆的问题。


<details>
  <summary>Details</summary>
Motivation: 在非目标试验中，治疗可能对“病态”群体有益但对“健康”群体有害，导致治疗效果难以准确估计。

Method: 提出PCM（预聚类合并）方法，通过非参数方式估计群体效应，适用于有限范围函数的估计。

Result: 在合成数据上，PCM的准确性比现有方法提高了10倍以上，并证明了其渐近一致性。

Conclusion: PCM方法能有效解决治疗效果异质性问题，适用于更广泛的函数估计场景。

Abstract: A treatment may be appropriate for some group (the ``sick" group) on whom it
has a positive effect, but it can also have a detrimental effect on subjects
from another group (the ``healthy" group). In a non-targeted trial both sick
and healthy subjects may be treated, producing heterogeneous effects within the
treated group. Inferring the correct treatment effect on the sick population is
then difficult, because the effects on the different groups get tangled. We
propose an efficient nonparametric approach to estimating the group effects,
called {\bf PCM} (pre-cluster and merge). We prove its asymptotic consistency
in a general setting and show, on synthetic data, more than a 10x improvement
in accuracy over existing state-of-the-art. Our approach applies more generally
to consistent estimation of functions with a finite range.

</details>

### [59] [SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains](https://arxiv.org/abs/2504.15897)
*Zherui Yang,Zhengyang Xue,Ligang Liu*

Main category: cs.LG

TLDR: SUPRA神经算子通过将注意力机制推广到函数空间，并利用拉普拉斯特征函数构造子空间，解决了传统神经算子在计算效率和几何适应性上的问题。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在处理大规模网格和不规则域时面临计算效率低和精度下降的挑战。

Method: 将标准注意力机制推广到函数空间，提出SUPRA神经算子，利用拉普拉斯特征函数构造子空间进行近似。

Result: 在多种PDE数据集上，SUPRA神经算子将错误率降低高达33%，同时保持计算效率。

Conclusion: SUPRA神经算子在提升精度的同时解决了计算效率和几何适应性问题，具有广泛应用潜力。

Abstract: Neural operators are efficient surrogate models for solving partial
differential equations (PDEs), but their key components face challenges: (1) in
order to improve accuracy, attention mechanisms suffer from computational
inefficiency on large-scale meshes, and (2) spectral convolutions rely on the
Fast Fourier Transform (FFT) on regular grids and assume a flat geometry, which
causes accuracy degradation on irregular domains. To tackle these problems, we
regard the matrix-vector operations in the standard attention mechanism on
vectors in Euclidean space as bilinear forms and linear operators in vector
spaces and generalize the attention mechanism to function spaces. This new
attention mechanism is fully equivalent to the standard attention but
impossible to compute due to the infinite dimensionality of function spaces. To
address this, inspired by model reduction techniques, we propose a Subspace
Parameterized Attention (SUPRA) neural operator, which approximates the
attention mechanism within a finite-dimensional subspace. To construct a
subspace on irregular domains for SUPRA, we propose using the Laplacian
eigenfunctions, which naturally adapt to domains' geometry and guarantee the
optimal approximation for smooth functions. Experiments show that the SUPRA
neural operator reduces error rates by up to 33% on various PDE datasets while
maintaining state-of-the-art computational efficiency.

</details>

### [60] [GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network](https://arxiv.org/abs/2504.15905)
*Wenjing Xiao,Chenglong Shi,Miaojiang Chen,Zhiquan Liu,Min Chen,H. Herbert Song*

Main category: cs.LG

TLDR: 提出了一种名为GraphEdge的高效边缘计算架构，结合图神经网络和深度强化学习，优化通信成本和任务卸载策略。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的快速增长，边缘计算在提供高效服务方面变得重要，但现有方法在图结构场景（如交通流量预测和社交推荐系统）中表现不佳，尤其是图神经网络方法通信成本高。

Method: GraphEdge架构通过感知用户拓扑结构，将数据关联表示为图布局，并利用分层遍历图切割算法（HiCut）优化布局，最小化通信成本；随后采用深度强化学习算法（DRLGO）生成基于子图的任务卸载策略，优化处理时间和能耗。

Result: 实验结果表明，GraphEdge在动态场景中表现出色，有效降低了通信成本并优化了任务处理。

Conclusion: GraphEdge架构通过结合图切割和强化学习，显著提升了边缘计算在图结构任务中的效率和适应性。

Abstract: With the exponential growth of Internet of Things (IoT) devices, edge
computing (EC) is gradually playing an important role in providing
cost-effective services. However, existing approaches struggle to perform well
in graph-structured scenarios where user data is correlated, such as traffic
flow prediction and social relationship recommender systems. In particular,
graph neural network (GNN)-based approaches lead to expensive server
communication cost. To address this problem, we propose GraphEdge, an efficient
GNN-based EC architecture. It considers the EC system of GNN tasks, where there
are associations between users and it needs to take into account the task data
of its neighbors when processing the tasks of a user. Specifically, the
architecture first perceives the user topology and represents their data
associations as a graph layout at each time step. Then the graph layout is
optimized by calling our proposed hierarchical traversal graph cut algorithm
(HiCut), which cuts the graph layout into multiple weakly associated subgraphs
based on the aggregation characteristics of GNN, and the communication cost
between different subgraphs during GNN inference is minimized. Finally, based
on the optimized graph layout, our proposed deep reinforcement learning (DRL)
based graph offloading algorithm (DRLGO) is executed to obtain the optimal
offloading strategy for the tasks of users, the offloading strategy is
subgraph-based, it tries to offload user tasks in a subgraph to the same edge
server as possible while minimizing the task processing time and energy
consumption of the EC system. Experimental results show the good effectiveness
and dynamic adaptation of our proposed architecture and it also performs well
even in dynamic scenarios.

</details>

### [61] [ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion](https://arxiv.org/abs/2504.15920)
*Xiang Li,Haobing Liu,Jianpeng Qi,Yuan Cao,Guoqing Chao,Yanwei Yu*

Main category: cs.LG

TLDR: ScaleGNN是一种新型图神经网络框架，通过自适应融合多级图特征解决过平滑和可扩展性问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: GNN在消息传递机制上存在过平滑和可扩展性不足的问题，影响其在深层网络和大规模图上的表现。

Method: 提出ScaleGNN框架，包括自适应高阶特征融合模块、基于LCS的高阶冗余特征掩码机制和低阶增强特征聚合。

Result: 在真实数据集上，ScaleGNN在准确性和计算效率上均优于现有GNN模型。

Conclusion: ScaleGNN通过自适应特征融合和冗余信息抑制，有效解决了GNN的两大挑战，为大规模图任务提供了高效解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across
various graph-based tasks by effectively capturing relational information
between nodes. These models rely on iterative message passing to propagate node
features, enabling nodes to aggregate information from their neighbors. Recent
research has significantly improved the message-passing mechanism, enhancing
GNN scalability on large-scale graphs. However, GNNs still face two main
challenges: over-smoothing, where excessive message passing results in
indistinguishable node representations, especially in deep networks
incorporating high-order neighbors; and scalability issues, as traditional
architectures suffer from high model complexity and increased inference time
due to redundant information aggregation. This paper proposes a novel framework
for large-scale graphs named ScaleGNN that simultaneously addresses both
challenges by adaptively fusing multi-level graph features. We first construct
neighbor matrices for each order, learning their relative information through
trainable weights through an adaptive high-order feature fusion module. This
allows the model to selectively emphasize informative high-order neighbors
while reducing unnecessary computational costs. Additionally, we introduce a
High-order redundant feature masking mechanism based on a Local Contribution
Score (LCS), which enables the model to retain only the most relevant neighbors
at each order, preventing redundant information propagation. Furthermore,
low-order enhanced feature aggregation adaptively integrates low-order and
high-order features based on task relevance, ensuring effective capture of both
local and global structural information without excessive complexity. Extensive
experiments on real-world datasets demonstrate that our approach consistently
outperforms state-of-the-art GNN models in both accuracy and computational
efficiency.

</details>

### [62] [Achieving Distributive Justice in Federated Learning via Uncertainty Quantification](https://arxiv.org/abs/2504.15924)
*Alycia Carey,Xintao Wu*

Main category: cs.LG

TLDR: UDJ-FL框架通过不确定性加权实现多种客户端公平性指标，包括平等主义、功利主义等，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习公平性指标缺乏理论基础，难以选择适合的公平性伦理。

Method: 结合公平资源分配和不确定性加权技术，实现多种公平性指标。

Result: UDJ-FL能够实现四种公平性指标，并在公平性上优于其他方法。

Conclusion: UDJ-FL提供了一种灵活且理论支持的公平性框架，适用于不同伦理需求。

Abstract: Client-level fairness metrics for federated learning are used to ensure that
all clients in a federation either: a) have similar final performance on their
local data distributions (i.e., client parity), or b) obtain final performance
on their local data distributions relative to their contribution to the
federated learning process (i.e., contribution fairness). While a handful of
works that propose either client-parity or contribution-based fairness metrics
ground their definitions and decisions in social theories of equality -- such
as distributive justice -- most works arbitrarily choose what notion of
fairness to align with which makes it difficult for practitioners to choose
which fairness metric aligns best with their fairness ethics. In this work, we
propose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),
a flexible federated learning framework that can achieve multiple distributive
justice-based client-level fairness metrics. Namely, by utilizing techniques
inspired by fair resource allocation, in conjunction with performing aleatoric
uncertainty-based client weighing, our UDJ-FL framework is able to achieve
egalitarian, utilitarian, Rawls' difference principle, or desert-based
client-level fairness. We empirically show the ability of UDJ-FL to achieve all
four defined distributive justice-based client-level fairness metrics in
addition to providing fairness equivalent to (or surpassing) other popular fair
federated learning works. Further, we provide justification for why aleatoric
uncertainty weighing is necessary to the construction of our UDJ-FL framework
as well as derive theoretical guarantees for the generalization bounds of
UDJ-FL. Our code is publicly available at
https://github.com/alycia-noel/UDJ-FL.

</details>

### [63] [StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation](https://arxiv.org/abs/2504.15930)
*Yinmin Zhong,Zili Zhang,Xiaoniu Song,Hanpeng Hu,Chao Jin,Bingyang Wu,Nuo Chen,Yukun Chen,Yu Zhou,Changyi Wan,Hongyu Zhou,Yimin Jiang,Yibo Zhu,Daxin Jiang*

Main category: cs.LG

TLDR: StreamRL通过解耦架构解决了传统RL在LLM训练中的资源耦合问题，并通过优化流水线气泡和偏态气泡显著提升了吞吐量和成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统RL在LLM训练中采用共享资源的共置架构，导致资源耦合问题，限制了扩展性和成本效率。StreamRL旨在通过解耦架构解决这一问题。

Method: StreamRL采用解耦架构，通过流生成和异步RL实现阶段重叠，并利用输出长度排序模型优化调度以减少长尾样本的影响。

Result: 实验表明，StreamRL的吞吐量提升至现有系统的2.66倍，成本效益提升1.33倍。

Conclusion: StreamRL通过解耦和优化设计，显著提升了RL在LLM训练中的效率和可扩展性。

Abstract: Reinforcement learning (RL) has become the core post-training technique for
large language models (LLMs). RL for LLMs involves two stages: generation and
training. The LLM first generates samples online, which are then used to derive
rewards for training. The conventional view holds that the colocated
architecture, where the two stages share resources via temporal multiplexing,
outperforms the disaggregated architecture, in which dedicated resources are
assigned to each stage. However, in real-world deployments, we observe that the
colocated architecture suffers from resource coupling, where the two stages are
constrained to use the same resources. This coupling compromises the
scalability and cost-efficiency of colocated RL in large-scale training. In
contrast, the disaggregated architecture allows for flexible resource
allocation, supports heterogeneous training setups, and facilitates
cross-datacenter deployment.
  StreamRL is designed with disaggregation from first principles and fully
unlocks its potential by addressing two types of performance bottlenecks in
existing disaggregated RL frameworks: pipeline bubbles, caused by stage
dependencies, and skewness bubbles, resulting from long-tail output length
distributions. To address pipeline bubbles, StreamRL breaks the traditional
stage boundary in synchronous RL algorithms through stream generation and
achieves full overlapping in asynchronous RL. To address skewness bubbles,
StreamRL employs an output-length ranker model to identify long-tail samples
and reduces generation time via skewness-aware dispatching and scheduling.
Experiments show that StreamRL improves throughput by up to 2.66x compared to
existing state-of-the-art systems, and improves cost-effectiveness by up to
1.33x in a heterogeneous, cross-datacenter setting.

</details>

### [64] [Universal Approximation with Softmax Attention](https://arxiv.org/abs/2504.15956)
*Jerry Yao-Chieh Hu,Hude Liu,Hong-Yu Chen,Weimin Wu,Han Liu*

Main category: cs.LG

TLDR: 论文证明了两层自注意力和一层自注意力加softmax函数在紧凑域上对连续序列到序列函数具有通用逼近能力。


<details>
  <summary>Details</summary>
Motivation: 探索自注意力机制在序列到序列任务中的通用逼近能力，揭示其内部机制。

Method: 提出基于插值的新方法分析注意力机制，证明自注意力可逼近广义ReLU。

Result: 两层多头自注意力即可实现通用逼近，无需依赖前馈网络。

Conclusion: 自注意力机制具有强大的逼近能力，可独立应用于统计模型逼近。

Abstract: We prove that with linear transformations, both (i) two-layer self-attention
and (ii) one-layer self-attention followed by a softmax function are universal
approximators for continuous sequence-to-sequence functions on compact domains.
Our main technique is a new interpolation-based method for analyzing
attention's internal mechanism. This leads to our key insight: self-attention
is able to approximate a generalized version of ReLU to arbitrary precision,
and hence subsumes many known universal approximators. Building on these, we
show that two-layer multi-head attention alone suffices as a
sequence-to-sequence universal approximator. In contrast, prior works rely on
feed-forward networks to establish universal approximation in Transformers.
Furthermore, we extend our techniques to show that, (softmax-)attention-only
layers are capable of approximating various statistical models in-context. We
believe these techniques hold independent interest.

</details>

### [65] [OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning](https://arxiv.org/abs/2504.15995)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Jin-Hee Cho*

Main category: cs.LG

TLDR: OPUS-VFL提出了一种新型的垂直联邦学习框架，通过隐私感知的激励机制和自适应差分隐私机制，优化隐私与效用的权衡，显著提升了效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VFL系统缺乏有效的激励机制，难以平衡隐私与效用的权衡，且无法适应异构资源能力的客户，限制了实际部署。

Method: OPUS-VFL引入隐私感知激励机制，基于模型贡献、隐私保护和资源投入的组合奖励客户；采用轻量级留一策略量化特征重要性，并集成自适应差分隐私机制动态调整噪声水平。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上，OPUS-VFL显著优于现有VFL基线，标签推断攻击成功率降低20%，特征推断重建误差增加30%，客户激励提升25%。

Conclusion: OPUS-VFL是一种安全、公平且性能驱动的解决方案，适用于实际VFL场景。

Abstract: Vertical Federated Learning (VFL) enables organizations with disjoint feature
spaces but shared user bases to collaboratively train models without sharing
raw data. However, existing VFL systems face critical limitations: they often
lack effective incentive mechanisms, struggle to balance privacy-utility
tradeoffs, and fail to accommodate clients with heterogeneous resource
capabilities. These challenges hinder meaningful participation, degrade model
performance, and limit practical deployment. To address these issues, we
propose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.
OPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards
clients based on a principled combination of model contribution, privacy
preservation, and resource investment. It employs a lightweight leave-one-out
(LOO) strategy to quantify feature importance per client, and integrates an
adaptive differential privacy mechanism that enables clients to dynamically
calibrate noise levels to optimize their individual utility. Our framework is
designed to be scalable, budget-balanced, and robust to inference and poisoning
attacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and
CIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art
VFL baselines in both efficiency and robustness. It reduces label inference
attack success rates by up to 20%, increases feature inference reconstruction
error (MSE) by over 30%, and achieves up to 25% higher incentives for clients
that contribute meaningfully while respecting privacy and cost constraints.
These results highlight the practicality and innovation of OPUS-VFL as a
secure, fair, and performance-driven solution for real-world VFL.

</details>

### [66] [AlphaGrad: Non-Linear Gradient Normalization Optimizer](https://arxiv.org/abs/2504.16020)
*Soham Sane*

Main category: cs.LG

TLDR: AlphaGrad是一种内存高效的优化器，通过梯度归一化和双曲正切变换解决自适应方法（如Adam）的内存和超参数复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 解决自适应优化器（如Adam）的内存开销和超参数复杂性问题，同时保持性能。

Method: 采用张量级L2梯度归一化和双曲正切变换（$g' = \tanh(\alpha \cdot \tilde{g})$），仅需一个陡度参数$\alpha$。

Result: 在DQN中表现不稳定，但在TD3中表现稳定（需调参），在PPO中性能显著优于Adam。

Conclusion: AlphaGrad是内存受限场景下的有力替代方案，特别适用于策略学习，其稳定性和效率优势显著。

Abstract: We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer
addressing the memory overhead and hyperparameter complexity of adaptive
methods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2
gradient normalization followed by a smooth hyperbolic tangent transformation,
$g' = \tanh(\alpha \cdot \tilde{g})$, controlled by a single steepness
parameter $\alpha$. Our contributions include: (1) the AlphaGrad algorithm
formulation; (2) a formal non-convex convergence analysis guaranteeing
stationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,
TD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent
performance profile. While exhibiting instability in off-policy DQN, it
provides enhanced training stability with competitive results in TD3 (requiring
careful $\alpha$ tuning) and achieves substantially superior performance in
on-policy PPO. These results underscore the critical importance of empirical
$\alpha$ selection, revealing strong interactions between the optimizer's
dynamics and the underlying RL algorithm. AlphaGrad presents a compelling
alternative optimizer for memory-constrained scenarios and shows significant
promise for on-policy learning regimes where its stability and efficiency
advantages can be particularly impactful.

</details>

### [67] [LLMs meet Federated Learning for Scalable and Secure IoT Management](https://arxiv.org/abs/2504.16032)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.LG

TLDR: 提出了一种基于联邦学习的大语言模型框架（FL-LLM），用于提升物联网系统的智能性，同时确保数据隐私和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统集中式架构在物联网大规模部署中存在延迟、隐私和资源消耗问题，需要更高效的解决方案。

Method: 结合生成式物联网模型（GIoT）和梯度感知联邦策略（GSFS），采用混合边缘-云处理架构动态优化模型更新。

Result: 在IoT-23数据集上验证，该框架提高了模型准确性、降低了响应延迟并提升了能源效率，优于传统联邦学习方法。

Conclusion: FL-LLM框架为大规模物联网生态系统提供了更安全、可扩展和自适应的管理方案。

Abstract: The rapid expansion of IoT ecosystems introduces severe challenges in
scalability, security, and real-time decision-making. Traditional centralized
architectures struggle with latency, privacy concerns, and excessive resource
consumption, making them unsuitable for modern large-scale IoT deployments.
This paper presents a novel Federated Learning-driven Large Language Model
(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring
data privacy and computational efficiency. The framework integrates Generative
IoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),
dynamically optimizing model updates based on real-time network conditions. By
leveraging a hybrid edge-cloud processing architecture, our approach balances
intelligence, scalability, and security in distributed IoT environments.
Evaluations on the IoT-23 dataset demonstrate that our framework improves model
accuracy, reduces response latency, and enhances energy efficiency,
outperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings
highlight the potential of integrating LLM-powered federated learning into
large-scale IoT ecosystems, paving the way for more secure, scalable, and
adaptive IoT management solutions.

</details>

### [68] [Muon Optimizer Accelerates Grokking](https://arxiv.org/abs/2504.16041)
*Amund Tveit,Bjørn Remseth,Arve Skogvold*

Main category: cs.LG

TLDR: 研究不同优化器对模型延迟泛化现象（grokking）的影响，发现Muon优化器显著加速泛化。


<details>
  <summary>Details</summary>
Motivation: 探讨优化器选择如何影响模型从记忆到泛化的过渡。

Method: 在七个数值任务上实验，比较Muon和AdamW优化器及不同softmax激活函数的效果。

Result: Muon优化器将平均泛化时间从153.09缩短至102.89，效果显著。

Conclusion: 优化器选择对模型的泛化能力有重要影响。

Abstract: This paper investigates the impact of different optimizers on the grokking
phenomenon, where models exhibit delayed generalization. We conducted
experiments across seven numerical tasks (primarily modular arithmetic) using a
modern Transformer architecture. The experimental configuration systematically
varied the optimizer (Muon vs. AdamW) and the softmax activation function
(standard softmax, stablemax, and sparsemax) to assess their combined effect on
learning dynamics. Our empirical evaluation reveals that the Muon optimizer,
characterized by its use of spectral norm constraints and second-order
information, significantly accelerates the onset of grokking compared to the
widely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch
from 153.09 to 102.89 across all configurations, a statistically significant
difference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice
plays a crucial role in facilitating the transition from memorization to
generalization.

</details>

### [69] [$π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization](https://arxiv.org/abs/2504.16054)
*Physical Intelligence,Kevin Black,Noah Brown,James Darpinian,Karan Dhabalia,Danny Driess,Adnan Esmail,Michael Equi,Chelsea Finn,Niccolo Fusai,Manuel Y. Galliker,Dibya Ghosh,Lachy Groom,Karol Hausman,Brian Ichter,Szymon Jakubczak,Tim Jones,Liyiming Ke,Devin LeBlanc,Sergey Levine,Adrian Li-Bell,Mohith Mothukuri,Suraj Nair,Karl Pertsch,Allen Z. Ren,Lucy Xiaoyang Shi,Laura Smith,Jost Tobias Springenberg,Kyle Stachowicz,James Tanner,Quan Vuong,Homer Walke,Anna Walling,Haohuan Wang,Lili Yu,Ury Zhilinsky*

Main category: cs.LG

TLDR: 论文提出了一种基于π₀的改进模型π₀.₅，通过异构任务协同训练实现广泛泛化，用于机器人操控任务。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉-语言-动作（VLA）模型在真实世界中泛化能力不足的问题。

Method: 利用多机器人数据、高层语义预测和网络数据等，结合协同训练和多模态示例（图像、语言指令、目标检测等）。

Result: 实验表明，该方法能实现长期和灵巧的操控任务（如清洁厨房或卧室），并在新环境中有效泛化。

Conclusion: 异构任务协同训练和多模态数据是实现机器人广泛泛化的关键。

Abstract: In order for robots to be useful, they must perform practically relevant
tasks in the real world, outside of the lab. While vision-language-action (VLA)
models have demonstrated impressive results for end-to-end robot control, it
remains an open question how far such models can generalize in the wild. We
describe $\pi_{0.5}$, a new model based on $\pi_{0}$ that uses co-training on
heterogeneous tasks to enable broad generalization. $\pi_{0.5}$\ uses data from
multiple robots, high-level semantic prediction, web data, and other sources to
enable broadly generalizable real-world robotic manipulation. Our system uses a
combination of co-training and hybrid multi-modal examples that combine image
observations, language commands, object detections, semantic subtask
prediction, and low-level actions. Our experiments show that this kind of
knowledge transfer is essential for effective generalization, and we
demonstrate for the first time that an end-to-end learning-enabled robotic
system can perform long-horizon and dexterous manipulation skills, such as
cleaning a kitchen or bedroom, in entirely new homes.

</details>

### [70] [LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities](https://arxiv.org/abs/2504.16078)
*Thomas Schmied,Jörg Bornschein,Jordi Grau-Moya,Markus Wulfmeier,Razvan Pascanu*

Main category: cs.LG

TLDR: LLMs在决策场景中存在探索不足和知行差距问题，通过RL微调自我生成的CoT推理可提升其决策能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在决策任务中表现不佳的原因，并提出改进方法。

Method: 通过RL微调LLMs的自我生成CoT推理，并测试经典探索机制和LLM特定方法。

Result: RL微调显著提升了LLMs的探索能力和决策表现。

Conclusion: RL微调是解决LLMs决策问题的有效方法，未来可进一步优化探索机制。

Abstract: The success of Large Language Models (LLMs) has sparked interest in various
agentic applications. A key hypothesis is that LLMs, leveraging common sense
and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently
solve complex domains. However, LLM agents have been found to suffer from
sub-optimal exploration and the knowing-doing gap, the inability to effectively
act on knowledge present in the model. In this work, we systematically study
why LLMs perform sub-optimally in decision-making scenarios. In particular, we
closely examine three prevalent failure modes: greediness, frequency bias, and
the knowing-doing gap. We propose mitigation of these shortcomings by
fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.
Our experiments across multi-armed bandits, contextual bandits, and
Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making
abilities of LLMs by increasing exploration and narrowing the knowing-doing
gap. Finally, we study both classic exploration mechanisms, such as
$\epsilon$-greedy, and LLM-specific approaches, such as self-correction and
self-consistency, to enable more effective fine-tuning of LLMs for
decision-making.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)](https://arxiv.org/abs/2504.15349)
*William Bruns*

Main category: cs.CL

TLDR: 论文通过RASP语言证明Transformer编码器-解码器可以在ReCOGS_pos任务上实现100%语义匹配，表明任务无需层次化解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在组合泛化任务（如COGS基准）中的表现，尤其是其在结构泛化上的0%准确率问题。

Method: 使用RASP编程语言构建Transformer等效模型，采用19个注意力头兼容的扁平模式匹配规则，结合介词短语和补语处理逻辑。

Result: 模型在ReCOGS测试集上实现100%语义匹配，除obj_pp_to_subj_pp外所有泛化分项均达100%。

Conclusion: ReCOGS_pos任务可通过非层次化方法解决，证明了Transformer在组合泛化中的潜力。

Abstract: Humans understand new combinations of words encountered if they are
combinations of words recognized from different contexts, an ability called
Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020)
arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural
generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted
Access Sequence Processing (RASP), a Transformer-equivalent programming
language, to prove by construction that a Transformer encoder-decoder can
perform the semantically equivalent ReCOGS_pos (Wu et al., 2024)
arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP
model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on
all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,
our RASP model shows the ReCOGS_pos task does not require a hierarchical or
tree-structured solution: we use word-level tokens with an "embedding" layer
that tags with possible parts of speech, applying just once per encoder pass 19
attention-head compatible flat pattern-matching rules, shown using grammar
coverage (Zeller et al., 2023) to be learnable from the training data, plus
general prepositional phrase (pp) handling and sentential complement (cp)
handling logic, and output the next logical form (LF) token (repeating until
the LF is complete). The model does not apply recursive, tree-structured rules
like 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact
match on pp recursion, cp recursion using the decoder loop.

</details>

### [72] [Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection](https://arxiv.org/abs/2504.15392)
*Myrthe Reuver,Indira Sen,Matteo Melis,Gabriella Lapesa*

Main category: cs.CL

TLDR: 论文研究了性别歧视研究者与大型语言模型（LLM）的协作，通过四步流程评估LLM在性别歧视研究中的适用性，并比较专家、LLM及合作定义在分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在性别歧视研究中的潜力，以及专家与LLM协作是否能提升分类性能。

Method: 1. 专家回答问题；2. 参与两个交互实验（评估LLM知识、创建定义）；3. 零样本分类实验评估三种定义。

Result: LLM生成的定义更长且复杂；专家定义平均表现较差，但部分合作定义提升了分类性能。

Conclusion: LLM在性别歧视研究中具有潜力，专家与LLM协作可能改善分类效果。

Abstract: This paper investigates hybrid intelligence and collaboration between
researchers of sexism and Large Language Models (LLMs), with a four-component
pipeline. First, nine sexism researchers answer questions about their knowledge
of sexism and of LLMs. They then participate in two interactive experiments
involving an LLM (GPT3.5). The first experiment has experts assessing the
model's knowledge about sexism and suitability for use in research. The second
experiment tasks them with creating three different definitions of sexism: an
expert-written definition, an LLM-written one, and a co-created definition.
Lastly, zero-shot classification experiments use the three definitions from
each expert in a prompt template for sexism detection, evaluating GPT4o on
2.500 texts sampled from five sexism benchmarks. We then analyze the resulting
67.500 classification decisions. The LLM interactions lead to longer and more
complex definitions of sexism. Expert-written definitions on average perform
poorly compared to LLM-generated definitions. However, some experts do improve
classification performance with their co-created definitions of sexism, also
experts who are inexperienced in using LLMs.

</details>

### [73] [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)
*Sungjun Han,Juyoung Suk,Suyeong An,Hyungguk Kim,Kyuseok Kim,Wonsuk Yang,Seungtaek Choi,Jamin Shin*

Main category: cs.CL

TLDR: Trillion-7B是一种高效的韩语为中心的多语言大语言模型，通过XLDA机制实现英语到目标语言的知识迁移，仅用10%的多语言数据和59.4K GPU小时即达到竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效的韩语为中心的多语言模型，通过优化数据混合和知识迁移机制，减少训练成本。

Method: 采用Cross-lingual Document Attention (XLDA)机制，结合优化的数据混合、语言特定过滤和定制分词器。

Result: 在27个基准测试中表现优异，展示了强大的多语言性能和跨语言一致性。

Conclusion: Trillion-7B是一种高效且成本低廉的多语言模型，适用于韩语及其他目标语言。

Abstract: We introduce Trillion-7B, the most token-efficient Korean-centric
multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)
mechanism enables highly efficient and effective knowledge transfer from
English to target languages like Korean and Japanese. Combined with optimized
data mixtures, language-specific filtering, and tailored tokenizer
construction, Trillion-7B achieves competitive performance while dedicating
only 10\% of its 2T training tokens to multilingual data and requiring just
59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations
across 27 benchmarks in four languages demonstrate Trillion-7B's robust
multilingual performance and exceptional cross-lingual consistency.

</details>

### [74] [Feeding LLM Annotations to BERT Classifiers at Your Own Risk](https://arxiv.org/abs/2504.15432)
*Yucheng Lu,Kazimier Smith*

Main category: cs.CL

TLDR: 研究表明，使用LLM生成的标签微调小型编码器模型在文本分类中存在性能下降和不稳定性，需谨慎应用于高风险任务。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成标签对小型模型性能的影响，揭示其在真实应用中的潜在风险。

Method: 通过实证分析比较使用LLM标签与黄金标签训练的模型，观察性能差异和稳定性问题。

Result: 发现LLM标签导致准确性、F1分数下降，训练不稳定，性能过早停滞。

Conclusion: 建议在高风险任务中谨慎使用LLM标签，并提出部分缓解策略，但问题未完全解决。

Abstract: Using LLM-generated labels to fine-tune smaller encoder-only models for text
classification has gained popularity in various settings. While this approach
may be justified in simple and low-stakes applications, we conduct empirical
analysis to demonstrate how the perennial curse of training on synthetic data
manifests itself in this specific setup. Compared to models trained on gold
labels, we observe not only the expected performance degradation in accuracy
and F1 score, but also increased instability across training runs and premature
performance plateaus. These findings cast doubts on the reliability of such
approaches in real-world applications. We contextualize the observed phenomena
through the lens of error propagation and offer several practical mitigation
strategies, including entropy-based filtering and ensemble techniques. Although
these heuristics offer partial relief, they do not fully resolve the inherent
risks of propagating non-random errors from LLM annotations to smaller
classifiers, underscoring the need for caution when applying this workflow in
high-stakes text classification tasks.

</details>

### [75] [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
*Tyler A. Chang,Benjamin K. Bergen*

Main category: cs.CL

TLDR: 论文研究了Transformer语言模型中激活向量从当前标记嵌入到下一个标记预测的转换过程，通过识别二元预测子网络，发现这些子网络对模型性能至关重要且集中在第一层MLP。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型中激活向量如何从当前标记转换为下一个标记预测，并寻找最小的转换形式。

Method: 识别并分析语言模型中的二元预测子网络，研究其在完全训练模型中的分布和作用。

Result: 发现二元子网络在1B参数模型中存在且关键，集中在第一层MLP，并与最优剪枝子网络重叠。

Conclusion: 二元子网络是语言模型中基本预测的最小必要参数集，为研究模型电路提供了新方法。

Abstract: In Transformer language models, activation vectors transform from current
token embeddings to next token predictions as they pass through the model. To
isolate a minimal form of this transformation, we identify language model
subnetworks that make bigram predictions, naive next token predictions based
only on the current token. We find that bigram subnetworks can be found in
fully trained language models up to 1B parameters, and these subnetworks are
critical for model performance even when they consist of less than 0.2% of
model parameters. Bigram subnetworks are concentrated in the first Transformer
MLP layer, and they overlap significantly with subnetworks trained to optimally
prune a given model. Mechanistically, the bigram subnetworks often recreate a
pattern from the full models where the first layer induces a sharp change that
aligns activations with next token predictions rather than current token
representations. Our results demonstrate that bigram subnetworks comprise a
minimal subset of parameters that are both necessary and sufficient for basic
next token predictions in language models, and they help drive the
transformation from current to next token activations in the residual stream.
These subnetworks can lay a foundation for studying language model circuits by
building up from a minimal circuit rather than the traditional approach of
ablating circuits from a full model.

</details>

### [76] [Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)
*Szymon Kobus,Deniz Gündüz*

Main category: cs.CL

TLDR: 论文通过建立推测解码与信道模拟的联系，提出了信息论分析框架，推导了生成加速与草稿模型生成令牌数的关系，并提出了一种新的ERSD方法。


<details>
  <summary>Details</summary>
Motivation: 探讨推测解码与信道模拟的关联，以信息论角度分析推测解码的加速潜力。

Method: 提出信息论分析框架，推导生成加速与令牌数的关系，并设计ERSD方法。

Result: 推导出生成加速与令牌数的显式关系，并提出性能优异的ERSD方法。

Conclusion: 论文通过理论分析和新方法，为推测解码提供了更深入的理解和实际应用方案。

Abstract: Speculative decoding accelerates large language model inference using a
smaller draft model. In this paper, we establish a surprising connection
between speculative decoding and channel simulation, which aims at simulating a
noisy channel using as few bits as possible. This connection allows us to
provide an information-theoretic analysis of the speed up that can be achieved
by speculative decoding. Leveraging this link, we derive an explicit relation
between generation speed-up and the number of tokens $k$ generated by the draft
model for large $k$, which serves as an upper bound for all $k$. We also
propose a novel speculative decoding method via exponential race ERSD that
matches state-of-the-art performance.

</details>

### [77] [SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation](https://arxiv.org/abs/2504.15509)
*Keqi Deng,Wenxi Chen,Xie Chen,Philip C. Woodland*

Main category: cs.CL

TLDR: SimulS2S-LLM是一种基于大型语言模型（LLM）的同步语音翻译方法，通过离线训练和测试时策略优化，实现低延迟高质量的语音翻译。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理流式语音输入时面临挑战，因为语音通常作为整个生成过程的提示，导致训练与推理不匹配。

Method: 提出SimulS2S-LLM，通过提取边界感知的语音提示，结合增量束搜索预测离散语音标记，并使用预训练声码器合成输出语音。

Result: 在CVSS语音数据上，SimulS2S-LLM在相同延迟下比现有方法提高了3点ASR-BLEU分数。

Conclusion: SimulS2S-LLM有效解决了流式语音翻译中的训练与推理不匹配问题，实现了更好的质量-延迟权衡。

Abstract: Simultaneous speech translation (SST) outputs translations in parallel with
streaming speech input, balancing translation quality and latency. While large
language models (LLMs) have been extended to handle the speech modality,
streaming remains challenging as speech is prepended as a prompt for the entire
generation process. To unlock LLM streaming capability, this paper proposes
SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy
to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between
training and inference by extracting boundary-aware speech prompts that allows
it to be better matched with text input data. SimulS2S-LLM achieves
simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete
output speech tokens and then synthesising output speech using a pre-trained
vocoder. An incremental beam search is designed to expand the search space of
speech token prediction without increasing latency. Experiments on the CVSS
speech data show that SimulS2S-LLM offers a better translation quality-latency
trade-off than existing methods that use the same training data, such as
improving ASR-BLEU scores by 3 points at similar latency.

</details>

### [78] [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)
*Minghao Wu,Weixuan Wang,Sinuo Liu,Huifeng Yin,Xintong Wang,Yu Zhao,Chenyang Lyu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TLDR: 本文分析了2000多个多语言基准测试，发现英语仍占主导地位，且翻译基准与本地化基准存在显著差异，强调需开发文化适配的基准。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，多语言评估的公平性成为关键问题，本文旨在揭示当前多语言基准测试的不足并提出改进方向。

Method: 研究分析了2021至2024年间148个国家的2000多个多语言基准测试，比较了其与人类评估的相关性。

Result: 英语在多语言基准中占比过高，本地化基准比翻译基准更贴合人类评估（0.68 vs. 0.47），STEM任务相关性高（0.70-0.85），传统NLP任务低（0.11-0.30）。

Conclusion: 需开发文化适配的基准测试，提出六项改进原则和五项研究方向，呼吁全球合作推动多语言评估发展。

Abstract: As large language models (LLMs) continue to advance in linguistic
capabilities, robust multilingual evaluation has become essential for promoting
equitable technological progress. This position paper examines over 2,000
multilingual (non-English) benchmarks from 148 countries, published between
2021 and 2024, to evaluate past, present, and future practices in multilingual
benchmarking. Our findings reveal that, despite significant investments
amounting to tens of millions of dollars, English remains significantly
overrepresented in these benchmarks. Additionally, most benchmarks rely on
original language content rather than translations, with the majority sourced
from high-resource countries such as China, India, Germany, the UK, and the
USA. Furthermore, a comparison of benchmark performance with human judgments
highlights notable disparities. STEM-related tasks exhibit strong correlations
with human evaluations (0.70 to 0.85), while traditional NLP tasks like
question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).
Moreover, translating English benchmarks into other languages proves
insufficient, as localized benchmarks demonstrate significantly higher
alignment with local human judgments (0.68) than their translated counterparts
(0.47). This underscores the importance of creating culturally and
linguistically tailored benchmarks rather than relying solely on translations.
Through this comprehensive analysis, we highlight six key limitations in
current multilingual evaluation practices, propose the guiding principles
accordingly for effective multilingual benchmarking, and outline five critical
research directions to drive progress in the field. Finally, we call for a
global collaborative effort to develop human-aligned benchmarks that prioritize
real-world applications.

</details>

### [79] [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
*Qiyao Wang,Guhong Chen,Hongbo Wang,Huaren Liu,Minghui Zhu,Zhifei Qin,Linwei Li,Yilin Yue,Shiqiang Wang,Jiayan Li,Yihang Wu,Ziqiang Liu,Longze Chen,Run Luo,Liyang Fan,Jiaming Li,Lei Zhang,Kan Xu,Hongfei Lin,Hamid Alinejad-Rokny,Shiwen Ni,Yuan Lin,Min Yang*

Main category: cs.CL

TLDR: 论文提出了首个全面的IP任务分类法和双语基准IPBench，用于评估LLMs在知识产权领域的实际应用表现，发现现有模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 知识产权领域复杂且知识密集，现有数据集和基准未能全面覆盖实际需求，亟需一个更全面的评估工具。

Method: 引入IPBench，涵盖8种IP机制和20项任务，对16种LLMs进行评估。

Result: 最佳模型准确率仅为75.8%，开源模型表现落后于闭源通用模型。

Conclusion: IPBench填补了现有空白，未来将持续更新以更好地反映实际挑战。

Abstract: Intellectual Property (IP) is a unique domain that integrates technical and
legal knowledge, making it inherently complex and knowledge-intensive. As large
language models (LLMs) continue to advance, they show great potential for
processing IP tasks, enabling more efficient analysis, understanding, and
generation of IP-related content. However, existing datasets and benchmarks
either focus narrowly on patents or cover limited aspects of the IP field,
lacking alignment with real-world scenarios. To bridge this gap, we introduce
the first comprehensive IP task taxonomy and a large, diverse bilingual
benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is
designed to evaluate LLMs in real-world intellectual property applications,
encompassing both understanding and generation. We benchmark 16 LLMs, ranging
from general-purpose to domain-specific models, and find that even the
best-performing model achieves only 75.8% accuracy, revealing substantial room
for improvement. Notably, open-source IP and law-oriented models lag behind
closed-source general-purpose models. We publicly release all data and code of
IPBench and will continue to update it with additional IP-related tasks to
better reflect real-world challenges in the intellectual property domain.

</details>

### [80] [Compass-V2 Technical Report](https://arxiv.org/abs/2504.15527)
*Sophia Maria*

Main category: cs.CL

TLDR: Compass-v2是一个轻量级的Mixture-of-Experts模型，专为东南亚语言和电子商务应用设计，通过高质量数据集和混合推理模型实现高性能和低成本。


<details>
  <summary>Details</summary>
Motivation: 解决主流LLMs对低资源语言（尤其是东南亚语言）和电子商务领域的忽视问题。

Method: 设计30B总参数和5B活跃参数的MoE模型，结合细粒度与共享专家模块，构建高质量数据集，并开发混合推理模型。

Result: 在30B以下模型中，Compass-v2在东南亚多语言和电子商务任务中表现最佳，且推理成本显著降低。

Conclusion: Compass-v2成功填补了低资源语言和电子商务领域的空白，同时平衡了性能与成本。

Abstract: Predominant LLMs focus on high-resource languages while leaving low-resource
languages, particularly those in Southeast Asia (SEA), underrepresented. In
addition, those models are general-purpose and pay limited attention to the
e-commerce domain. To overcome these limitations, we introduce Compass-v2, a
lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast
Asian languages and e-commerce applications. To balance model performance and
inference cost, the model is designed with 30B total parameters and 5B active
parameters, incorporating both fine-grained and shared expert modules. To
enhance multilingual performance, we curated and constructed a high-quality,
industry-leading SEA dataset, to the best of our knowledge. To boost
performance in the e-commerce domain, we built a dataset comprising hundreds of
billions of tokens, sourced through external data mining and internal platform
collection. Besides, we pioneered a hybrid reasoning model that supports both
fast thinking and deep thinking within a unified framework to enhance the
reasoning capabilities, diverging from the conventional industry practice of
deploying two separate models. Through extensive experimental evaluations, our
model demonstrates state-of-the-art SEA multilingual and e-commerce performance
among sub-30B models, while maintaining significantly lower inference cost.

</details>

### [81] [llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length](https://arxiv.org/abs/2504.15544)
*Issa Sugiura,Kouta Nakayama,Yusuke Oda*

Main category: cs.CL

TLDR: 论文介绍了llm-jp-modernbert，一种基于大规模日语语料库训练的ModernBERT模型，支持8192个token的长上下文。尽管在下游任务中未超越现有基线，但在填充掩码测试中表现良好，并分析了上下文长度扩展的影响。


<details>
  <summary>Details</summary>
Motivation: 探索在大规模语料库和长上下文条件下预训练编码器模型（如BERT）的效果，填补与解码器模型相比的研究空白。

Method: 使用公开的大规模日语语料库训练ModernBERT模型，支持8192个token的上下文长度，并通过填充掩码测试和伪困惑度实验分析模型性能。

Result: 模型在填充掩码测试中表现良好，但未超越现有下游任务基线；上下文长度扩展的分析显示了其影响。

Conclusion: 研究支持长上下文BERT的发展，并公开了模型和代码以促进可重复性和进一步研究。

Abstract: Encoder-only transformer models like BERT are widely adopted as a pre-trained
backbone for tasks like sentence classification and retrieval. However,
pretraining of encoder models with large-scale corpora and long contexts has
been relatively underexplored compared to decoder-only transformers. In this
work, we present llm-jp-modernbert, a ModernBERT model trained on a publicly
available, massive Japanese corpus with a context length of 8192 tokens. While
our model does not surpass existing baselines on downstream tasks, it achieves
good results on fill-mask test evaluations. We also analyze the effect of
context length expansion through pseudo-perplexity experiments. Furthermore, we
investigate sentence embeddings in detail, analyzing their transitions during
training and comparing them with those from other existing models, confirming
similar trends with models sharing the same architecture. To support
reproducibility and foster the development of long-context BERT, we release our
model, along with the training and evaluation code.

</details>

### [82] [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548)
*Elyas Meguellati,Assaad Zeghina,Shazia Sadiq,Gianluca Demartini*

Main category: cs.CL

TLDR: 论文提出了一种利用LLM进行文本预处理和语义增强的方法，以提升复杂社交媒体分类任务的性能，效果接近人工标注数据，但成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM生成合成数据，忽视了其在文本预处理和语义增强中的潜力，尤其是在复杂社交媒体任务中LLM的零样本分类表现不佳。

Method: 通过提示LLM清理噪声文本并提供上下文丰富的解释，增强训练集，无需大幅增加数据量。在多个数据集上系统评估。

Result: 零样本LLM分类在高上下文任务中表现不如监督模型，但LLM语义增强的效果接近人工标注数据，成本更低。

Conclusion: 策略性地将LLM整合到机器学习流程中，对社交媒体分类任务具有重要意义，为在线有害内容治理提供了新思路。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
performance on simple text classification tasks, frequently under zero-shot
settings. However, their efficacy declines when tackling complex social media
challenges such as propaganda detection, hateful meme classification, and
toxicity identification. Much of the existing work has focused on using LLMs to
generate synthetic training data, overlooking the potential of LLM-based text
preprocessing and semantic augmentation. In this paper, we introduce an
approach that prompts LLMs to clean noisy text and provide context-rich
explanations, thereby enhancing training sets without substantial increases in
data volume. We systematically evaluate on the SemEval 2024 multi-label
Persuasive Meme dataset and further validate on the Google Jigsaw toxic
comments and Facebook hateful memes datasets to assess generalizability. Our
results reveal that zero-shot LLM classification underperforms on these
high-context tasks compared to supervised models. In contrast, integrating
LLM-based semantic augmentation yields performance on par with approaches that
rely on human-annotated data, at a fraction of the cost. These findings
underscore the importance of strategically incorporating LLMs into machine
learning (ML) pipeline for social media classification tasks, offering broad
implications for combating harmful content online.

</details>

### [83] [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
*Yuxin Jiang,Yufei Wang,Chuhan Wu,Xinyi Dai,Yan Xu,Weinan Gan,Yasheng Wang,Xin Jiang,Lifeng Shang,Ruiming Tang,Wei Wang*

Main category: cs.CL

TLDR: WebR是一种自动化框架，直接从原始网页文档合成高质量的指令调优数据，减少对种子数据或网页结构假设的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有自动数据合成方法依赖种子数据质量或对网页结构的强假设，限制了指令调优数据的多样性和质量。

Method: 提出WebR框架，通过双视角范式（网页作为指令或响应）从原始网页文档合成数据。

Result: WebR生成的数据在四个指令跟随基准测试中表现优于现有方法，最高提升16.65%。

Conclusion: WebR具有兼容性、数据效率和可扩展性优势，能轻松适应不同领域。

Abstract: The improvement of LLMs' instruction-following capabilities depends
critically on the availability of high-quality instruction-response pairs.
While existing automatic data synthetic methods alleviate the burden of manual
curation, they often rely heavily on either the quality of seed data or strong
assumptions about the structure and content of web documents. To tackle these
challenges, we propose Web Reconstruction (WebR), a fully automated framework
for synthesizing high-quality instruction-tuning (IT) data directly from raw
web documents with minimal assumptions. Leveraging the inherent diversity of
raw web content, we conceptualize web reconstruction as an instruction-tuning
data synthesis task via a novel dual-perspective paradigm--Web as Instruction
and Web as Response--where each web document is designated as either an
instruction or a response to trigger the reconstruction process. Comprehensive
experiments show that datasets generated by WebR outperform state-of-the-art
baselines by up to 16.65% across four instruction-following benchmarks.
Notably, WebR demonstrates superior compatibility, data efficiency, and
scalability, enabling enhanced domain adaptation with minimal effort. The data
and code are publicly available at https://github.com/YJiangcm/WebR.

</details>

### [84] [Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models](https://arxiv.org/abs/2504.15604)
*Pavan Yadav,Nikhil Khandalkar,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.CL

TLDR: 该研究比较了GPT-2和Llama-2-7b-chat-hf在心理理论任务中的表现，发现Llama-2在低温度设置下表现更优，但随着上下文复杂性和推理难度的增加，模型预测准确性下降。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在心理理论任务中的表现，分析上下文复杂性和推理难度对模型预测的影响。

Method: 使用GPT-4增强的短故事数据集，测试两种模型在不同温度和推理级别下的表现。

Result: Llama-2在低温度下表现更优，但上下文复杂性增加会降低预测准确性。模型在高阶推理任务中表现差异显著。

Conclusion: 模型架构、温度和上下文复杂性显著影响预测性能，揭示了当前语言模型的优势和局限性。

Abstract: Language models have made significant progress in generating coherent text
and predicting next tokens based on input prompts. This study compares the
next-token prediction performance of two well-known models: OpenAI's GPT-2 and
Meta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their
capabilities, we built a dataset from 10 short stories sourced from the Explore
ToM Dataset. We enhanced these stories by programmatically inserting additional
sentences (infills) using GPT-4, creating variations that introduce different
levels of contextual complexity. This setup enables analysis of how increasing
context affects model performance. We tested both models under four temperature
settings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next
token across three reasoning levels. Zero-order reasoning involves tracking the
state, either current (ground truth) or past (memory). First-order reasoning
concerns understanding another's mental state (e.g., "Does Anne know the apple
is salted?"). Second-order reasoning adds recursion (e.g., "Does Anne think
that Charles knows the apple is salted?").
  Our results show that adding more infill sentences slightly reduces
prediction accuracy, as added context increases complexity and ambiguity.
Llama-2 consistently outperforms GPT-2 in prediction accuracy, especially at
lower temperatures, demonstrating greater confidence in selecting the most
probable token. As reasoning complexity rises, model responses diverge more.
Notably, GPT-2 and Llama-2 display greater variability in predictions during
first- and second-order reasoning tasks. These findings illustrate how model
architecture, temperature, and contextual complexity influence next-token
prediction, contributing to a better understanding of the strengths and
limitations of current language models.

</details>

### [85] [Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement](https://arxiv.org/abs/2504.15630)
*Xiaowei Yuan,Zhao Yang,Ziyang Huang,Yequan Wang,Siqi Fan,Yiming Ju,Jun Zhao,Kang Liu*

Main category: cs.CL

TLDR: 论文提出了一种名为CaLE的新方法，通过增强LLMs内部表示中的上下文知识利用，改进了上下文忠实生成的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注解码策略的改进，而忽略了LLMs内部状态中上下文信息处理的机制，导致LLMs无法充分利用上下文知识。

Method: CaLE通过V-usable信息分析，在最优层级上战略性地放大上下文信息的增长，从而丰富最终层的表示。

Result: 实验表明，CaLE在问答任务中有效提升了上下文忠实生成的能力，尤其是在涉及未知或冲突上下文知识的场景中。

Conclusion: CaLE通过优化LLMs内部表示中的上下文知识利用，显著提升了生成结果的上下文忠实性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, yet they often struggle with context-faithfulness generations
that properly reflect contextual knowledge. While existing approaches focus on
enhancing the decoding strategies, they ignore the fundamental mechanism of how
contextual information is processed within LLMs' internal states. As a result,
LLMs remain limited in their ability to fully leverage contextual knowledge. In
this paper, we propose Context-aware Layer Enhancement (CaLE), a novel
intervention method that enhances the utilization of contextual knowledge
within LLMs' internal representations. By employing V-usable information
analysis, CaLE strategically amplifies the growth of contextual information at
an optimal layer, thereby enriching representations in the final layer. Our
experiments demonstrate that CaLE effectively improves context-faithful
generation in Question-Answering tasks, particularly in scenarios involving
unknown or conflicting contextual knowledge.

</details>

### [86] [Cost-Effective Text Clustering with Large Language Models](https://arxiv.org/abs/2504.15640)
*Hongtao Wang,Taiyan Zhang,Renchi Yang,Jianliang Xu*

Main category: cs.CL

TLDR: 本文提出了一种名为TECL的成本效益框架，利用LLMs的反馈在有限查询预算内实现高精度文本聚类。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在文本聚类中提供了高精度的嵌入和相似性评分，但其计算和财务成本高昂，亟需一种成本效益高的解决方案。

Method: TECL通过EdgeLLM或TriangleLLM构建文本对的must-link/cannot-link约束，并利用这些约束作为监督信号输入加权约束聚类方法。

Result: 实验表明，TECL在相同查询成本下显著优于现有无监督文本聚类方法。

Conclusion: TECL是一种高效且成本可控的文本聚类框架，能够充分利用LLMs的优势。

Abstract: Text clustering aims to automatically partition a collection of text
documents into distinct clusters based on linguistic features. In the
literature, this task is usually framed as metric clustering based on text
embeddings from pre-trained encoders or a graph clustering problem upon
pairwise similarities from an oracle, e.g., a large ML model. Recently, large
language models (LLMs) bring significant advancement in this field by offering
contextualized text embeddings and highly accurate similarity scores, but
meanwhile, present grand challenges to cope with substantial computational
and/or financial overhead caused by numerous API-based queries or inference
calls to the models.
  In response, this paper proposes TECL, a cost-effective framework that taps
into the feedback from LLMs for accurate text clustering within a limited
budget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or
TriangleLLM to construct must-link/cannot-link constraints for text pairs, and
further leverages such constraints as supervision signals input to our weighted
constrained clustering approach to generate clusters. Particularly, EdgeLLM
(resp. TriangleLLM) enables the identification of informative text pairs (resp.
triplets) for querying LLMs via well-thought-out greedy algorithms and accurate
extraction of pairwise constraints through carefully-crafted prompting
techniques. Our experiments on multiple benchmark datasets exhibit that TECL
consistently and considerably outperforms existing solutions in unsupervised
text clustering under the same query cost for LLMs.

</details>

### [87] [Computational Typology](https://arxiv.org/abs/2504.15642)
*Gerhard Jäger*

Main category: cs.CL

TLDR: 计算统计模型在语言类型学研究中的应用及其优势。


<details>
  <summary>Details</summary>
Motivation: 探讨计算统计方法如何帮助分析大规模语言数据，验证语言结构和演化的假设。

Method: 利用计算统计建模分析语言类型学数据。

Result: 展示了计算统计模型在语言类型学研究中的实际效果。

Conclusion: 计算统计方法为语言类型学研究提供了新的工具和视角。

Abstract: Typology is a subfield of linguistics that focuses on the study and
classification of languages based on their structural features. Unlike
genealogical classification, which examines the historical relationships
between languages, typology seeks to understand the diversity of human
languages by identifying common properties and patterns, known as universals.
In recent years, computational methods have played an increasingly important
role in typological research, enabling the analysis of large-scale linguistic
data and the testing of hypotheses about language structure and evolution. This
article provides an illustration of the benefits of computational statistical
modeling in typology.

</details>

### [88] [FinTextSim: Enhancing Financial Text Analysis with BERTopic](https://arxiv.org/abs/2504.15683)
*Simon Jehnen,Joaquín Ordieres-Meré,Javier Villalba-Díez*

Main category: cs.CL

TLDR: 本研究探讨了BERTopic与FinTextSim结合在分析10-K文件中的有效性，FinTextSim显著提升了主题模型的性能。


<details>
  <summary>Details</summary>
Motivation: 信息可用性和计算能力的进步推动了财务文本分析的需求，传统方法与文本数据的结合需要更高效的工具。

Method: 使用BERTopic和FinTextSim（优化的句子转换模型）分析S&P 500公司2016-2022年的10-K文件。

Result: FinTextSim将主题内相似性提高81%，主题间相似性降低100%，显著优于通用模型。

Conclusion: FinTextSim对财务文本分析至关重要，能提升研究质量和决策效率，并可能优化商业估值和股价预测。

Abstract: Recent advancements in information availability and computational
capabilities have transformed the analysis of annual reports, integrating
traditional financial metrics with insights from textual data. To extract
valuable insights from this wealth of textual data, automated review processes,
such as topic modeling, are crucial. This study examines the effectiveness of
BERTopic, a state-of-the-art topic model relying on contextual embeddings, for
analyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies
(2016-2022). Moreover, we introduce FinTextSim, a finetuned
sentence-transformer model optimized for clustering and semantic search in
financial contexts. Compared to all-MiniLM-L6-v2, the most widely used
sentence-transformer, FinTextSim increases intratopic similarity by 81% and
reduces intertopic similarity by 100%, significantly enhancing organizational
clarity. We assess BERTopic's performance using embeddings from both FinTextSim
and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and
distinct economic topic clusters when paired with FinTextSim's embeddings.
Without FinTextSim, BERTopic struggles with misclassification and overlapping
topics. Thus, FinTextSim is pivotal for advancing financial text analysis.
FinTextSim's enhanced contextual embeddings, tailored for the financial domain,
elevate the quality of future research and financial information. This improved
quality of financial information will enable stakeholders to gain a competitive
advantage, streamlining resource allocation and decision-making processes.
Moreover, the improved insights have the potential to leverage business
valuation and stock price prediction models.

</details>

### [89] [Subject islands do not reduce to construction-specific discourse function](https://arxiv.org/abs/2504.15688)
*Mandy Cartner,Matthew Kogan,Nikolas Webster,Matthew Wagers,Ivy Sichel*

Main category: cs.CL

TLDR: 论文探讨了语言学中的'岛屿'现象，特别是主语作为岛屿的成因，通过实验验证了主语岛屿效应在不同句法结构中的普遍性，支持了抽象的句法解释。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证主语岛屿效应是否仅与疑问句的信息结构冲突有关，还是普遍存在于其他句法结构中。

Method: 通过三个大规模的可接受性研究，采用超加性设计，测试了疑问句、关系从句和话题化结构中的主语岛屿效应。

Result: 实验结果显示主语岛屿效应在所有测试结构中均存在，与信息结构冲突无关。

Conclusion: 研究支持主语岛屿效应的抽象句法解释，独立于句法结构的交际功能。

Abstract: The term islands in linguistics refers to phrases from which extracting an
element results in ungrammaticality (Ross, 1967). Grammatical subjects are
considered islands because extracting a sub-part of a subject results in an
ill-formed sentence, despite having a clear intended meaning (e.g., "Which
topic did the article about inspire you?"). The generative tradition, which
views syntax as autonomous of meaning and function, attributes this
ungrammaticality to the abstract movement dependency between the wh-phrase and
the subject-internal position with which it is associated for interpretation.
However, research on language that emphasizes its communicative function
suggests instead that syntactic constraints, including islands, can be
explained based on the way different constructions package information.
Accordingly, Abeill\'e et al. (2020) suggest that the islandhood of subjects is
specific to the information structure of wh-questions, and propose that
subjects are not islands for movement, but for focusing, due to their
discourse-backgroundedness. This predicts that other constructions that differ
in their information structure from wh-questions, but still involve movement,
should not create a subject island effect. We test this prediction in three
large-scale acceptability studies, using a super-additive design that singles
out subject island violations, in three different constructions: wh-questions,
relative clauses, and topicalization. We report evidence for a subject island
effect in each construction type, despite only wh-questions introducing what
Abeill\'e et al. (2020) call "a clash in information structure." We argue that
this motivates an account of islands in terms of abstract, syntactic
representations, independent of the communicative function associated with the
constructions.

</details>

### [90] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Willie Neiswanger*

Main category: cs.CL

TLDR: Tina是一种高效的小型推理模型，通过参数高效更新和低秩适应（LoRA）在强化学习中实现低成本高性能推理。


<details>
  <summary>Details</summary>
Motivation: 探索如何在语言模型中低成本高效地实现强大的推理能力。

Method: 使用LoRA对1.5B参数的基模型进行参数高效更新，结合强化学习训练。

Result: Tina在推理性能上优于或媲美SOTA模型，且计算成本极低（仅9美元）。

Conclusion: LoRA在高效RL推理中表现出色，能快速适应推理结构，同时保留基模型知识。

Abstract: How cost-effectively can strong reasoning abilities be achieved in language
models? Driven by this fundamental question, we present Tina, a family of tiny
reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates
that substantial reasoning performance can be developed using only minimal
resources, by applying parameter-efficient updates during reinforcement
learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B
parameter base model. This minimalist approach produces models that achieve
reasoning performance which is competitive with, and sometimes surpasses, SOTA
RL reasoning models built upon the same base model. Crucially, this is achieved
at a tiny fraction of the computational post-training cost employed by existing
SOTA models. In fact, the best Tina model achieves a >20\% reasoning
performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD
post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our
work reveals the surprising effectiveness of efficient RL reasoning via LoRA.
We validate this across multiple open-source reasoning datasets and various
ablation settings starting with a single, fixed set of hyperparameters.
Furthermore, we hypothesize that this effectiveness and efficiency stem from
LoRA rapidly adapting the model to the structural format of reasoning rewarded
by RL, while largely preserving the base model's underlying knowledge. In
service of accessibility and open research, we fully open-source all code,
training logs, and model weights \& checkpoints.

</details>

### [91] [Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach](https://arxiv.org/abs/2504.15784)
*Ruizhe Li,Chiwei Zhu,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TLDR: 提出了一种基于Torrance创造性写作测试（TTCW）的自动化评估方法，用于评估大语言模型生成文本的创造力，显著提高了与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖昂贵的人工标注或与人类评估不一致，需要更有效的自动化方法。

Method: 采用基于参考的Likert式评分方法，将生成文本与高质量参考文本进行多维度比较。

Result: 实验结果显示，该方法显著提升了评估与人类判断的一致性，配对准确率达到0.75（提升15%）。

Conclusion: 该方法为机器生成文本的创造力评估提供了高效且可靠的自动化解决方案。

Abstract: Creative writing is a key capability of Large Language Models (LLMs), with
potential applications in literature, storytelling, and various creative
domains. However, evaluating the creativity of machine-generated texts remains
a significant challenge, as existing methods either rely on costly manual
annotations or fail to align closely with human assessments. In this paper, we
propose an effective automated evaluation method based on the Torrance Test of
Creative Writing (TTCW), which evaluates creativity as product. Our method
employs a reference-based Likert-style approach, scoring generated creative
texts relative to high-quality reference texts across various tests.
Experimental results demonstrate that our method significantly improves the
alignment between LLM evaluations and human assessments, achieving a pairwise
accuracy of 0.75 (+15\%).

</details>

### [92] [A closer look at how large language models trust humans: patterns and biases](https://arxiv.org/abs/2504.15801)
*Valeria Lerman,Yaniv Dover*

Main category: cs.CL

TLDR: 研究探讨了大型语言模型（LLM）对人类的有效信任形成机制，发现其与人类信任机制相似，但也存在偏见。


<details>
  <summary>Details</summary>
Motivation: 理解LLM与人类在决策中的信任动态，填补LLM对人类信任研究的空白。

Method: 基于行为理论，研究LLM对人类信任的三大维度（能力、善意、正直）及人口变量影响，通过43,200次模拟实验验证。

Result: LLM信任机制与人类相似，但受信任维度和人口变量（如年龄、宗教、性别）影响，部分模型存在偏见。

Conclusion: 需进一步研究AI对人类的信任动态，监控偏见以避免信任敏感应用中的潜在危害。

Abstract: As large language models (LLMs) and LLM-based agents increasingly interact
with humans in decision-making contexts, understanding the trust dynamics
between humans and AI agents becomes a central concern. While considerable
literature studies how humans trust AI agents, it is much less understood how
LLM-based agents develop effective trust in humans. LLM-based agents likely
rely on some sort of implicit effective trust in trust-related contexts (e.g.,
evaluating individual loan applications) to assist and affect decision making.
Using established behavioral theories, we develop an approach that studies
whether LLMs trust depends on the three major trustworthiness dimensions:
competence, benevolence and integrity of the human subject. We also study how
demographic variables affect effective trust. Across 43,200 simulated
experiments, for five popular language models, across five different scenarios
we find that LLM trust development shows an overall similarity to human trust
development. We find that in most, but not all cases, LLM trust is strongly
predicted by trustworthiness, and in some cases also biased by age, religion
and gender, especially in financial scenarios. This is particularly true for
scenarios common in the literature and for newer models. While the overall
patterns align with human-like mechanisms of effective trust formation,
different models exhibit variation in how they estimate trust; in some cases,
trustworthiness and demographic factors are weak predictors of effective trust.
These findings call for a better understanding of AI-to-human trust dynamics
and monitoring of biases and trust development patterns to prevent unintended
and potentially harmful outcomes in trust-sensitive applications of AI.

</details>

### [93] [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
*Michael A. Hedderich,Anyi Wang,Raoyuan Zhao,Florian Eichin,Barbara Plank*

Main category: cs.CL

TLDR: Spotlight结合自动化与人工分析，通过数据挖掘区分语言模型输出的随机与系统差异，提供标记模式以高效分析提示和模型变化的影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（自动指标或人工评估）存在局限性，无法全面洞察语言模型输出的系统差异。

Method: 基于数据挖掘技术自动区分输出中的随机与系统差异，生成标记模式指导人工分析。

Result: 创建三个基准测试验证标记模式提取方法的可靠性，并揭示提示和模型变化的系统差异（如性别或文化相关）。

Conclusion: Spotlight方法支持提示工程和以人为中心的模型行为研究，帮助用户理解语言模型输出的系统差异。

Abstract: Prompt engineering for large language models is challenging, as even small
prompt perturbations or model changes can significantly impact the generated
output texts. Existing evaluation methods, either automated metrics or human
evaluation, have limitations, such as providing limited insights or being
labor-intensive. We propose Spotlight, a new approach that combines both
automation and human analysis. Based on data mining techniques, we
automatically distinguish between random (decoding) variations and systematic
differences in language model outputs. This process provides token patterns
that describe the systematic differences and guide the user in manually
analyzing the effects of their prompt and model changes efficiently. We create
three benchmarks to quantitatively test the reliability of token pattern
extraction methods and demonstrate that our approach provides new insights into
established prompt data. From a human-centric perspective, through
demonstration studies and a user study, we show that our token pattern approach
helps users understand the systematic differences of language model outputs,
and we are able to discover relevant differences caused by prompt and model
changes (e.g. related to gender or culture), thus supporting the prompt
engineering process and human-centric model behavior research.

</details>

### [94] [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
*Junshu Pan,Wei Shen,Shulin Huang,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TLDR: Pre-DPO是一种改进的偏好优化方法，通过引入引导参考模型提升DPO和SimPO的性能。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法中参考模型与策略模型初始化相同导致数据利用效率低，而SimPO缺乏参考模型导致训练鲁棒性不足。

Method: 提出Pre-DPO，利用引导参考模型动态调整样本权重，优化偏好训练。

Result: 在AlpacaEval 2.0和Arena-Hard v0.1基准测试中，Pre-DPO显著提升了DPO和SimPO的性能。

Conclusion: Pre-DPO通过参考模型的引导机制有效提升了偏好优化的效果，无需依赖外部模型或额外数据。

Abstract: Direct Preference Optimization (DPO) simplifies reinforcement learning from
human feedback (RLHF) for large language models (LLMs) by directly optimizing
human preferences without an explicit reward model. We find that during DPO
training, the reference model plays the role of a data weight adjuster.
However, the common practice of initializing the policy and reference models
identically in DPO can lead to inefficient data utilization and impose a
performance ceiling. Meanwhile, the lack of a reference model in Simple
Preference Optimization (SimPO) reduces training robustness and necessitates
stricter conditions to prevent catastrophic forgetting. In this work, we
propose Pre-DPO, a simple yet effective DPO-based training paradigm that
enhances preference optimization performance by leveraging a guiding reference
model. This reference model provides foresight into the optimal policy state
achievable through the training preference data, serving as a guiding mechanism
that adaptively assigns higher weights to samples more suitable for the model
and lower weights to those less suitable. Extensive experiments on AlpacaEval
2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently
improves the performance of both DPO and SimPO, without relying on external
models or additional data.

</details>

### [95] [Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.15848)
*Luwei Xiao,Rui Mao,Shuai Zhao,Qika Lin,Yanhao Jia,Liang He,Erik Cambria*

Main category: cs.CL

TLDR: 该论文提出了一种名为Chimera的多模态情感分类框架，结合了语义内容和情感认知共振，以提升对细粒度视觉内容和情感表达的理解。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体上多模态内容的增加，现有方法在理解细粒度视觉内容和情感认知机制方面存在不足，因此需要一种更全面的框架。

Method: Chimera框架通过视觉补丁特征对齐、细粒度视觉区域提取和文本描述转换，结合大语言模型生成的情感原因和印象，增强模型对情感线索的感知。

Result: 在标准MASC数据集上的实验表明，该模型优于GPT-4o等大语言模型，具有更高的灵活性。

Conclusion: Chimera框架通过结合语义和情感认知共振，显著提升了多模态情感分类的性能，并公开了实现和数据集。

Abstract: Multimodal aspect-based sentiment classification (MASC) is an emerging task
due to an increase in user-generated multimodal content on social platforms,
aimed at predicting sentiment polarity toward specific aspect targets (i.e.,
entities or attributes explicitly mentioned in text-image pairs). Despite
extensive efforts and significant achievements in existing MASC, substantial
gaps remain in understanding fine-grained visual content and the cognitive
rationales derived from semantic content and impressions (cognitive
interpretations of emotions evoked by image content). In this study, we present
Chimera: a cognitive and aesthetic sentiment causality understanding framework
to derive fine-grained holistic features of aspects and infer the fundamental
drivers of sentiment expression from both semantic perspectives and
affective-cognitive resonance (the synergistic effect between emotional
responses and cognitive interpretations). Specifically, this framework first
incorporates visual patch features for patch-word alignment. Meanwhile, it
extracts coarse-grained visual features (e.g., overall image representation)
and fine-grained visual regions (e.g., aspect-related regions) and translates
them into corresponding textual descriptions (e.g., facial, aesthetic).
Finally, we leverage the sentimental causes and impressions generated by a
large language model (LLM) to enhance the model's awareness of sentimental cues
evoked by semantic content and affective-cognitive resonance. Experimental
results on standard MASC datasets demonstrate the effectiveness of the proposed
model, which also exhibits greater flexibility to MASC compared to LLMs such as
GPT-4o. We have publicly released the complete implementation and dataset at
https://github.com/Xillv/Chimera

</details>

### [96] [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
*Chenxu Yang,Qingyi Si,Yongjie Duan,Zheliang Zhu,Chenyu Zhu,Zheng Lin,Li Cao,Weiping Wang*

Main category: cs.CL

TLDR: 论文提出了一种动态截断长链式推理（CoT）的方法，通过提前退出生成来提升推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 长链式推理可能导致效率低下和准确性损失，因此需要一种动态截断方法。

Method: 通过监控模型行为（如“Wait”标记）动态终止推理链生成，无需额外训练。

Result: 在多个推理基准测试中，CoT序列长度平均减少31%-43%，准确性提升1.7%-5.7%。

Conclusion: 该方法简单有效，可无缝集成到现有推理模型中，显著提升性能。

Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time
scaling, which extends long chain-of-thought (CoT) generation to solve complex
tasks. However, overthinking in long CoT not only slows down the efficiency of
problem solving, but also risks accuracy loss due to the extremely detailed or
redundant reasoning steps. We propose a simple yet effective method that allows
LLMs to self-truncate CoT sequences by early exit during generation. Instead of
relying on fixed heuristics, the proposed method monitors model behavior at
potential reasoning transition points (e.g.,"Wait" tokens) and dynamically
terminates the next reasoning chain's generation when the model exhibits high
confidence in a trial answer. Our method requires no additional training and
can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments
on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024
show that the proposed method is consistently effective on deepseek-series
reasoning LLMs, reducing the length of CoT sequences by an average of 31% to
43% while improving accuracy by 1.7% to 5.7%.

</details>

### [97] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
*Cheng Wen,Tingwei Guo,Shuaijiang Zhao,Wei Zou,Xiangang Li*

Main category: cs.CL

TLDR: 该论文通过强化学习（RL）提升大型音频语言模型（LALM）的推理能力，提出结构化音频推理模型SARI，并在多个实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在音频语言模型推理能力上的应用，填补现有研究在音频语言推理领域的空白。

Method: 采用两阶段训练方法：先进行监督微调（SFT），再通过课程引导的GRPO框架优化模型。比较结构化与非结构化推理方式。

Result: SARI模型在基准模型上平均准确率提升16.35%，并在MMAU测试中达到67.08%的最新性能。

Conclusion: 结构化推理和课程学习显著提升音频语言理解能力，为未来研究提供了有效方法。

Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the
reasoning ability of large language models (LLMs) by prompting them to "think
before answering." Yet whether and how these gains transfer to audio-language
reasoning remains largely unexplored. We extend the Group-Relative Policy
Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model
(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage
regimen supervised fine-tuning on structured and unstructured
chains-of-thought, followed by curriculum-guided GRPO, we systematically
compare implicit vs. explicit, and structured vs. free form reasoning under
identical architectures. Our structured audio reasoning model, SARI (Structured
Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a
16.35% improvement in average accuracy over the base model
Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni
reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.
Ablation experiments show that on the base model we use: (i) SFT warm-up is
important for stable RL training, (ii) structured chains yield more robust
generalization than unstructured ones, and (iii) easy-to-hard curricula
accelerate convergence and improve final performance. These findings
demonstrate that explicit, structured reasoning and curriculum learning
substantially enhances audio-language understanding.

</details>

### [98] [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/abs/2504.15941)
*Fanny Jourdan,Yannick Chevalier,Cécile Favre*

Main category: cs.CL

TLDR: FairTranslate是一个新数据集，用于评估LLM在英语到法语翻译中的非二元性别偏见，结果显示LLM存在显著偏见。


<details>
  <summary>Details</summary>
Motivation: LLM在翻译包容性语言（如单数'they'代词）时表现不佳，需评估其公平性。

Method: 创建FairTranslate数据集（2418句对），评估四种LLM在不同提示下的表现。

Result: LLM在性别表示上存在显著偏见，公平翻译仍具挑战。

Conclusion: 需针对性策略确保LLM翻译的公平与包容性，数据集和代码已公开。

Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks
but often fall short when translating inclusive language -- such as texts
containing the singular 'they' pronoun or otherwise reflecting fair linguistic
protocols. Because these challenges span both computational and societal
domains, it is imperative to critically evaluate how well LLMs handle inclusive
translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset
designed to evaluate non-binary gender biases in machine translation systems
from English to French. FairTranslate includes 2418 English-French sentence
pairs related to occupations, annotated with rich metadata such as the
stereotypical alignment of the occupation, grammatical gender indicator
ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,
Llama3.3-70B) on this dataset under different prompting procedures. Our results
reveal substantial biases in gender representation across LLMs, highlighting
persistent challenges in achieving equitable outcomes in machine translation.
These findings underscore the need for focused strategies and interventions
aimed at ensuring fair and inclusive language usage in LLM-based translation
systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and
disclose the code for all experiments on GitHub.

</details>

### [99] [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://arxiv.org/abs/2504.15983)
*Shang Wang*

Main category: cs.CL

TLDR: 本文提出了一种名为W-PCA的零样本神经架构搜索方法，用于高效设计和评估轻量级语言模型，显著减少了训练时间并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前零样本NAS方法存在评估指标偏差和计算效率低的问题，需要一种更高效的方法来设计轻量级语言模型。

Method: 采用权重加权PCA（W-PCA）方法，结合参数数量和FFN层主成分贡献率作为评估代理，无需梯度计算。

Result: 在GLUE和SQuAD数据集上，W-PCA显著减少训练时间，并在测试阶段优于现有方法。在FlexiBERT搜索空间中的排名评估中，表现出更高的相关性和更短的解决时间。

Conclusion: W-PCA是一种高效的零样本NAS方法，适用于轻量级语言模型的设计和评估，具有显著的时间和性能优势。

Abstract: The demand for efficient natural language processing (NLP) systems has led to
the development of lightweight language models. Previous work in this area has
primarily focused on manual design or training-based neural architecture search
(NAS) methods. Recently, zero-shot NAS methods have been proposed for
evaluating language models without the need for training. However, prevailing
approaches to zero-shot NAS often face challenges such as biased evaluation
metrics and computational inefficiencies. In this paper, we introduce
weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored
for lightweight language models. Our approach utilizes two evaluation proxies:
the parameter count and the number of principal components with cumulative
contribution exceeding $\eta$ in the feed-forward neural (FFN) layer.
Additionally, by eliminating the need for gradient computations, we optimize
the evaluation time, thus enhancing the efficiency of designing and evaluating
lightweight language models. We conduct a comparative analysis on the GLUE and
SQuAD datasets to evaluate our approach. The results demonstrate that our
method significantly reduces training time compared to one-shot NAS methods and
achieves higher scores in the testing phase compared to previous
state-of-the-art training-based methods. Furthermore, we perform ranking
evaluations on a dataset sampled from the FlexiBERT search space. Our approach
exhibits superior ranking correlation and further reduces solving time compared
to other zero-shot NAS methods that require gradient computation.

</details>

### [100] [Few-shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987)
*Zhenkai Qin,Dongze Wu,Yuxin Liu,Guifang Yang*

Main category: cs.CL

TLDR: 提出了一种名为MS-FSLHate的少样本仇恨言论检测框架，结合可学习提示嵌入、CNN-BiLSTM主干网络和对抗数据增强，在低资源环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 社交媒体仇恨言论泛滥，现有深度学习模型在少样本或低资源场景下性能下降，需改进。

Method: 采用MindSpore平台，整合可学习提示嵌入、CNN-BiLSTM主干网络与注意力池化，以及基于同义词的对抗数据增强。

Result: 在HateXplain和HSOL数据集上，模型在精确率、召回率和F1分数上优于基线方法，且高效可扩展。

Conclusion: 结合提示学习和对抗增强的方法在少样本仇恨言论检测中具有鲁棒性和适应性，适合资源受限环境。

Abstract: The proliferation of hate speech on social media poses a significant threat
to online communities, requiring effective detection systems. While deep
learning models have shown promise, their performance often deteriorates in
few-shot or low-resource settings due to reliance on large annotated corpora.
To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for
few-shot hate speech detection implemented on the MindSpore deep learning
platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM
backbone with attention pooling, and synonym-based adversarial data
augmentation to improve generalization. Experimental results on two benchmark
datasets-HateXplain and HSOL-demonstrate that our approach outperforms
competitive baselines in precision, recall, and F1-score. Additionally, the
framework shows high efficiency and scalability, suggesting its suitability for
deployment in resource-constrained environments. These findings highlight the
potential of combining prompt-based learning with adversarial augmentation for
robust and adaptable hate speech detection in few-shot scenarios.

</details>

### [101] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
*Tom Zehle,Moritz Schlager,Timo Heiß,Matthias Feurer*

Main category: cs.CL

TLDR: CAPO是一种成本感知的提示优化算法，通过结合AutoML技术提高效率，减少LLM调用次数和输入标记，同时优化指令和少样本示例。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的性能对提示的表述高度敏感，而现有的提示优化方法成本高昂，需要大量LLM调用和输入标记。

Method: CAPO采用进化算法，结合AutoML技术，通过竞赛机制减少评估次数，并利用多目标优化平衡性能与提示长度。

Result: 在11/15的案例中，CAPO优于现有离散提示优化方法，性能提升高达21%，且在较小预算下表现更好。

Conclusion: CAPO通过提高成本效率，使提示优化更强大和易用。

Abstract: Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automated prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves
better performances already with smaller budgets, saves evaluations through
racing, and decreases average prompt length via a length penalty, making it
both cost-efficient and cost-aware. Even without few-shot examples, CAPO
outperforms its competitors and generally remains robust to initial prompts.
CAPO represents an important step toward making prompt optimization more
powerful and accessible by improving cost-efficiency.

</details>

### [102] [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
*Igor Rozhkov,Natalia Loukachevitch*

Main category: cs.CL

TLDR: 本文介绍了在RuTermEval竞赛中应用Binder模型提取嵌套术语的研究，取得了最佳成绩，并探索了从非嵌套标注数据中识别嵌套术语的新任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证Binder模型在提取嵌套术语中的有效性，并探索无需嵌套标注的术语识别方法。

Method: 采用Binder模型，该模型曾成功用于嵌套命名实体识别，并应用于嵌套术语提取。

Result: 在RuTermEval竞赛的三个赛道中均取得最佳成绩，证明了方法的有效性。

Conclusion: 提出的方法能够有效提取嵌套术语，且无需嵌套标注数据支持。

Abstract: In this paper, we describe our participation in the RuTermEval competition
devoted to extracting nested terms. We apply the Binder model, which was
previously successfully applied to the recognition of nested named entities, to
extract nested terms. We obtained the best results of term recognition in all
three tracks of the RuTermEval competition. In addition, we study the new task
of recognition of nested terms from flat training data annotated with terms
without nestedness. We can conclude that several approaches we proposed in this
work are viable enough to retrieve nested terms effectively without nested
labeling of them.

</details>

### [103] [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
*Jingyu Zhang,Jiacan Yu,Marc Marone,Benjamin Van Durme,Daniel Khashabi*

Main category: cs.CL

TLDR: 论文提出了一种名为BloomScrub的轻量级推理时方法，用于解决大型语言模型（LLMs）在预训练中接触受版权保护材料后可能引发的侵权风险。该方法通过结合引用检测和重写技术，提供可认证的版权删除效果。


<details>
  <summary>Details</summary>
Motivation: 预训练中LLMs接触受版权保护材料可能导致部署后的无意侵权，现有方法未能有效解决最坏情况下的侵权风险（如长段直接引用）。

Method: 提出BloomScrub方法，通过重复结合引用检测与重写技术，利用高效数据草图（Bloom过滤器）实现可扩展的版权筛查，并在必要时通过弃权响应降低风险。

Result: 实验表明，BloomScrub能有效降低侵权风险，保持模型实用性，并通过自适应弃权适应不同严格程度的执法需求。

Conclusion: 轻量级推理时方法（如BloomScrub）在版权预防中表现出意外的高效性。

Abstract: The exposure of large language models (LLMs) to copyrighted material during
pre-training raises concerns about unintentional copyright infringement post
deployment. This has driven the development of "copyright takedown" methods,
post-training approaches aimed at preventing models from generating content
substantially similar to copyrighted ones. While current mitigation approaches
are somewhat effective for average-case risks, we demonstrate that they
overlook worst-case copyright risks exhibits by the existence of long, verbatim
quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet
highly effective inference-time approach that provides certified copyright
takedown. Our method repeatedly interleaves quote detection with rewriting
techniques to transform potentially infringing segments. By leveraging
efficient data sketches (Bloom filters), our approach enables scalable
copyright screening even for large-scale real-world corpora. When quotes beyond
a length threshold cannot be removed, the system can abstain from responding,
offering certified risk reduction. Experimental results show that BloomScrub
reduces infringement risk, preserves utility, and accommodates different levels
of enforcement stringency with adaptive abstention. Our results suggest that
lightweight, inference-time methods can be surprisingly effective for copyright
prevention.

</details>

### [104] [LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement](https://arxiv.org/abs/2504.16053)
*Zhifan Ye,Kejing Xia,Yonggan Fu,Xin Dong,Jihoon Hong,Xiangchi Yuan,Shizhe Diao,Jan Kautz,Pavlo Molchanov,Yingyan Celine Lin*

Main category: cs.CL

TLDR: LongMamba是一种无需训练的改进技术，通过识别和过滤全局通道中的关键令牌，显著提升Mamba模型的长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管状态空间模型（SSMs）在长上下文处理中具有计算效率，但其性能仍不及Transformer模型，尤其是在长上下文理解任务中表现不佳。

Method: LongMamba通过分类隐藏通道为局部和全局通道，并聚焦于全局通道的关键令牌，防止不重要令牌的积累，从而提升性能。

Result: 在合成和真实长上下文场景中，LongMamba显著提升了Mamba模型的长上下文性能，无需额外训练。

Conclusion: LongMamba为Mamba模型的长上下文能力设定了新标准，解决了其性能瓶颈问题。

Abstract: State space models (SSMs) have emerged as an efficient alternative to
Transformer models for language modeling, offering linear computational
complexity and constant memory usage as context length increases. However,
despite their efficiency in handling long contexts, recent studies have shown
that SSMs, such as Mamba models, generally underperform compared to
Transformers in long-context understanding tasks. To address this significant
shortfall and achieve both efficient and accurate long-context understanding,
we propose LongMamba, a training-free technique that significantly enhances the
long-context capabilities of Mamba models. LongMamba builds on our discovery
that the hidden channels in Mamba can be categorized into local and global
channels based on their receptive field lengths, with global channels primarily
responsible for long-context capability. These global channels can become the
key bottleneck as the input context lengthens. Specifically, when input lengths
largely exceed the training sequence length, global channels exhibit
limitations in adaptively extend their receptive fields, leading to Mamba's
poor long-context performance. The key idea of LongMamba is to mitigate the
hidden state memory decay in these global channels by preventing the
accumulation of unimportant tokens in their memory. This is achieved by first
identifying critical tokens in the global channels and then applying token
filtering to accumulate only those critical tokens. Through extensive
benchmarking across synthetic and real-world long-context scenarios, LongMamba
sets a new standard for Mamba's long-context performance, significantly
extending its operational range without requiring additional training. Our code
is available at https://github.com/GATECH-EIC/LongMamba.

</details>

### [105] [Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability](https://arxiv.org/abs/2504.16056)
*Daniel Hendriks,Philipp Spitzer,Niklas Kühl,Gerhard Satzger*

Main category: cs.CL

TLDR: 本文通过应用批判性修订提示和综合现有方法，扩展了知识蒸馏的方法集，并系统比较了其在性能和可解释性上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高计算和存储需求限制了其在资源受限环境中的部署，知识蒸馏可以解决这一问题，但现有方法的性能和可解释性尚未充分研究。

Method: 应用批判性修订提示进行数据生成，综合现有方法进行训练，并在Commonsense Question-Answering（CQA）数据集上系统比较。

Result: 通过学生模型准确率衡量性能，并通过人类基础研究评估可解释性。

Conclusion: 本文贡献了新的蒸馏方法及其在性能和可解释性上的比较，有助于小型语言模型的蒸馏和LLM技术的更广泛应用。

Abstract: Artificial Intelligence (AI) has increasingly influenced modern society,
recently in particular through significant advancements in Large Language
Models (LLMs). However, high computational and storage demands of LLMs still
limit their deployment in resource-constrained environments. Knowledge
distillation addresses this challenge by training a small student model from a
larger teacher model. Previous research has introduced several distillation
methods for both generating training data and for training the student model.
Despite their relevance, the effects of state-of-the-art distillation methods
on model performance and explainability have not been thoroughly investigated
and compared. In this work, we enlarge the set of available methods by applying
critique-revision prompting to distillation for data generation and by
synthesizing existing methods for training. For these methods, we provide a
systematic comparison based on the widely used Commonsense Question-Answering
(CQA) dataset. While we measure performance via student model accuracy, we
employ a human-grounded study to evaluate explainability. We contribute new
distillation methods and their comparison in terms of both performance and
explainability. This should further advance the distillation of small language
models and, thus, contribute to broader applicability and faster diffusion of
LLM technology.

</details>

### [106] [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
*Ziqiao Ma,Jing Ding,Xuejun Zhang,Dezhi Luo,Jiahe Ding,Sihan Xu,Yuchen Huang,Run Peng,Joyce Chai*

Main category: cs.CL

TLDR: 论文提出从语用学角度重新审视Referring Expression Generation (REG)，指出当前视觉语言模型(VLMs)在语用能力上的不足，并引入新数据集RefOI进行系统性评估。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型(VLMs)的评估忽视了语用维度，将REG简化为基于区域的描述任务，忽略了Gricean准则。

Method: 通过新数据集RefOI（包含1.5k图像及书面和口语指代表达）对VLMs进行系统性评估。

Result: 发现VLMs在语用能力上的三大失败：(1)未能唯一识别指称对象，(2)包含过多无关信息，(3)与人类语用偏好不一致。

Conclusion: 呼吁开发更符合人类真实沟通的语用模型和评估框架。

Abstract: Referring Expression Generation (REG) is a core task for evaluating the
pragmatic competence of vision-language systems, requiring not only accurate
semantic grounding but also adherence to principles of cooperative
communication (Grice, 1975). However, current evaluations of vision-language
models (VLMs) often overlook the pragmatic dimension, reducing REG to a
region-based captioning task and neglecting Gricean maxims. In this work, we
revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of
1.5k images annotated with both written and spoken referring expressions.
Through a systematic evaluation of state-of-the-art VLMs, we identify three key
failures of pragmatic competence: (1) failure to uniquely identify the
referent, (2) inclusion of excessive or irrelevant information, and (3)
misalignment with human pragmatic preference, such as the underuse of minimal
spatial cues. We also show that standard automatic evaluations fail to capture
these pragmatic violations, reinforcing superficial cues rather than genuine
referential success. Our findings call for a renewed focus on pragmatically
informed models and evaluation frameworks that align with real human
communication.

</details>

### [107] [A Python Tool for Reconstructing Full News Text from GDELT](https://arxiv.org/abs/2504.16063)
*A. Fronzetti Colladon,R. Vestrelli*

Main category: cs.CL

TLDR: 本文提出了一种利用GDELT Web News NGrams 3.0数据集低成本获取全文新闻文章的方法，通过Python代码重构全文，解决了现有新闻数据集的访问限制问题。


<details>
  <summary>Details</summary>
Motivation: 新闻数据在多个学科中具有重要价值，但现有数据集通常成本高昂或数据不完整，限制了研究。

Method: 利用GDELT数据集中的n-grams，通过识别和合并重叠文本片段重构全文新闻文章。

Result: 提供了一种低成本、大规模的新闻数据获取方法，适用于文本分析。

Conclusion: 该方法提高了新闻数据的可访问性，支持经济预测、计算社会科学和自然语言处理等应用。

Abstract: News data have become an essential resource across various disciplines,
including economics, finance, management, social sciences, and computer
science. Researchers leverage newspaper articles to study economic trends,
market dynamics, corporate strategies, public perception, political discourse,
and the evolution of public opinion. Additionally, news datasets have been
instrumental in training large-scale language models, with applications in
sentiment analysis, fake news detection, and automated news summarization.
Despite their significance, access to comprehensive news corpora remains a key
challenge. Many full-text news providers, such as Factiva and LexisNexis,
require costly subscriptions, while free alternatives often suffer from
incomplete data and transparency issues. This paper presents a novel approach
to obtaining full-text newspaper articles at near-zero cost by leveraging data
from the Global Database of Events, Language, and Tone (GDELT). Specifically,
we focus on the GDELT Web News NGrams 3.0 dataset, which provides
high-frequency updates of n-grams extracted from global online news sources. We
provide Python code to reconstruct full-text articles from these n-grams by
identifying overlapping textual fragments and intelligently merging them. Our
method enables researchers to access structured, large-scale newspaper data for
text analysis while overcoming the limitations of existing proprietary
datasets. The proposed approach enhances the accessibility of news data for
empirical research, facilitating applications in economic forecasting,
computational social science, and natural language processing.

</details>

### [108] [Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation](https://arxiv.org/abs/2504.16073)
*Zhiyuan Hu,Shiyun Xiong,Yifan Zhang,See-Kiong Ng,Anh Tuan Luu,Bo An,Shuicheng Yan,Bryan Hooi*

Main category: cs.CL

TLDR: 提出了一种通过过程监督优化视觉语言模型（VLM）在GUI任务中表现的方法，显著提升了静态和动态环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在复杂GUI任务中表现不足，且商业VLM为黑盒，开源VLM调优成本高。现有评估方法因延迟反馈和局部优化问题效果有限。

Method: 在推理时通过奖励模型对VLM代理进行过程监督，优化每一步动作，并结合轨迹反思和重试机制。

Result: 静态环境中单步动作准确率提升3.4%，动态环境中任务成功率提高约33%。

Conclusion: 该方法有效提升了VLM在GUI任务中的表现，尤其在动态环境中效果显著。

Abstract: Recent advancements in visual language models (VLMs) have notably enhanced
their capabilities in handling complex Graphical User Interface (GUI)
interaction tasks. Despite these improvements, current frameworks often
struggle to generate correct actions in challenging GUI environments.
State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source
VLMs for GUI tasks requires significant resources. Additionally, existing
trajectory-level evaluation and refinement techniques frequently fall short due
to delayed feedback and local optimization issues. To address these challenges,
we propose an approach that guides VLM agents with process supervision by a
reward model during GUI navigation and control at inference time. This guidance
allows the VLM agent to optimize actions at each inference step, thereby
improving performance in both static and dynamic environments. In particular,
our method demonstrates significant performance gains in three GUI navigation
tasks, achieving a 3.4% improvement in single step action accuracy for static
environments, along with a around 33% increase in task success rate in one
dynamic environment. With further integration of trajectory reflection and
retry mechanisms, we also demonstrate even greater enhancement in task success.

</details>

### [109] [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
*Shi Qiu,Shaoyang Guo,Zhuo-Yang Song,Yunbo Sun,Zeyu Cai,Jiashen Wei,Tianyu Luo,Yixuan Yin,Haoxu Zhang,Yi Hu,Chenyang Wang,Chencheng Tang,Haoling Chang,Qi Liu,Ziheng Zhou,Tianyu Zhang,Jingtian Zhang,Zhangyi Liu,Minghao Li,Yuku Zhang,Boxuan Jing,Xianqi Yin,Yutong Ren,Zizhuo Fu,Weike Wang,Xudong Tian,Anqi Lv,Laifu Man,Jianxiang Li,Feiyu Tao,Qihua Sun,Zhou Liang,Yushu Mu,Zhongxuan Li,Jing-Jun Zhang,Shutao Zhang,Xiaotian Li,Xingqi Xia,Jiawei Lin,Zheyu Shen,Jiahang Chen,Qiuhao Xiong,Binran Wang,Fengyuan Wang,Ziyang Ni,Bohan Zhang,Fan Cui,Changkun Shao,Qing-Hong Cao,Ming-xing Luo,Muhan Zhang,Hua Xing Zhu*

Main category: cs.CL

TLDR: PHYBench是一个用于评估大型语言模型在物理场景中推理能力的高质量基准，包含500个精心设计的物理问题，并提出了一种新的评估指标EED Score。实验结果显示，当前最先进的模型在复杂物理推理上仍显著落后于人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法难以全面衡量大型语言模型在物理推理中的表现，因此需要一种更全面的基准和评估指标。

Method: PHYBench包含500个基于真实物理场景的问题，涵盖多个物理领域和难度级别，并提出了基于数学表达式编辑距离的EED Score评估指标。

Result: 实验表明，即使是当前最先进的模型，在复杂物理推理任务中的表现仍显著落后于人类专家。

Conclusion: PHYBench为评估和改进大型语言模型的物理推理能力提供了重要工具，揭示了现有模型的局限性。

Abstract: We introduce PHYBench, a novel, high-quality benchmark designed for
evaluating reasoning capabilities of large language models (LLMs) in physical
contexts. PHYBench consists of 500 meticulously curated physics problems based
on real-world physical scenarios, designed to assess the ability of models to
understand and reason about realistic physical processes. Covering mechanics,
electromagnetism, thermodynamics, optics, modern physics, and advanced physics,
the benchmark spans difficulty levels from high school exercises to
undergraduate problems and Physics Olympiad challenges. Additionally, we
propose the Expression Edit Distance (EED) Score, a novel evaluation metric
based on the edit distance between mathematical expressions, which effectively
captures differences in model reasoning processes and results beyond
traditional binary scoring methods. We evaluate various LLMs on PHYBench and
compare their performance with human experts. Our results reveal that even
state-of-the-art reasoning models significantly lag behind human experts,
highlighting their limitations and the need for improvement in complex physical
reasoning scenarios. Our benchmark results and dataset are publicly available
at https://phybench-official.github.io/phybench-demo/.

</details>

### [110] [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
*Yuxin Zuo,Kaiyan Zhang,Shang Qu,Li Sheng,Xuekai Zhu,Biqing Qi,Youbang Sun,Ganqu Cui,Ning Ding,Bowen Zhou*

Main category: cs.CL

TLDR: 论文提出了一种名为TTRL的新方法，通过强化学习在无标签数据上训练大型语言模型，利用测试时缩放技术生成奖励信号，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决大型语言模型在无标签数据上进行推理任务时奖励估计的挑战，探索如何利用测试时缩放技术生成有效的奖励信号。

Method: 方法是通过测试时强化学习（TTRL），利用预训练模型的先验知识，结合多数投票等测试时缩放技术生成奖励信号，驱动强化学习训练。

Result: 实验结果表明，TTRL在多种任务和模型上均能显著提升性能，例如在AIME 2024任务中将Qwen-2.5-Math-7B的pass@1性能提高了约159%。

Conclusion: 结论是TTRL是一种通用且有效的方法，能够在无标签数据上实现模型性能的自我进化，并接近有标签数据训练的模型性能。

Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model, and
approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks, and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [Can Machine Learning Agents Deal with Hard Choices?](https://arxiv.org/abs/2504.15304)
*Kangyu Wang*

Main category: cs.AI

TLDR: 论文探讨了机器学习（ML）代理在决策中无法识别和解决“硬选择”问题，与人类决策的差异，并提出了可能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解ML代理与人类在决策过程中的差异，特别是在面对“硬选择”时，ML代理的局限性可能导致对齐问题。

Method: 通过分析多目标优化（MOO）方法的局限性，评估了两种潜在技术解决方案，并推荐了一种集成方法。

Result: 发现当前ML代理无法识别或解决硬选择问题，但集成方法显示出一定的潜力。

Conclusion: 结论强调了人类决策的独特性，呼吁ML研究者重新思考机器自主性，并开发新框架以填补这一根本差距。

Abstract: Machine Learning ML agents have been increasingly used in decision-making
across a wide range of tasks and environments. These ML agents are typically
designed to balance multiple objectives when making choices. Understanding how
their decision-making processes align with or diverge from human reasoning is
essential. Human agents often encounter hard choices, that is, situations where
options are incommensurable; neither option is preferred, yet the agent is not
indifferent between them. In such cases, human agents can identify hard choices
and resolve them through deliberation. In contrast, current ML agents, due to
fundamental limitations in Multi-Objective Optimisation or MOO methods, cannot
identify hard choices, let alone resolve them. Neither Scalarised Optimisation
nor Pareto Optimisation, the two principal MOO approaches, can capture
incommensurability. This limitation generates three distinct alignment
problems: the alienness of ML decision-making behaviour from a human
perspective; the unreliability of preference-based alignment strategies for
hard choices; and the blockage of alignment strategies pursuing multiple
objectives. Evaluating two potential technical solutions, I recommend an
ensemble solution that appears most promising for enabling ML agents to
identify hard choices and mitigate alignment problems. However, no known
technique allows ML agents to resolve hard choices through deliberation, as
they cannot autonomously change their goals. This underscores the
distinctiveness of human agency and urges ML researchers to reconceptualise
machine autonomy and develop frameworks and methods that can better address
this fundamental gap.

</details>

### [112] [PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind](https://arxiv.org/abs/2504.15313)
*Yajie Yu,Yue Feng*

Main category: cs.AI

TLDR: 论文提出PolicyEvol-Agent框架，通过系统获取他人意图和自适应优化策略，解决多智能体交互中的认知偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究在多智能体动态交互场景中，对认知链（推理、规划、决策、反思）的研究不足，且基于提示的响应在心理状态感知和经验校准方面存在挑战。

Method: PolicyEvol-Agent通过获取反思性专业知识模式，结合心理理论和内外视角，整合多种认知操作。

Result: 仿真结果表明，PolicyEvol-Agent优于基于强化学习和智能体的方法，实现最终游戏胜利。

Conclusion: PolicyEvol-Agent通过动态策略调整机制，在自动和人工评估中均表现出有效性。

Abstract: Multi-agents has exhibited significant intelligence in real-word simulations
with Large language models (LLMs) due to the capabilities of social cognition
and knowledge retrieval. However, existing research on agents equipped with
effective cognition chains including reasoning, planning, decision-making and
reflecting remains limited, especially in the dynamically interactive
scenarios. In addition, unlike human, prompt-based responses face challenges in
psychological state perception and empirical calibration during uncertain
gaming process, which can inevitably lead to cognition bias. In light of above,
we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework
characterized by systematically acquiring intentions of others and adaptively
optimizing irrational strategies for continual enhancement. Specifically,
PolicyEvol-Agent first obtains reflective expertise patterns and then
integrates a range of cognitive operations with Theory of Mind alongside
internal and external perspectives. Simulation results, outperforming RL-based
models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent
for final gaming victory. Moreover, the policy evolution mechanism reveals the
effectiveness of dynamic guideline adjustments in both automatic and human
evaluation.

</details>

### [113] [Reliable Classification with Conformal Learning and Interval-Type 2 Fuzzy Sets](https://arxiv.org/abs/2504.15360)
*Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TLDR: 本文提出使用共形学习与模糊规则系统结合的分类方法，并探讨了类型2模糊集对系统输出质量的提升效果。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习分类器在实验室基准测试之外可能不可靠且过于自信，因此需要可靠的方法评估模型输出的质量。

Method: 采用共形学习与模糊规则系统结合的方法，并引入类型2模糊集以提升输出质量。

Result: 共形学习能提供更可靠的预测覆盖，类型2模糊集相比传统模糊和清晰规则进一步提升了系统性能。

Conclusion: 共形学习与模糊规则系统的结合，尤其是类型2模糊集的使用，显著提升了分类任务的可靠性和输出质量。

Abstract: Classical machine learning classifiers tend to be overconfident can be
unreliable outside of the laboratory benchmarks. Properly assessing the
reliability of the output of the model per sample is instrumental for real-life
scenarios where these systems are deployed. Because of this, different
techniques have been employed to properly quantify the quality of prediction
for a given model. These are most commonly Bayesian statistics and, more
recently, conformal learning. Given a calibration set, conformal learning can
produce outputs that are guaranteed to cover the target class with a desired
significance level, and are more reliable than the standard confidence
intervals used by Bayesian methods. In this work, we propose to use conformal
learning with fuzzy rule-based systems in classification and show some metrics
of their performance. Then, we discuss how the use of type 2 fuzzy sets can
improve the quality of the output of the system compared to both fuzzy and
crisp rules. Finally, we also discuss how the fine-tuning of the system can be
adapted to improve the quality of the conformal prediction.

</details>

### [114] [KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments](https://arxiv.org/abs/2504.15364)
*Junyoung Park,Dalton Jones,Matt Morse,Raghavv Goel,Mingu Lee,Chris Lott*

Main category: cs.AI

TLDR: KeyDiff是一种无需训练的KV缓存淘汰方法，基于键相似性，适用于资源受限环境中的长输入提示LLM应用。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限环境中部署LLM应用时处理长输入提示的挑战。

Method: 提出KeyDiff方法，通过最大化键多样性优化KV缓存选择，不依赖注意力分数。

Result: 在LongBench基准测试中，KeyDiff在8K缓存预算下性能差距小于0.04%，减少约23% KV缓存。

Conclusion: KeyDiff是一种高效且理论支持的KV缓存优化方法，适用于资源受限场景。

Abstract: In this work, we demonstrate that distinctive keys during LLM inference tend
to have high attention scores. We explore this phenomenon and propose KeyDiff,
a training-free KV cache eviction method based on key similarity. This method
facilitates the deployment of LLM-based application requiring long input
prompts in resource-constrained environments with limited memory and compute
budgets. Unlike other KV cache eviction methods, KeyDiff can process
arbitrarily long prompts within strict resource constraints and efficiently
generate responses. We demonstrate that KeyDiff computes the optimal solution
to a KV cache selection problem that maximizes key diversity, providing a
theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on
attention scores, allowing the use of optimized attention mechanisms like
FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse
tasks and models, illustrating a performance gap of less than 0.04\% with 8K
cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on
the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

</details>

### [115] [AGI Is Coming... Right After AI Learns to Play Wordle](https://arxiv.org/abs/2504.15434)
*Sarath Shekkizhar,Romain Cosentino*

Main category: cs.AI

TLDR: 研究评估了OpenAI的计算机用户代理（CUA）在《纽约时报》Wordle游戏中的表现，发现其在颜色识别上存在显著差异，成功率仅为5.36%。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态代理（如CUA）在简单任务中的表现，揭示当前前沿AI模型的局限性。

Method: 通过让CUA在Wordle游戏中完成任务，分析其行为和识别能力。

Result: 模型在颜色识别上表现不佳，成功率低，表明简单任务对AI仍具挑战性。

Conclusion: 讨论了潜在原因、未来发展的影响及改进AI系统的研究方向。

Abstract: This paper investigates multimodal agents, in particular, OpenAI's
Computer-User Agent (CUA), trained to control and complete tasks through a
standard computer interface, similar to humans. We evaluated the agent's
performance on the New York Times Wordle game to elicit model behaviors and
identify shortcomings. Our findings revealed a significant discrepancy in the
model's ability to recognize colors correctly depending on the context. The
model had a $5.36\%$ success rate over several hundred runs across a week of
Wordle. Despite the immense enthusiasm surrounding AI agents and their
potential to usher in Artificial General Intelligence (AGI), our findings
reinforce the fact that even simple tasks present substantial challenges for
today's frontier AI models. We conclude with a discussion of the potential
underlying causes, implications for future development, and research directions
to improve these AI systems.

</details>

### [116] [Improving Human-AI Coordination through Adversarial Training and Generative Models](https://arxiv.org/abs/2504.15457)
*Paresh Chaudhary,Yancheng Liang,Daphne Chen,Simon S. Du,Natasha Jaques*

Main category: cs.AI

TLDR: 论文提出了一种名为GOAT的新方法，结合生成模型和对抗训练，以解决合作任务中对抗策略的自我破坏问题，并在Overcooked基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 合作任务中对抗训练难以模拟有效合作行为，导致泛化能力受限。

Method: 结合预训练生成模型和对抗训练，动态生成挑战性协调策略，同时冻结生成模型参数以避免对抗性利用。

Result: 在Overcooked基准测试中实现了最先进的性能，能够泛化到多样化的人类行为。

Conclusion: GOAT方法有效解决了合作任务中的对抗训练问题，提升了泛化能力。

Abstract: Being able to cooperate with new people is an important component of many
economically valuable AI tasks, from household robotics to autonomous driving.
However, generalizing to novel humans requires training on data that captures
the diversity of human behaviors. Adversarial training is one avenue for
searching for such data and ensuring that agents are robust. However, it is
difficult to apply in the cooperative setting because adversarial policies
intentionally learn to sabotage the task instead of simulating valid
cooperation partners. To address this challenge, we propose a novel strategy
for overcoming self-sabotage that combines a pre-trained generative model to
simulate valid cooperative agent policies with adversarial training to maximize
regret. We call our method GOAT: Generative Online Adversarial Training. In
this framework, the GOAT dynamically searches for and generates coordination
strategies where the learning policy -- the Cooperator agent -- underperforms.
GOAT enables better generalization by exposing the Cooperator to various
challenging interaction scenarios. We maintain realistic coordination
strategies by updating only the generative model's embedding while keeping its
parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT
with real human partners, and the results demonstrate state-of-the-art
performance on the Overcooked benchmark, highlighting its effectiveness in
generalizing to diverse human behaviors.

</details>

### [117] [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
*Jiayi Pan,Xiuyu Li,Long Lian,Charlie Snell,Yifei Zhou,Adam Yala,Trevor Darrell,Kurt Keutzer,Alane Suhr*

Main category: cs.AI

TLDR: 论文提出了一种名为自适应并行推理（APR）的新框架，通过结合串行和并行计算优化语言模型的推理能力，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法（如链式思维或自一致性）存在输出过长、协调不足等问题，限制了推理性能和效率。

Method: APR采用自适应多线程推理，结合spawn()和join()操作，并通过端到端强化学习优化推理线程。

Result: 实验表明，APR在相同上下文窗口下性能更高（83.4% vs. 60.0%），计算扩展性更好（80.1% vs. 66.6%），且在相同延迟下准确性更高（75.2% vs. 57.3%）。

Conclusion: APR为语言模型通过自适应计算分配优化推理过程提供了新方向。

Abstract: Scaling inference-time computation has substantially improved the reasoning
capabilities of language models. However, existing methods have significant
limitations: serialized chain-of-thought approaches generate overly long
outputs, leading to increased latency and exhausted context windows, while
parallel methods such as self-consistency suffer from insufficient
coordination, resulting in redundant computations and limited performance
gains. To address these shortcomings, we propose Adaptive Parallel Reasoning
(APR), a novel reasoning framework that enables language models to orchestrate
both serialized and parallel computations end-to-end. APR generalizes existing
reasoning methods by enabling adaptive multi-threaded inference using spawn()
and join() operations. A key innovation is our end-to-end reinforcement
learning strategy, optimizing both parent and child inference threads to
enhance task success rate without requiring predefined reasoning structures.
Experiments on the Countdown reasoning task demonstrate significant benefits of
APR: (1) higher performance within the same context window (83.4% vs. 60.0% at
4k context); (2) superior scalability with increased computation (80.1% vs.
66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%
vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling
language models to autonomously optimize their reasoning processes through
adaptive allocation of computation.

</details>

### [118] [A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models](https://arxiv.org/abs/2504.15552)
*Gengxian Cao,Fengyuan Li,Hong Duan,Ye Yang,Bofeng Wang,Donghe Li*

Main category: cs.AI

TLDR: 本文提出了一种多智能体框架，通过整合大语言模型、视觉生成和文本转语音技术，自动化完成秦腔戏曲的端到端制作。三个智能体协作：Agent1生成剧本，Agent2生成舞台场景，Agent3生成语音表演。案例研究显示系统表现优于单智能体基线，模块化协作具有重要价值。


<details>
  <summary>Details</summary>
Motivation: 通过AI技术保护和规模化传统表演艺术的传承，提升秦腔戏曲的制作效率和质量。

Method: 采用三个专门智能体协作：Agent1（剧本生成）、Agent2（视觉生成）、Agent3（语音合成）。

Result: 在《窦娥冤》案例中，系统获得专家评分3.6分，比单智能体基线高0.3分。去除Agent2或Agent3分别导致评分下降0.4和0.5分。

Conclusion: AI驱动的多智能体框架能有效提升传统艺术制作效率，未来可优化跨模态对齐、情感表达和支持更多戏曲类型。

Abstract: This paper introduces a novel multi-Agent framework that automates the end to
end production of Qinqiang opera by integrating Large Language Models , visual
generation, and Text to Speech synthesis. Three specialized agents collaborate
in sequence: Agent1 uses an LLM to craft coherent, culturally grounded
scripts;Agent2 employs visual generation models to render contextually accurate
stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally
expressive vocal performances. In a case study on Dou E Yuan, the system
achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,
and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point
improvement over a Single Agent baseline. Ablation experiments demonstrate that
removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,
underscoring the value of modular collaboration. This work showcases how AI
driven pipelines can streamline and scale the preservation of traditional
performing arts, and points toward future enhancements in cross modal
alignment, richer emotional nuance, and support for additional opera genres.

</details>

### [119] [A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings](https://arxiv.org/abs/2504.15610)
*Md Millat,Md Motiur*

Main category: cs.AI

TLDR: 该研究提出了一种低成本方法，通过LoRA和4位量化技术优化Mistral-7B-Instruct模型，用于学术留学咨询，并在低资源环境中实现高效训练和性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为低资源环境下的学术留学咨询提供高效、低成本的语言模型解决方案，同时保持计算效率。

Method: 采用LoRA和4位量化技术，分两阶段训练模型：第一阶段使用合成数据集（Gemini Pro API生成），第二阶段使用手动标注数据集（StudyAbroadGPT项目）。

Result: 训练损失减少52.7%，领域推荐准确率达92%，支持95%的Markdown格式，并在普通GPU上实现每秒100样本的处理速度。

Conclusion: 该方法适用于教育咨询领域，尤其是在低资源环境中，但存在通用性不足和依赖合成数据的局限性。未来可扩展多语言和实时数据库集成。

Abstract: The current study describes a cost-effective method for adapting large
language models (LLMs) for academic advising with study-abroad contexts in mind
and for application in low-resource methods for acculturation. With the
Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and
a 4-bit quantization method, the model underwent training in two distinct
stages related to this study's purpose to enhance domain specificity while
maintaining computational efficiency. In Phase 1, the model was conditioned
with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained
with manually curated datasets from the StudyAbroadGPT project to achieve
enhanced, contextualized responses. Technical innovations entailed
memory-efficient quantization, parameter-efficient adaptation, and continuous
training analytics via Weights & Biases. After training, this study
demonstrated a reduction in training loss by 52.7%, 92% accuracy in
domain-specific recommendations, achieved 95% markdown-based formatting
support, and a median run-rate of 100 samples per second on off-the-shelf GPU
equipment. These findings support the effective application of
instruction-tuned LLMs within educational advisers, especially in low-resource
institutional scenarios. Limitations included decreased generalizability and
the application of a synthetically generated dataset, but this framework is
scalable for adding new multilingual-augmented and real-time academic advising
processes. Future directions may include plans for the integration of
retrieval-augmented generation, applying dynamic quantization routines, and
connecting to real-time academic databases to increase adaptability and
accuracy.

</details>

### [120] [Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems](https://arxiv.org/abs/2504.15668)
*Mir Md Sajid Sarwar,Rajarshi Ray*

Main category: cs.AI

TLDR: 该论文提出了一种通过识别通用障碍点（waypoints）来解释规划问题不可解性的方法，并将其建模为最长公共子序列问题。


<details>
  <summary>Details</summary>
Motivation: 当前解释规划问题不可解性的研究较少，而分解任务为子问题的方法在规划中广泛使用，因此作者希望利用类似方法分析不可解性。

Method: 提出识别通用障碍点作为子问题，并将其建模为最长公共子序列问题，通过符号可达性分析确定最早不可达点作为解释。

Result: 实验结果表明，该方法能有效识别并解释混合域中不可解规划问题的原因。

Conclusion: 通过识别通用障碍点并提供可达性分析，该方法为解释规划问题不可解性提供了新思路。

Abstract: Explaining unsolvability of planning problems is of significant research
interest in Explainable AI Planning. AI planning literature has reported
several research efforts on generating explanations of solutions to planning
problems. However, explaining the unsolvability of planning problems remains a
largely open and understudied problem. A widely practiced approach to plan
generation and automated problem solving, in general, is to decompose tasks
into sub-problems that help progressively converge towards the goal. In this
paper, we propose to adopt the same philosophy of sub-problem identification as
a mechanism for analyzing and explaining unsolvability of planning problems in
hybrid systems. In particular, for a given unsolvable planning problem, we
propose to identify common waypoints, which are universal obstacles to plan
existence; in other words, they appear on every plan from the source to the
planning goal. This work envisions such waypoints as sub-problems of the
planning problem and the unreachability of any of these waypoints as an
explanation for the unsolvability of the original planning problem. We propose
a novel method of waypoint identification by casting the problem as an instance
of the longest common subsequence problem, a widely popular problem in computer
science, typically considered as an illustrative example for the dynamic
programming paradigm. Once the waypoints are identified, we perform symbolic
reachability analysis on them to identify the earliest unreachable waypoint and
report it as the explanation of unsolvability. We present experimental results
on unsolvable planning problems in hybrid domains.

</details>

### [121] [Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation](https://arxiv.org/abs/2504.15699)
*Ning Wang,Zihan Yan,Weiyang Li,Chuan Ma,He Chen,Tao Xiang*

Main category: cs.AI

TLDR: 本文提出了一种专为具身代理设计的输入审核框架，包括EAsafetyBench安全基准和Pinpoint方案，显著提升了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通用大语言模型的安全性，缺乏针对具身代理的专门方法，需填补这一空白。

Method: 提出输入审核框架，涵盖分类定义、数据集构建、审核架构、模型训练和评估；引入EAsafetyBench基准和Pinpoint方案。

Result: 实验显示，方法平均检测准确率达94.58%，处理时间仅0.002秒/实例，优于现有技术。

Conclusion: 该框架为具身代理提供了高效、精准的安全保障，具有广泛应用潜力。

Abstract: Embodied agents exhibit immense potential across a multitude of domains,
making the assurance of their behavioral safety a fundamental prerequisite for
their widespread deployment. However, existing research predominantly
concentrates on the security of general large language models, lacking
specialized methodologies for establishing safety benchmarks and input
moderation tailored to embodied agents. To bridge this gap, this paper
introduces a novel input moderation framework, meticulously designed to
safeguard embodied agents. This framework encompasses the entire pipeline,
including taxonomy definition, dataset curation, moderator architecture, model
training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a
meticulously crafted safety benchmark engineered to facilitate both the
training and stringent assessment of moderators specifically designed for
embodied agents. Furthermore, we propose Pinpoint, an innovative
prompt-decoupled input moderation scheme that harnesses a masked attention
mechanism to effectively isolate and mitigate the influence of functional
prompts on moderation tasks. Extensive experiments conducted on diverse
benchmark datasets and models validate the feasibility and efficacy of the
proposed approach. The results demonstrate that our methodologies achieve an
impressive average detection accuracy of 94.58%, surpassing the performance of
existing state-of-the-art techniques, alongside an exceptional moderation
processing time of merely 0.002 seconds per instance.

</details>

### [122] [DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models](https://arxiv.org/abs/2504.15716)
*Jie Zhu,Qian Chen,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.AI

TLDR: DianJin-R1是一个增强推理能力的框架，通过监督学习和强化学习提升大型语言模型在金融领域的表现。


<details>
  <summary>Details</summary>
Motivation: 金融领域任务需要专业知识、精确计算和合规性，现有模型推理能力不足。

Method: 构建高质量数据集DianJin-R1-Data，结合监督学习和GRPO强化学习方法优化模型推理。

Result: DianJin-R1模型在金融和通用推理任务中表现优异，尤其在复杂金融任务上超越非推理模型。

Conclusion: DianJin-R1通过结构化监督和奖励对齐学习，为金融推理提供了可扩展的解决方案。

Abstract: Effective reasoning remains a core challenge for large language models (LLMs)
in the financial domain, where tasks often require domain-specific knowledge,
precise numerical calculations, and strict adherence to compliance rules. We
propose DianJin-R1, a reasoning-enhanced framework designed to address these
challenges through reasoning-augmented supervision and reinforcement learning.
Central to our approach is DianJin-R1-Data, a high-quality dataset constructed
from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance
Check, CCC), combining diverse financial reasoning scenarios with verified
annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from
Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that
generates both reasoning steps and final answers. To further refine reasoning
quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement
learning method that incorporates dual reward signals: one encouraging
structured outputs and another rewarding answer correctness. We evaluate our
models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and
two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental
results show that DianJin-R1 models consistently outperform their non-reasoning
counterparts, especially on complex financial tasks. Moreover, on the
real-world CCC dataset, our single-call reasoning models match or even surpass
the performance of multi-agent systems that require significantly more
computational cost. These findings demonstrate the effectiveness of DianJin-R1
in enhancing financial reasoning through structured supervision and
reward-aligned learning, offering a scalable and practical solution for
real-world applications.

</details>

### [123] [Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences](https://arxiv.org/abs/2504.15719)
*Anna Karnysheva,Christian Drescher,Dietrich Klakow*

Main category: cs.AI

TLDR: 本文探讨了大型语言模型（LLMs）作为决策代理时与用户偏好对齐的问题，提出了一种更灵活的偏好对齐方法，并通过实证研究验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在智能用户界面（IUIs）中的广泛应用，其作为决策代理的对齐问题（尤其是与用户偏好的对齐）尚未得到充分研究。本文旨在填补这一空白。

Method: 作者扩展了现有方法，利用LLMs对替代结果进行排序，并引入更广泛的用户偏好概念（包括严格偏好和无差异）。提出了设计原则和测量工具。

Result: 通过汽车领域的实证研究，验证了所提方法的适用性。

Conclusion: 本文为LLMs在决策代理中的偏好对齐提供了理论和实践工具，具有实际应用价值。

Abstract: As large language models (LLMs) become integral to intelligent user
interfaces (IUIs), their role as decision-making agents raises critical
concerns about alignment. Although extensive research has addressed issues such
as factuality, bias, and toxicity, comparatively little attention has been paid
to measuring alignment to preferences, i.e., the relative desirability of
different alternatives, a concept used in decision making, economics, and
social choice theory. However, a reliable decision-making agent makes choices
that align well with user preferences.
  In this paper, we generalize existing methods that exploit LLMs for ranking
alternative outcomes by addressing alignment with the broader and more flexible
concept of user preferences, which includes both strict preferences and
indifference among alternatives. To this end, we put forward design principles
for using LLMs to implement rational choice functions, and provide the
necessary tools to measure preference satisfaction. We demonstrate the
applicability of our approach through an empirical study in a practical
application of an IUI in the automotive domain.

</details>

### [124] [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
*Daocheng Fu,Zijun Chen,Renqiu Xia,Qi Liu,Yuan Feng,Hongbin Zhou,Renrui Zhang,Shiyang Feng,Peng Gao,Junchi Yan,Botian Shi,Bo Zhang,Yu Qiao*

Main category: cs.AI

TLDR: 本文提出了一种名为TrustGeoGen的可扩展数据引擎，用于生成几何问题，并通过形式化验证提供基准，解决了现有合成几何问题基准的噪声和自我矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在几何问题解决中面临方法学和基准的不足，尤其是合成基准的噪声和自我矛盾问题。

Method: TrustGeoGen引擎通过多模态对齐生成、形式化验证、递归状态生成和GeoExplore算法，生成具有模态完整性的几何问题数据集。

Result: 实验表明，现有模型在GeoTrust-test上的准确率仅为49.17%，而基于GeoTrust训练的模型在GeoQA上表现出更强的泛化能力。

Conclusion: TrustGeoGen为几何问题解决提供了可靠的基准和方法基础，显著减少了逻辑不一致性。

Abstract: Mathematical geometric problem solving (GPS) often requires effective
integration of multimodal information and verifiable logical coherence. Despite
the fast development of large language models in general problem solving, it
remains unresolved regarding with both methodology and benchmarks, especially
given the fact that exiting synthetic GPS benchmarks are often not
self-verified and contain noise and self-contradicted information due to the
illusion of LLMs. In this paper, we propose a scalable data engine called
TrustGeoGen for problem generation, with formal verification to provide a
principled benchmark, which we believe lays the foundation for the further
development of methods for GPS. The engine synthesizes geometric data through
four key innovations: 1) multimodal-aligned generation of diagrams, textual
descriptions, and stepwise solutions; 2) formal verification ensuring
rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling
complexity escalation via recursive state generation and 4) our devised
GeoExplore series algorithms simultaneously produce multi-solution variants and
self-reflective backtracking traces. By formal logical verification,
TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,
along with GeoTrust-test testset. Experiments reveal the state-of-the-art
models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its
evaluation stringency. Crucially, models trained on GeoTrust achieve OOD
generalization on GeoQA, significantly reducing logical inconsistencies
relative to pseudo-label annotated by OpenAI-o1. Our code is available at
https://github.com/Alpha-Innovator/TrustGeoGen

</details>

### [125] [WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents](https://arxiv.org/abs/2504.15785)
*Siyu Zhou,Tianyi Zhou,Yijun Yang,Guodong Long,Deheng Ye,Jing Jiang,Chengqi Zhang*

Main category: cs.AI

TLDR: 论文提出了一种无需训练的'世界对齐'方法，通过提取环境符号知识（如动作规则、知识图谱和场景图）来补充LLMs的先验知识，并构建了RL-free的模型预测控制代理WALL-E 2.0，显著提升了在新环境中的学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs作为世界模型时，其先验知识与特定环境动态之间的差距问题。

Method: 提出'世界对齐'方法，提取环境符号知识并编码为可执行代码；构建基于模型预测控制的代理WALL-E 2.0，利用LLM作为高效的前瞻优化器。

Result: 在Mars和ALFWorld环境中，WALL-E 2.0显著优于现有方法，如Mars中成功率提升16.1%-51.6%，ALFWorld中仅4次迭代即达到98%成功率。

Conclusion: 通过结合LLMs的启发式能力和对齐世界模型的准确预测，WALL-E 2.0高效且性能优越。

Abstract: Can we build accurate world models out of large language models (LLMs)? How
can world models benefit LLM agents? The gap between the prior knowledge of
LLMs and the specified environment's dynamics usually bottlenecks LLMs'
performance as world models. To bridge the gap, we propose a training-free
"world alignment" that learns an environment's symbolic knowledge complementary
to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and
scene graphs, which are extracted by LLMs from exploration trajectories and
encoded into executable codes to regulate LLM agents' policies. We further
propose an RL-free, model-based agent "WALL-E 2.0" through the model-predictive
control (MPC) framework. Unlike classical MPC requiring costly optimization on
the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future
steps' actions by interacting with the neurosymbolic world model. While the LLM
agent's strong heuristics make it an efficient planner in MPC, the quality of
its planned actions is also secured by the accurate predictions of the aligned
world model. They together considerably improve learning efficiency in a new
environment. On open-world challenges in Mars (Minecraft like) and ALFWorld
(embodied indoor environments), WALL-E 2.0 significantly outperforms existing
methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and
by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success
rate after only 4 iterations.

</details>

### [126] [Crisp complexity of fuzzy classifiers](https://arxiv.org/abs/2504.15791)
*Raquel Fernandez-Peralta,Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TLDR: 提出一种将模糊规则分类器简化为清晰规则分类器的方法，分析其复杂性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 模糊规则分类器在非模糊领域应用受限，因其不易解释。希望通过简化提升其普适性。

Method: 研究不同清晰描述方法，并实现算法将其从模糊规则转化为清晰规则。

Result: 提出复杂度度量，帮助选择模糊分类器及其清晰等效形式。

Conclusion: 该方法有助于模糊和非模糊领域用户理解规则分类器的空间划分和转换。

Abstract: Rule-based systems are a very popular form of explainable AI, particularly in
the fuzzy community, where fuzzy rules are widely used for control and
classification problems. However, fuzzy rule-based classifiers struggle to
reach bigger traction outside of fuzzy venues, because users sometimes do not
know about fuzzy and because fuzzy partitions are not so easy to interpret in
some situations. In this work, we propose a methodology to reduce fuzzy
rule-based classifiers to crisp rule-based classifiers. We study different
possible crisp descriptions and implement an algorithm to obtain them. Also, we
analyze the complexity of the resulting crisp classifiers. We believe that our
results can help both fuzzy and non-fuzzy practitioners understand better the
way in which fuzzy rule bases partition the feature space and how easily one
system can be translated to another and vice versa. Our complexity metric can
also help to choose between different fuzzy classifiers based on what the
equivalent crisp partitions look like.

</details>

### [127] [Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases](https://arxiv.org/abs/2504.15829)
*Modhurita Mitra,Martine G. de Vos,Nicola Cortinovis,Dawa Ometto*

Main category: cs.AI

TLDR: 探索性研究验证生成式AI在复杂数据处理任务中的可行性，包括信息提取、自然语言理解和文本分类，并总结了使用生成式AI的适用性和优化方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如ChatGPT）的兴起引发了对输出准确性和一致性的担忧，研究旨在探索其在研究数据处理中的应用潜力。

Method: 选择传统方法难以处理的任务，使用生成式AI模型Claude 3 Opus完成三项复杂数据处理任务：信息提取、自然语言理解和文本分类。

Result: 成功验证了生成式AI在三种复杂任务中的可行性，并总结了如何判断其适用性及优化结果准确性和一致性的经验。

Conclusion: 生成式AI在特定复杂数据处理任务中具有潜力，但需谨慎评估适用性并优化方法以确保结果质量。

Abstract: There has been enormous interest in generative AI since ChatGPT was launched
in 2022. However, there are concerns about the accuracy and consistency of the
outputs of generative AI. We have carried out an exploratory study on the
application of this new technology in research data processing. We identified
tasks for which rule-based or traditional machine learning approaches were
difficult to apply, and then performed these tasks using generative AI.
  We demonstrate the feasibility of using the generative AI model Claude 3 Opus
in three research projects involving complex data processing tasks:
  1) Information extraction: We extract plant species names from historical
seedlists (catalogues of seeds) published by botanical gardens.
  2) Natural language understanding: We extract certain data points (name of
drug, name of health indication, relative effectiveness, cost-effectiveness,
etc.) from documents published by Health Technology Assessment organisations in
the EU.
  3) Text classification: We assign industry codes to projects on the
crowdfunding website Kickstarter.
  We share the lessons we learnt from these use cases: How to determine if
generative AI is an appropriate tool for a given data processing task, and if
so, how to maximise the accuracy and consistency of the results obtained.

</details>

### [128] [CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters](https://arxiv.org/abs/2504.15847)
*Xiang Liu,Hau Chan,Minming Li,Xianlong Zeng,Chenchen Fu,Weiwei Wu*

Main category: cs.AI

TLDR: 本文研究了联邦学习（FL）中多预算请求者与不兼容工人之间的激励机制，提出了兼容性感知的激励机制CARE-CO和CARE-NO，以解决工人不兼容和预算限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有激励机制忽略了工人不兼容性和请求者预算限制，导致FL效率下降。本文旨在解决这些问题。

Method: 提出两种兼容性感知激励机制（CARE-CO和CARE-NO），分别针对合作预算和非合作预算场景，确保工人真实成本揭示和预算可行性。

Result: 实验表明，所提机制在真实数据集上显著优于现有基线，满足个体理性、真实性、预算可行性和近似性能。

Conclusion: CARE-CO和CARE-NO有效解决了FL中工人不兼容和预算限制问题，提升了整体效用。

Abstract: Federated learning (FL) is a promising approach that allows requesters (\eg,
servers) to obtain local training models from workers (e.g., clients). Since
workers are typically unwilling to provide training services/models freely and
voluntarily, many incentive mechanisms in FL are designed to incentivize
participation by offering monetary rewards from requesters. However, existing
studies neglect two crucial aspects of real-world FL scenarios. First, workers
can possess inherent incompatibility characteristics (e.g., communication
channels and data sources), which can lead to degradation of FL efficiency
(e.g., low communication efficiency and poor model generalization). Second, the
requesters are budgeted, which limits the amount of workers they can hire for
their tasks. In this paper, we investigate the scenario in FL where multiple
budgeted requesters seek training services from incompatible workers with
private training costs. We consider two settings: the cooperative budget
setting where requesters cooperate to pool their budgets to improve their
overall utility and the non-cooperative budget setting where each requester
optimizes their utility within their own budgets. To address efficiency
degradation caused by worker incompatibility, we develop novel
compatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both
settings to elicit true private costs and determine workers to hire for
requesters and their rewards while satisfying requester budget constraints. Our
mechanisms guarantee individual rationality, truthfulness, budget feasibility,
and approximation performance. We conduct extensive experiments using
real-world datasets to show that the proposed mechanisms significantly
outperform existing baselines.

</details>

### [129] [Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations](https://arxiv.org/abs/2504.15903)
*Nikhil Khandalkar,Pavan Yadav,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.AI

TLDR: 论文研究了大型语言模型（LLMs）在抽象推理任务中的表现，发现即使如GPT-4o在无噪声条件下表现优异，但其他模型如DeepSeek R1和LLaMA 3.2表现不佳。噪声的引入会显著降低所有模型的性能，揭示了当前LLMs在现实场景中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在抽象推理任务中的表现差异，特别是在噪声环境下的鲁棒性，以评估其实际应用潜力。

Method: 通过系统性地评估不同模型（如GPT-4o、DeepSeek R1和LLaMA 3.2）在不同噪声水平和温度设置下的表现，分析其推理能力。

Result: 噪声显著降低了所有模型的性能，表明当前LLMs在输入扰动下表现脆弱，缺乏现实场景所需的鲁棒性。

Conclusion: 当前LLMs在抽象推理任务中仍存在局限性，需开发更具鲁棒性和适应性的AI系统以应对现实世界的不确定性。

Abstract: Recent advancements in Large Language Models (LLMs) have generated growing
interest in their structured reasoning capabilities, particularly in tasks
involving abstraction and pattern recognition. The Abstraction and Reasoning
Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by
testing how well AI models generalize to novel problems. While GPT-4o
demonstrates strong performance by solving all ARC tasks under zero-noise
conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,
suggesting limitations in their ability to reason beyond simple pattern
matching. To explore this gap, we systematically evaluate these models across
different noise levels and temperature settings. Our results reveal that the
introduction of noise consistently impairs model performance, regardless of
architecture. This decline highlights a shared vulnerability: current LLMs,
despite showing signs of abstract reasoning, remain highly sensitive to input
perturbations. Such fragility raises concerns about their real-world
applicability, where noise and uncertainty are common. By comparing how
different model architectures respond to these challenges, we offer insights
into the structural weaknesses of modern LLMs in reasoning tasks. This work
underscores the need for developing more robust and adaptable AI systems
capable of handling the ambiguity and variability inherent in real-world
scenarios. Our findings aim to guide future research toward enhancing model
generalization, robustness, and alignment with human-like cognitive
flexibility.

</details>

### [130] [Approximate matrices of systems of max-min fuzzy relational equations](https://arxiv.org/abs/2504.16042)
*Ismaïl Baaj*

Main category: cs.AI

TLDR: 本文提出了一种通过最小化修改矩阵来解决max-min模糊关系方程系统不一致性的方法，确保一致性同时尽量保留原系统特性。


<details>
  <summary>Details</summary>
Motivation: 解决max-min模糊关系方程系统的不一致性问题，同时最小化对原系统的修改。

Method: 通过最小化修改矩阵的条目来达到一致性，并研究不同范数（L1、L2、L∞）下矩阵间的距离。

Result: 方法能直接计算与原系统右端向量一致的矩阵，且L∞范数下距离最小，同时给出了L∞距离的解析公式。

Conclusion: 该方法有效且计算高效，适用于min-max模糊关系方程系统，并具有潜在应用价值。

Abstract: In this article, we address the inconsistency of a system of max-min fuzzy
relational equations by minimally modifying the matrix governing the system in
order to achieve consistency. Our method yields consistent systems that
approximate the original inconsistent system in the following sense: the
right-hand side vector of each consistent system is that of the inconsistent
system, and the coefficients of the matrix governing each consistent system are
obtained by modifying, exactly and minimally, the entries of the original
matrix that must be corrected to achieve consistency, while leaving all other
entries unchanged.
  To obtain a consistent system that closely approximates the considered
inconsistent system, we study the distance (in terms of a norm among $L_1$,
$L_2$ or $L_\infty$) between the matrix of the inconsistent system and the set
formed by the matrices of consistent systems that use the same right-hand side
vector as the inconsistent system. We show that our method allows us to
directly compute matrices of consistent systems that use the same right-hand
side vector as the inconsistent system whose distance in terms of $L_\infty$
norm to the matrix of the inconsistent system is minimal (the computational
costs are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit
analytical formula for computing this minimal $L_\infty$ distance. Finally, we
translate our results for systems of min-max fuzzy relational equations and
present some potential applications.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [131] [LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation](https://arxiv.org/abs/2504.15309)
*Anran Yu,Wei Feng,Yaochen Zhang,Xiang Li,Lei Meng,Lei Wu,Xiangxu Meng*

Main category: cs.CV

TLDR: 本文提出了一种改进个性化文本到图像生成的方法，通过风格优化和内容保留策略，解决了现有方法在风格化和内容准确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化文本到图像生成中常因嵌入标识符的微调而导致风格化不足和内容不准确，本文旨在解决这些问题。

Method: 采用风格优化策略（利用视觉推理提示和参考图像的语义信息优化风格嵌入）和内容保留策略（保持模型泛化能力以增强文本可控性）。

Result: 实验结果表明，该方法在生成一致且个性化的文本到图像输出方面表现优异。

Conclusion: 本文提出的策略有效提升了文本到图像生成的风格化和内容准确性。

Abstract: The personalized text-to-image generation has rapidly advanced with the
emergence of Stable Diffusion. Existing methods, which typically fine-tune
models using embedded identifiers, often struggle with insufficient stylization
and inaccurate image content due to reduced textual controllability. In this
paper, we propose style refinement and content preservation strategies. The
style refinement strategy leverages the semantic information of visual
reasoning prompts and reference images to optimize style embeddings, allowing a
more precise and consistent representation of style information. The content
preservation strategy addresses the content bias problem by preserving the
model's generalization capabilities, ensuring enhanced textual controllability
without compromising stylization. Experimental results verify that our approach
achieves superior performance in generating consistent and personalized
text-to-image outputs.

</details>

### [132] [LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception](https://arxiv.org/abs/2504.15362)
*Yuan-Hong Liao,Sven Elflein,Liu He,Laura Leal-Taixé,Yejin Choi,Sanja Fidler,David Acuna*

Main category: cs.CV

TLDR: 论文提出LongPerceptualThoughts数据集，通过三阶段合成框架生成长思维链，显著提升视觉推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 探索长思维链在感知任务中的潜力，解决现有模型缺乏此类行为且验证困难的问题。

Method: 三阶段数据合成框架：1) 从密集图像描述生成可验证选择题；2) 从视觉语言模型提取简单思维链；3) 扩展为长思维链。

Result: 在5个视觉基准测试中平均提升3.4分，V$^*$ Bench提升11.8分；文本推理任务MMLU-Pro也提升2分。

Conclusion: 长思维链对感知任务有效，且能泛化到文本推理任务。

Abstract: Recent reasoning models through test-time scaling have demonstrated that long
chain-of-thoughts can unlock substantial performance boosts in hard reasoning
tasks such as math and code. However, the benefit of such long thoughts for
system-2 reasoning is relatively less explored in other domains such as
perceptual tasks where shallower, system-1 reasoning seems sufficient. In this
paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K
long-thought traces for perceptual tasks. The key challenges in synthesizing
elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models
are not yet equipped with such thinking behavior and that it is not
straightforward to build a reliable process verifier for perceptual tasks.
Thus, we propose a novel three-stage data synthesis framework that first
synthesizes verifiable multiple-choice questions from dense image descriptions,
then extracts simple CoTs from VLMs for those verifiable problems, and finally
expands those simple thoughts to elaborate long thoughts via frontier reasoning
models. In controlled experiments with a strong instruction-tuned 7B model, we
demonstrate notable improvements over existing visual reasoning data-generation
methods. Our model, trained on the generated dataset, achieves an average +3.4
points improvement over 5 vision-centric benchmarks, including +11.8 points on
V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves
performance on the text reasoning benchmark, MMLU-Pro, by +2 points.

</details>

### [133] [Event2Vec: Processing neuromorphic events directly by representations in vector space](https://arxiv.org/abs/2504.15371)
*Wei Fang,Priyadarshini Panda*

Main category: cs.CV

TLDR: 论文提出了一种名为event2vec的事件表示方法，将事件数据转换为向量形式，解决了事件相机数据与主流计算机视觉方法不兼容的问题，并在ASL-DVS数据集上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据具有异步、稀疏和不规则的特点，与传统计算机视觉方法不兼容，现有解决方案存在预处理复杂、丢失时间分辨率或并行计算不兼容等问题。

Method: 受词向量（word2vec）启发，提出event2vec方法，将事件数据转换为向量表示。

Result: 在ASL-DVS数据集分类任务中，event2vec表现出更高的参数效率、准确性和速度，优于基于图/图像/体素的表示方法。

Conclusion: event2vec不仅性能优越，还能将事件数据与自然语言处理领域对齐，为事件数据融入大型语言和多模态模型提供了可能。

Abstract: The neuromorphic event cameras have overwhelming advantages in temporal
resolution, power efficiency, and dynamic range compared to traditional
cameras. However, the event cameras output asynchronous, sparse, and irregular
events, which are not compatible with mainstream computer vision and deep
learning methods. Various methods have been proposed to solve this issue but at
the cost of long preprocessing procedures, losing temporal resolutions, or
being incompatible with massively parallel computation. Inspired by the great
success of the word to vector, we summarize the similarities between words and
events, then propose the first event to vector (event2vec) representation. We
validate event2vec on classifying the ASL-DVS dataset, showing impressive
parameter efficiency, accuracy, and speed than previous graph/image/voxel-based
representations. Beyond task performance, the most attractive advantage of
event2vec is that it aligns events to the domain of natural language
processing, showing the promising prospect of integrating events into large
language and multimodal models. Our codes, models, and training logs are
available at https://github.com/fangwei123456/event2vec.

</details>

### [134] [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
*Zhiqiu Lin,Siyuan Cen,Daniel Jiang,Jay Karhade,Hewei Wang,Chancharik Mitra,Tiffany Ling,Yuhan Huang,Sifan Liu,Mingyu Chen,Rushikesh Zawar,Xue Bai,Yilun Du,Chuang Gan,Deva Ramanan*

Main category: cs.CV

TLDR: CameraBench是一个大规模数据集和基准测试，用于评估和改进相机运动理解。它包含约3000个多样化视频，由专家标注，并提出了相机运动分类法。研究发现，专家标注和培训能显著提高准确性。通过评估SfM和VLMs模型，发现各自在语义和几何运动理解上的不足，并通过微调生成式VLM展示了应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前对相机运动理解的研究缺乏标准化数据集和评估方法，CameraBench旨在填补这一空白，推动相机运动理解的进一步发展。

Method: 构建了包含3000个多样化视频的数据集，设计了相机运动分类法，并通过专家标注和培训提高标注质量。评估了SfM和VLMs模型，并微调生成式VLM。

Result: SfM模型在语义运动理解上表现不佳，VLMs在几何运动理解上不足。微调后的生成式VLM在多个应用中表现优异。

Conclusion: CameraBench为相机运动理解提供了标准化工具，未来有望推动该领域的进一步发展。

Abstract: We introduce CameraBench, a large-scale dataset and benchmark designed to
assess and improve camera motion understanding. CameraBench consists of ~3,000
diverse internet videos, annotated by experts through a rigorous multi-stage
quality control process. One of our contributions is a taxonomy of camera
motion primitives, designed in collaboration with cinematographers. We find,
for example, that some motions like "follow" (or tracking) require
understanding scene content like moving subjects. We conduct a large-scale
human study to quantify human annotation performance, revealing that domain
expertise and tutorial-based training can significantly enhance accuracy. For
example, a novice may confuse zoom-in (a change of intrinsics) with translating
forward (a change of extrinsics), but can be trained to differentiate the two.
Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language
Models (VLMs), finding that SfM models struggle to capture semantic primitives
that depend on scene content, while VLMs struggle to capture geometric
primitives that require precise estimation of trajectories. We then fine-tune a
generative VLM on CameraBench to achieve the best of both worlds and showcase
its applications, including motion-augmented captioning, video question
answering, and video-text retrieval. We hope our taxonomy, benchmark, and
tutorials will drive future efforts towards the ultimate goal of understanding
camera motions in any video.

</details>

### [135] [Physics Driven Image Simulation from Commercial Satellite Imagery](https://arxiv.org/abs/2504.15378)
*Scott Sorensen,Wayne Treible,Robert Wagner,Andrew D. Gilliam,Todd Rovito,Joseph L. Mundy*

Main category: cs.CV

TLDR: 利用卫星图像自动生成物理真实的3D场景，无需激光雷达，提高仿真效率和保真度。


<details>
  <summary>Details</summary>
Motivation: 通过物理驱动的图像仿真，超越传统渲染管道的限制，自动生成真实场景，减少人工干预。

Method: 基于数字表面模型（DSM）构建场景几何，利用卫星图像估计材质并填充动态元素（如植被、车辆）。

Result: 实现了无需激光雷达的高保真3D场景仿真，适用于从紫外线到长波红外的多种波段。

Conclusion: 该方法显著提升了场景构建效率，支持算法开发和图像处理管道的优化。

Abstract: Physics driven image simulation allows for the modeling and creation of
realistic imagery beyond what is afforded by typical rendering pipelines. We
aim to automatically generate a physically realistic scene for simulation of a
given region using satellite imagery to model the scene geometry, drive
material estimates, and populate the scene with dynamic elements. We present
automated techniques to utilize satellite imagery throughout the simulated
scene to expedite scene construction and decrease manual overhead. Our
technique does not use lidar, enabling simulations that could not be
constructed previously. To develop a 3D scene, we model the various components
of the real location, addressing the terrain, modelling man-made structures,
and populating the scene with smaller elements such as vegetation and vehicles.
To create the scene we begin with a Digital Surface Model, which serves as the
basis for scene geometry, and allows us to reason about the real location in a
common 3D frame of reference. These simulated scenes can provide increased
fidelity with less manual intervention for novel locations on earth, and can
facilitate algorithm development, and processing pipelines for imagery ranging
from UV to LWIR $(200nm-20\mu m)$.

</details>

### [136] [Plug-and-Play Versatile Compressed Video Enhancement](https://arxiv.org/abs/2504.15380)
*Huimin Zeng,Jiacheng Li,Zhiwei Xiong*

Main category: cs.CV

TLDR: 论文提出了一种基于编解码信息的视频增强框架，通过复用编解码信息自适应提升不同压缩设置下的视频质量，支持多种下游视觉任务，且不引入计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 视频压缩虽能减少文件大小，但会降低视觉质量，影响下游视觉模型的鲁棒性。本文旨在解决这一问题。

Method: 框架包含压缩感知适应网络（CAA）和比特流感知增强网络（BAE），利用编解码信息自适应增强视频质量。

Result: 实验表明，该框架在质量增强和辅助下游任务方面优于现有方法。

Conclusion: 该框架是一种高效的即插即用模块，适用于压缩视频的增强和下游任务。

Abstract: As a widely adopted technique in data transmission, video compression
effectively reduces the size of files, making it possible for real-time cloud
computing. However, it comes at the cost of visual quality, posing challenges
to the robustness of downstream vision models. In this work, we present a
versatile codec-aware enhancement framework that reuses codec information to
adaptively enhance videos under different compression settings, assisting
various downstream vision tasks without introducing computation bottleneck.
Specifically, the proposed codec-aware framework consists of a
compression-aware adaptation (CAA) network that employs a hierarchical
adaptation mechanism to estimate parameters of the frame-wise enhancement
network, namely the bitstream-aware enhancement (BAE) network. The BAE network
further leverages temporal and spatial priors embedded in the bitstream to
effectively improve the quality of compressed input frames. Extensive
experimental results demonstrate the superior quality enhancement performance
of our framework over existing enhancement methods, as well as its versatility
in assisting multiple downstream tasks on compressed videos as a plug-and-play
module. Code and models are available at
https://huimin-zeng.github.io/PnP-VCVE/.

</details>

### [137] [ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images](https://arxiv.org/abs/2504.15384)
*Chen Zhao,Anjum Shaik,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Qiuying Sha,Hui Shen,Hong-Wen Deng,Weihua Zhou*

Main category: cs.CV

TLDR: ICGM-FRAX是一种基于DXA图像和图形匹配的新方法，用于预测髋部骨折风险，具有高灵敏度。


<details>
  <summary>Details</summary>
Motivation: 髋部骨折对老年人健康影响重大，早期准确识别高风险个体是关键。

Method: 通过将DXA图像转换为图形，迭代比较测试图形与模板图形，评估相似性以预测风险。

Result: 在547名受试者中，ICGM-FRAX的灵敏度达到0.9869。

Conclusion: ICGM-FRAX能高效预测髋部骨折风险，适用于临床干预。

Abstract: Hip fractures represent a major health concern, particularly among the
elderly, often leading decreased mobility and increased mortality. Early and
accurate detection of at risk individuals is crucial for effective
intervention. In this study, we propose Iterative Cross Graph Matching for Hip
Fracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip
fractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX
involves iteratively comparing a test (subject) graph with multiple template
graphs representing the characteristics of hip fracture subjects to assess the
similarity and accurately to predict hip fracture risk. These graphs are
obtained as follows. The DXA images are separated into multiple regions of
interest (RoIs), such as the femoral head, shaft, and lesser trochanter.
Radiomic features are then calculated for each RoI, with the central
coordinates used as nodes in a graph. The connectivity between nodes is
established according to the Euclidean distance between these coordinates. This
process transforms each DXA image into a graph, where each node represents a
RoI, and edges derived by the centroids of RoIs capture the spatial
relationships between them. If the test graph closely matches a set of template
graphs representing subjects with incident hip fractures, it is classified as
indicating high hip fracture risk. We evaluated our method using 547 subjects
from the UK Biobank dataset, and experimental results show that ICGM-FRAX
achieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip
fractures.

</details>

### [138] [MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World](https://arxiv.org/abs/2504.15397)
*Ankit Dhiman,Manan Shah,R Venkatesh Babu*

Main category: cs.CV

TLDR: 提出了一种改进扩散模型生成逼真镜面反射的方法，通过数据增强和训练策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成镜面反射时难以完全遵循物理规律，且泛化能力不足。

Method: 引入随机物体定位、旋转和接地等数据增强，并采用三阶段训练课程。

Result: 生成的反射效果更逼真，泛化能力显著提升。

Conclusion: 方法有效解决了镜面反射生成的挑战，为实际场景应用提供了可能。

Abstract: Diffusion models have become central to various image editing tasks, yet they
often fail to fully adhere to physical laws, particularly with effects like
shadows, reflections, and occlusions. In this work, we address the challenge of
generating photorealistic mirror reflections using diffusion-based generative
models. Despite extensive training data, existing diffusion models frequently
overlook the nuanced details crucial to authentic mirror reflections. Recent
approaches have attempted to resolve this by creating synhetic datasets and
framing reflection generation as an inpainting task; however, they struggle to
generalize across different object orientations and positions relative to the
mirror. Our method overcomes these limitations by introducing key augmentations
into the synthetic data pipeline: (1) random object positioning, (2) randomized
rotations, and (3) grounding of objects, significantly enhancing generalization
across poses and placements. To further address spatial relationships and
occlusions in scenes with multiple objects, we implement a strategy to pair
objects during dataset generation, resulting in a dataset robust enough to
handle these complex scenarios. Achieving generalization to real-world scenes
remains a challenge, so we introduce a three-stage training curriculum to
develop the MirrorFusion 2.0 model to improve real-world performance. We
provide extensive qualitative and quantitative evaluations to support our
approach. The project page is available at: https://mirror-verse.github.io/.

</details>

### [139] [Context Aware Grounded Teacher for Source Free Object Detection](https://arxiv.org/abs/2504.15404)
*Tajamul Ashraf,Rajes Manna,Partha Sarathi Purkayastha,Tavaheed Tariq,Janibul Bashir*

Main category: cs.CV

TLDR: 论文提出了一种名为Grounded Teacher（GT）的框架，用于解决源数据不可用时的目标域适应问题，通过关系上下文模块和专家分支减少上下文偏差。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，源数据不可用时，传统的半监督师生架构可能因上下文不平衡和领域偏移导致伪标签不准确，从而影响学生模型性能。

Method: 引入关系上下文模块建模上下文关系，并通过专家分支监督学生模型，同时应用增强技术改善少数类性能。

Result: 在三个医学数据集上的实验验证了GT框架在减少上下文偏差和提高模型性能方面的有效性。

Conclusion: GT框架通过关系上下文模块和专家分支有效解决了SFOD设置中的上下文偏差问题，提升了模型性能。

Abstract: We focus on the Source Free Object Detection (SFOD) problem, when source data
is unavailable during adaptation, and the model must adapt to the unlabeled
target domain. In medical imaging, several approaches have leveraged a
semi-supervised student-teacher architecture to bridge domain discrepancy.
Context imbalance in labeled training data and significant domain shifts
between domains can lead to biased teacher models that produce inaccurate
pseudolabels, degrading the student model's performance and causing a mode
collapse. Class imbalance, particularly when one class significantly outnumbers
another, leads to contextual bias. To tackle the problem of context bias and
the significant performance drop of the student model in the SFOD setting, we
introduce Grounded Teacher (GT) as a standard framework. In this study, we
model contextual relationships using a dedicated relational context module and
leverage it to mitigate inherent biases in the model. This approach enables us
to apply augmentations to closely related classes, across and within domains,
enhancing the performance of underrepresented classes while keeping the effect
on dominant classes minimal. We further improve the quality of predictions by
implementing an expert foundational branch to supervise the student model. We
validate the effectiveness of our approach in mitigating context bias under the
SFOD setting through experiments on three medical datasets supported by
comprehensive ablation studies. All relevant resources, including preprocessed
data, trained model weights, and code, are publicly available at this
https://github.com/Tajamul21/Grounded_Teacher.

</details>

### [140] [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)
*David Ma,Yuanxing Zhang,Jincheng Ren,Jarvis Guo,Yifan Yao,Zhenlin Wei,Zhenzhu Yang,Zhongyuan Peng,Boyu Feng,Jun Ma,Xiao Gu,Zhoufutu Wen,King Zhu,Yancheng He,Meng Cao,Shiwen Ni,Jiaheng Liu,Wenhao Huang,Ge Zhang,Xiaojie Jin*

Main category: cs.CV

TLDR: IV-Bench是一个新的基准测试，专注于评估图像背景在视频理解中的作用，填补了现有MLLM评估框架的空白。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估框架主要关注图像推理或通用视频理解任务，忽视了图像背景在视频理解中的重要作用。

Method: 提出了IV-Bench，包含967个视频和2,585个标注的图像-文本查询，覆盖13个任务和5个类别。评估了开源和闭源MLLM的性能。

Result: 当前模型在图像背景视频感知和推理任务中表现不佳，最高准确率仅为28.9%。关键影响因素包括推理模式、帧数和分辨率。

Conclusion: IV-Bench揭示了当前模型的局限性，并提供了未来研究的方向。代码和数据已开源。

Abstract: Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)
primarily focus on image reasoning or general video understanding tasks,
largely overlooking the significant role of image context in video
comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive
benchmark for evaluating Image-Grounded Video Perception and Reasoning.
IV-Bench consists of 967 videos paired with 2,585 meticulously annotated
image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5
representative categories. Extensive evaluations of state-of-the-art
open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,
Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models
substantially underperform in image-grounded video Perception and Reasoning,
merely achieving at most 28.9% accuracy. Further analysis reveals key factors
influencing model performance on IV-Bench, including inference pattern, frame
number, and resolution. Additionally, through a simple data synthesis approach,
we demonstratethe challenges of IV- Bench extend beyond merely aligning the
data format in the training proecss. These findings collectively provide
valuable insights for future research. Our codes and data are released in
https://github.com/multimodal-art-projection/IV-Bench.

</details>

### [141] [Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images](https://arxiv.org/abs/2504.15470)
*Jonathan Brokman,Amit Giloni,Omer Hofman,Roman Vainshtein,Hisashi Kojima,Guy Gilboa*

Main category: cs.CV

TLDR: 该论文提出了一种基于生成内容偏见的零样本和少样本图像检测方法，通过分析预训练扩散模型的概率流形偏向来区分真实与AI生成图像。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像检测方法在零样本和少样本场景下缺乏理论支持和性能不足的问题。

Method: 利用预训练扩散模型分析概率流形的偏向性，通过评分函数近似曲率、梯度和偏向性，并采用专家混合方法扩展至少样本场景。

Result: 在20种生成模型上的实验表明，该方法在零样本和少样本场景下均优于现有方法。

Conclusion: 通过流形分析，该研究提升了生成内容偏见的理论理解和实际应用能力。

Abstract: Distinguishing between real and AI-generated images, commonly referred to as
'image detection', presents a timely and significant challenge. Despite
extensive research in the (semi-)supervised regime, zero-shot and few-shot
solutions have only recently emerged as promising alternatives. Their main
advantage is in alleviating the ongoing data maintenance, which quickly becomes
outdated due to advances in generative technologies. We identify two main gaps:
(1) a lack of theoretical grounding for the methods, and (2) significant room
for performance improvements in zero-shot and few-shot regimes. Our approach is
founded on understanding and quantifying the biases inherent in generated
content, where we use these quantities as criteria for characterizing generated
images. Specifically, we explore the biases of the implicit probability
manifold, captured by a pre-trained diffusion model. Through score-function
analysis, we approximate the curvature, gradient, and bias towards points on
the probability manifold, establishing criteria for detection in the zero-shot
regime. We further extend our contribution to the few-shot setting by employing
a mixture-of-experts methodology. Empirical results across 20 generative models
demonstrate that our method outperforms current approaches in both zero-shot
and few-shot settings. This work advances the theoretical understanding and
practical usage of generated content biases through the lens of manifold
analysis.

</details>

### [142] [Emergence and Evolution of Interpretable Concepts in Diffusion Models](https://arxiv.org/abs/2504.15473)
*Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TLDR: 本文利用稀疏自编码器（SAEs）探索文本到图像扩散模型的内部机制，揭示了其激活中的人类可解释概念，并展示了这些概念对生成过程的因果影响。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现出色，但其内部机制仍不透明。通过机械可解释性技术（如SAEs）揭示其工作原理，有助于更好地控制和优化生成过程。

Method: 采用稀疏自编码器（SAEs）框架分析扩散模型的激活，识别人类可解释概念，并通过干预技术验证这些概念的因果作用。

Result: 研究发现，扩散过程的早期阶段可以预测最终场景的构图，中期阶段可控制风格，而后期阶段仅能调整纹理细节。

Conclusion: SAEs成功揭示了扩散模型的内部机制，并提供了干预生成过程的方法，为模型的可控性和优化开辟了新途径。

Abstract: Diffusion models have become the go-to method for text-to-image generation,
producing high-quality images from noise through a process called reverse
diffusion. Understanding the dynamics of the reverse diffusion process is
crucial in steering the generation and achieving high sample quality. However,
the inner workings of diffusion models is still largely a mystery due to their
black-box nature and complex, multi-step generation process. Mechanistic
Interpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at
uncovering the operating principles of models through granular analysis of
their internal representations. These MI techniques have been successful in
understanding and steering the behavior of large language models at scale.
However, the great potential of SAEs has not yet been applied toward gaining
insight into the intricate generative process of diffusion models. In this
work, we leverage the SAE framework to probe the inner workings of a popular
text-to-image diffusion model, and uncover a variety of human-interpretable
concepts in its activations. Interestingly, we find that even before the first
reverse diffusion step is completed, the final composition of the scene can be
predicted surprisingly well by looking at the spatial distribution of activated
concepts. Moreover, going beyond correlational analysis, we show that the
discovered concepts have a causal effect on the model output and can be
leveraged to steer the generative process. We design intervention techniques
aimed at manipulating image composition and style, and demonstrate that (1) in
early stages of diffusion image composition can be effectively controlled, (2)
in the middle stages of diffusion image composition is finalized, however
stylistic interventions are effective, and (3) in the final stages of diffusion
only minor textural details are subject to change.

</details>

### [143] [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
*Atin Pothiraj,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TLDR: 论文提出了一项新任务CAPTURe，用于测试视觉语言模型（VLMs）对遮挡物体的识别和推理能力，发现现有模型在遮挡情况下表现较差，而人类表现优异。


<details>
  <summary>Details</summary>
Motivation: 遮挡物体在现实场景中常见，但现有视觉语言模型对遮挡物体的推理能力不足，需要新的测试方法来评估和改进。

Method: 设计了CAPTURe任务，分为真实图像（CAPTURe-real）和合成图像（CAPTURe-synthetic）两部分，评估了四种VLMs在遮挡和非遮挡情况下的表现。

Result: 模型在遮挡情况下表现更差，尤其是最强的GPT-4o也无法准确计数；人类表现优异；提供遮挡物体位置信息可提升模型性能。

Conclusion: 现有VLMs在遮挡推理和计数能力上存在不足，需进一步改进；CAPTURe为评估模型提供了有效工具。

Abstract: Recognizing and reasoning about occluded (partially or fully hidden) objects
is vital to understanding visual scenes, as occlusions frequently occur in
real-world environments and act as obstacles for spatial comprehension. To test
models' ability to reason about multiple occluded objects, we introduce a novel
task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which
requires a model to count objects arranged in a pattern by inferring how the
pattern continues behind an occluder (an object which blocks parts of the
scene). CAPTURe requires both recognizing visual patterns and reasoning, making
it a useful testbed for evaluating vision-language models (VLMs) on whether
they understand occluded patterns and possess spatial understanding skills. By
requiring models to reason about occluded objects, CAPTURe also tests VLMs'
ability to form world models that would allow them to fill in missing
information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually
filtered images of real objects in patterns and (2) CAPTURe-synthetic, a
controlled diagnostic with generated patterned images. We evaluate four strong
VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models
struggle to count on both occluded and unoccluded patterns. Crucially, we find
that models perform worse with occlusion, suggesting that VLMs are also
deficient in inferring unseen spatial relationships: even the strongest VLMs
like GPT-4o fail to count with occlusion. In contrast, we find that humans
achieve very little error on CAPTURe. We also find that providing auxiliary
information of occluded object locations increases performance, underscoring
that the model error comes both from an inability to handle occlusion as well
as difficulty counting in images.

</details>

### [144] [InstaRevive: One-Step Image Enhancement via Dynamic Score Matching](https://arxiv.org/abs/2504.15513)
*Yixuan Zhu,Haolin Wang,Ao Li,Wenliang Zhao,Yansong Tang,Jingxuan Niu,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TLDR: InstaRevive是一种基于扩散蒸馏的图像增强框架，通过动态控制减少采样步骤，同时结合文本提示提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散方法计算成本高、采样步骤多的问题，同时充分利用预训练扩散模型的潜力。

Method: 采用动态控制的扩散蒸馏管道，结合文本提示作为辅助条件，优化生成过程。

Result: 在多种任务和数据集上验证了高效性和高质量结果。

Conclusion: InstaRevive在图像增强中表现出色，兼具高效性和视觉吸引力。

Abstract: Image enhancement finds wide-ranging applications in real-world scenarios due
to complex environments and the inherent limitations of imaging devices. Recent
diffusion-based methods yield promising outcomes but necessitate prolonged and
computationally intensive iterative sampling. In response, we propose
InstaRevive, a straightforward yet powerful image enhancement framework that
employs score-based diffusion distillation to harness potent generative
capability and minimize the sampling steps. To fully exploit the potential of
the pre-trained diffusion model, we devise a practical and effective diffusion
distillation pipeline using dynamic control to address inaccuracies in updating
direction during score matching. Our control strategy enables a dynamic
diffusing scope, facilitating precise learning of denoising trajectories within
the diffusion model and ensuring accurate distribution matching gradients
during training. Additionally, to enrich guidance for the generative power, we
incorporate textual prompts via image captioning as auxiliary conditions,
fostering further exploration of the diffusion model. Extensive experiments
substantiate the efficacy of our framework across a diverse array of
challenging tasks and datasets, unveiling the compelling efficacy and
efficiency of InstaRevive in delivering high-quality and visually appealing
results. Code is available at https://github.com/EternalEvan/InstaRevive.

</details>

### [145] [Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness](https://arxiv.org/abs/2504.15599)
*Shichen Li,Chenhui Shao*

Main category: cs.CV

TLDR: 提出了一种多模态数据融合框架，用于实时预测食品干燥状态，显著降低了预测误差。


<details>
  <summary>Details</summary>
Motivation: 食品干燥的实时预测对节能、生产效率和产品质量至关重要，但现有方法因数据有限和动态性难以满足需求。

Method: 采用端到端多模态数据融合框架，结合视频数据和工艺参数，使用编码器-解码器架构和基于Transformer的解码器。

Result: 模型在糖饼干干燥实验中平均预测误差仅15秒，优于现有方法65.69%和纯视频模型11.30%。

Conclusion: 该模型在精度、规模和效率上表现优异，适用于工业多模态数据融合任务。

Abstract: Food drying is essential for food production, extending shelf life, and
reducing transportation costs. Accurate real-time forecasting of drying
readiness is crucial for minimizing energy consumption, improving productivity,
and ensuring product quality. However, this remains challenging due to the
dynamic nature of drying, limited data availability, and the lack of effective
predictive analytical methods. To address this gap, we propose an end-to-end
multi-modal data fusion framework that integrates in-situ video data with
process parameters for real-time food drying readiness forecasting. Our
approach leverages a new encoder-decoder architecture with modality-specific
encoders and a transformer-based decoder to effectively extract features while
preserving the unique structure of each modality. We apply our approach to
sugar cookie drying, where time-to-ready is predicted at each timestamp.
Experimental results demonstrate that our model achieves an average prediction
error of only 15 seconds, outperforming state-of-the-art data fusion methods by
65.69% and a video-only model by 11.30%. Additionally, our model balances
prediction accuracy, model size, and computational efficiency, making it
well-suited for heterogenous industrial datasets. The proposed model is
extensible to various other industrial modality fusion tasks for online
decision-making.

</details>

### [146] [SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking](https://arxiv.org/abs/2504.15609)
*Yunfeng Li,Bo Wang,Jiahao Wan,Xueyi Wu,Ye Li*

Main category: cs.CV

TLDR: 论文提出了首个大规模水下声学目标跟踪基准SonarT165，并设计了高效框架STFTrack，通过多视角模板融合和轨迹校正模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 水下观测系统在能见度不足时依赖声呐数据，但缺乏统一评估基准限制了现有方法的实用性。

Method: 提出SonarT165基准，并设计STFTrack框架，包含多视角模板融合模块（MTFM）和最优轨迹校正模块（OTCM），结合声学图像增强和频率增强模块。

Result: STFTrack在SonarT165上表现优异，超越现有方法。

Conclusion: SonarT165填补了评估空白，STFTrack为水下声学目标跟踪提供了高效解决方案。

Abstract: Underwater observation systems typically integrate optical cameras and
imaging sonar systems. When underwater visibility is insufficient, only sonar
systems can provide stable data, which necessitates exploration of the
underwater acoustic object tracking (UAOT) task. Previous studies have explored
traditional methods and Siamese networks for UAOT. However, the absence of a
unified evaluation benchmark has significantly constrained the value of these
methods. To alleviate this limitation, we propose the first large-scale UAOT
benchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and
205K high-quality annotations. Experimental results demonstrate that SonarT165
reveals limitations in current state-of-the-art SOT trackers. To address these
limitations, we propose STFTrack, an efficient framework for acoustic object
tracking. It includes two novel modules, a multi-view template fusion module
(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module
integrates multi-view feature of both the original image and the binary image
of the dynamic template, and introduces a cross-attention-like layer to fuse
the spatio-temporal target representations. The OTCM module introduces the
acoustic-response-equivalent pixel property and proposes normalized pixel
brightness response scores, thereby suppressing suboptimal matches caused by
inaccurate Kalman filter prediction boxes. To further improve the model
feature, STFTrack introduces a acoustic image enhancement method and a
Frequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive
experiments show the proposed STFTrack achieves state-of-the-art performance on
the proposed benchmark. The code is available at
https://github.com/LiYunfengLYF/SonarT165.

</details>

### [147] [HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2504.15612)
*Hongxing Peng,Kang Lin,Huanai Liu*

Main category: cs.CV

TLDR: 提出了一种基于Mamba架构的HS-Mamba框架，结合局部和全局特征，实现高光谱图像的高精度分类。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像的高维度和特征内联特性对Mamba架构的应用提出了挑战，需要一种新的方法结合局部和全局特征。

Method: HS-Mamba采用双通道空间-光谱编码器模块和多组Mamba建模局部特征，结合轻量级全局内联注意力模块增强全局特征。

Result: 在四个基准数据集上优于现有方法。

Conclusion: HS-Mamba通过融合局部和全局特征，显著提升了高光谱图像分类性能。

Abstract: Hyperspectral image (HSI) classification has been one of the hot topics in
remote sensing fields. Recently, the Mamba architecture based on selective
state-space models (S6) has demonstrated great advantages in long sequence
modeling. However, the unique properties of hyperspectral data, such as high
dimensionality and feature inlining, pose challenges to the application of
Mamba to HSI classification. To compensate for these shortcomings, we propose
an full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts
a strategy different from pixel-patch based or whole-image based, but combines
the advantages of both. The patches cut from the whole image are sent to
multi-groups Mamba, combined with positional information to perceive local
inline features in the spatial and spectral domains, and the whole image is
sent to a lightweight attention module to enhance the global feature
representation ability. Specifically, HS-Mamba consists of a dual-channel
spatial-spectral encoder (DCSS-encoder) module and a lightweight global inline
attention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of
Mamba to decouple and model the local features of dual-channel sequences with
non-overlapping patches. The LGI-Att branch uses a lightweight compressed and
extended attention module to perceive the global features of the spatial and
spectral domains of the unsegmented whole image. By fusing local and global
features, high-precision classification of hyperspectral images is achieved.
Extensive experiments demonstrate the superiority of the proposed HS-Mamba,
outperforming state-of-the-art methods on four benchmark HSI datasets.

</details>

### [148] [AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization](https://arxiv.org/abs/2504.15619)
*Jinda Lu,Jinghan Li,Yuan Gao,Junkang Wu,Jiancan Wu,Xiang Wang,Xiangnan He*

Main category: cs.CV

TLDR: AdaViP通过视觉增强的偏好优化方法，显著提升了多模态大语言模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注语言偏好，忽视了视觉上下文的重要性。

Method: 提出AdaViP，包括基于视觉的偏好对构建和动态平衡视觉与语言偏好的优化方法。

Result: AdaViP-7B在Object HalBench上显著减少了幻觉现象，表现优于现有方法。

Conclusion: AdaViP有效解决了视觉上下文缺失的问题，提升了模型对齐效果。

Abstract: Preference alignment through Direct Preference Optimization (DPO) has
demonstrated significant effectiveness in aligning multimodal large language
models (MLLMs) with human preferences. However, existing methods focus
primarily on language preferences while neglecting the critical visual context.
In this paper, we propose an Adaptive Vision-enhanced Preference optimization
(AdaViP) that addresses these limitations through two key innovations: (1)
vision-based preference pair construction, which integrates multiple visual
foundation models to strategically remove key visual elements from the image,
enhancing MLLMs' sensitivity to visual details; and (2) adaptive preference
optimization that dynamically balances vision- and language-based preferences
for more accurate alignment. Extensive evaluations across different benchmarks
demonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%
reductions in response-level and mentioned-level hallucination respectively on
the Object HalBench, significantly outperforming current state-of-the-art
methods.

</details>

### [149] [FaceInsight: A Multimodal Large Language Model for Face Perception](https://arxiv.org/abs/2504.15624)
*Jingzhi Li,Changjiang Luo,Ruoyu Chen,Hua Zhang,Wenqi Ren,Jianhou Gan,Xiaochun Cao*

Main category: cs.CV

TLDR: FaceInsight是一种多功能面部感知MLLM，通过视觉-文本对齐和辅助感知模态提升面部任务性能。


<details>
  <summary>Details</summary>
Motivation: 通用领域MLLM在面部感知任务中表现不佳，需要针对性解决方案。

Method: 引入视觉-文本对齐和面部分割图作为辅助模态，优化面部信息建模。

Result: 在三种面部感知任务中，FaceInsight显著优于其他九种MLLM。

Conclusion: FaceInsight填补了通用MLLM在面部感知领域的不足，表现卓越。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
strong capabilities in understanding general visual content. However, these
general-domain MLLMs perform poorly in face perception tasks, often producing
inaccurate or misleading responses to face-specific queries. To address this
gap, we propose FaceInsight, the versatile face perception MLLM that provides
fine-grained facial information. Our approach introduces visual-textual
alignment of facial knowledge to model both uncertain dependencies and
deterministic relationships among facial information, mitigating the
limitations of language-driven reasoning. Additionally, we incorporate face
segmentation maps as an auxiliary perceptual modality, enriching the visual
input with localized structural cues to enhance semantic understanding.
Comprehensive experiments and analyses across three face perception tasks
demonstrate that FaceInsight consistently outperforms nine compared MLLMs under
both training-free and fine-tuned settings.

</details>

### [150] [ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?](https://arxiv.org/abs/2504.15627)
*Doanh C. Bui,Hoai Luan Pham,Vu Trung Duong Le,Tuan Hai Vu,Van Duy Tran,Yasuhiko Nakashima*

Main category: cs.CV

TLDR: 比较传统持续学习方法与视觉语言零样本分类在WSI终身学习中的效果。


<details>
  <summary>Details</summary>
Motivation: 解决WSI多任务学习的存储、处理和传输问题，避免重复训练新模型。

Method: 应用正则化和基于记忆的方法，并与视觉语言基础模型的零样本分类对比。

Result: 首次比较两种方法在WSI终身学习中的表现，结果即将公布。

Conclusion: 需进一步研究持续学习策略是否优于零样本分类。

Abstract: Lifelong learning for whole slide images (WSIs) poses the challenge of
training a unified model to perform multiple WSI-related tasks, such as cancer
subtyping and tumor classification, in a distributed, continual fashion. This
is a practical and applicable problem in clinics and hospitals, as WSIs are
large, require storage, processing, and transfer time. Training new models
whenever new tasks are defined is time-consuming. Recent work has applied
regularization- and rehearsal-based methods to this setting. However, the rise
of vision-language foundation models that align diagnostic text with pathology
images raises the question: are these models alone sufficient for lifelong WSI
learning using zero-shot classification, or is further investigation into
continual learning strategies needed to improve performance? To our knowledge,
this is the first study to compare conventional continual-learning approaches
with vision-language zero-shot classification for WSIs. Our source code and
experimental results will be available soon.

</details>

### [151] [AffordanceSAM: Segment Anything Once More in Affordance Grounding](https://arxiv.org/abs/2504.15650)
*Dengyang Jiang,Mengmeng Wang,Teli Ma,Hengzhuang Li,Yong liu,Guang Dai,Lei Zhang*

Main category: cs.CV

TLDR: AffordanceSAM通过扩展SAM的分割能力到功能区域识别，提升了模型对未见对象和功能的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前模型在功能区域识别任务中泛化能力不足，难以处理未见对象和功能。

Method: 提出AffordanceSAM，结合SAM的分割能力，通过功能适配模块和粗到细训练策略优化功能区域识别。

Result: 在AGD20K基准测试中超越现有方法，并能处理新对象和功能。

Conclusion: AffordanceSAM展示了强大的泛化能力，适用于实际应用。

Abstract: Improving the generalization ability of an affordance grounding model to
recognize regions for unseen objects and affordance functions is crucial for
real-world application. However, current models are still far away from such
standards. To address this problem, we introduce AffordanceSAM, an effective
approach that extends SAM's generalization capacity to the domain of affordance
grounding. For the purpose of thoroughly transferring SAM's robust performance
in segmentation to affordance, we initially propose an affordance-adaption
module in order to help modify SAM's segmentation output to be adapted to the
specific functional regions required for affordance grounding. We concurrently
make a coarse-to-fine training recipe to make SAM first be aware of affordance
objects and actions coarsely, and then be able to generate affordance heatmaps
finely. Both quantitative and qualitative experiments show the strong
generalization capacity of our AffordanceSAM, which not only surpasses previous
methods under AGD20K benchmark but also shows evidence to handle the task with
novel objects and affordance functions.

</details>

### [152] [DiTPainter: Efficient Video Inpainting with Diffusion Transformers](https://arxiv.org/abs/2504.15661)
*Xian Wu,Chang Liu*

Main category: cs.CV

TLDR: DiTPainter是一种基于扩散变换器（DiT）的视频修复模型，通过高效设计的变换器网络解决现有方法因光流不准确或大掩码导致的模糊和不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复算法依赖光流传播像素，但易因光流不准确或大掩码导致模糊和不一致；预训练DiT模型参数过多，难以直接应用于视频修复。

Method: 提出DiTPainter，一种端到端基于DiT的视频修复模型，采用专为视频修复设计的高效变换器网络，从头训练而非依赖预训练模型。

Result: DiTPainter能处理任意长度视频，适用于视频去字幕和视频补全任务，实验显示其质量和时空一致性优于现有方法。

Conclusion: DiTPainter通过高效设计和从头训练，显著提升了视频修复的质量和效率。

Abstract: Many existing video inpainting algorithms utilize optical flows to construct
the corresponding maps and then propagate pixels from adjacent frames to
missing areas by mapping. Despite the effectiveness of the propagation
mechanism, they might encounter blurry and inconsistencies when dealing with
inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)
has emerged as a revolutionary technique for video generation tasks. However,
pretrained DiT models for video generation all contain a large amount of
parameters, which makes it very time consuming to apply to video inpainting
tasks. In this paper, we present DiTPainter, an end-to-end video inpainting
model based on Diffusion Transformer (DiT). DiTPainter uses an efficient
transformer network designed for video inpainting, which is trained from
scratch instead of initializing from any large pretrained models. DiTPainter
can address videos with arbitrary lengths and can be applied to video
decaptioning and video completion tasks with an acceptable time cost.
Experiments show that DiTPainter outperforms existing video inpainting
algorithms with higher quality and better spatial-temporal consistency.

</details>

### [153] [Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection](https://arxiv.org/abs/2504.15665)
*Pei Liu,Yisi Luo,Wenzhen Wang,Xiangyong Cao*

Main category: cs.CV

TLDR: 论文提出了一种基于运动增强的非局部相似性隐式神经表示（INR）框架，用于红外弱小目标检测，解决了动态多帧场景和弱目标特征的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统低秩加稀疏模型难以捕捉动态背景和全局时空相关性，导致背景泄漏或目标丢失。

Method: 结合光流进行运动估计以捕捉目标细微运动，提出多帧融合增强运动显著性；利用非局部相似性构建具有强低秩特性的块张量，提出基于张量分解的INR模型编码非局部低秩性和时空相关性。

Result: 实验表明，该方法能有效从复杂红外背景中分离弱小目标，在检测精度和鲁棒性上优于现有方法。

Conclusion: 提出的INR框架通过运动增强和非局部相似性，显著提升了红外弱小目标检测的性能。

Abstract: Infrared dim and small target detection presents a significant challenge due
to dynamic multi-frame scenarios and weak target signatures in the infrared
modality. Traditional low-rank plus sparse models often fail to capture dynamic
backgrounds and global spatial-temporal correlations, which results in
background leakage or target loss. In this paper, we propose a novel
motion-enhanced nonlocal similarity implicit neural representation (INR)
framework to address these challenges. We first integrate motion estimation via
optical flow to capture subtle target movements, and propose multi-frame fusion
to enhance motion saliency. Second, we leverage nonlocal similarity to
construct patch tensors with strong low-rank properties, and propose an
innovative tensor decomposition-based INR model to represent the nonlocal patch
tensor, effectively encoding both the nonlocal low-rankness and
spatial-temporal correlations of background through continuous neural
representations. An alternating direction method of multipliers is developed
for the nonlocal INR model, which enjoys theoretical fixed-point convergence.
Experimental results show that our approach robustly separates dim targets from
complex infrared backgrounds, outperforming state-of-the-art methods in
detection accuracy and robustness.

</details>

### [154] [DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining](https://arxiv.org/abs/2504.15669)
*Wei Zhuo,Zhiyue Tang,Wufeng Xue,Hao Ding,Linlin Shen*

Main category: cs.CV

TLDR: FS-DINO是一个统一的少样本语义分割模型，结合了DINOv2和SAM的知识，通过轻量级分割器和跨模型蒸馏实现高效分割。


<details>
  <summary>Details</summary>
Motivation: 研究如何将DINOv2和SAM两种基础模型的知识整合到一个统一模型中，以提升少样本语义分割的性能。

Method: 提出FS-DINO模型，仅使用DINOv2的编码器和轻量级分割器，通过瓶颈适配器、元视觉提示生成器和解码器实现分割，并结合跨模型蒸馏和4D相关性挖掘。

Result: 在COCO-20i、PASCAL-5i和FSS-1000数据集上验证了方法的有效性和优越性。

Conclusion: FS-DINO成功整合了两种基础模型的知识，为少样本语义分割提供了高效且性能优越的解决方案。

Abstract: Few-shot semantic segmentation has gained increasing interest due to its
generalization capability, i.e., segmenting pixels of novel classes requiring
only a few annotated images. Prior work has focused on meta-learning for
support-query matching, with extensive development in both prototype-based and
aggregation-based methods. To address data scarcity, recent approaches have
turned to foundation models to enhance representation transferability for novel
class segmentation. Among them, a hybrid dual-modal framework including both
DINOv2 and SAM has garnered attention due to their complementary capabilities.
We wonder "can we build a unified model with knowledge from both foundation
models?" To this end, we propose FS-DINO, with only DINOv2's encoder and a
lightweight segmenter. The segmenter features a bottleneck adapter, a
meta-visual prompt generator based on dense similarities and semantic
embeddings, and a decoder. Through coarse-to-fine cross-model distillation, we
effectively integrate SAM's knowledge into our lightweight segmenter, which can
be further enhanced by 4D correlation mining on support-query pairs. Extensive
experiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness
and superiority of our method.

</details>

### [155] [Vidi: Large Multimodal Models for Video Understanding and Editing](https://arxiv.org/abs/2504.15681)
*Vidi Team,Celong Liu,Chia-Wen Kuo,Dawei Du,Fan Chen,Guang Chen,Jiamin Yuan,Lingxi Zhang,Lu Guo,Lusha Li,Longyin Wen,Qingyu Chen,Rachel Deng,Sijie Zhu,Stuart Siew,Tong Jin,Wei Lu,Wen Zhong,Xiaohui Shen,Xin Gu,Xing Mei,Xueqiong Qu*

Main category: cs.CV

TLDR: Vidi是一个大型多模态模型家族，专注于视频理解和编辑任务，特别是时间范围检索，能够处理长时间视频并在真实场景中显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 视频已成为互联网上主要的沟通和表达媒介，但传统模型在处理多模态和长时间视频时面临挑战，需要更强大的模型支持高质量视频编辑。

Method: Vidi是一个大型多模态模型（LMM），专注于时间范围检索任务，能够处理小时级视频并支持多模态输入（如视觉、音频、文本）。

Result: Vidi在时间范围检索任务中显著优于GPT-4o和Gemini等领先专有模型，并通过VUE-TR基准测试展示了其优越性。

Conclusion: Vidi在视频编辑场景中表现出色，为大规模高质量视频内容创作提供了强大支持。

Abstract: Humans naturally share information with those they are connected to, and
video has become one of the dominant mediums for communication and expression
on the Internet. To support the creation of high-quality large-scale video
content, a modern pipeline requires a comprehensive understanding of both the
raw input materials (e.g., the unedited footage captured by cameras) and the
editing components (e.g., visual effects). In video editing scenarios, models
must process multiple modalities (e.g., vision, audio, text) with strong
background knowledge and handle flexible input lengths (e.g., hour-long raw
videos), which poses significant challenges for traditional models. In this
report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a
wide range of video understand editing scenarios. The first release focuses on
temporal retrieval, i.e., identifying the time ranges within the input videos
corresponding to a given text query, which plays a critical role in intelligent
editing. The model is capable of processing hour-long videos with strong
temporal understanding capability, e.g., retrieve time ranges for certain
queries. To support a comprehensive evaluation in real-world scenarios, we also
present the VUE-TR benchmark, which introduces five key advancements. 1) Video
duration: significantly longer than existing temporal retrival datasets, 2)
Audio support: includes audio-based queries, 3) Query format: diverse query
lengths/formats, 4) Annotation quality: ground-truth time ranges are manually
annotated. 5) Evaluation metric: a refined IoU metric to support evaluation
over multiple time ranges. Remarkably, Vidi significantly outperforms leading
proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,
indicating its superiority in video editing scenarios.

</details>

### [156] [You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection](https://arxiv.org/abs/2504.15694)
*Jun Dong,Wenli Wu,Jintao Cheng,Xiaoyu Tang*

Main category: cs.CV

TLDR: 提出了一种超轻量实时水下目标检测框架YSOOB，通过多频谱小波编码和动态信息增强技术，显著提升了模型在低质量水下图像中的性能，同时实现了高效推理。


<details>
  <summary>Details</summary>
Motivation: 水下环境中的低图像质量和有限计算资源对目标检测模型的精度和效率提出了挑战，需要一种轻量且高效的解决方案。

Method: 采用多频谱小波编码（MSWE）减少光学颜色失真，动态选择关键信息增强模型泛化能力，并通过通道压缩和重构大核卷积（RLKC）实现模型轻量化。

Result: YSOOB仅120万参数，在URPC2020和DUO数据集上分别达到83.1%和82.9%的mAP50，推理速度显著优于YOLOv12-N。

Conclusion: YSOOB是一种高效且轻量的水下目标检测框架，在性能和速度上均优于现有方法。

Abstract: Despite the remarkable achievements in object detection, the model's accuracy
and efficiency still require further improvement under challenging underwater
conditions, such as low image quality and limited computational resources. To
address this, we propose an Ultra-Light Real-Time Underwater Object Detection
framework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a
Multi-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on
the input image, minimizing the semantic loss caused by underwater optical
color distortion. Furthermore, we revisit the unique characteristics of
even-sized and transposed convolutions, allowing the model to dynamically
select and enhance key information during the resampling process, thereby
improving its generalization ability. Finally, we eliminate model redundancy
through a simple yet effective channel compression and reconstructed large
kernel convolution (RLKC) to achieve model lightweight. As a result, forms a
high-performance underwater object detector YSOOB with only 1.2 million
parameters. Extensive experimental results demonstrate that, with the fewest
parameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO
datasets, respectively, comparable to the current SOTA detectors. The inference
speed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge
computing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by
28.1% and 22.5%, respectively.

</details>

### [157] [RePOPE: Impact of Annotation Errors on the POPE Benchmark](https://arxiv.org/abs/2504.15707)
*Yannic Neuhaus,Matthias Hein*

Main category: cs.CV

TLDR: 研究评估了MSCOCO标签错误对POPE基准的影响，通过重新标注发现标注错误分布不均，修正后的RePOPE显著改变了模型排名。


<details>
  <summary>Details</summary>
Motivation: 由于数据标注成本高，基准数据集常使用现有标签，但标签错误可能影响评估结果。

Method: 重新标注POPE基准图像，分析标签错误分布，并基于修正标签评估多个模型。

Result: 修正后的RePOPE标签导致模型排名显著变化，凸显标签质量的重要性。

Conclusion: 标签质量对基准评估有重要影响，需谨慎处理现有标签的使用。

Abstract: Since data annotation is costly, benchmark datasets often incorporate labels
from established image datasets. In this work, we assess the impact of label
errors in MSCOCO on the frequently used object hallucination benchmark POPE. We
re-annotate the benchmark images and identify an imbalance in annotation errors
across different subsets. Evaluating multiple models on the revised labels,
which we denote as RePOPE, we observe notable shifts in model rankings,
highlighting the impact of label quality. Code and data are available at
https://github.com/YanNeu/RePOPE .

</details>

### [158] [Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models](https://arxiv.org/abs/2504.15723)
*Dasol Jeong,Donggoo Kang,Jiwon Park,Hyebean Lee,Joonki Paik*

Main category: cs.CV

TLDR: 提出一种基于扩散的零样本图像编辑框架，统一文本引导和参考引导方法，无需微调。


<details>
  <summary>Details</summary>
Motivation: 解决零样本图像编辑中结构保持和精细修改的挑战，同时支持多样化的编辑场景。

Method: 利用扩散反演和特定时间步的空文本嵌入，结合分阶段潜在注入策略（早期形状注入，后期属性注入），并通过参考潜力的交叉注意力实现语义对齐。

Result: 在表情迁移、纹理变换和风格注入等任务中表现优异，验证了方法的可扩展性和适应性。

Conclusion: 该方法在零样本图像编辑中实现了高精度和全局一致性，适用于多样化场景。

Abstract: We propose a diffusion-based framework for zero-shot image editing that
unifies text-guided and reference-guided approaches without requiring
fine-tuning. Our method leverages diffusion inversion and timestep-specific
null-text embeddings to preserve the structural integrity of the source image.
By introducing a stage-wise latent injection strategy-shape injection in early
steps and attribute injection in later steps-we enable precise, fine-grained
modifications while maintaining global consistency. Cross-attention with
reference latents facilitates semantic alignment between the source and
reference. Extensive experiments across expression transfer, texture
transformation, and style infusion demonstrate state-of-the-art performance,
confirming the method's scalability and adaptability to diverse image editing
scenarios.

</details>

### [159] [SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems](https://arxiv.org/abs/2504.15728)
*Manjunath D,Aniruddh Sikdar,Prajwal Gurunath,Sumanth Udupa,Suresh Sundaram*

Main category: cs.CV

TLDR: 论文提出了一种名为SAGA的新策略，用于解决RGB到IR图像适应中的颜色偏差问题，并引入了一个多传感器数据集IndraEye，以验证方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于IR图像缺乏颜色和纹理信息，RGB训练的模型在适应IR图像时会产生较多误检和低质量伪标签，因此需要一种方法来减少颜色偏差并缩小域差距。

Method: 提出了Semantic-Aware Gray color Augmentation (SAGA)策略，通过提取与IR图像相关的对象级特征来缓解颜色偏差。同时，引入了多传感器数据集IndraEye用于验证。

Result: 实验结果表明，SAGA在RGB到IR适应中显著提升了性能，在自动驾驶和IndraEye数据集上实现了0.4%到7.6%的mAP提升。

Conclusion: SAGA和IndraEye数据集为多模态学习和域适应提供了有效工具，有助于开发更鲁棒的空中感知系统。

Abstract: Domain-adaptive thermal object detection plays a key role in facilitating
visible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered
image pairs and minimizing reliance on large annotated IR datasets. However,
inherent limitations of IR images, such as the lack of color and texture cues,
pose challenges for RGB-trained models, leading to increased false positives
and poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray
color Augmentation (SAGA), a novel strategy for mitigating color bias and
bridging the domain gap by extracting object-level features relevant to IR
images. Additionally, to validate the proposed SAGA for drone imagery, we
introduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse
applications. The dataset contains 5,612 images with 145,666 instances,
captured from diverse angles, altitudes, backgrounds, and times of day,
offering valuable opportunities for multimodal learning, domain adaptation for
object detection and segmentation, and exploration of sensor-specific strengths
and weaknesses. IndraEye aims to enhance the development of more robust and
accurate aerial perception systems, especially in challenging environments.
Experimental results show that SAGA significantly improves RGB-to-IR adaptation
for autonomous driving and IndraEye dataset, achieving consistent performance
gains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain
adaptation techniques. The dataset and codes are available at
https://github.com/airliisc/IndraEye.

</details>

### [160] [GADS: A Super Lightweight Model for Head Pose Estimation](https://arxiv.org/abs/2504.15751)
*Menan Velayuthan,Asiri Gawesha,Purushoth Velayuthan,Nuwan Kodagoda,Dharshana Kasthurirathna,Pradeepa Samarasinghe*

Main category: cs.CV

TLDR: 提出了一种名为GADS的新架构，用于头部姿态估计，通过分组地标和注意力机制显著减小模型规模和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有基于地标的方法过于复杂且模型庞大，难以在边缘设备或计算资源有限的环境中部署。

Method: 采用Deep Set框架，将地标分组并使用小型Deep Set层，结合多头注意力机制提取和组合组间信息。

Result: 模型比当前最轻量级模型小7.5倍、快25倍，比性能最佳模型小4321倍。

Conclusion: GADS架构为资源受限的头部姿态估计方法提供了强大基准。

Abstract: In human-computer interaction, head pose estimation profoundly influences
application functionality. Although utilizing facial landmarks is valuable for
this purpose, existing landmark-based methods prioritize precision over
simplicity and model size, limiting their deployment on edge devices and in
compute-poor environments. To bridge this gap, we propose \textbf{Grouped
Attention Deep Sets (GADS)}, a novel architecture based on the Deep Set
framework. By grouping landmarks into regions and employing small Deep Set
layers, we reduce computational complexity. Our multihead attention mechanism
extracts and combines inter-group information, resulting in a model that is
$7.5\times$ smaller and executes $25\times$ faster than the current lightest
state-of-the-art model. Notably, our method achieves an impressive reduction,
being $4321\times$ smaller than the best-performing model. We introduce vanilla
GADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three
benchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture
as a robust baseline for resource-constrained head pose estimation methods.

</details>

### [161] [DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy](https://arxiv.org/abs/2504.15756)
*Qirui Yang,Fangpu Zhang,Yeying Jin,Qihua Cheng,Pengtao Jiang,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TLDR: 提出了一种单阶段原始域去摩尔纹框架DSDNet，通过利用原始和YCbCr图像的协同作用，在保持亮度和颜色保真度的同时去除摩尔纹。


<details>
  <summary>Details</summary>
Motivation: 移动成像中摩尔纹问题严重，现有方法存在信息丢失或效率低下的问题。

Method: 设计了DSDNet框架，包括Raw-to-YCbCr映射管道、SADM模块和LCAT模块，用于亮度和颜色保真度优化。

Result: DSDNet在视觉质量和定量评估上优于现有方法，推理速度快2.4倍。

Conclusion: DSDNet是一种高效且实用的去摩尔纹解决方案。

Abstract: With the rapid advancement of mobile imaging, capturing screens using
smartphones has become a prevalent practice in distance learning and conference
recording. However, moir\'e artifacts, caused by frequency aliasing between
display screens and camera sensors, are further amplified by the image signal
processing pipeline, leading to severe visual degradation. Existing sRGB domain
demoir\'eing methods struggle with irreversible information loss, while recent
two-stage raw domain approaches suffer from information bottlenecks and
inference inefficiency. To address these limitations, we propose a single-stage
raw domain demoir\'eing framework, Dual-Stream Demoir\'eing Network (DSDNet),
which leverages the synergy of raw and YCbCr images to remove moir\'e while
preserving luminance and color fidelity. Specifically, to guide luminance
correction and moir\'e removal, we design a raw-to-YCbCr mapping pipeline and
introduce the Synergic Attention with Dynamic Modulation (SADM) module. This
module enriches the raw-to-sRGB conversion with cross-domain contextual
features. Furthermore, to better guide color fidelity, we develop a
Luminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance
and chrominance representations. Extensive experiments demonstrate that DSDNet
outperforms state-of-the-art methods in both visual quality and quantitative
evaluation, and achieves an inference speed $\mathrm{\textbf{2.4x}}$ faster
than the second-best method, highlighting its practical advantages. We provide
an anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.

</details>

### [162] [Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection](https://arxiv.org/abs/2504.15770)
*Lei Xu,Mehmet Yamac,Mete Ahishali,Moncef Gabbouj*

Main category: cs.CV

TLDR: 提出了一种基于多尺度张量求和（MTS）降维模块的神经网络MTS-DR-Net，用于边缘检测任务，通过减少冗余信息并聚焦相关子空间，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 边缘检测在计算机视觉任务中具有重要作用，传统深度学习方法需要大量层数才能获得大感受野，而MTS算子可以在初始层实现大感受野，因此提出MTS-DR模块以优化网络结构。

Method: 使用MTS层和MTS-DR模块作为新骨干网络，减少冗余信息并聚焦相关子空间，随后采用U形细化模块进一步优化。

Result: 在BSDS500和BIPEDv2数据集上的实验验证了MTS-DR-Net的有效性。

Conclusion: MTS-DR-Net通过降维模块和细化模块显著提升了边缘检测性能，为相关任务提供了新思路。

Abstract: Edge detection has attracted considerable attention thanks to its exceptional
ability to enhance performance in downstream computer vision tasks. In recent
years, various deep learning methods have been explored for edge detection
tasks resulting in a significant performance improvement compared to
conventional computer vision algorithms. In neural networks, edge detection
tasks require considerably large receptive fields to provide satisfactory
performance. In a typical convolutional operation, such a large receptive field
can be achieved by utilizing a significant number of consecutive layers, which
yields deep network structures. Recently, a Multi-scale Tensorial Summation
(MTS) factorization operator was presented, which can achieve very large
receptive fields even from the initial layers. In this paper, we propose a
novel MTS Dimensional Reduction (MTS-DR) module guided neural network,
MTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and
corresponding MTS-DR blocks as a new backbone to remove redundant information
initially. Such a dimensional reduction module enables the neural network to
focus specifically on relevant information (i.e., necessary subspaces).
Finally, a weight U-shaped refinement module follows MTS-DR blocks in the
MTS-DR-Net. We conducted extensive experiments on two benchmark edge detection
datasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The
implementation of the proposed MTS-DR-Net can be found at
https://github.com/LeiXuAI/MTS-DR-Net.git.

</details>

### [163] [Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models](https://arxiv.org/abs/2504.15776)
*Quentin Herau,Nathan Piasco,Moussab Bennehar,Luis Rolado,Dzmitry Tsishkou,Bingbing Liu,Cyrille Migniot,Pascal Vasseur,Cédric Demonceaux*

Main category: cs.CV

TLDR: 提出了一种基于NeRF的优化方法，用于改进自动驾驶数据集中传感器位姿和校准参数，提升数据集基准的准确性。


<details>
  <summary>Details</summary>
Motivation: 公共数据集中传感器校准和车辆位姿的不准确性可能导致下游任务评估错误，影响自动驾驶系统的可靠性。

Method: 采用NeRF进行传感器位姿和校准参数的鲁棒优化，并通过重投影指标、新视角合成渲染质量和几何对齐进行无真值验证。

Result: 方法显著提升了传感器位姿的准确性，优化后的参数公开供研究社区使用。

Conclusion: 该方法不仅提升了现有数据集的实用性，还为更可靠的自动驾驶模型奠定了基础。

Abstract: Autonomous driving systems rely on accurate perception and localization of
the ego car to ensure safety and reliability in challenging real-world driving
scenarios. Public datasets play a vital role in benchmarking and guiding
advancement in research by providing standardized resources for model
development and evaluation. However, potential inaccuracies in sensor
calibration and vehicle poses within these datasets can lead to erroneous
evaluations of downstream tasks, adversely impacting the reliability and
performance of the autonomous systems. To address this challenge, we propose a
robust optimization method based on Neural Radiance Fields (NeRF) to refine
sensor poses and calibration parameters, enhancing the integrity of dataset
benchmarks. To validate improvement in accuracy of our optimized poses without
ground truth, we present a thorough evaluation process, relying on reprojection
metrics, Novel View Synthesis rendering quality, and geometric alignment. We
demonstrate that our method achieves significant improvements in sensor pose
accuracy. By optimizing these critical parameters, our approach not only
improves the utility of existing datasets but also paves the way for more
reliable autonomous driving models. To foster continued progress in this field,
we make the optimized sensor poses publicly available, providing a valuable
resource for the research community.

</details>

### [164] [Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos](https://arxiv.org/abs/2504.15782)
*Daniele Baieri,Riccardo Cicciarella,Michael Krützen,Emanuele Rodolà,Silvia Zuffi*

Main category: cs.CV

TLDR: 提出一种基于模型的方法，通过单目视频估计野生海豚的3D形状和运动，以评估其身体状况。


<details>
  <summary>Details</summary>
Motivation: 水生动物在自然水下环境中的观察难度大，现有方法主要针对陆地四足动物，海豚等水生动物未被探索。

Method: 采用基于模型的方法，结合传输模型以解决水导致的遮挡问题，应用于不同海况下的视频数据。

Result: 估计了海豚的质量和体积，并与基于手动2D测量的方法进行了比较。

Conclusion: 该方法为水生动物3D重建提供了可行方案，并展示了其潜在应用价值。

Abstract: We address the problem of estimating the metric 3D shape and motion of wild
dolphins from monocular video, with the aim of assessing their body condition.
While considerable progress has been made in reconstructing 3D models of
terrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty
of observing them in their natural underwater environment. To address this, we
propose a model-based approach that incorporates a transmission model to
account for water-induced occlusion. We apply our method to video captured
under different sea conditions. We estimate mass and volume, and compare our
results to a manual 2D measurements-based method.

</details>

### [165] [Towards prediction of morphological heart age from computed tomography angiography](https://arxiv.org/abs/2504.15783)
*Johan Öfverstedt,Elin Lundström,Håkan Ahlström,Joel Kullberg*

Main category: cs.CV

TLDR: 该研究通过CTA图像预测年龄，开发了一种新的心脏形态年龄生物标志物，并研究了形态与衰老的关系。使用图像配准和机器学习方法，预测误差约为2.7年。


<details>
  <summary>Details</summary>
Motivation: 探索心脏形态与年龄的关系，并开发一种新的心脏形态年龄生物标志物。

Method: 采用图像配准方法标准化图像，提取密度和局部体积特征，训练机器学习模型预测年龄。

Result: 预测误差为女性2.74年，男性2.77年。形态预测高度一致，显著区域分析揭示了与年龄相关的关键区域。

Conclusion: 心脏形态可用于预测年龄，显著区域分析提高了模型的可解释性。

Abstract: Age prediction from medical images or other health-related non-imaging data
is an important approach to data-driven aging research, providing knowledge of
how much information a specific tissue or organ carries about the chronological
age of the individual. In this work, we studied the prediction of age from
computed tomography angiography (CTA) images, which provide detailed
representations of the heart morphology, with the goals of (i) studying the
relationship between morphology and aging, and (ii) developing a novel
\emph{morphological heart age} biomarker. We applied an image
registration-based method that standardizes the images from the whole cohort
into a single space. We then extracted supervoxels (using unsupervised
segmentation), and corresponding robust features of density and local volume,
which provide a detailed representation of the heart morphology while being
robust to registration errors. Machine learning models are then trained to fit
regression models from these features to the chronological age. We applied the
method to a subset of the images from the Swedish CArdioPulomonary bioImage
Study (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a
mean absolute error of $2.74$ years for females and $2.77$ years for males. The
predictions from different sub-regions of interest were observed to be more
highly correlated with the predictions from the whole heart, compared to the
chronological age, revealing a high consistency in the predictions from
morphology. Saliency analysis was also performed on the prediction models to
study what regions are associated positively and negatively with the predicted
age. This resulted in detailed association maps where the density and volume of
known, as well as some novel sub-regions of interest, are determined to be
important. The saliency analysis aids in the interpretability of the models and
their predictions.

</details>

### [166] [Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views](https://arxiv.org/abs/2504.15786)
*Ningli Xu,Rongjun Qin*

Main category: cs.CV

TLDR: 提出了一种基于固定潜在扩散模型的新方法，通过卫星引导去噪和卫星-时间去噪模块，解决了卫星图像生成地面视图时的一致性问题，并贡献了一个大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 卫星与地面视图之间存在视角和分辨率差异，导致生成的地面视图在相邻区域不一致。

Method: 采用固定潜在扩散模型，引入卫星引导去噪和卫星-时间去噪模块，确保多视图生成的一致性。

Result: 实验表明，该方法在感知和时间指标上优于现有方法，生成的多视图输出具有高真实感和一致性。

Conclusion: 该方法有效解决了跨视图合成中的一致性问题，为大规模地面场景或视频生成提供了支持。

Abstract: Generating consistent ground-view images from satellite imagery is
challenging, primarily due to the large discrepancies in viewing angles and
resolution between satellite and ground-level domains. Previous efforts mainly
concentrated on single-view generation, often resulting in inconsistencies
across neighboring ground views. In this work, we propose a novel cross-view
synthesis approach designed to overcome these challenges by ensuring
consistency across ground-view images generated from satellite views. Our
method, based on a fixed latent diffusion model, introduces two conditioning
modules: satellite-guided denoising, which extracts high-level scene layout to
guide the denoising process, and satellite-temporal denoising, which captures
camera motion to maintain consistency across multiple generated views. We
further contribute a large-scale satellite-ground dataset containing over
100,000 perspective pairs to facilitate extensive ground scene or video
generation. Experimental results demonstrate that our approach outperforms
existing methods on perceptual and temporal metrics, achieving high
photorealism and consistency in multi-view outputs.

</details>

### [167] [Development and evaluation of a deep learning algorithm for German word recognition from lip movements](https://arxiv.org/abs/2504.15792)
*Dinh Nam Pham,Torsten Rahne*

Main category: cs.CV

TLDR: 该论文开发了一种基于神经网络的德语唇读算法，通过3D CNN和GRU模型的结合（GRUConv），在已知和未知说话者中分别达到87%和63%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有唇读算法多针对英语，缺乏德语支持，且传统方法依赖视觉信息易出错。

Method: 使用1806个德语视频片段，提取38,391个单词片段，训练3D CNN、GRU及GRUConv模型，比较不同图像区域和颜色空间的效果。

Result: GRUConv模型在已知说话者中准确率达87%，未知说话者中为63%；唇部区域裁剪显著提高准确率至70%。

Conclusion: 首次开发的德语唇读神经网络表现优异，准确率与英语算法相当，且可泛化至更多词汇类别。

Abstract: When reading lips, many people benefit from additional visual information
from the lip movements of the speaker, which is, however, very error prone.
Algorithms for lip reading with artificial intelligence based on artificial
neural networks significantly improve word recognition but are not available
for the German language. A total of 1806 video clips with only one
German-speaking person each were selected, split into word segments, and
assigned to word classes using speech-recognition software. In 38,391 video
segments with 32 speakers, 18 polysyllabic, visually distinguishable words were
used to train and validate a neural network. The 3D Convolutional Neural
Network and Gated Recurrent Units models and a combination of both models
(GRUConv) were compared, as were different image sections and color spaces of
the videos. The accuracy was determined in 5000 training epochs. Comparison of
the color spaces did not reveal any relevant different correct classification
rates in the range from 69% to 72%. With a cut to the lips, a significantly
higher accuracy of 70% was achieved than when cut to the entire speaker's face
(34%). With the GRUConv model, the maximum accuracies were 87% with known
speakers and 63% in the validation with unknown speakers. The neural network
for lip reading, which was first developed for the German language, shows a
very high level of accuracy, comparable to English-language algorithms. It
works with unknown speakers as well and can be generalized with more word
classes.

</details>

### [168] [Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness](https://arxiv.org/abs/2504.15796)
*Jiaqi Tang,Yinsong Xu,Qingchao Chen*

Main category: cs.CV

TLDR: 提出了一种基于显著性图的采样块（SM-DSB），用于解决点云无监督域适应中自监督任务梯度冲突的问题，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在点云无监督域适应中采用多任务学习框架，但部分自监督任务的梯度可能对分类性能产生负面影响。

Method: 设计了一种基于3D显著性图偏度的评分机制，动态筛选对分类有益的自监督样本，减少梯度冲突。

Result: 实验表明，该方法优于现有技术，且计算开销小，可扩展性强。

Conclusion: SM-DSB为无监督域适应问题提供了新视角，并通过反向传播分析深化了对该问题的理解。

Abstract: Object classification models utilizing point cloud data are fundamental for
3D media understanding, yet they often struggle with unseen or
out-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain
adaptation (UDA) methods typically employ a multi-task learning (MTL) framework
that combines primary classification tasks with auxiliary self-supervision
tasks to bridge the gap between cross-domain feature distributions. However,
our further experiments demonstrate that not all gradients from
self-supervision tasks are beneficial and some may negatively impact the
classification performance. In this paper, we propose a novel solution, termed
Saliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient
conflicts. Specifically, our method designs a new scoring mechanism based on
the skewness of 3D saliency maps to estimate gradient conflicts without
requiring target labels. Leveraging this, we develop a sample selection
strategy that dynamically filters out samples whose self-supervision gradients
are not beneficial for the classification. Our approach is scalable,
introducing modest computational overhead, and can be integrated into all the
point cloud UDA MTL frameworks. Extensive evaluations demonstrate that our
method outperforms state-of-the-art approaches. In addition, we provide a new
perspective on understanding the UDA problem through back-propagation analysis.

</details>

### [169] [Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models](https://arxiv.org/abs/2504.15823)
*Songyan Xie,Jinghang Wen,Encheng Su,Qiucheng Yu*

Main category: cs.CV

TLDR: 提出了一种新型、隐蔽且实用的对抗性补丁方法，用于攻击近红外（NIR）人脸识别系统，在数字和物理领域均显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 近红外人脸识别系统在低光或化妆条件下表现良好，但易受物理对抗攻击，研究旨在揭示其实际应用中的潜在风险。

Method: 利用人眼不可见的红外吸收墨水生成数字优化的补丁形状和位置，并通过模拟NIR光反射的皮肤反射模型减少数字与物理成像的差异。

Result: 在物理领域中，该方法平均攻击成功率达82.46%，优于现有方法的64.18%，且在不同姿态下保持高效。

Conclusion: 该方法显著提升了NIR人脸识别系统的对抗攻击效果，为实际应用中的安全风险提供了新视角。

Abstract: Near-infrared (NIR) face recognition systems, which can operate effectively
in low-light conditions or in the presence of makeup, exhibit vulnerabilities
when subjected to physical adversarial attacks. To further demonstrate the
potential risks in real-world applications, we design a novel, stealthy, and
practical adversarial patch to attack NIR face recognition systems in a
black-box setting. We achieved this by utilizing human-imperceptible
infrared-absorbing ink to generate multiple patches with digitally optimized
shapes and positions for infrared images. To address the optimization mismatch
between digital and real-world NIR imaging, we develop a light reflection model
for human skin to minimize pixel-level discrepancies by simulating NIR light
reflection.
  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition
systems, the experimental results show that our method improves the attack
success rate in both digital and physical domains, particularly maintaining
effectiveness across various face postures. Notably, the proposed approach
outperforms SOTA methods, achieving an average attack success rate of 82.46% in
the physical domain across different models, compared to 64.18% for existing
methods. The artifact is available at
https://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.

</details>

### [170] [Text-based Animatable 3D Avatars with Morphable Model Alignment](https://arxiv.org/abs/2504.15835)
*Yiqian Wu,Malte Prinzler,Xiaogang Jin,Siyu Tang*

Main category: cs.CV

TLDR: 本文提出了一种名为AnimPortrait3D的新框架，用于从文本生成高质量、可动画的3D头像，解决了现有方法在细节合成和语义对齐上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的3D头像生成方法在细节真实性和动画自然性上存在局限，主要源于2D扩散预测的模糊性和与参数化模型的语义对齐不足。

Method: 结合预训练的文本到3D模型初始化头像，并使用ControlNet基于参数化模型的语义和法线图进行动态表情细化。

Result: 实验表明，该方法在合成质量、对齐精度和动画保真度上优于现有方法。

Conclusion: AnimPortrait3D在文本驱动的可动画3D头像生成领域取得了显著进展。

Abstract: The generation of high-quality, animatable 3D head avatars from text has
enormous potential in content creation applications such as games, movies, and
embodied virtual assistants. Current text-to-3D generation methods typically
combine parametric head models with 2D diffusion models using score
distillation sampling to produce 3D-consistent results. However, they struggle
to synthesize realistic details and suffer from misalignments between the
appearance and the driving parametric model, resulting in unnatural animation
results. We discovered that these limitations stem from ambiguities in the 2D
diffusion predictions during 3D avatar distillation, specifically: i) the
avatar's appearance and geometry is underconstrained by the text input, and ii)
the semantic alignment between the predictions and the parametric head model is
insufficient because the diffusion model alone cannot incorporate information
from the parametric model. In this work, we propose a novel framework,
AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with
morphable model alignment, and introduce two key strategies to address these
challenges. First, we tackle appearance and geometry ambiguities by utilizing
prior information from a pretrained text-to-3D model to initialize a 3D avatar
with robust appearance, geometry, and rigging relationships to the morphable
model. Second, we refine the initial 3D avatar for dynamic expressions using a
ControlNet that is conditioned on semantic and normal maps of the morphable
model to ensure accurate alignment. As a result, our method outperforms
existing approaches in terms of synthesis quality, alignment, and animation
fidelity. Our experiments show that the proposed method advances the state of
the art in text-based, animatable 3D head avatar generation.

</details>

### [171] [DERD-Net: Learning Depth from Event-based Ray Densities](https://arxiv.org/abs/2504.15863)
*Diego de Oliveira Hitzges,Suman Ghosh,Guillermo Gallego*

Main category: cs.CV

TLDR: 提出了一种用于事件相机深度估计的可扩展、灵活且适应性强的框架，通过处理局部子区域的DSIs，结合3D卷积和循环结构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习框架难以处理事件数据的异步流特性，需要一种适应事件相机特性的新方法。

Method: 将3D场景结构编码为DSIs，通过3D卷积和循环结构处理局部子区域，实现像素级深度估计。

Result: 在标准数据集上表现优异：单目数据媲美现有立体方法，立体数据性能超越SOTA，误差降低至少42%，深度完整性提升3倍以上。

Conclusion: 该框架在事件相机深度估计和SLAM中具有成为标准方法的潜力。

Abstract: Event cameras offer a promising avenue for multi-view stereo depth estimation
and Simultaneous Localization And Mapping (SLAM) due to their ability to detect
blur-free 3D edges at high-speed and over broad illumination conditions.
However, traditional deep learning frameworks designed for conventional cameras
struggle with the asynchronous, stream-like nature of event data, as their
architectures are optimized for discrete, image-like inputs. We propose a
scalable, flexible and adaptable framework for pixel-wise depth estimation with
event cameras in both monocular and stereo setups. The 3D scene structure is
encoded into disparity space images (DSIs), representing spatial densities of
rays obtained by back-projecting events into space via known camera poses. Our
neural network processes local subregions of the DSIs combining 3D convolutions
and a recurrent structure to recognize valuable patterns for depth prediction.
Local processing enables fast inference with full parallelization and ensures
constant ultra-low model complexity and memory costs, regardless of camera
resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)
demonstrate unprecedented effectiveness: (i) using purely monocular data, our
method achieves comparable results to existing stereo methods; (ii) when
applied to stereo data, it strongly outperforms all state-of-the-art (SOTA)
approaches, reducing the mean absolute error by at least 42%; (iii) our method
also allows for increases in depth completeness by more than 3-fold while still
yielding a reduction in median absolute error of at least 30%. Given its
remarkable performance and effective processing of event-data, our framework
holds strong potential to become a standard approach for using deep learning
for event-based depth estimation and SLAM. Project page:
https://github.com/tub-rip/DERD-Net

</details>

### [172] [MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search](https://arxiv.org/abs/2504.15865)
*Lotfi Abdelkrim Mecharbat,Ibrahim Elmakky,Martin Takac,Mohammed Yaqub*

Main category: cs.CV

TLDR: MedNNS是一个针对医学影像的神经网络搜索框架，联合优化架构选择和权重初始化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学影像中深度学习模型的适应性问题，包括架构选择和权重初始化的挑战，以及ImageNet迁移学习的局限性。

Method: 通过构建一个基于Supernetwork的元空间，结合rank loss和FID loss优化模型与数据集的匹配。

Result: 在多个数据集上，MedNNS平均准确率提升1.7%，且收敛速度更快。

Conclusion: MedNNS为医学影像任务提供了一种高效的神经网络搜索解决方案。

Abstract: Deep learning (DL) has achieved remarkable progress in the field of medical
imaging. However, adapting DL models to medical tasks remains a significant
challenge, primarily due to two key factors: (1) architecture selection, as
different tasks necessitate specialized model designs, and (2) weight
initialization, which directly impacts the convergence speed and final
performance of the models. Although transfer learning from ImageNet is a widely
adopted strategy, its effectiveness is constrained by the substantial
differences between natural and medical images. To address these challenges, we
introduce Medical Neural Network Search (MedNNS), the first Neural Network
Search framework for medical imaging applications. MedNNS jointly optimizes
architecture selection and weight initialization by constructing a meta-space
that encodes datasets and models based on how well they perform together. We
build this space using a Supernetwork-based approach, expanding the model zoo
size by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we
introduce rank loss and Fr\'echet Inception Distance (FID) loss into the
construction of the space to capture inter-model and inter-dataset
relationships, thereby achieving more accurate alignment in the meta-space.
Experimental results across multiple datasets demonstrate that MedNNS
significantly outperforms both ImageNet pre-trained DL models and SOTA Neural
Architecture Search (NAS) methods, achieving an average accuracy improvement of
1.7% across datasets while converging substantially faster. The code and the
processed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.

</details>

### [173] [Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading](https://arxiv.org/abs/2504.15883)
*Farida Mohsen,Samir Belhaouari,Zubair Shah*

Main category: cs.CV

TLDR: RadFuse是一种多表示深度学习框架，结合了RadEx变换的sinogram图像和传统眼底图像，显著提升了糖尿病视网膜病变的检测和分级性能。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变的复杂和不规则病变模式使得传统方法难以捕捉细微变化，需要更有效的特征提取方法。

Method: 提出RadFuse框架，整合RadEx变换生成的sinogram图像和传统眼底图像，利用ResNeXt-50、MobileNetV2和VGG19三种CNN进行实验。

Result: 在APTOS-2019和DDR数据集上，RadFuse在五级严重性分级和二元分类中均显著优于现有方法，表现出更高的准确性和鲁棒性。

Conclusion: RadFuse通过捕捉复杂的非线性特征，推动了糖尿病视网膜病变分类的进步，并展示了高级数学变换在医学图像分析中的潜力。

Abstract: Diabetic retinopathy is a serious ocular complication that poses a
significant threat to patients' vision and overall health. Early detection and
accurate grading are essential to prevent vision loss. Current automatic
grading methods rely heavily on deep learning applied to retinal fundus images,
but the complex, irregular patterns of lesions in these images, which vary in
shape and distribution, make it difficult to capture subtle changes. This study
introduces RadFuse, a multi-representation deep learning framework that
integrates non-linear RadEx-transformed sinogram images with traditional fundus
images to enhance diabetic retinopathy detection and grading. Our RadEx
transformation, an optimized non-linear extension of the Radon transform,
generates sinogram representations to capture complex retinal lesion patterns.
By leveraging both spatial and transformed domain information, RadFuse enriches
the feature set available to deep learning models, improving the
differentiation of severity levels. We conducted extensive experiments on two
benchmark datasets, APTOS-2019 and DDR, using three convolutional neural
networks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant
improvements over fundus-image-only models across all three CNN architectures
and outperformed state-of-the-art methods on both datasets. For severity
grading across five stages, RadFuse achieved a quadratic weighted kappa of
93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary
classification between healthy and diabetic retinopathy cases, the method
reached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,
surpassing previously established models. These results demonstrate RadFuse's
capacity to capture complex non-linear features, advancing diabetic retinopathy
classification and promoting the integration of advanced mathematical
transforms in medical image analysis.

</details>

### [174] [MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2504.15888)
*Zhiqiang Wei,Lianqing Zheng,Jianan Liu,Tao Huang,Qing-Long Han,Wenwen Zhang,Fengdeng Zhang*

Main category: cs.CV

TLDR: MS-Occ提出了一种多阶段LiDAR-相机融合框架，通过中间和后期融合结合LiDAR的几何精度与相机的语义丰富性，显著提升了3D语义占用感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉方法几何不准确和LiDAR方法语义信息不足的问题，提升自动驾驶在复杂环境中的感知能力。

Method: 采用多阶段融合框架，包括中间阶段的Gaussian-Geo模块和Semantic-Aware模块，以及后期阶段的Adaptive Fusion和HCCVF模块。

Result: 在nuScenes-OpenOccupancy基准测试中，IoU为32.1%，mIoU为25.3%，优于现有方法。

Conclusion: MS-Occ通过模块化设计显著提升了3D语义占用感知性能，尤其在小型物体感知上表现突出，适用于安全关键的自动驾驶场景。

Abstract: Accurate 3D semantic occupancy perception is essential for autonomous driving
in complex environments with diverse and irregular objects. While
vision-centric methods suffer from geometric inaccuracies, LiDAR-based
approaches often lack rich semantic information. To address these limitations,
MS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes
middle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's
geometric fidelity with camera-based semantic richness via hierarchical
cross-modal fusion. The framework introduces innovations at two critical
stages: (1) In the middle-stage feature fusion, the Gaussian-Geo module
leverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D
image features with dense geometric priors, and the Semantic-Aware module
enriches LiDAR voxels with semantic context via deformable cross-attention; (2)
In the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically
balances voxel features across modalities, while the High Classification
Confidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using
self-attention-based refinement. Experiments on the nuScenes-OpenOccupancy
benchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%
and a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU
and +2.4% mIoU. Ablation studies further validate the contribution of each
module, with substantial improvements in small-object perception, demonstrating
the practical value of MS-Occ for safety-critical autonomous driving scenarios.

</details>

### [175] [Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions](https://arxiv.org/abs/2504.15918)
*Chang Zong,Bin Li,Shoujun Zhou,Jian Wan,Lei Zhang*

Main category: cs.CV

TLDR: 论文提出了一种新任务In-VAL，模拟人与视频的多次交互以获取视觉答案，并提出了Ask2Loc框架解决语义差距问题，实验显示其性能显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 用户在获取视频指导知识时需多次交互以精确定位内容，现有方法无法有效模拟这一过程，因此提出In-VAL任务。

Method: 提出Ask2Loc框架，包含聊天模块（澄清意图）、重写模块（生成完整描述）和搜索模块（整合内容）。

Result: 在三个重构的In-VAL数据集上，Ask2Loc性能提升高达14.91（mIoU）。

Conclusion: Ask2Loc通过交互式提问有效解决了In-VAL任务中的语义差距问题，性能优于传统方法。

Abstract: Locating specific segments within an instructional video is an efficient way
to acquire guiding knowledge. Generally, the task of obtaining video segments
for both verbal explanations and visual demonstrations is known as visual
answer localization (VAL). However, users often need multiple interactions to
obtain answers that align with their expectations when using the system. During
these interactions, humans deepen their understanding of the video content by
asking themselves questions, thereby accurately identifying the location.
Therefore, we propose a new task, named In-VAL, to simulate the multiple
interactions between humans and videos in the procedure of obtaining visual
answers. The In-VAL task requires interactively addressing several semantic gap
issues, including 1) the ambiguity of user intent in the input questions, 2)
the incompleteness of language in video subtitles, and 3) the fragmentation of
content in video segments. To address these issues, we propose Ask2Loc, a
framework for resolving In-VAL by asking questions. It includes three key
modules: 1) a chatting module to refine initial questions and uncover clear
intentions, 2) a rewriting module to generate fluent language and create
complete descriptions, and 3) a searching module to broaden local context and
provide integrated content. We conduct extensive experiments on three
reconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage
methods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on
the In-VAL task. Our code and datasets can be accessed at
https://github.com/changzong/Ask2Loc.

</details>

### [176] [ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](https://arxiv.org/abs/2504.15921)
*Jian Hu,Dimitrios Korkinof,Shaogang Gong,Mariano Beguerisse-Diaz*

Main category: cs.CV

TLDR: ViSMaP是一种无监督视频摘要系统，利用LLM生成伪摘要，避免对长视频进行昂贵标注。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型在长视频摘要上表现不佳，且依赖昂贵标注。ViSMaP旨在填补短视频（标注丰富）和长视频（标注稀缺）之间的差距。

Method: 采用元提示策略，通过三个LLM迭代生成、评估和优化伪摘要，利用短视频片段描述指导长视频摘要生成。

Result: ViSMaP在多个数据集上表现与全监督SOTA模型相当，且能跨领域泛化。

Conclusion: ViSMaP通过无监督方法实现了高效的长视频摘要，避免了标注成本。

Abstract: We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a
system to summarise hour long videos with no-supervision. Most existing video
understanding models work well on short videos of pre-segmented events, yet
they struggle to summarise longer videos where relevant events are sparsely
distributed and not pre-segmented. Moreover, long-form video understanding
often relies on supervised hierarchical training that needs extensive
annotations which are costly, slow and prone to inconsistency. With ViSMaP we
bridge the gap between short videos (where annotated data is plentiful) and
long ones (where it's not). We rely on LLMs to create optimised
pseudo-summaries of long videos using segment descriptions from short ones.
These pseudo-summaries are used as training data for a model that generates
long-form video summaries, bypassing the need for expensive annotations of long
videos. Specifically, we adopt a meta-prompting strategy to iteratively
generate and refine creating pseudo-summaries of long videos. The strategy
leverages short clip descriptions obtained from a supervised short video model
to guide the summary. Each iteration uses three LLMs working in sequence: one
to generate the pseudo-summary from clip descriptions, another to evaluate it,
and a third to optimise the prompt of the generator. This iteration is
necessary because the quality of the pseudo-summaries is highly dependent on
the generator prompt, and varies widely among videos. We evaluate our summaries
extensively on multiple datasets; our results show that ViSMaP achieves
performance comparable to fully supervised state-of-the-art models while
generalising across domains without sacrificing performance. Code will be
released upon publication.

</details>

### [177] [A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers](https://arxiv.org/abs/2504.15928)
*Meng Wang,Tian Lin,Qingshan Hou,Aidi Lin,Jingcheng Wang,Qingsheng Peng,Truong X. Nguyen,Danqi Fang,Ke Zou,Ting Xu,Cancan Xue,Ten Cheer Quek,Qinkai Yu,Minxin Liu,Hui Zhou,Zixuan Xiao,Guiqin He,Huiyu Liang,Tingkun Shi,Man Chen,Linna Liu,Yuanyuan Peng,Lianyu Wang,Qiuming Hu,Junhong Chen,Zhenhua Zhang,Cheng Chen,Yitian Zhao,Dianbo Liu,Jianhua Wu,Xinjian Chen,Changqing Zhang,Triet Thanh Nguyen,Yanda Meng,Yalin Zheng,Yih Chung Tham,Carol Y. Cheung,Huazhu Fu,Haoyu Chen,Ching-Yu Cheng*

Main category: cs.CV

TLDR: GlobeReady是一个无需重新训练即可跨临床中心应用的AI平台，用于眼科疾病诊断，具有高准确性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在不同临床中心部署时需要重新训练，限制了其广泛应用。GlobeReady旨在解决这一问题。

Method: 通过训练无关的局部特征增强技术，应对不同中心和人群的领域偏移，并结合置信度量化诊断方法。

Result: 在多个国家和数据集上表现出高准确性（如中国88.9%，越南86.3%，英国90.2%），临床评分高（4.6/5）。

Conclusion: GlobeReady展示了无需技术障碍的稳健、可扩展的眼科诊断潜力。

Abstract: Artificial intelligence (AI) shows remarkable potential in medical imaging
diagnostics, but current models typically require retraining when deployed
across different clinical centers, limiting their widespread adoption. We
introduce GlobeReady, a clinician-friendly AI platform that enables ocular
disease diagnosis without retraining/fine-tuning or technical expertise.
GlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an
11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.
Through training-free local feature augmentation, it addresses domain shifts
across centers and populations, reaching an average accuracy of 88.9% across
five centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in
confidence-quantifiable diagnostic approach further boosted accuracy to
94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution
cases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians
from multiple countries rated GlobeReady highly (average 4.6 out of 5) for its
usability and clinical relevance. These results demonstrate GlobeReady's
robust, scalable diagnostic capability and potential to support ophthalmic care
without technical barriers.

</details>

### [178] [Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models](https://arxiv.org/abs/2504.15929)
*Saban Ozturk,Melih B. Yilmaz,Muti Kara,M. Talat Yavuz,Aykut Koç,Tolga Çukur*

Main category: cs.CV

TLDR: MedTrim提出了一种新的医学视觉语言模型对齐方法，通过多模态三元组学习优化图像和文本的对齐，提升细粒度病理属性的区分能力。


<details>
  <summary>Details</summary>
Motivation: 医学影像和报告数据量增长导致专家压力增加，现有对齐方法忽视细粒度病理属性，影响模型性能。

Method: MedTrim结合疾病类别和病理描述符，通过元实体识别模块和新型评分函数优化三元组挖掘，实现多模态对齐。

Result: MedTrim在下游检索和分类任务中表现优于现有对齐方法。

Conclusion: MedTrim通过细粒度对齐提升了医学视觉语言模型的性能，具有临床应用潜力。

Abstract: Diagnostic imaging relies on interpreting both images and radiology reports,
but the growing data volumes place significant pressure on medical experts,
yielding increased errors and workflow backlogs. Medical vision-language models
(med-VLMs) have emerged as a powerful framework to efficiently process
multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit
their performance hinges on how well image and text representations are
aligned. Existing alignment methods, predominantly based on contrastive
learning, prioritize separation between disease classes over segregation of
fine-grained pathology attributes like location, size or severity, leading to
suboptimal representations. Here, we propose MedTrim (Meta-entity-driven
Triplet mining), a novel method that enhances image-text alignment through
multimodal triplet learning synergistically guided by disease class as well as
adjectival and directional pathology descriptors. Unlike common alignment
methods that separate broad disease classes, MedTrim leverages structured
meta-entity information to preserve subtle but clinically significant
intra-class variations. For this purpose, we first introduce an ontology-based
entity recognition module that extracts pathology-specific meta-entities from
CXR reports, as annotations on pathology attributes are rare in public
datasets. For refined sample selection in triplet mining, we then introduce a
novel score function that captures an aggregate measure of inter-sample
similarity based on disease classes and adjectival/directional descriptors.
Lastly, we introduce a multimodal triplet alignment objective for explicit
within- and cross-modal alignment between samples sharing detailed pathology
characteristics. Our demonstrations indicate that MedTrim improves performance
in downstream retrieval and classification tasks compared to state-of-the-art
alignment methods.

</details>

### [179] [Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time](https://arxiv.org/abs/2504.15931)
*Ekaterina Kondrateva,Sandzhi Barg,Mikhail Vasiliev*

Main category: cs.CV

TLDR: 该研究评估了FastSurfer和SynthSeg两种现代脑部分割流程在纵向和多中心研究中的表现，揭示了小脑区体积变异性问题，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 脑部MRI的准确性和可重复性对于监测神经解剖变化至关重要，但扫描仪引起的变异性限制了深度学习分割流程的可靠性。

Method: 使用SIMON纵向队列和SRPBS多中心测试-重测队列，通过Dice系数、Surface Dice、Hausdorff距离和MAPE量化分割变异性。

Result: 研究发现小脑区（如杏仁核）体积变异性高达7-8%，并分析了配准模板和插值模式的影响。

Conclusion: 研究强调了脑部MRI研究中标准化策略的必要性，并提出了基于表面的质量过滤方法以提高分割可靠性。

Abstract: Accurate and reproducible brain morphometry from structural MRI is critical
for monitoring neuroanatomical changes across time and across imaging domains.
Although deep learning has accelerated segmentation workflows, scanner-induced
variability and reproducibility limitations remain-especially in longitudinal
and multi-site settings. In this study, we benchmark two modern segmentation
pipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the
most widely adopted tools in neuroimaging.
  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and
a 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation
variability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),
and Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume
variation in small subcortical structures such as the amygdala and ventral
diencephalon, even under controlled test-retest conditions. This raises a key
question: is it feasible to detect subtle longitudinal changes on the order of
5-10% in pea-sized brain regions, given the magnitude of domain-induced
morphometric noise?
  We further analyze the effects of registration templates and interpolation
modes, and propose surface-based quality filtering to improve segmentation
reliability. This study provides a reproducible benchmark for morphometric
reproducibility and emphasizes the need for harmonization strategies in
real-world neuroimaging studies.
  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation

</details>

### [180] [Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning](https://arxiv.org/abs/2504.15932)
*Wang Lin,Liyu Jia,Wentao Hu,Kaihang Pan,Zhongqi Yue,Wei Zhao,Jingyuan Chen,Fei Wu,Hanwang Zhang*

Main category: cs.CV

TLDR: 论文提出了一种结合符号推理和强化学习的方法（Phys-AR框架），通过Diffusion Timestep Tokenizer（DDT）和两阶段训练，生成符合物理规律的视频。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的方法难以生成符合物理规律的视频，尤其是对未见过的物理条件（如速度）的泛化能力不足。

Method: 1. 引入DDT学习离散递归视觉标记，支持符号推理；2. Phys-AR框架分两阶段：监督微调传递符号知识，强化学习优化推理能力。

Result: 实验证明Phys-AR能生成物理一致的视频。

Conclusion: 结合符号推理和强化学习的方法有效提升了视频生成的物理一致性。

Abstract: Despite recent progress in video generation, producing videos that adhere to
physical laws remains a significant challenge. Traditional diffusion-based
methods struggle to extrapolate to unseen physical conditions (eg, velocity)
due to their reliance on data-driven approximations. To address this, we
propose to integrate symbolic reasoning and reinforcement learning to enforce
physical consistency in video generation. We first introduce the Diffusion
Timestep Tokenizer (DDT), which learns discrete, recursive visual tokens by
recovering visual attributes lost during the diffusion process. The recursive
visual tokens enable symbolic reasoning by a large language model. Based on it,
we propose the Phys-AR framework, which consists of two stages: The first stage
uses supervised fine-tuning to transfer symbolic knowledge, while the second
stage applies reinforcement learning to optimize the model's reasoning
abilities through reward functions based on physical conditions. Our approach
allows the model to dynamically adjust and improve the physical properties of
generated videos, ensuring adherence to physical laws. Experimental results
demonstrate that PhysAR can generate videos that are physically consistent.

</details>

### [181] [FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation](https://arxiv.org/abs/2504.15958)
*Zebin Yao,Lei Ren,Huixing Jiang,Chen Wei,Xiaojie Wang,Ruifan Li,Fangxiang Feng*

Main category: cs.CV

TLDR: FreeGraftor提出了一种无需训练的框架，通过跨图像特征嫁接解决主题驱动图像生成中保真度与效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在主题驱动图像生成中难以平衡保真度和效率，调优方法耗时耗资源，零样本方法无法保持主题一致性。

Method: FreeGraftor采用语义匹配和位置约束注意力融合，将参考主题的视觉细节转移到生成图像中，并结合噪声初始化策略保留几何先验。

Result: 实验表明，FreeGraftor在主题保真度和文本对齐方面显著优于现有零样本和无训练方法，并可扩展到多主题生成。

Conclusion: FreeGraftor是一种高效且无需训练的主题驱动图像生成框架，适用于实际应用。

Abstract: Subject-driven image generation aims to synthesize novel scenes that
faithfully preserve subject identity from reference images while adhering to
textual guidance, yet existing methods struggle with a critical trade-off
between fidelity and efficiency. Tuning-based approaches rely on time-consuming
and resource-intensive subject-specific optimization, while zero-shot methods
fail to maintain adequate subject consistency. In this work, we propose
FreeGraftor, a training-free framework that addresses these limitations through
cross-image feature grafting. Specifically, FreeGraftor employs semantic
matching and position-constrained attention fusion to transfer visual details
from reference subjects to the generated image. Additionally, our framework
incorporates a novel noise initialization strategy to preserve geometry priors
of reference subjects for robust feature matching. Extensive qualitative and
quantitative experiments demonstrate that our method enables precise subject
identity transfer while maintaining text-aligned scene synthesis. Without
requiring model fine-tuning or additional training, FreeGraftor significantly
outperforms existing zero-shot and training-free approaches in both subject
fidelity and text alignment. Furthermore, our framework can seamlessly extend
to multi-subject generation, making it practical for real-world deployment. Our
code is available at https://github.com/Nihukat/FreeGraftor.

</details>

### [182] [Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications](https://arxiv.org/abs/2504.15991)
*Leonardo Olivi,Edoardo Santero Mormile,Enzo Tartaglione*

Main category: cs.CV

TLDR: 该论文探讨了在月球和火星地形中，通过适配器技术实现高效迁移学习用于岩石分割的可行性，并提出了两种节省内存的策略。


<details>
  <summary>Details</summary>
Motivation: 解决地外探索中标记数据稀缺的问题，同时减少目标设备的带宽和内存需求。

Method: 在预训练骨干模型中集成适配器，采用层融合和适配器排名两种策略。

Result: 适配器技术成功减少了内存和带宽需求，同时任务性能得到保持。

Conclusion: 研究为地外设备上的高效迁移学习提供了新思路，并指出了未来研究方向。

Abstract: In recent years, the application of Deep Learning techniques has shown
remarkable success in various computer vision tasks, paving the way for their
deployment in extraterrestrial exploration. Transfer learning has emerged as a
powerful strategy for addressing the scarcity of labeled data in these novel
environments. This paper represents one of the first efforts in evaluating the
feasibility of employing adapters toward efficient transfer learning for rock
segmentation in extraterrestrial landscapes, mainly focusing on lunar and
martian terrains. Our work suggests that the use of adapters, strategically
integrated into a pre-trained backbone model, can be successful in reducing
both bandwidth and memory requirements for the target extraterrestrial device.
In this study, we considered two memory-saving strategies: layer fusion (to
reduce to zero the inference overhead) and an ``adapter ranking'' (to also
reduce the transmission cost). Finally, we evaluate these results in terms of
task performance, memory, and computation on embedded devices, evidencing
trade-offs that open the road to more research in the field.

</details>

### [183] [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
*Yimu Wang,Xuye Liu,Wei Pang,Li Ma,Shuai Yuan,Paul Debevec,Ning Yu*

Main category: cs.CV

TLDR: 本文综述了基于扩散模型的视频生成技术，涵盖其发展、技术基础、应用及挑战，提供了比现有综述更全面和细致的视角。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成中展现出优于传统方法的潜力，但在运动一致性、计算效率和伦理问题方面仍面临挑战，因此需要系统性的综述。

Method: 通过系统分类现有方法，分析架构创新和优化策略，并探讨其在低层视觉任务及相关领域的应用。

Result: 提供了对扩散模型在视频生成中的全面视角，包括评估指标、行业解决方案和训练工程技术。

Conclusion: 本文为研究者和从业者提供了理论和实践资源，推动了这一快速发展领域的进步。

Abstract: Recent advances in diffusion models have revolutionized video generation,
offering superior temporal consistency and visual quality compared to
traditional generative adversarial networks-based approaches. While this
emerging field shows tremendous promise in applications, it faces significant
challenges in motion consistency, computational efficiency, and ethical
considerations. This survey provides a comprehensive review of diffusion-based
video generation, examining its evolution, technical foundations, and practical
applications. We present a systematic taxonomy of current methodologies,
analyze architectural innovations and optimization strategies, and investigate
applications across low-level vision tasks such as denoising and
super-resolution. Additionally, we explore the synergies between diffusionbased
video generation and related domains, including video representation learning,
question answering, and retrieval. Compared to the existing surveys (Lei et
al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which
focus on specific aspects of video generation, such as human video synthesis
(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our
work provides a broader, more updated, and more fine-grained perspective on
diffusion-based approaches with a special section for evaluation metrics,
industry solutions, and training engineering techniques in video generation.
This survey serves as a foundational resource for researchers and practitioners
working at the intersection of diffusion models and video generation, providing
insights into both the theoretical frameworks and practical implementations
that drive this rapidly evolving field. A structured list of related works
involved in this survey is also available on
https://github.com/Eyeline-Research/Survey-Video-Diffusion.

</details>

### [184] [MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment](https://arxiv.org/abs/2504.16003)
*Yachun Mi,Yu Li,Weicheng Meng,Chaofeng Chen,Chen Hui,Shaohui Liu*

Main category: cs.CV

TLDR: MVQA结合Mamba模型和USDS采样方法，高效实现视频质量评估，性能接近SOTA，速度提升2倍，GPU内存需求仅为1/5。


<details>
  <summary>Details</summary>
Motivation: 长时长高清视频的增长使得高效视频质量评估（VQA）成为关键挑战，现有方法在效率和性能平衡上存在不足。

Method: 提出MVQA模型，基于Mamba结构，结合USDS采样方法，通过语义和失真采样捕获关键信息，并使用预定义掩码融合策略。

Result: MVQA性能接近SOTA方法，速度提升2倍，GPU内存需求仅为1/5。

Conclusion: MVQA和USDS为高效VQA提供了新思路，平衡了性能和效率。

Abstract: The rapid growth of long-duration, high-definition videos has made efficient
video quality assessment (VQA) a critical challenge. Existing research
typically tackles this problem through two main strategies: reducing model
parameters and resampling inputs. However, light-weight Convolution Neural
Networks (CNN) and Transformers often struggle to balance efficiency with high
performance due to the requirement of long-range modeling capabilities.
Recently, the state-space model, particularly Mamba, has emerged as a promising
alternative, offering linear complexity with respect to sequence length.
Meanwhile, efficient VQA heavily depends on resampling long sequences to
minimize computational costs, yet current resampling methods are often weak in
preserving essential semantic information. In this work, we present MVQA, a
Mamba-based model designed for efficient VQA along with a novel Unified
Semantic and Distortion Sampling (USDS) approach. USDS combines semantic patch
sampling from low-resolution videos and distortion patch sampling from
original-resolution videos. The former captures semantically dense regions,
while the latter retains critical distortion details. To prevent computation
increase from dual inputs, we propose a fusion mechanism using pre-defined
masks, enabling a unified sampling strategy that captures both semantic and
quality information without additional computational burden. Experiments show
that the proposed MVQA, equipped with USDS, achieve comparable performance to
state-of-the-art methods while being $2\times$ as fast and requiring only $1/5$
GPU memory.

</details>

### [185] [Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework](https://arxiv.org/abs/2504.16016)
*Xinyuan Song,Yangfan He,Sida Li,Jianhui Wang,Hongyang He,Xinhang Yuan,Ruoyu Wang,Jiaqi Chen,Keqin Li,Kuan Lu,Menghao Huo,Binxu Li,Pei Liu*

Main category: cs.CV

TLDR: 本文提出了一个通用的理论框架，用于在DDIM模型中通过适配器保持帧一致性，并证明了时间一致性目标的可微性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 适配器方法在视频编辑任务中能高效保持帧间一致性，但缺乏理论支持。本文旨在填补这一空白。

Method: 在DDIM模型中引入适配器，通过时间一致性损失优化，证明目标的可微性和梯度下降的收敛性。

Result: 证明了时间一致性目标的可微性、梯度下降的单调收敛性，以及DDIM反转过程中模块的稳定性。

Conclusion: 该理论框架增强了基于适配器的扩散视频编辑方法的可靠性，并为视频生成任务提供了理论依据。

Abstract: Adapter-based methods are commonly used to enhance model performance with
minimal additional complexity, especially in video editing tasks that require
frame-to-frame consistency. By inserting small, learnable modules into
pretrained diffusion models, these adapters can maintain temporal coherence
without extensive retraining. Approaches that incorporate prompt learning with
both shared and frame-specific tokens are particularly effective in preserving
continuity across frames at low training cost. In this work, we want to provide
a general theoretical framework for adapters that maintain frame consistency in
DDIM-based models under a temporal consistency loss. First, we prove that the
temporal consistency objective is differentiable under bounded feature norms,
and we establish a Lipschitz bound on its gradient. Second, we show that
gradient descent on this objective decreases the loss monotonically and
converges to a local minimum if the learning rate is within an appropriate
range. Finally, we analyze the stability of modules in the DDIM inversion
procedure, showing that the associated error remains controlled. These
theoretical findings will reinforce the reliability of diffusion-based video
editing methods that rely on adapter strategies and provide theoretical
insights in video generation tasks.

</details>

### [186] [PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning](https://arxiv.org/abs/2504.16023)
*Song Wang,Xiaolu Liu,Lingdong Kong,Jianyun Xu,Chunyong Hu,Gongfan Fang,Wentong Li,Jianke Zhu,Xinchao Wang*

Main category: cs.CV

TLDR: PointLoRA是一种结合低秩适应（LoRA）和多尺度令牌选择的高效点云模型微调方法，显著减少可调参数数量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型复杂度增加，完全微调需要大量计算和存储资源，而现有参数高效微调方法依赖复杂机制，增加了可调参数。

Method: 在点云变换器参数密集组件中嵌入LoRA层，结合多尺度令牌选择提取关键局部信息作为下游微调的提示。

Result: 实验表明，仅需3.43%的可调参数即可在多个预训练模型和公开数据集上实现竞争性性能。

Conclusion: PointLoRA是一种高效且资源友好的点云模型微调方法，适用于资源受限的应用场景。

Abstract: Self-supervised representation learning for point cloud has demonstrated
effectiveness in improving pre-trained model performance across diverse tasks.
However, as pre-trained models grow in complexity, fully fine-tuning them for
downstream applications demands substantial computational and storage
resources. Parameter-efficient fine-tuning (PEFT) methods offer a promising
solution to mitigate these resource requirements, yet most current approaches
rely on complex adapter and prompt mechanisms that increase tunable parameters.
In this paper, we propose PointLoRA, a simple yet effective method that
combines low-rank adaptation (LoRA) with multi-scale token selection to
efficiently fine-tune point cloud models. Our approach embeds LoRA layers
within the most parameter-intensive components of point cloud transformers,
reducing the need for tunable parameters while enhancing global feature
capture. Additionally, multi-scale token selection extracts critical local
information to serve as prompts for downstream fine-tuning, effectively
complementing the global context captured by LoRA. The experimental results
across various pre-trained models and three challenging public datasets
demonstrate that our approach achieves competitive performance with only 3.43%
of the trainable parameters, making it highly effective for
resource-constrained applications. Source code is available at:
https://github.com/songw-zju/PointLoRA.

</details>

### [187] [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030)
*Joya Chen,Ziyun Zeng,Yiqi Lin,Wei Li,Zejun Ma,Mike Zheng Shou*

Main category: cs.CV

TLDR: 本文提出了一种利用自动语音识别（ASR）转录文本进行大规模视频语言模型训练的方法，通过流式训练和时序对齐技术，显著提升了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型依赖昂贵的人工标注或专有API，限制了其规模化训练。本文旨在探索低成本ASR转录文本的应用，以解决这一问题。

Method: 提出了一种流式训练方法，将ASR文本与视频帧按时间戳密集交错对齐，并构建了Live-CC-5M和Live-WhisperX-526K数据集支持训练。

Result: ASR预训练的LiveCC-7B-Base模型在视频问答任务中表现优异，并具备实时视频评论能力。最终模型LiveCC-7B-Instruct在评论质量和视频问答任务中超越更大规模模型。

Conclusion: 该方法展示了ASR转录文本在大规模视频语言模型训练中的潜力，具有广泛的通用性和实际应用价值。

Abstract: Recent video large language models (Video LLMs) often depend on costly human
annotations or proprietary model APIs (e.g., GPT-4o) to produce training data,
which limits their training at scale. In this paper, we explore large-scale
training for Video LLM with cheap automatic speech recognition (ASR)
transcripts. Specifically, we propose a novel streaming training approach that
densely interleaves the ASR words and video frames according to their
timestamps. Compared to previous studies in vision-language representation with
ASR, our method naturally fits the streaming characteristics of ASR, thus
enabling the model to learn temporally-aligned, fine-grained vision-language
modeling. To support the training algorithm, we introduce a data production
pipeline to process YouTube videos and their closed captions (CC, same as ASR),
resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset
for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,
the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general
video QA performance and exhibits a new capability in real-time video
commentary. To evaluate this, we carefully design a new LiveSports-3K
benchmark, using LLM-as-a-judge to measure the free-form commentary.
Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B
models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even
working in a real-time mode. Meanwhile, it achieves state-of-the-art results at
the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,
demonstrating the broad generalizability of our approach. All resources of this
paper have been released at https://showlab.github.io/livecc.

</details>

### [188] [Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis](https://arxiv.org/abs/2504.16047)
*Frank Li,Hari Trivedi,Bardia Khosravi,Theo Dapamede,Mohammadreza Chavoshi,Abdulhameed Dere,Rohan Satya Isaac,Aawez Mansuri,Janice Newsome,Saptarshi Purkayastha,Judy Gichoya*

Main category: cs.CV

TLDR: 研究评估了三种视觉语言基础模型（RAD-DINO、CheXagent和BiomedCLIP）在放射学任务中的表现，发现预训练方法对下游任务性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在医学影像任务中的应用潜力，尤其是针对细粒度特征捕捉能力。

Method: 评估三种模型在胸部X光片的分类、分割和回归任务中的表现，并设计了一种结合全局和局部特征的自定义分割模型。

Result: RAD-DINO在分割任务中表现最佳，CheXagent在分类任务中表现优异，BiomedCLIP表现不稳定。自定义分割模型显著提升了性能。

Conclusion: 预训练方法对模型性能有显著影响，无文本监督模型适合分割任务，文本监督模型在分类和可解释性方面有优势。

Abstract: Foundation models, trained on vast amounts of data using self-supervised
techniques, have emerged as a promising frontier for advancing artificial
intelligence (AI) applications in medicine. This study evaluates three
different vision-language foundation models (RAD-DINO, CheXagent, and
BiomedCLIP) on their ability to capture fine-grained imaging features for
radiology tasks. The models were assessed across classification, segmentation,
and regression tasks for pneumothorax and cardiomegaly on chest radiographs.
Self-supervised RAD-DINO consistently excelled in segmentation tasks, while
text-supervised CheXagent demonstrated superior classification performance.
BiomedCLIP showed inconsistent performance across tasks. A custom segmentation
model that integrates global and local features substantially improved
performance for all foundation models, particularly for challenging
pneumothorax segmentation. The findings highlight that pre-training methodology
significantly influences model performance on specific downstream tasks. For
fine-grained segmentation tasks, models trained without text supervision
performed better, while text-supervised models offered advantages in
classification and interpretability. These insights provide guidance for
selecting foundation models based on specific clinical applications in
radiology.

</details>

### [189] [Vision language models are unreliable at trivial spatial cognition](https://arxiv.org/abs/2504.16061)
*Sangeet Khemlani,Tyler Tran,Nathaniel Gyory,Anthony M. Harrison,Wallace E. Lawson,Ravenna Thielstrom,Hunter Thompson,Taaren Singh,J. Gregory Trafton*

Main category: cs.CV

TLDR: 论文研究了视觉语言模型（VLMs）在空间认知任务中的可靠性，发现其性能受提示语微小变化影响，揭示了其在空间关系推理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs在空间认知任务中的表现，以评估其在实际应用中的可靠性。

Method: 开发了TableTest基准数据集，用于测试VLMs在简单空间关系任务中的表现。

Result: VLMs的性能因提示语的微小变化而下降，表明其在空间关系推理中存在局限性。

Conclusion: 研究揭示了VLMs的局限性，并提出了改进图像描述语料库以优化训练和测试的机会。

Abstract: Vision language models (VLMs) are designed to extract relevant visuospatial
information from images. Some research suggests that VLMs can exhibit humanlike
scene understanding, while other investigations reveal difficulties in their
ability to process relational information. To achieve widespread applicability,
VLMs must perform reliably, yielding comparable competence across a wide
variety of related tasks. We sought to test how reliable these architectures
are at engaging in trivial spatial cognition, e.g., recognizing whether one
object is left of another in an uncluttered scene. We developed a benchmark
dataset -- TableTest -- whose images depict 3D scenes of objects arranged on a
table, and used it to evaluate state-of-the-art VLMs. Results show that
performance could be degraded by minor variations of prompts that use logically
equivalent descriptions. These analyses suggest limitations in how VLMs may
reason about spatial relations in real-world applications. They also reveal
novel opportunities for bolstering image caption corpora for more efficient
training and testing.

</details>

### [190] [Boosting Generative Image Modeling via Joint Image-Feature Synthesis](https://arxiv.org/abs/2504.16064)
*Theodoros Kouzelis,Efstathios Karypidis,Ioannis Kakogeorgiou,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CV

TLDR: 提出了一种新的生成图像建模框架，通过联合建模低层次图像潜在变量和高层次语义特征，显著提升了生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决潜在扩散模型（LDMs）中表示学习与生成建模难以结合的问题。

Method: 利用扩散模型联合建模变分自编码器的低层次图像潜在变量和预训练自监督编码器（如DINO）的高层次语义特征。

Result: 在条件和非条件设置下，图像质量和训练收敛速度均有显著提升。

Conclusion: 为表示感知生成建模开辟了新方向，简化了训练并提供了新的推理策略（表示引导）。

Abstract: Latent diffusion models (LDMs) dominate high-quality image generation, yet
integrating representation learning with generative modeling remains a
challenge. We introduce a novel generative image modeling framework that
seamlessly bridges this gap by leveraging a diffusion model to jointly model
low-level image latents (from a variational autoencoder) and high-level
semantic features (from a pretrained self-supervised encoder like DINO). Our
latent-semantic diffusion approach learns to generate coherent image-feature
pairs from pure noise, significantly enhancing both generative quality and
training efficiency, all while requiring only minimal modifications to standard
Diffusion Transformer architectures. By eliminating the need for complex
distillation objectives, our unified design simplifies training and unlocks a
powerful new inference strategy: Representation Guidance, which leverages
learned semantics to steer and refine image generation. Evaluated in both
conditional and unconditional settings, our method delivers substantial
improvements in image quality and training convergence speed, establishing a
new direction for representation-aware generative modeling.

</details>

### [191] [Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/abs/2504.16072)
*Long Lian,Yifan Ding,Yunhao Ge,Sifei Liu,Hanzi Mao,Boyi Li,Marco Pavone,Ming-Yu Liu,Trevor Darrell,Adam Yala,Yin Cui*

Main category: cs.CV

TLDR: DAM模型通过焦点提示和局部视觉骨干网络实现详细局部描述生成，结合半监督学习数据管道解决数据稀缺问题，并在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在生成图像和视频特定区域详细描述时的挑战。

Method: 提出DAM模型，结合焦点提示和局部视觉骨干网络；设计半监督学习数据管道DLC-SDP；引入DLC-Bench基准测试。

Result: DAM在7个基准测试中达到最优性能，涵盖关键词、短语和多句子级别的局部描述任务。

Conclusion: DAM通过创新设计和数据管道，显著提升了局部描述生成的性能，为相关领域提供了新工具。

Abstract: Generating detailed and accurate descriptions for specific regions in images
and videos remains a fundamental challenge for vision-language models. We
introduce the Describe Anything Model (DAM), a model designed for detailed
localized captioning (DLC). DAM preserves both local details and global context
through two key innovations: a focal prompt, which ensures high-resolution
encoding of targeted regions, and a localized vision backbone, which integrates
precise localization with its broader context. To tackle the scarcity of
high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data
Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and
expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark
designed to evaluate DLC without relying on reference captions. DAM sets new
state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and
detailed multi-sentence localized image and video captioning.

</details>

### [192] [From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning](https://arxiv.org/abs/2504.16080)
*Le Zhuo,Liangbing Zhao,Sayak Paul,Yue Liao,Renrui Zhang,Yi Xin,Peng Gao,Mohamed Elhoseiny,Hongsheng Li*

Main category: cs.CV

TLDR: ReflectionFlow是一个推理时框架，通过噪声级、提示级和反射级三种扩展轴，帮助扩散模型迭代优化输出质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在复杂场景和细节处理上表现不足，受大型语言模型自反思能力启发，提出改进方法。

Method: 引入噪声级、提示级和反射级三种扩展轴，构建GenRef数据集进行反射调优。

Result: ReflectionFlow显著优于传统噪声级扩展方法，提升图像合成质量。

Conclusion: ReflectionFlow为高质量图像合成提供了可扩展且计算高效的解决方案。

Abstract: Recent text-to-image diffusion models achieve impressive visual quality
through extensive scaling of training data and model parameters, yet they often
struggle with complex scenes and fine-grained details. Inspired by the
self-reflection capabilities emergent in large language models, we propose
ReflectionFlow, an inference-time framework enabling diffusion models to
iteratively reflect upon and refine their outputs. ReflectionFlow introduces
three complementary inference-time scaling axes: (1) noise-level scaling to
optimize latent initialization; (2) prompt-level scaling for precise semantic
guidance; and most notably, (3) reflection-level scaling, which explicitly
provides actionable reflections to iteratively assess and correct previous
generations. To facilitate reflection-level scaling, we construct GenRef, a
large-scale dataset comprising 1 million triplets, each containing a
reflection, a flawed image, and an enhanced image. Leveraging this dataset, we
efficiently perform reflection tuning on state-of-the-art diffusion
transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified
framework. Experimental results show that ReflectionFlow significantly
outperforms naive noise-level scaling methods, offering a scalable and
compute-efficient solution toward higher-quality image synthesis on challenging
tasks.

</details>

### [193] [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082)
*Ziqi Pang,Yu-Xiong Wang*

Main category: cs.CV

TLDR: MR. Video是一个基于MapReduce原则的长视频理解框架，通过独立感知短视频片段（Map）和联合聚合信息（Reduce）提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有序列到序列视觉语言模型（VLMs）和视频代理在长视频理解中的上下文长度限制和关键片段选择问题。

Method: 采用两阶段MapReduce：1) 字幕生成（Map生成片段字幕，Reduce标准化）；2) 分析（Map提取相关信息，Reduce整合答案）。

Result: 在LVBench上比现有VLMs和视频代理准确率提升超过10%。

Conclusion: MapReduce原则简单有效，适用于VLMs和视频代理，显著提升长视频理解性能。

Abstract: We propose MR. Video, an agentic long video understanding framework that
demonstrates the simple yet effective MapReduce principle for processing long
videos: (1) Map: independently and densely perceiving short video clips, and
(2) Reduce: jointly aggregating information from all clips. Compared with
sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed
short video perception without being limited by context length. Compared with
existing video agents that typically rely on sequential key segment selection,
the Map operation enables simpler and more scalable sequence parallel
perception of short video segments. Its Reduce step allows for more
comprehensive context aggregation and reasoning, surpassing explicit key
segment retrieval. This MapReduce principle is applicable to both VLMs and
video agents, and we use LLM agents to validate its effectiveness.
  In practice, MR. Video employs two MapReduce stages: (A) Captioning:
generating captions for short video clips (map), then standardizing repeated
characters and objects into shared names (reduce); (B) Analysis: for each user
question, analyzing relevant information from individual short videos (map),
and integrating them into a final answer (reduce). MR. Video achieves over 10%
accuracy improvement on the challenging LVBench compared to state-of-the-art
VLMs and video agents.
  Code is available at: https://github.com/ziqipang/MR-Video

</details>

### [194] [MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](https://arxiv.org/abs/2504.16083)
*Yucheng Li,Huiqiang Jiang,Chengruidong Zhang,Qianhui Wu,Xufang Luo,Surin Ahn,Amir H. Abdi,Dongsheng Li,Jianfeng Gao,Yuqing Yang,Lili Qiu*

Main category: cs.CV

TLDR: MMInference是一种动态稀疏注意力方法，用于加速长上下文多模态输入的预填充阶段，提升视觉语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型（VLMs）在长上下文多模态输入中预填充阶段二次注意力复杂度的瓶颈问题。

Method: 利用视频输入的时空局部性发现独特的Grid稀疏模式，并通过基于排列的方法动态构建稀疏分布，优化GPU内核。

Result: 在1M tokens下，预填充阶段加速高达8.3倍，同时保持准确性。

Conclusion: MMInference无需修改模型即可无缝集成到现有VLM流程中，显著提升效率。

Abstract: The integration of long-context capabilities with visual understanding
unlocks unprecedented potential for Vision Language Models (VLMs). However, the
quadratic attention complexity during the pre-filling phase remains a
significant obstacle to real-world deployment. To overcome this limitation, we
introduce MMInference (Multimodality Million tokens Inference), a dynamic
sparse attention method that accelerates the prefilling stage for long-context
multi-modal inputs. First, our analysis reveals that the temporal and spatial
locality of video input leads to a unique sparse pattern, the Grid pattern.
Simultaneously, VLMs exhibit markedly different sparse distributions across
different modalities. We introduce a permutation-based method to leverage the
unique Grid pattern and handle modality boundary issues. By offline search the
optimal sparse patterns for each head, MMInference constructs the sparse
distribution dynamically based on the input. We also provide optimized GPU
kernels for efficient sparse computations. Notably, MMInference integrates
seamlessly into existing VLM pipelines without any model modifications or
fine-tuning. Experiments on multi-modal benchmarks-including Video QA,
Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art
long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that
MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while
maintaining accuracy. Our code is available at https://aka.ms/MMInference.

</details>

<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [195] [On the Boolean Network Theory of Datalog$^\neg$](https://arxiv.org/abs/2504.15417)
*Van-Giang Trinh,Belaid Benhamou,Sylvain Soliman,François Fages*

Main category: cs.LO

TLDR: 本文建立了Datalog$^\neg$与布尔网络理论之间的形式化联系，证明了在无奇环或无偶环条件下稳定模型与正则模型的关系，并给出了模型数量的上界。


<details>
  <summary>Details</summary>
Motivation: Datalog$^\neg$在多个领域有广泛应用，但其模型理论与布尔网络理论的联系尚未被充分探索。本文旨在填补这一空白。

Method: 利用布尔网络理论的结果，分析Datalog$^\neg$程序的奇偶环条件，证明稳定模型与正则模型的关系，并引入陷阱空间概念。

Result: 在无奇环时，正则模型与稳定模型一致；无偶环时，稳定偏模型唯一。此外，给出了模型数量的上界。

Conclusion: 本文通过布尔网络理论为Datalog$^\neg$提供了新的理论支持，并修正了前人工作中的问题。

Abstract: Datalog$^\neg$ is a central formalism used in a variety of domains ranging
from deductive databases and abstract argumentation frameworks to answer set
programming. Its model theory is the finite counterpart of the logical
semantics developed for normal logic programs, mainly based on the notions of
Clark's completion and two-valued or three-valued canonical models including
supported, stable, regular and well-founded models. In this paper we establish
a formal link between Datalog$^\neg$ and Boolean network theory, which was
initially introduced by Stuart Kaufman and Ren\'e Thomas to reason about gene
regulatory networks. We use previous results from Boolean network theory to
prove that in the absence of odd cycles in a Datalog$^\neg$ program, the
regular models coincide with the stable models, which entails the existence of
stable models, and in the absence of even cycles, we show the uniqueness of
stable partial models, which entails the uniqueness of regular models. These
results on regular models have been claimed by You and Yuan in 1994 for normal
logic programs but we show problems in their definition of well-founded
stratification and in their proofs that we can fix for negative normal logic
programs only. We also give upper bounds on the numbers of stable partial,
regular, and stable models of a Datalog$^\neg$ program using the cardinality of
a feedback vertex set in its atom dependency graph. Interestingly, our
connection to Boolean network theory also points us to the notion of trap
spaces for Datalog$^\neg$ programs. We relate the notions of supported or
stable trap spaces to the other semantics of Datalog$^\neg$, and show the
equivalence between subset-minimal stable trap spaces and regular models.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [196] [Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling](https://arxiv.org/abs/2504.15296)
*Yihong Jin,Ze Yang*

Main category: cs.DC

TLDR: 本文提出了一种结合强化学习和深度神经网络的混合框架，用于优化云AI推理服务的可扩展性，显著提升了负载均衡效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 随着云AI推理服务的快速扩展，需要一种强大的可扩展性解决方案来管理动态工作负载并保持高性能。

Method: 采用混合方法，结合强化学习进行自适应负载分配和深度神经网络进行需求预测，实现多层优化。

Result: 实验结果显示，负载均衡效率提升35%，响应延迟减少28%。

Conclusion: 该模型在可扩展性优化方面表现出显著效果，优于传统解决方案。

Abstract: The rapid expansion of AI inference services in the cloud necessitates a
robust scalability solution to manage dynamic workloads and maintain high
performance. This study proposes a comprehensive scalability optimization
framework for cloud AI inference services, focusing on real-time load balancing
and autoscaling strategies. The proposed model is a hybrid approach that
combines reinforcement learning for adaptive load distribution and deep neural
networks for accurate demand forecasting. This multi-layered approach enables
the system to anticipate workload fluctuations and proactively adjust
resources, ensuring maximum resource utilisation and minimising latency.
Furthermore, the incorporation of a decentralised decision-making process
within the model serves to enhance fault tolerance and reduce response time in
scaling operations. Experimental results demonstrate that the proposed model
enhances load balancing efficiency by 35\ and reduces response delay by 28\,
thereby exhibiting a substantial optimization effect in comparison with
conventional scalability solutions.

</details>

### [197] [D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving](https://arxiv.org/abs/2504.15299)
*Haodong Wang,Qihua Zhou,Zicong Hong,Song Guo*

Main category: cs.DC

TLDR: D$^2$MoE是一个算法-系统协同设计框架，通过动态分配位宽优化MoE模型在边缘设备上的部署，提升推理吞吐量并减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在边缘设备上部署时因静态优化策略无法满足多样化任务需求，导致服务质量下降。

Method: 提出嵌套式权重量化（MWQ）和启发式调度算法（HEBF），动态分配位宽并优化I/O-计算流水线。

Result: 在边缘设备上，D$^2$MoE将推理吞吐量提升1.39倍，峰值内存占用减少53%，同时保持与INT8相当的精度。

Conclusion: D$^2$MoE通过动态位宽分配和系统优化，显著提升了MoE模型在资源受限设备上的性能。

Abstract: The mixture of experts (MoE) model is a sparse variant of large language
models (LLMs), designed to hold a better balance between intelligent capability
and computational overhead. Despite its benefits, MoE is still too expensive to
deploy on resource-constrained edge devices, especially with the demands of
on-device inference services. Recent research efforts often apply model
compression techniques, such as quantization, pruning and merging, to restrict
MoE complexity. Unfortunately, due to their predefined static model
optimization strategies, they cannot always achieve the desired
quality-overhead trade-off when handling multiple requests, finally degrading
the on-device quality of service. These limitations motivate us to propose the
D$^2$MoE, an algorithm-system co-design framework that matches diverse task
requirements by dynamically allocating the most proper bit-width to each
expert. Specifically, inspired by the nested structure of matryoshka dolls, we
propose the matryoshka weight quantization (MWQ) to progressively compress
expert weights in a bit-nested manner and reduce the required runtime memory.
On top of it, we further optimize the I/O-computation pipeline and design a
heuristic scheduling algorithm following our hottest-expert-bit-first (HEBF)
principle, which maximizes the expert parallelism between I/O and computation
queue under constrained memory budgets, thus significantly reducing the idle
temporal bubbles waiting for the experts to load. Evaluations on real edge
devices show that D$^2$MoE improves the overall inference throughput by up to
1.39$\times$ and reduces the peak memory footprint by up to 53% over the latest
on-device inference frameworks, while still preserving comparable serving
accuracy as its INT8 counterparts.

</details>

### [198] [High-Throughput LLM inference on Heterogeneous Clusters](https://arxiv.org/abs/2504.15303)
*Yi Xiong,Jinqi Huang,Wenjie Huang,Xuebing Yu,Entong Li,Zhixiong Ning,Jinhua Zhou,Li Zeng,Xin Chen*

Main category: cs.DC

TLDR: 论文提出了一种在异构集群上实现高吞吐量LLM推理服务的系统，通过优化部署配置和请求调度算法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 异构集群中LLM推理面临配置优化和请求调度的挑战，需高效利用资源以降低成本并加速任务处理。

Method: 采用穷举搜索方法优化部署配置，并提出考虑实例处理能力差异的请求调度机制。

Result: 实验表明，所提调度器在两个异构集群上分别提升了122.5%和33.6%的吞吐量。

Conclusion: 该系统有效解决了异构集群中LLM推理的配置和调度问题，显著提升了性能。

Abstract: Nowadays, many companies possess various types of AI accelerators, forming
heterogeneous clusters. Efficiently leveraging these clusters for
high-throughput large language model (LLM) inference services can significantly
reduce costs and expedite task processing. However, LLM inference on
heterogeneous clusters presents two main challenges. Firstly, different
deployment configurations can result in vastly different performance. The
number of possible configurations is large, and evaluating the effectiveness of
a specific setup is complex. Thus, finding an optimal configuration is not an
easy task. Secondly, LLM inference instances within a heterogeneous cluster
possess varying processing capacities, leading to different processing speeds
for handling inference requests. Evaluating these capacities and designing a
request scheduling algorithm that fully maximizes the potential of each
instance is challenging. In this paper, we propose a high-throughput inference
service system on heterogeneous clusters. First, the deployment configuration
is optimized by modeling the resource amount and expected throughput and using
the exhaustive search method. Second, a novel mechanism is proposed to schedule
requests among instances, which fully considers the different processing
capabilities of various instances. Extensive experiments show that the proposed
scheduler improves throughput by 122.5% and 33.6% on two heterogeneous
clusters, respectively.

</details>

### [199] [DR.FIX: Automatically Fixing Data Races at Industry Scale](https://arxiv.org/abs/2504.15637)
*Farnaz Behrang,Zhizhou Zhang,Georgian-Vlad Saioc,Peng Liu,Milind Chabbi*

Main category: cs.DC

TLDR: Dr.Fix结合大型语言模型和程序分析，自动修复工业规模代码中的数据竞争问题，已在Uber的开发流程中成功应用。


<details>
  <summary>Details</summary>
Motivation: 数据竞争是共享内存并行程序中的常见并发错误，对软件可靠性和可重现性构成挑战，但自动修复工具的研究较少。

Method: Dr.Fix结合大型语言模型（LLMs）和程序分析，针对Go语言设计，支持复杂代码环境中的多种竞争模式。

Result: 在18个月内，Dr.Fix为404个数据竞争中的224个（55%）生成补丁，其中193个（86%）被开发者接受并集成。

Conclusion: Dr.Fix展示了在工业规模下自动修复数据竞争的可行性，并成功集成到实际开发流程中。

Abstract: Data races are a prevalent class of concurrency bugs in shared-memory
parallel programs, posing significant challenges to software reliability and
reproducibility. While there is an extensive body of research on detecting data
races and a wealth of practical detection tools across various programming
languages, considerably less effort has been directed toward automatically
fixing data races at an industrial scale. In large codebases, data races are
continuously introduced and exhibit myriad patterns, making automated fixing
particularly challenging.
  In this paper, we tackle the problem of automatically fixing data races at an
industrial scale. We present Dr.Fix, a tool that combines large language models
(LLMs) with program analysis to generate fixes for data races in real-world
settings, effectively addressing a broad spectrum of racy patterns in complex
code contexts. Implemented for Go--the programming language widely used in
modern microservice architectures where concurrency is pervasive and data races
are common--Dr.Fix seamlessly integrates into existing development workflows.
We detail the design of Dr.Fix and examine how individual design choices
influence the quality of the fixes produced. Over the past 18 months, Dr.Fix
has been integrated into developer workflows at Uber demonstrating its
practical utility. During this period, Dr.Fix produced patches for 224 (55%)
from a corpus of 404 data races spanning various categories; 193 of these
patches (86%) were accepted by more than a hundred developers via code reviews
and integrated into the codebase.

</details>

### [200] [Collaborative Split Federated Learning with Parallel Training and Aggregation](https://arxiv.org/abs/2504.15724)
*Yiannis Papageorgiou,Yannis Thomas,Alexios Filippakopoulos,Ramin Khalili,Iordanis Koutsopoulos*

Main category: cs.DC

TLDR: 提出了一种新的协作式分割联邦学习（C-SFL）方法，通过将模型分为三部分并并行训练，减少了训练延迟和通信开销，同时提高了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有分割联邦学习（SFL）方法在计算能力不同的客户端参与时仍存在训练延迟和通信开销大的问题。

Method: 将模型分为三部分（弱计算客户端、强计算客户端和服务器端），实现并行训练和聚合。

Result: 实验证明C-SFL在减少训练延迟、通信开销和提高模型准确性方面优于现有方法。

Conclusion: C-SFL是一种高效的联邦学习方案，适用于异构计算能力的客户端场景。

Abstract: Federated learning (FL) operates based on model exchanges between the server
and the clients, and it suffers from significant client-side computation and
communication burden. Split federated learning (SFL) arises a promising
solution by splitting the model into two parts, that are trained sequentially:
the clients train the first part of the model (client-side model) and transmit
it to the server that trains the second (server-side model). Existing SFL
schemes though still exhibit long training delays and significant communication
overhead, especially when clients of different computing capability
participate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a
novel scheme that splits the model into three parts, namely the model parts
trained at the computationally weak clients, the ones trained at the
computationally strong clients, and the ones at the server. Unlike existing
works, C-SFL enables parallel training and aggregation of model's parts at the
clients and at the server, resulting in reduced training delays and
commmunication overhead while improving the model's accuracy. Experiments
verify the multiple gains of C-SFL against the existing schemes.

</details>

<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [201] [Assessing Surrogate Heterogeneity in Real World Data Using Meta-Learners](https://arxiv.org/abs/2504.15386)
*Rebecca Knowlton,Layla Parast*

Main category: stat.ME

TLDR: 该论文提出了一种评估真实世界非随机数据中替代标记异质性的框架，并利用元学习器实现。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的公共卫生和社会科学研究中，随机试验往往不切实际，但现有方法多依赖随机化假设，且缺乏对替代标记异质性的研究。

Method: 提出一个框架，结合灵活的机器学习方法处理混杂因素，量化替代标记强度与患者特征的异质性。

Result: 通过模拟研究和实际应用（如血红蛋白A1c作为空腹血糖的替代标记），验证了方法的性能。

Conclusion: 该框架为真实世界数据中替代标记的异质性分析提供了有效工具，并能识别替代标记有效的个体。

Abstract: Surrogate markers are most commonly studied within the context of randomized
clinical trials. However, the need for alternative outcomes extends beyond
these settings and may be more pronounced in real-world public health and
social science research, where randomized trials are often impractical.
Research on identifying surrogates in real-world non-randomized data is scarce,
as available statistical approaches for evaluating surrogate markers tend to
rely on the assumption that treatment is randomized. While the few methods that
allow for non-randomized treatment/exposure appropriately handle confounding
individual characteristics, they do not offer a way to examine surrogate
heterogeneity with respect to patient characteristics. In this paper, we
propose a framework to assess surrogate heterogeneity in real-world, i.e.,
non-randomized, data and implement this framework using various meta-learners.
Our approach allows us to quantify heterogeneity in surrogate strength with
respect to patient characteristics while accommodating confounders through the
use of flexible, off-the-shelf machine learning methods. In addition, we use
our framework to identify individuals for whom the surrogate is a valid
replacement of the primary outcome. We examine the performance of our methods
via a simulation study and application to examine heterogeneity in the
surrogacy of hemoglobin A1c as a surrogate for fasting plasma glucose.

</details>

### [202] [Deep learning with missing data](https://arxiv.org/abs/2504.15388)
*Tianyi Ma,Tengyao Wang,Richard J. Samworth*

Main category: stat.ME

TLDR: 论文提出了一种名为PENNs的神经网络方法，用于处理多元非参数回归中缺失协变量的问题，结合任意现有插补技术，通过三个神经网络模块提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决多元非参数回归中缺失协变量的问题，提升预测准确性。

Method: 提出PENNs方法，结合三个神经网络模块：一个用于插补数据训练，一个用于观测指示向量压缩表示，一个用于最终预测。

Result: 理论分析表明PENNs在典型情况下达到最小最大收敛速率，实验验证其在模拟和真实数据上显著优于标准神经网络。

Conclusion: PENNs在处理缺失协变量问题时表现优异，理论支持其高效性，实际应用效果显著。

Abstract: In the context of multivariate nonparametric regression with missing
covariates, we propose Pattern Embedded Neural Networks (PENNs), which can be
applied in conjunction with any existing imputation technique. In addition to a
neural network trained on the imputed data, PENNs pass the vectors of
observation indicators through a second neural network to provide a compact
representation. The outputs are then combined in a third neural network to
produce final predictions. Our main theoretical result exploits an assumption
that the observation patterns can be partitioned into cells on which the Bayes
regression function behaves similarly, and belongs to a compositional H\"older
class. It provides a finite-sample excess risk bound that holds for an
arbitrary missingness mechanism, and in combination with a complementary
minimax lower bound, demonstrates that our PENN estimator attains in typical
cases the minimax rate of convergence as if the cells of the partition were
known in advance, up to a poly-logarithmic factor in the sample size. Numerical
experiments on simulated, semi-synthetic and real data confirm that the PENN
estimator consistently improves, often dramatically, on standard neural
networks without pattern embedding. Code to reproduce our experiments, as well
as a tutorial on how to apply our method, is publicly available.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [203] [FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning](https://arxiv.org/abs/2504.15663)
*Ju Yeon Kang,Ji Won Yoon,Semin Kim,Min Hyun Han,Nam Soo Kim*

Main category: eess.AS

TLDR: 提出了一种名为FADEL的新框架，通过证据学习改进假音频检测，解决了现有方法在未知攻击下的过自信问题。


<details>
  <summary>Details</summary>
Motivation: 随着语音合成和声音转换技术的发展，自动说话人验证系统更容易受到欺骗攻击，现有方法因使用softmax分类而存在过自信问题。

Method: 提出FADEL框架，利用Dirichlet分布建模类别概率，将模型不确定性纳入预测，提升对未知攻击的鲁棒性。

Result: 在ASVspoof2019和2021数据集上，FADEL显著优于基线模型，并通过不确定性估计与错误率的强相关性验证了其有效性。

Conclusion: FADEL通过引入不确定性估计，显著提升了假音频检测在未知攻击场景下的性能。

Abstract: Recently, fake audio detection has gained significant attention, as
advancements in speech synthesis and voice conversion have increased the
vulnerability of automatic speaker verification (ASV) systems to spoofing
attacks. A key challenge in this task is generalizing models to detect unseen,
out-of-distribution (OOD) attacks. Although existing approaches have shown
promising results, they inherently suffer from overconfidence issues due to the
usage of softmax for classification, which can produce unreliable predictions
when encountering unpredictable spoofing attempts. To deal with this
limitation, we propose a novel framework called fake audio detection with
evidential learning (FADEL). By modeling class probabilities with a Dirichlet
distribution, FADEL incorporates model uncertainty into its predictions,
thereby leading to more robust performance in OOD scenarios. Experimental
results on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets
indicate that the proposed method significantly improves the performance of
baseline models. Furthermore, we demonstrate the validity of uncertainty
estimation by analyzing a strong correlation between average uncertainty and
equal error rate (EER) across different spoofing algorithms.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [204] [New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics](https://arxiv.org/abs/2504.15927)
*Ling Cheng,Jiashu Pu,Ruicheng Liang,Qian Shao,Hezhe Qiao,Feida Zhu*

Main category: cs.SI

TLDR: 论文提出CLANN方法，通过模拟晶体退火动力学改进半监督社区检测，解决了现有方法计算成本高和候选核心不合理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有半监督社区检测方法依赖强化学习和生成对抗网络，计算成本高且候选核心不合理，限制了可扩展性。

Method: 将社区检测类比为晶体退火过程，提出CLANN方法，通过优化过程增强社区核心一致性，并使用无学习的Transitive Annealer细化候选。

Result: 在43种网络设置下的实验表明，CLANN在多个真实数据集上优于现有方法，具有高效性和有效性。

Conclusion: CLANN通过动力学概念改进了社区检测，显著提升了性能和可扩展性。

Abstract: Semi-supervised community detection methods are widely used for identifying
specific communities due to the label scarcity. Existing semi-supervised
community detection methods typically involve two learning stages learning in
both initial identification and subsequent adjustment, which often starts from
an unreasonable community core candidate. Moreover, these methods encounter
scalability issues because they depend on reinforcement learning and generative
adversarial networks, leading to higher computational costs and restricting the
selection of candidates. To address these limitations, we draw a parallel
between crystallization kinetics and community detection to integrate the
spontaneity of the annealing process into community detection. Specifically, we
liken community detection to identifying a crystal subgrain (core) that expands
into a complete grain (community) through a process similar to annealing. Based
on this finding, we propose CLique ANNealing (CLANN), which applies kinetics
concepts to community detection by integrating these principles into the
optimization process to strengthen the consistency of the community core.
Subsequently, a learning-free Transitive Annealer was employed to refine the
first-stage candidates by merging neighboring cliques and repositioning the
community core, enabling a spontaneous growth process that enhances
scalability. Extensive experiments on \textbf{43} different network settings
demonstrate that CLANN outperforms state-of-the-art methods across multiple
real-world datasets, showcasing its exceptional efficacy and efficiency in
community detection.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [205] [State-Aware IoT Scheduling Using Deep Q-Networks and Edge-Based Coordination](https://arxiv.org/abs/2504.15577)
*Qingyuan He,Chang Liu,Juecen Zhan,Weiqiang Huang,Ran Hao*

Main category: cs.NI

TLDR: 提出了一种结合深度Q网络（DQN）和边缘协作机制的优化方法，用于智能物联网设备的能效管理，实验证明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂应用环境中智能物联网设备的能效管理挑战。

Method: 结合DQN与边缘协作机制，构建状态-动作-奖励交互模型，引入协作图结构建模多设备环境。

Result: 在平均能耗、处理延迟和资源利用率方面优于现有基线方法。

Conclusion: 该方法在智能物联网场景中具有有效性和实用性。

Abstract: This paper addresses the challenge of energy efficiency management faced by
intelligent IoT devices in complex application environments. A novel
optimization method is proposed, combining Deep Q-Network (DQN) with an edge
collaboration mechanism. The method builds a state-action-reward interaction
model and introduces edge nodes as intermediaries for state aggregation and
policy scheduling. This enables dynamic resource coordination and task
allocation among multiple devices. During the modeling process, device status,
task load, and network resources are jointly incorporated into the state space.
The DQN is used to approximate and learn the optimal scheduling strategy. To
enhance the model's ability to perceive inter-device relationships, a
collaborative graph structure is introduced to model the multi-device
environment and assist in decision optimization. Experiments are conducted
using real-world IoT data collected from the FastBee platform. Several
comparative and validation tests are performed, including energy efficiency
comparisons across different scheduling strategies, robustness analysis under
varying task loads, and evaluation of state dimension impacts on policy
convergence speed. The results show that the proposed method outperforms
existing baseline approaches in terms of average energy consumption, processing
latency, and resource utilization. This confirms its effectiveness and
practicality in intelligent IoT scenarios.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [206] [Quantifying Source Speaker Leakage in One-to-One Voice Conversion](https://arxiv.org/abs/2504.15822)
*Scott Wellington,Xuechen Liu,Junichi Yamagishi*

Main category: cs.SD

TLDR: 通过多口音语料库研究，量化一对一语音转换中源说话人身份的可信度，强调合成语音提供者保护说话人隐私的责任。


<details>
  <summary>Details</summary>
Motivation: 探讨在语音转换技术中如何量化源说话人身份的可信度，以应对隐私泄露的风险。

Method: 使用HiFi-GAN声码器进行语音转换，分析不同说话人特征的信息泄漏情况，假设“最坏情况”白盒场景。

Result: 能够量化推断源说话人身份的置信度，缩小可能的源说话人范围。

Conclusion: 研究强调了合成语音提供者在保护说话人隐私方面的监管义务和道德责任。

Abstract: Using a multi-accented corpus of parallel utterances for use with commercial
speech devices, we present a case study to show that it is possible to quantify
a degree of confidence about a source speaker's identity in the case of
one-to-one voice conversion. Following voice conversion using a HiFi-GAN
vocoder, we compare information leakage for a range speaker characteristics;
assuming a "worst-case" white-box scenario, we quantify our confidence to
perform inference and narrow the pool of likely source speakers, reinforcing
the regulatory obligation and moral duty that providers of synthetic voices
have to ensure the privacy of their speakers' data.

</details>

<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [207] [Shannon invariants: A scalable approach to information decomposition](https://arxiv.org/abs/2504.15779)
*Aaron J. Gutknecht,Fernando E. Rosas,David A. Ehrlich,Abdullah Makkeh,Pedro A. M. Mediano,Michael Wibral*

Main category: cs.IT

TLDR: 论文提出了一种基于“香农不变量”的新框架，用于分析分布式系统中的高阶信息处理，解决了多变量度量定义和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 研究分布式系统（如生物和人工神经网络）中高阶信息处理的挑战，包括多变量度量的定义和大系统可扩展性问题。

Method: 引入“香农不变量”框架，这些量仅依赖于熵的定义，并能高效计算大规模系统的高阶信息处理特性。

Result: 理论结果澄清了多变量信息论度量的解释，实践结果揭示了深度学习架构在不同层中的信息处理特征。

Conclusion: 该框架解决了分析高阶现象的基本限制，为理论发展和实证分析提供了广阔机会。

Abstract: Distributed systems, such as biological and artificial neural networks,
process information via complex interactions engaging multiple subsystems,
resulting in high-order patterns with distinct properties across scales.
Investigating how these systems process information remains challenging due to
difficulties in defining appropriate multivariate metrics and ensuring their
scalability to large systems. To address these challenges, we introduce a novel
framework based on what we call "Shannon invariants" -- quantities that capture
essential properties of high-order information processing in a way that depends
only on the definition of entropy and can be efficiently calculated for large
systems. Our theoretical results demonstrate how Shannon invariants can be used
to resolve long-standing ambiguities regarding the interpretation of widely
used multivariate information-theoretic measures. Moreover, our practical
results reveal distinctive information-processing signatures of various deep
learning architectures across layers, which lead to new insights into how these
systems process information and how this evolves during training. Overall, our
framework resolves fundamental limitations in analyzing high-order phenomena
and offers broad opportunities for theoretical developments and empirical
analyses.

</details>

<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [208] [Structural Properties of Non-Linear Cellular Automata: Permutivity, Surjectivity and Reversibility](https://arxiv.org/abs/2504.15949)
*Firas Ben Ramdhane,Alberto Dennunzio,Luciano Margara,Giuliamaria Menara*

Main category: cs.DM

TLDR: 本文研究了非线性局部规则的元胞自动机在代数条件下的满射性和可逆性，并分析了置换性对这些性质的关键影响。


<details>
  <summary>Details</summary>
Motivation: 探讨非线性元胞自动机的基本性质（如满射性和可逆性）及其与置换性的关系，以深入理解其动力学行为。

Method: 通过理论分析和示例，研究非线性元胞自动机的代数条件，特别是置换性对满射性和可逆性的影响。

Result: 确定了非线性元胞自动机是否具有（双）置换性的条件，并揭示了这些基本性质之间的关系。

Conclusion: 研究为非线性元胞自动机的动力学行为提供了新的见解，特别是在满射性、可逆性和置换性方面的联系。

Abstract: This paper explores the algebraic conditions under which a cellular automaton
with a non-linear local rule exhibits surjectivity and reversibility. We also
analyze the role of permutivity as a key factor influencing these properties
and provide conditions that determine whether a non-linear CA is (bi)permutive.
Through theoretical results and illustrative examples, we characterize the
relationships between these fundamental properties, offering new insights into
the dynamical behavior of non-linear CA.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [209] [On the Price of Differential Privacy for Hierarchical Clustering](https://arxiv.org/abs/2504.15580)
*Chengyuan Deng,Jie Gao,Jalaj Upadhyay,Chen Wang,Samson Zhou*

Main category: cs.DS

TLDR: 该论文提出了一种在权重隐私模型下的差分隐私层次聚类算法，显著优于边级差分隐私的已知不可能结果，并展示了其在实际数据集上的良好表现。


<details>
  <summary>Details</summary>
Motivation: 层次聚类涉及敏感用户信息，需在差分隐私框架下保护数据隐私，但现有边级差分隐私方法误差较大。论文旨在解决这一问题，提出更优的权重隐私模型算法。

Method: 论文提出了一种新颖的算法，适用于权重隐私模型，其中输入图的每条边至少具有单位权重。算法在多项式时间内运行，并实现了较低的乘法误差。

Result: 算法在ε-DP下实现了O(log^1.5n/ε)乘法误差，且成本不高于现有工作的最优加法误差。实验表明算法在额外成本和可扩展性方面表现良好。

Conclusion: 权重隐私模型下的层次聚类算法优于边级差分隐私方法，同时论文还揭示了权重隐私模型的新下界，为相关领域提供了新的理论支持。

Abstract: Hierarchical clustering is a fundamental unsupervised machine learning task
with the aim of organizing data into a hierarchy of clusters. Many applications
of hierarchical clustering involve sensitive user information, therefore
motivating recent studies on differentially private hierarchical clustering
under the rigorous framework of Dasgupta's objective. However, it has been
shown that any privacy-preserving algorithm under edge-level differential
privacy necessarily suffers a large error. To capture practical applications of
this problem, we focus on the weight privacy model, where each edge of the
input graph is at least unit weight. We present a novel algorithm in the weight
privacy model that shows significantly better approximation than known
impossibility results in the edge-level DP setting. In particular, our
algorithm achieves $O(\log^{1.5}n/\varepsilon)$ multiplicative error for
$\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the
input graph, and the cost is never worse than the optimal additive error in
existing work. We complement our algorithm by showing if the unit-weight
constraint does not apply, the lower bound for weight-level DP hierarchical
clustering is essentially the same as the edge-level DP, i.e.
$\Omega(n^2/\varepsilon)$ additive error. As a result, we also obtain a new
lower bound of $\tilde{\Omega}(1/\varepsilon)$ additive error for balanced
sparsest cuts in the weight-level DP model, which may be of independent
interest. Finally, we evaluate our algorithm on synthetic and real-world
datasets. Our experimental results show that our algorithm performs well in
terms of extra cost and has good scalability to large graphs.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [210] [Markov Kernels, Distances and Optimal Control: A Parable of Linear Quadratic Non-Gaussian Distribution Steering](https://arxiv.org/abs/2504.15753)
*Alexis M. H. Teter,Wenqing Wang,Sachin Shivakumar,Abhishek Halder*

Main category: math.OC

TLDR: 论文推导了线性时变（LTV）系统的马尔可夫核，并将其应用于求解线性二次非高斯薛定谔桥问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过动态Sinkhorn递归精确求解受控LTV扩散的路径规划问题。

Method: 通过求解确定性最优控制问题，提出了一种新的状态-时间依赖距离函数方法。

Result: 成功推导了广义的马尔可夫核，并证明了线性二次非高斯薛定谔桥问题的可解性。

Conclusion: 该方法揭示了马尔可夫核、距离与最优控制之间的新联系，具有广泛的应用潜力。

Abstract: For a controllable linear time-varying (LTV) pair
$(\boldsymbol{A}_t,\boldsymbol{B}_t)$ and $\boldsymbol{Q}_{t}$ positive
semidefinite, we derive the Markov kernel for the It\^{o} diffusion
${\mathrm{d}}\boldsymbol{x}_{t}=\boldsymbol{A}_{t}\boldsymbol{x}_t {\mathrm{d}}
t + \sqrt{2}\boldsymbol{B}_{t}{\mathrm{d}}\boldsymbol{w}_{t}$ with an
accompanying killing of probability mass at rate
$\frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{Q}_{t}\boldsymbol{x}$. This Markov
kernel is the Green's function for an associated linear
reaction-advection-diffusion partial differential equation. Our result
generalizes the recently derived kernel for the special case
$\left(\boldsymbol{A}_t,\boldsymbol{B}_t\right)=\left(\boldsymbol{0},\boldsymbol{I}\right)$,
and depends on the solution of an associated Riccati matrix ODE. A consequence
of this result is that the linear quadratic non-Gaussian Schr\"{o}dinger bridge
is exactly solvable. This means that the problem of steering a controlled LTV
diffusion from a given non-Gaussian distribution to another over a fixed
deadline while minimizing an expected quadratic cost can be solved using
dynamic Sinkhorn recursions performed with the derived kernel. Our derivation
for the
$\left(\boldsymbol{A}_t,\boldsymbol{B}_t,\boldsymbol{Q}_t\right)$-parametrized
kernel pursues a new idea that relies on finding a state-time dependent
distance-like functional given by the solution of a deterministic optimal
control problem. This technique breaks away from existing methods, such as
generalizing Hermite polynomials or Weyl calculus, which have seen limited
success in the reaction-diffusion context. Our technique uncovers a new
connection between Markov kernels, distances, and optimal control. This
connection is of interest beyond its immediate application in solving the
linear quadratic Schr\"{o}dinger bridge problem.

</details>

<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [211] [Transferable Learning of Reaction Pathways from Geometric Priors](https://arxiv.org/abs/2504.15370)
*Juno Nam,Miguel Steiner,Max Misterka,Soojung Yang,Avni Singhal,Rafael Gómez-Bombarelli*

Main category: physics.chem-ph

TLDR: MEPIN是一种可扩展的机器学习方法，用于高效预测化学反应的最小能量路径（MEPs），无需依赖过渡态几何或预优化的反应路径。


<details>
  <summary>Details</summary>
Motivation: 理解化学反应机制需要识别最小能量路径，但传统方法计算成本高。

Method: 使用对称性破缺的等变神经网络构建连续反应路径模型，通过几何插值预测反应坐标的偏差，并结合能量目标和几何先验进行训练。

Result: 方法在多种小分子反应和[3+2]环加成反应中表现出色，能准确对齐参考反应坐标。

Conclusion: MEPIN为大规模化学反应空间探索提供了高效、数据驱动的反应路径预测工具。

Abstract: Identifying minimum-energy paths (MEPs) is crucial for understanding chemical
reaction mechanisms but remains computationally demanding. We introduce MEPIN,
a scalable machine-learning method for efficiently predicting MEPs from
reactant and product configurations, without relying on transition-state
geometries or pre-optimized reaction paths during training. The task is defined
as predicting deviations from geometric interpolations along reaction
coordinates. We address this task with a continuous reaction path model based
on a symmetry-broken equivariant neural network that generates a flexible
number of intermediate structures. The model is trained using an energy-based
objective, with efficiency enhanced by incorporating geometric priors from
geodesic interpolation as initial interpolations or pre-training objectives.
Our approach generalizes across diverse chemical reactions and achieves
accurate alignment with reference intrinsic reaction coordinates, as
demonstrated on various small molecule reactions and [3+2] cycloadditions. Our
method enables the exploration of large chemical reaction spaces with
efficient, data-driven predictions of reaction pathways.

</details>

<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [212] [Benchmarking machine learning models for predicting aerofoil performance](https://arxiv.org/abs/2504.15993)
*Oliver Summerell,Gerardo Aragon-Camarasa,Stephanie Ordonez Sanchez*

Main category: physics.flu-dyn

TLDR: 本文研究了神经网络（NNs）作为传统方法的替代方案，用于分析风能和潮汐能行业中翼型的性能表现。通过比较四种神经网络（MLP、PointNet、GraphSAGE、GUNet），发现MLP和PointNet表现最佳，其中MLP在流体行为预测上更准确，而PointNet在计算升力系数（$C_L$）上更精确。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如CFD、薄翼理论和面板法）在计算速度和结果准确性之间存在权衡，神经网络被探索为一种既能快速又能准确分析翼型性能的替代方案。

Method: 研究使用了四种神经网络（MLP、PointNet、GraphSAGE、GUNet），在25个攻角（4$^\circ$至20$^\circ$）下训练，预测流体流动并通过面板法计算升力系数（$C_L$）。

Result: GraphSAGE和GUNet在测试阶段表现良好，但在验证阶段表现不佳。MLP和PointNet表现最强，MLP在流体行为预测上更准确，而PointNet在计算$C_L$上更精确。

Conclusion: 神经网络（尤其是MLP和PointNet）可作为传统方法的有效替代，但需根据具体任务选择模型：MLP适用于流体行为预测，PointNet适用于升力系数计算。

Abstract: This paper investigates the capability of Neural Networks (NNs) as
alternatives to the traditional methods to analyse the performance of aerofoils
used in the wind and tidal energy industry. The current methods used to assess
the characteristic lift and drag coefficients include Computational Fluid
Dynamics (CFD), thin aerofoil and panel methods, all face trade-offs between
computational speed and the accuracy of the results and as such NNs have been
investigated as an alternative with the aim that it would perform both quickly
and accurately. As such, this paper provides a benchmark for the windAI_bench
dataset published by the National Renewable Energy Laboratory (NREL) in the
USA. In order to validate the methodology of the benchmarking, the AirfRANS
{\tt arXiv:2212.07564v3} dataset is used as both a starting point and a point
of comparison. This study evaluates four neural networks (MLP, PointNet,
GraphSAGE, GUNet) trained on a range aerofoils at 25 angles of attack
(4$^\circ$ to 20$^\circ$). to predict fluid flow and calculate lift
coefficients ($C_L$) via the panel method. GraphSAGE and GUNet performed well
during the testing phase, but underperformed during validation. Accordingly,
this paper has identified PointNet and MLP as the two strongest models tested,
however whilst the results from MLP are more commonly correct for predicting
the behaviour of the fluid, the results from PointNet provide the more accurate
results for calculating $C_L$.

</details>

<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [213] [A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition](https://arxiv.org/abs/2504.15975)
*Peter Fletcher*

Main category: cs.FL

TLDR: 本文提出了一种直接且声明式的方法来表示递归结构的图模式语法，避免了传统图语法的产生式规则，将语法和模式都表示为网络，解析过程视为从模式到语法的同态构造。


<details>
  <summary>Details</summary>
Motivation: 传统图语法使用产生式规则，不够直接和灵活。本文旨在提出一种更直观的方法来表示和解析递归结构的图模式，以支持并行解析和多维结构的处理。

Method: 将语法和模式表示为网络，解析过程构造从模式到语法的同态。支持并行解析，整合特征检测、分割、解析等步骤为一个统一过程。

Result: 能够处理50-1000符号的复杂递归结构模式，支持几何关系变化、模糊符号、重叠符号、杂乱图像和缺失区域等情况的容错解析。

Conclusion: 本文方法在理论和实践上均展示了处理复杂递归结构模式的能力，为并行解析和多维结构处理提供了新思路。

Abstract: I introduce a formalism for representing the syntax of recursively structured
graph-like patterns. It does not use production rules, like a conventional
graph grammar, but represents the syntactic structure in a more direct and
declarative way. The grammar and the pattern are both represented as networks,
and parsing is seen as the construction of a homomorphism from the pattern to
the grammar. The grammars can represent iterative, hierarchical and nested
recursive structure in more than one dimension.
  This supports a highly parallel style of parsing, in which all aspects of
pattern recognition (feature detection, segmentation, parsing, filling in
missing symbols, top-down and bottom-up inference) are integrated into a single
process, to exploit the synergy between them.
  The emphasis of this paper is on underlying theoretical issues, but I also
give some example runs to illustrate the error-tolerant parsing of complex
recursively structured patterns of 50-1000 symbols, involving variability in
geometric relationships, blurry and indistinct symbols, overlapping symbols,
cluttered images, and erased patches.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [214] [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000)
*Soham Bonnerjee,Zhen Wei,Yeon,Anna Asch,Sagnik Nandy,Promit Ghosal*

Main category: stat.ML

TLDR: 本文研究了在形式隐私约束下，基于上下文学习（ICL）的可行性，提出了一种差分隐私预训练算法，并首次对线性回归中的ICL隐私-准确性权衡进行了理论分析。


<details>
  <summary>Details</summary>
Motivation: 探索在隐私约束下，现代语言模型基于上下文学习的能力的可行性。

Method: 提出了一种差分隐私预训练算法，用于线性注意力头，并分析了线性回归中ICL的隐私-准确性权衡。

Result: 结果表明，优化与隐私引入的噪声之间存在基本张力，且该方法对训练提示的对抗性扰动具有鲁棒性。

Conclusion: 通过理论分析和广泛模拟，验证了差分隐私预训练算法在ICL中的有效性。

Abstract: In-context learning (ICL)-the ability of transformer-based models to perform
new tasks from examples provided at inference time-has emerged as a hallmark of
modern language models. While recent works have investigated the mechanisms
underlying ICL, its feasibility under formal privacy constraints remains
largely unexplored. In this paper, we propose a differentially private
pretraining algorithm for linear attention heads and present the first
theoretical analysis of the privacy-accuracy trade-off for ICL in linear
regression. Our results characterize the fundamental tension between
optimization and privacy-induced noise, formally capturing behaviors observed
in private training via iterative methods. Additionally, we show that our
method is robust to adversarial perturbations of training prompts, unlike
standard ridge regression. All theoretical findings are supported by extensive
simulations across diverse settings.

</details>

### [215] [Transfer Learning for High-dimensional Reduced Rank Time Series Models](https://arxiv.org/abs/2504.15691)
*Mingliang Ma Abolfazl Safikhani*

Main category: stat.ML

TLDR: 提出了一种针对具有低秩和稀疏结构的高维VAR模型的迁移学习算法，并展示了其理论保证和实证效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注独立观测的迁移学习，而对时间序列模型的研究较少，尤其是具有时间依赖性和复杂参数结构的模型。

Method: 提出了一种新的迁移学习算法，用于估计高维VAR模型，并开发了一种选择辅助数据集中信息观测的新方法。

Result: 理论保证包括模型参数一致性、信息集选择和估计量的渐近分布，实证结果验证了方法的有效性。

Conclusion: 该方法在理论和实证上均表现出色，为时间序列数据的迁移学习提供了新工具。

Abstract: The objective of transfer learning is to enhance estimation and inference in
a target data by leveraging knowledge gained from additional sources. Recent
studies have explored transfer learning for independent observations in
complex, high-dimensional models assuming sparsity, yet research on time series
models remains limited. Our focus is on transfer learning for sequences of
observations with temporal dependencies and a more intricate model parameter
structure. Specifically, we investigate the vector autoregressive model (VAR),
a widely recognized model for time series data, where the transition matrix can
be deconstructed into a combination of a sparse matrix and a low-rank one. We
propose a new transfer learning algorithm tailored for estimating
high-dimensional VAR models characterized by low-rank and sparse structures.
Additionally, we present a novel approach for selecting informative
observations from auxiliary datasets. Theoretical guarantees are established,
encompassing model parameter consistency, informative set selection, and the
asymptotic distribution of estimators under mild conditions. The latter
facilitates the construction of entry-wise confidence intervals for model
parameters. Finally, we demonstrate the empirical efficacy of our methodologies
through both simulated and real-world datasets.

</details>

### [216] [From predictions to confidence intervals: an empirical study of conformal prediction methods for in-context learning](https://arxiv.org/abs/2504.15722)
*Zhe Huang,Simone Rossi,Rui Yuan,Thomas Hannagan*

Main category: stat.ML

TLDR: 本文提出了一种基于共形预测的方法，利用Transformer的上下文学习能力，实现高效且分布自由的预测区间估计。


<details>
  <summary>Details</summary>
Motivation: Transformer在上下文学习中表现出色，但其不确定性量化仍是一个挑战，尤其是在噪声回归任务中。本文旨在探索如何利用上下文学习实现分布自由的不确定性估计。

Method: 提出了一种基于共形预测的方法，通过单次前向传播高效生成置信区间，避免了传统方法需要重复拟合模型的计算开销。

Result: 实验表明，该方法在噪声回归任务中表现稳健且可扩展，优于基于岭回归的共形预测方法，并在分布偏移下保持良好性能。

Conclusion: 本文结合了上下文学习与共形预测，为基于Transformer的模型提供了理论支持的不确定性量化框架。

Abstract: Transformers have become a standard architecture in machine learning,
demonstrating strong in-context learning (ICL) abilities that allow them to
learn from the prompt at inference time. However, uncertainty quantification
for ICL remains an open challenge, particularly in noisy regression tasks. This
paper investigates whether ICL can be leveraged for distribution-free
uncertainty estimation, proposing a method based on conformal prediction to
construct prediction intervals with guaranteed coverage. While traditional
conformal methods are computationally expensive due to repeated model fitting,
we exploit ICL to efficiently generate confidence intervals in a single forward
pass. Our empirical analysis compares this approach against ridge
regression-based conformal methods, showing that conformal prediction with
in-context learning (CP with ICL) achieves robust and scalable uncertainty
estimates. Additionally, we evaluate its performance under distribution shifts
and establish scaling laws to guide model training. These findings bridge ICL
and conformal prediction, providing a theoretically grounded and new framework
for uncertainty quantification in transformer-based models.

</details>

### [217] [Explainable Unsupervised Anomaly Detection with Random Forest](https://arxiv.org/abs/2504.16075)
*Joshua S. Harvey,Joshua Rosaler,Mingshu Li,Dhruv Desai,Dhagash Mehta*

Main category: stat.ML

TLDR: 提出了一种基于无监督随机森林的相似性学习方法，用于改进无监督异常检测。通过训练随机森林区分真实数据和均匀分布的合成数据，获得一种距离度量，可各向异性地变换数据，扩展数据流形边界处的距离。


<details>
  <summary>Details</summary>
Motivation: 改进无监督异常检测的准确性，并解决现有方法在数据预处理、缺失数据处理和可视化方面的局限性。

Method: 训练随机森林区分真实数据和均匀分布的合成数据，获得距离度量并用于异常检测。

Result: 在多个基准数据集上验证了该方法优于其他常用检测器，且具有数据预处理要求低、支持缺失数据和可视化潜力等优势。

Conclusion: 该方法不仅提高了异常检测性能，还支持局部可解释的异常预测，通过特征重要性提供解释。

Abstract: We describe the use of an unsupervised Random Forest for similarity learning
and improved unsupervised anomaly detection. By training a Random Forest to
discriminate between real data and synthetic data sampled from a uniform
distribution over the real data bounds, a distance measure is obtained that
anisometrically transforms the data, expanding distances at the boundary of the
data manifold. We show that using distances recovered from this transformation
improves the accuracy of unsupervised anomaly detection, compared to other
commonly used detectors, demonstrated over a large number of benchmark
datasets. As well as improved performance, this method has advantages over
other unsupervised anomaly detection methods, including minimal requirements
for data preprocessing, native handling of missing data, and potential for
visualizations. By relating outlier scores to partitions of the Random Forest,
we develop a method for locally explainable anomaly predictions in terms of
feature importance.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [218] [RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network](https://arxiv.org/abs/2504.15311)
*Fei Shang,Haohua Du,Dawei Yan,Panlong Yang,Xiang-Yang Li*

Main category: eess.IV

TLDR: 论文提出了一种名为RINN的网络，利用物理约束而非真实值比较约束，结合PINN思想，实现了仅需单一样本且无需相位信息的射频成像。


<details>
  <summary>Details</summary>
Motivation: 射频成像技术在非视距和低光环境下具有潜力，但现有设备难以提供高精度电磁测量和大规模数据集，限制了其应用。

Method: 结合PINN思想设计RINN网络，利用物理约束适应射频信号特性，实现无需相位且仅需单一样本的成像。

Result: 数值评估显示，RINN在无相位数据下的成像效果与基于相位数据的经典算法相当（如RRMSE为0.11）。

Conclusion: RINN为射频成像技术的普及提供了新的可能性。

Abstract: Due to its ability to work in non-line-of-sight and low-light environments,
radio frequency (RF) imaging technology is expected to bring new possibilities
for embodied intelligence and multimodal sensing. However, widely used RF
devices (such as Wi-Fi) often struggle to provide high-precision
electromagnetic measurements and large-scale datasets, hindering the
application of RF imaging technology. In this paper, we combine the ideas of
PINN to design the RINN network, using physical constraints instead of true
value comparison constraints and adapting it with the characteristics of
ubiquitous RF signals, allowing the RINN network to achieve RF imaging using
only one sample without phase and with amplitude noise. Our numerical
evaluation results show that compared with 5 classic algorithms based on phase
data for imaging results, RINN's imaging results based on phaseless data are
good, with indicators such as RRMSE (0.11) performing similarly well. RINN
provides new possibilities for the universal development of radio frequency
imaging technology.

</details>

### [219] [Enhancing DR Classification with Swin Transformer and Shifted Window Attention](https://arxiv.org/abs/2504.15317)
*Meher Boulaabi,Takwa Ben Aïcha Gader,Afef Kacem Echi,Zied Bouraoui*

Main category: eess.IV

TLDR: 提出了一种结合图像预处理和Swin Transformer的方法，用于糖尿病视网膜病变（DR）分类，在Aptos和IDRiD数据集上分别达到89.65%和97.40%的准确率。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球致盲的主要原因，早期检测对治疗至关重要，但自动化分类面临图像质量、类别不平衡和像素级相似性等挑战。

Method: 采用图像裁剪、CLAHE增强和针对性数据增强的预处理流程，结合Swin Transformer的层次化token处理和窗口注意力机制。

Result: 在Aptos和IDRiD数据集上分别实现89.65%和97.40%的准确率，尤其在早期DR检测中表现突出。

Conclusion: 该方法在自动视网膜筛查中具有临床潜力，能有效提升DR分类的准确性和泛化能力。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide,
underscoring the importance of early detection for effective treatment.
However, automated DR classification remains challenging due to variations in
image quality, class imbalance, and pixel-level similarities that hinder model
training. To address these issues, we propose a robust preprocessing pipeline
incorporating image cropping, Contrast-Limited Adaptive Histogram Equalization
(CLAHE), and targeted data augmentation to improve model generalization and
resilience. Our approach leverages the Swin Transformer, which utilizes
hierarchical token processing and shifted window attention to efficiently
capture fine-grained features while maintaining linear computational
complexity. We validate our method on the Aptos and IDRiD datasets for
multi-class DR classification, achieving accuracy rates of 89.65% and 97.40%,
respectively. These results demonstrate the effectiveness of our model,
particularly in detecting early-stage DR, highlighting its potential for
improving automated retinal screening in clinical settings.

</details>

### [220] [Split-quaternions for perceptual white balance](https://arxiv.org/abs/2504.15481)
*Michel Berthier,Nicoletta Prencipe,Edoardo Provenzi*

Main category: eess.IV

TLDR: 提出了一种基于分裂四元数的感知色适应变换方法，用于白平衡。


<details>
  <summary>Details</summary>
Motivation: 受近期开发的量子化颜色感知模型启发，强调该模型中代数结构与分裂四元数子代数之间的联系。

Method: 通过分裂四元数乘法实现色适应变换。

Result: 与广泛使用的von Kries色适应变换进行了定量比较，展示了该方法的潜力。

Conclusion: 该方法在颜色图像处理应用中具有潜力。

Abstract: We propose a perceptual chromatic adaptation transform for white balance that
makes use of split-quaternions. The novelty of the present work, which is
motivated by a recently developed quantum-like model of color perception,
consists at stressing the link between the algebraic structures appearing in
this model and a certain sub-algebra of the split-quaternions. We show the
potentiality of this approach for color image processing applications by
proposing a chromatic adaptation transform, implemented via an appropriate use
of the split-quaternion multiplication. Moreover, quantitative comparisons with
the widely used state-of-the art von Kries chromatic adaptation transform are
provided.

</details>

### [221] [VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining](https://arxiv.org/abs/2504.15545)
*Zizhi Chen,Xinyu Zhang,Minghao Han,Yizhou Liu,Ziyun Qian,Weifeng Zhang,Xukun Zhang,Jingwei Wei,Lihua Zhang*

Main category: eess.IV

TLDR: 论文提出了一种基于病理视觉-语言大模型（VLM）的虚拟染色方法，结合对比学习提示和组织概念锚点，解决了传统虚拟染色中忽略病理知识和物理特性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟染色方法仅实现风格迁移，忽略了病理知识和染色物理特性，导致效果有限。

Method: 引入病理VLM作为辅助工具，结合对比学习提示和组织概念锚点，开发了基于VLM约束的数据增强方法。

Result: 在公开数据集上验证了方法的高真实性和下游任务（如肾小球检测）的准确性提升。

Conclusion: 该方法显著提升了虚拟染色的真实性和病理诊断的准确性。

Abstract: In histopathology, tissue sections are typically stained using common H&E
staining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific
tissue structures. The rapid advancement of deep learning offers an effective
solution for generating virtually stained images, significantly reducing the
time and labor costs associated with traditional histochemical staining.
However, a new challenge arises in separating the fundamental visual
characteristics of tissue sections from the visual differences induced by
staining agents. Additionally, virtual staining often overlooks essential
pathological knowledge and the physical properties of staining, resulting in
only style-level transfer. To address these issues, we introduce, for the first
time in virtual staining tasks, a pathological vision-language large model
(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,
foundational concept anchors for tissue sections, and staining-specific concept
anchors to leverage the extensive knowledge of the pathological VLM. This
approach is designed to describe, frame, and enhance the direction of virtual
staining. Furthermore, we have developed a data augmentation method based on
the constraints of the VLM. This method utilizes the VLM's powerful image
interpretation capabilities to further integrate image style and structural
information, proving beneficial in high-precision pathological diagnostics.
Extensive evaluations on publicly available multi-domain unpaired staining
datasets demonstrate that our method can generate highly realistic images and
enhance the accuracy of downstream tasks, such as glomerular detection and
segmentation. Our code is available at:
https://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR

</details>

### [222] [RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2504.15649)
*Biao Wu,Diankai Zhang,Shaoli Liu,Si Gao,Chengjian Zheng,Ning Wang*

Main category: eess.IV

TLDR: 提出了一种名为RepNet-VSR的可重参数化架构，用于实时4倍视频超分辨率，在资源受限的边缘设备上实现了高质量与高效部署的平衡。


<details>
  <summary>Details</summary>
Motivation: 视频超分辨率在资源受限的边缘设备（如移动设备）上部署时，面临计算密集和实时性挑战，需要一种高效且高质量的解决方案。

Method: 采用可重参数化架构（RepNet-VSR），优化计算效率，适用于实时4倍视频超分辨率任务。

Result: 在REDS验证集上，模型处理180p到720p帧时达到27.79 dB PSNR，每10帧耗时103毫秒，优于之前的冠军算法。

Conclusion: RepNet-VSR在恢复质量和部署效率之间取得了优异平衡，适用于实时移动视频处理场景。

Abstract: As a fundamental challenge in visual computing, video super-resolution (VSR)
focuses on reconstructing highdefinition video sequences from their degraded
lowresolution counterparts. While deep convolutional neural networks have
demonstrated state-of-the-art performance in spatial-temporal super-resolution
tasks, their computationally intensive nature poses significant deployment
challenges for resource-constrained edge devices, particularly in real-time
mobile video processing scenarios where power efficiency and latency
constraints coexist. In this work, we propose a Reparameterizable Architecture
for High Fidelity Video Super Resolution method, named RepNet-VSR, for
real-time 4x video super-resolution. On the REDS validation set, the proposed
model achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per
10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an
excellent balance between restoration quality and deployment efficiency. The
proposed method scores higher than the previous champion algorithm of MAI video
super-resolution challenge.

</details>

### [223] [Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg](https://arxiv.org/abs/2504.15667)
*Jingchen Zou,Jianqiang Li,Gabriel Jimenez,Qing Zhao,Daniel Racoceanu,Matias Cosarinsky,Enzo Ferrante,Guanghui Fu*

Main category: eess.IV

TLDR: 提出了一种无需标注数据的医学图像分割模型性能评估框架SPE，适用于多种评估指标和模型架构，实验证明其高效可靠。


<details>
  <summary>Details</summary>
Motivation: 在临床等实际应用中，标注所有数据不现实，导致模型性能难以评估。

Method: 设计了Segmentation Performance Evaluator (SPE)框架，通过实验验证其在多种评估指标和数据集上的表现。

Result: SPE在独立测试集上与实际Dice分数高度相关（0.956±0.046），且MAE低（0.025±0.019）。

Conclusion: SPE框架无需标注即可可靠评估模型性能，适用于实际应用。

Abstract: The performance of medical image segmentation models is usually evaluated
using metrics like the Dice score and Hausdorff distance, which compare
predicted masks to ground truth annotations. However, when applying the model
to unseen data, such as in clinical settings, it is often impractical to
annotate all the data, making the model's performance uncertain. To address
this challenge, we propose the Segmentation Performance Evaluator (SPE), a
framework for estimating segmentation models' performance on unlabeled data.
This framework is adaptable to various evaluation metrics and model
architectures. Experiments on six publicly available datasets across six
evaluation metrics including pixel-based metrics such as Dice score and
distance-based metrics like HD95, demonstrated the versatility and
effectiveness of our approach, achieving a high correlation (0.956$\pm$0.046)
and low MAE (0.025$\pm$0.019) compare with real Dice score on the independent
test set. These results highlight its ability to reliably estimate model
performance without requiring annotations. The SPE framework integrates
seamlessly into any model training process without adding training overhead,
enabling performance estimation and facilitating the real-world application of
medical image segmentation algorithms. The source code is publicly available

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [224] [EditLord: Learning Code Transformation Rules for Code Editing](https://arxiv.org/abs/2504.15284)
*Weichen Li,Albert Jan,Baishakhi Ray,Chengzhi Mao,Junfeng Yang,Kexin Pei*

Main category: cs.SE

TLDR: EditLord是一个显式代码编辑框架，通过语言模型提取编辑规则，显著提升编辑性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑方法通常将任务隐式处理，忽略其离散步骤，导致性能不足和缺乏鲁棒性。

Method: 使用语言模型从训练代码对中提取编辑规则，生成元规则集，用于微调或辅助提示和迭代编辑。

Result: EditLord在编辑性能、鲁棒性和功能正确性上显著优于现有方法。

Conclusion: 显式处理代码编辑步骤能显著提升效果，EditLord为代码编辑任务提供了新方向。

Abstract: Code editing is a foundational task in software development, where its
effectiveness depends on whether it introduces desired code property changes
without changing the original code's intended functionality. Existing
approaches often formulate code editing as an implicit end-to-end task,
omitting the fact that code-editing procedures inherently consist of discrete
and explicit steps. Thus, they suffer from suboptimal performance and lack of
robustness and generalization. We introduce EditLord, a code editing framework
that makes the code transformation steps explicit. Our key insight is to employ
a language model (LM) as an inductive learner to extract code editing rules
from the training code pairs as concise meta-rule sets. Such rule sets will be
manifested for each training sample to augment them for finetuning or assist in
prompting- and iterative-based code editing. EditLordoutperforms the
state-of-the-art by an average of 22.7% in editing performance and 58.1% in
robustness while achieving 20.2% higher functional correctness across critical
software engineering and security applications, LM models, and editing modes.

</details>

### [225] [A Study On Mixup-inspired Augmentation Methods For Software Vulnerability Detection](https://arxiv.org/abs/2504.15632)
*Seyed Shayan Daneshvar,Da Tan,Shaowei Wang,Carson Leung*

Main category: cs.SE

TLDR: 论文探讨了在表示层面增强漏洞数据的方法，以改善深度学习模型对软件漏洞的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现实中的软件漏洞数据集稀缺且不平衡，现有方法生成漏洞数据不实用且需人工检查。

Method: 提出并评估了5种嵌入层面的数据增强技术，并引入条件化版本以确保不改变漏洞部分的向量表示。

Result: 这些方法能将f1-score提高最多9.67%，但仍不及随机过采样的10.82%提升。

Conclusion: 表示层面的数据增强对漏洞检测有帮助，但平衡数据集时随机过采样效果更佳。

Abstract: Various Deep Learning (DL) methods have recently been utilized to detect
software vulnerabilities. Real-world software vulnerability datasets are rare
and hard to acquire as there's no simple metric for classifying vulnerability.
Such datasets are heavily imbalanced, and none of the current datasets are
considered huge for DL models. To tackle these problems a recent work has tried
to augment the dataset using the source code and generate realistic
single-statement vulnerabilities which is not quite practical and requires
manual checking of the generated vulnerabilities. In this regard, we aim to
explore the augmentation of vulnerabilities at the representation level to help
current models learn better which has never been done before to the best of our
knowledge. We implement and evaluate the 5 augmentation techniques that augment
the embedding of the data and recently have been used for code search which is
a completely different software engineering task. We also introduced a
conditioned version of those augmentation methods, which ensures the
augmentation does not change the vulnerable section of the vector
representation. We show that such augmentation methods can be helpful and
increase the f1-score by up to 9.67%, yet they cannot beat Random Oversampling
when balancing datasets which increases the f1-score by 10.82%!

</details>

### [226] [CUBETESTERAI: Automated JUnit Test Generation using the LLaMA Model](https://arxiv.org/abs/2504.15286)
*Daniele Gorla,Shivam Kumar,Pietro Nicolaus Roselli Lorenzini,Alireza Alipourfaz*

Main category: cs.SE

TLDR: 本文提出了一种利用LLaMA模型自动化生成Spring Boot应用JUnit测试的方法，开发了工具CUBETESTERAI，结合CI/CD流程，显著提升测试效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统JUnit测试生成效率低且易出错，需要自动化工具提升开发效率。

Method: 结合LLaMA模型和CI/CD流程，通过RunPod执行模型，生成高覆盖率测试用例。

Result: CUBETESTERAI在代码覆盖率和准确性上优于现有工具。

Conclusion: CUBETESTERAI为Java Spring Boot应用提供高效、准确的自动化测试解决方案。

Abstract: This paper presents an approach to automating JUnit test generation for Java
applications using the Spring Boot framework, leveraging the LLaMA (Large
Language Model Architecture) model to enhance the efficiency and accuracy of
the testing process. The resulting tool, called CUBETESTERAI, includes a
user-friendly web interface and the integration of a CI/CD pipeline using
GitLab and Docker. These components streamline the automated test generation
process, allowing developers to generate JUnit tests directly from their code
snippets with minimal manual intervention. The final implementation executes
the LLaMA models through RunPod, an online GPU service, which also enhances the
privacy of our tool. Using the advanced natural language processing
capabilities of the LLaMA model, CUBETESTERAI is able to generate test cases
that provide high code coverage and accurate validation of software
functionalities in Java-based Spring Boot applications. Furthermore, it
efficiently manages resource-intensive operations and refines the generated
tests to address common issues like missing imports and handling of private
methods. By comparing CUBETESTERAI with some state-of-the-art tools, we show
that our proposal consistently demonstrates competitive and, in many cases,
better performance in terms of code coverage in different real-life Java
programs.

</details>

### [227] [LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study](https://arxiv.org/abs/2504.15424)
*Nishath Rajiv Ranasinghe,Shawn M. Jones,Michal Kucer,Ayan Biswas,Daniel O'Malley,Alexander Buschmann Most,Selma Liliane Wanna,Ajay Sreekumar*

Main category: cs.SE

TLDR: 该论文研究了利用大型语言模型（LLMs）将Fortran代码翻译为C++代码的适用性，并量化了其编译准确性、代码相似性和输出相似性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在科学计算代码生成和翻译中应用广泛，但其对Fortran等传统高性能计算语言的翻译效果尚未充分评估。

Method: 研究使用开源LLMs在两种计算平台上构建代理工作流，对Fortran到C++的翻译进行统计量化，包括编译准确性、代码相似性和输出相似性。

Result: 论文统计量化了翻译后的C++代码的编译准确性、与人工翻译代码的相似性以及输出结果的相似性。

Conclusion: 研究表明LLM在Fortran到C++的代码翻译中具有一定适用性，但需进一步评估其实际可用性。

Abstract: Large Language Models (LLMs) are increasingly being leveraged for generating
and translating scientific computer codes by both domain-experts and non-domain
experts. Fortran has served as one of the go to programming languages in legacy
high-performance computing (HPC) for scientific discoveries. Despite growing
adoption, LLM-based code translation of legacy code-bases has not been
thoroughly assessed or quantified for its usability. Here, we studied the
applicability of LLM-based translation of Fortran to C++ as a step towards
building an agentic-workflow using open-weight LLMs on two different
computational platforms. We statistically quantified the compilation accuracy
of the translated C++ codes, measured the similarity of the LLM translated code
to the human translated C++ code, and statistically quantified the output
similarity of the Fortran to C++ translation.

</details>

### [228] [A Framework for Testing and Adapting REST APIs as LLM Tools](https://arxiv.org/abs/2504.15546)
*Jayachandu Bandlamudi,Ritwik Chaudhuri,Neelamadhav Gantayat,Kushal Mukherjee,Prerna Agarwal,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TLDR: 提出了一种新的测试框架，用于评估和增强REST API作为基于LLM的代理工具的可用性。


<details>
  <summary>Details</summary>
Motivation: 当前API作为工具使用时存在输入模式复杂、响应繁琐和文档模糊等问题，现有测试基准未能充分解决这些复杂性。

Method: 开发了一个测试框架，将API转化为工具，生成全面的测试用例，并将其转化为适合代理的自然语言指令，同时评估代理调用API和处理输入输出的能力。

Result: 分析了750个测试用例的结果，提出了错误分类（如输入误解、输出处理不一致和模式不匹配），并分类测试用例以简化调试。

Conclusion: 该框架为将企业API作为工具提供了基础，提升了其在基于代理的应用中的可用性。

Abstract: Large Language Models (LLMs) are enabling autonomous agents to perform
complex workflows using external tools or functions, often provided via REST
APIs in enterprise systems. However, directly utilizing these APIs as tools
poses challenges due to their complex input schemas, elaborate responses, and
often ambiguous documentation. Current benchmarks for tool testing do not
adequately address these complexities, leading to a critical gap in evaluating
API readiness for agent-driven automation. In this work, we present a novel
testing framework aimed at evaluating and enhancing the readiness of REST APIs
to function as tools for LLM-based agents. Our framework transforms apis as
tools, generates comprehensive test cases for the APIs, translates tests cases
into natural language instructions suitable for agents, enriches tool
definitions and evaluates the agent's ability t correctly invoke the API and
process its inputs and responses. To provide actionable insights, we analyze
the outcomes of 750 test cases, presenting a detailed taxonomy of errors,
including input misinterpretation, output handling inconsistencies, and schema
mismatches. Additionally, we classify these test cases to streamline debugging
and refinement of tool integrations. This work offers a foundational step
toward enabling enterprise APIs as tools, improving their usability in
agent-based applications.

</details>

### [229] [A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs](https://arxiv.org/abs/2504.15564)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TLDR: 论文介绍了一个基于真实开源项目的Python类级别数据集，用于提升大语言模型在代码生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多关注孤立函数，未能捕捉真实世界类级软件的复杂性。

Method: 从13,174个开源项目中提取842,000个类骨架，保留结构和上下文依赖，并添加静态代码指标。

Result: 使用GPT-4生成类实现，结果显示生成的类与人工编写的类在词汇和结构上高度相似（ROUGE@L 0.80，BLEU 0.59，TSED 0.73）。

Conclusion: 真实类骨架的结构化提示显著提升LLM在类级代码生成中的表现，数据集为LLM的基准测试和训练提供了宝贵资源。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
promising capabilities in code generation tasks. However, most existing
benchmarks focus on isolated functions and fail to capture the complexity of
real-world, class-level software structures. To address this gap, we introduce
a large-scale, Python class-level dataset curated from $13{,}174$ real-world
open-source projects. The dataset contains over 842,000 class skeletons, each
including class and method signatures, along with associated docstrings when
available. We preserve structural and contextual dependencies critical to
realistic software development scenarios and enrich the dataset with static
code metrics to support downstream analysis. To evaluate the usefulness of this
dataset, we use extracted class skeletons as prompts for GPT-4 to generate full
class implementations. Results show that the LLM-generated classes exhibit
strong lexical and structural similarity to human-written counterparts, with
average ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.
These findings confirm that well-structured prompts derived from real-world
class skeletons significantly enhance LLM performance in class-level code
generation. This dataset offers a valuable resource for benchmarking, training,
and improving LLMs in realistic software engineering contexts.

</details>

### [230] [Automated Bug Report Prioritization in Large Open-Source Projects](https://arxiv.org/abs/2504.15912)
*Riley Pierson,Armin Moin*

Main category: cs.SE

TLDR: 本文提出了一种基于自然语言文本的自动化错误优先级排序方法，结合TopicMiner-MTM和BERT模型，在Eclipse项目中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开源项目面临大量问题和有限资源，需要高效优先级排序。

Method: 使用TopicMiner-MTM进行主题建模，结合BERT进行文本分类。

Result: 在85,156个错误报告数据集上，准确率、精确率、召回率和F1值均优于现有方法。

Conclusion: 该方法显著提升了错误报告优先级预测的性能。

Abstract: Large open-source projects receive a large number of issues (known as bugs),
including software defect (i.e., bug) reports and new feature requests from
their user and developer communities at a fast rate. The often limited project
resources do not allow them to deal with all issues. Instead, they have to
prioritize them according to the project's priorities and the issues'
severities. In this paper, we propose a novel approach to automated bug
prioritization based on the natural language text of the bug reports that are
stored in the open bug repositories of the issue-tracking systems. We conduct
topic modeling using a variant of LDA called TopicMiner-MTM and text
classification with the BERT large language model to achieve a higher
performance level compared to the state-of-the-art. Experimental results using
an existing reference dataset containing 85,156 bug reports of the Eclipse
Platform project indicate that we outperform existing approaches in terms of
Accuracy, Precision, Recall, and F1-measure of the bug report priority
prediction.

</details>

### [231] [Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling](https://arxiv.org/abs/2504.15972)
*Sophie C. Pope,Andrew Barovic,Armin Moin*

Main category: cs.SE

TLDR: 该研究提出了一种结合情感分析和BERTopic模型的新方法，用于预测Bugzilla Eclipse项目中Bug的解决时间、修复时间和最终状态，发现情感分析对预测Bug是否修复特别有效。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合情感分析和BERTopic模型，提高对Bug相关结果的预测准确性，尤其是解决时间和修复时间。

Method: 利用情感分析生成情感分数和分类，结合BERTopic提取的主题和Bug优先级，使用CNN和MLP进行预测。

Result: 情感分析对预测Bug是否修复效果显著，但在复杂分类任务中效果较弱；平衡输入数据会降低准确性。

Conclusion: 情感分析是预测Bug结果的有用工具，但需权衡输入数据的平衡与准确性。

Abstract: This study explores a novel approach to predicting key bug-related outcomes,
including the time to resolution, time to fix, and ultimate status of a bug,
using data from the Bugzilla Eclipse Project. Specifically, we leverage
features available before a bug is resolved to enhance predictive accuracy. Our
methodology incorporates sentiment analysis to derive both an emotionality
score and a sentiment classification (positive or negative). Additionally, we
integrate the bug's priority level and its topic, extracted using a BERTopic
model, as features for a Convolutional Neural Network (CNN) and a Multilayer
Perceptron (MLP). Our findings indicate that the combination of BERTopic and
sentiment analysis can improve certain model performance metrics. Furthermore,
we observe that balancing model inputs enhances practical applicability, albeit
at the cost of a significant reduction in accuracy in most cases. To address
our primary objectives, predicting time-to-resolution, time-to-fix, and bug
destiny, we employ both binary classification and exact time value predictions,
allowing for a comparative evaluation of their predictive effectiveness.
Results demonstrate that sentiment analysis serves as a valuable predictor of a
bug's eventual outcome, particularly in determining whether it will be fixed.
However, its utility is less pronounced when classifying bugs into more complex
or unconventional outcome categories.

</details>

### [232] [Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3](https://arxiv.org/abs/2504.16027)
*Ahmed R. Sadik,Siddhata Govind*

Main category: cs.SE

TLDR: 研究提出了一种评估大型语言模型（LLM）在代码异味检测中表现的方法，比较了GPT-4.0和DeepSeek-V3的性能与成本效益。


<details>
  <summary>Details</summary>
Motivation: 确定哪种LLM在代码异味检测中最有效是一个复杂问题，需要系统化的评估方法。

Method: 使用标注好的代码数据集，跨四种编程语言（Java、Python、JavaScript、C++），以精确率、召回率和F1分数为指标，评估GPT-4.0和DeepSeek-V3的性能。

Result: 研究提供了模型在整体、类别和具体异味类型上的表现，并对比了与传统静态分析工具的成本效益。

Conclusion: 结果为从业者选择高效且经济的自动化代码异味检测方案提供了指导。

Abstract: Determining the most effective Large Language Model for code smell detection
presents a complex challenge. This study introduces a structured methodology
and evaluation matrix to tackle this issue, leveraging a curated dataset of
code samples consistently annotated with known smells. The dataset spans four
prominent programming languages Java, Python, JavaScript, and C++; allowing for
cross language comparison. We benchmark two state of the art LLMs, OpenAI GPT
4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation
metrics. Our analysis covers three levels of detail: overall performance,
category level performance, and individual code smell type performance.
Additionally, we explore cost effectiveness by comparing the token based
detection approach of GPT 4.0 with the pattern-matching techniques employed by
DeepSeek V3. The study also includes a cost analysis relative to traditional
static analysis tools such as SonarQube. The findings offer valuable guidance
for practitioners in selecting an efficient, cost effective solution for
automated code smell detection

</details>

<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [233] [High-performance training and inference for deep equivariant interatomic potentials](https://arxiv.org/abs/2504.16068)
*Chuin Wei Tan,Marc L. Descoteaux,Mit Kotak,Gabriel de Miranda Nascimento,Seán R. Kavanagh,Laura Zichi,Menghang Wang,Aadit Saluja,Yizhong R. Hu,Tess Smidt,Anders Johansson,William C. Witt,Boris Kozinsky,Albert Musaelian*

Main category: physics.comp-ph

TLDR: NequIP框架的重大升级，专注于多节点并行、计算性能和可扩展性，显著提升了分子动力学计算的效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模和下游工作流需求的快速增长，需要更强大和可扩展的软件来支持机器学习原子间势的建模任务。

Method: 对NequIP框架进行了全面改造，支持分布式训练和大规模数据集，优化了PyTorch 2.0编译器的使用，并引入了自定义内核以加速关键操作。

Result: 在SPICE 2数据集上的案例研究中，展示了训练速度的提升；通过PyTorch Ahead-of-Time Inductor编译器和自定义内核，分子动力学计算速度提高了18倍。

Conclusion: 升级后的NequIP框架显著提升了计算效率和可扩展性，为大规模原子建模任务提供了更强大的工具。

Abstract: Machine learning interatomic potentials, particularly those based on deep
equivariant neural networks, have demonstrated state-of-the-art accuracy and
computational efficiency in atomistic modeling tasks like molecular dynamics
and high-throughput screening. The size of datasets and demands of downstream
workflows are growing rapidly, making robust and scalable software essential.
This work presents a major overhaul of the NequIP framework focusing on
multi-node parallelism, computational performance, and extensibility. The
redesigned framework supports distributed training on large datasets and
removes barriers preventing full utilization of the PyTorch 2.0 compiler at
train time. We demonstrate this acceleration in a case study by training
Allegro models on the SPICE 2 dataset of organic molecular systems. For
inference, we introduce the first end-to-end infrastructure that uses the
PyTorch Ahead-of-Time Inductor compiler for machine learning interatomic
potentials. Additionally, we implement a custom kernel for the Allegro model's
most expensive operation, the tensor product. Together, these advancements
speed up molecular dynamics calculations on system sizes of practical relevance
by up to a factor of 18.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [234] [Med-CoDE: Medical Critique based Disagreement Evaluation Framework](https://arxiv.org/abs/2504.15330)
*Mohit Gupta,Akiko Aizawa,Rajiv Ratn Shah*

Main category: cs.IR

TLDR: 提出Med-CoDE框架，用于评估医疗领域大型语言模型的可靠性和准确性，填补现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法在医疗领域缺乏鲁棒性，可能导致临床风险，需要更全面的评估框架。

Method: 采用基于批判的方法，量化模型生成响应与医学事实之间的不一致程度。

Result: 通过实验和案例研究验证了框架的实用性和可靠性。

Conclusion: Med-CoDE为医疗LLM提供了系统化的评估方法，提升了其质量和可信度。

Abstract: The emergence of large language models (LLMs) has significantly influenced
numerous fields, including healthcare, by enhancing the capabilities of
automated systems to process and generate human-like text. However, despite
their advancements, the reliability and accuracy of LLMs in medical contexts
remain critical concerns. Current evaluation methods often lack robustness and
fail to provide a comprehensive assessment of LLM performance, leading to
potential risks in clinical settings. In this work, we propose Med-CoDE, a
specifically designed evaluation framework for medical LLMs to address these
challenges. The framework leverages a critique-based approach to quantitatively
measure the degree of disagreement between model-generated responses and
established medical ground truths. This framework captures both accuracy and
reliability in medical settings. The proposed evaluation framework aims to fill
the existing gap in LLM assessment by offering a systematic method to evaluate
the quality and trustworthiness of medical LLMs. Through extensive experiments
and case studies, we illustrate the practicality of our framework in providing
a comprehensive and reliable evaluation of medical LLMs.

</details>

### [235] [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
*Harsh Maheshwari,Srikanth Tenneti,Alwarappan Nakkiran*

Main category: cs.IR

TLDR: 论文提出了一种高效的后期处理算法，通过关键词+语义匹配、BERTScore微调模型和轻量级LLM技术，显著提升了RAG系统中LLM生成响应的引用准确性。实验结果显示，引用准确性相对提高了15.46%，同时降低了成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中LLM在引用准确性上表现不佳（约74%），影响了生成内容的可靠性和用户信任。

Method: 采用三种方法：关键词+语义匹配、BERTScore微调模型和轻量级LLM技术，对生成的引用进行交叉验证。

Result: 引用准确性相对提升15.46%，同时实现了12倍的成本节省和3倍的推理速度提升。

Conclusion: 该研究提升了AI生成内容的可靠性，尤其在商业产品中，有助于增强用户信任。

Abstract: Retrieval Augmented Generation (RAG) has emerged as a powerful application of
Large Language Models (LLMs), revolutionizing information search and
consumption. RAG systems combine traditional search capabilities with LLMs to
generate comprehensive answers to user queries, ideally with accurate
citations. However, in our experience of developing a RAG product, LLMs often
struggle with source attribution, aligning with other industry studies
reporting citation accuracy rates of only about 74% for popular generative
search engines. To address this, we present efficient post-processing
algorithms to improve citation accuracy in LLM-generated responses, with
minimal impact on latency and cost. Our approaches cross-check generated
citations against retrieved articles using methods including keyword + semantic
matching, fine tuned model with BERTScore, and a lightweight LLM-based
technique. Our experimental results demonstrate a relative improvement of
15.46% in the overall accuracy metrics of our RAG system. This significant
enhancement potentially enables a shift from our current larger language model
to a relatively smaller model that is approximately 12x more cost-effective and
3x faster in inference time, while maintaining comparable performance. This
research contributes to enhancing the reliability and trustworthiness of
AI-generated content in information retrieval and summarization tasks which is
critical to gain customer trust especially in commercial products.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [236] [Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL](https://arxiv.org/abs/2504.15425)
*Songyuan Zhang,Oswin So,Mitchell Black,Zachary Serlin,Chuchu Fan*

Main category: cs.RO

TLDR: 论文提出了一种名为Def-MARL的新型多智能体强化学习算法，用于解决多机器人系统中的安全协作问题，通过分布式执行实现稳定训练。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在协作完成任务时需要确保安全性，现有算法在零约束违反的安全定义下训练不稳定。

Method: 采用约束优化的epigraph形式，证明其可通过分布式方式解决，提出Def-MARL算法。

Result: 在8个任务和2个模拟器中表现最佳，满足安全约束且训练稳定，并在真实四旋翼飞行器实验中验证其有效性。

Conclusion: Def-MARL在安全协作任务中优于其他方法，具有实际应用潜力。

Abstract: Tasks for multi-robot systems often require the robots to collaborate and
complete a team goal while maintaining safety. This problem is usually
formalized as a constrained Markov decision process (CMDP), which targets
minimizing a global cost and bringing the mean of constraint violation below a
user-defined threshold. Inspired by real-world robotic applications, we define
safety as zero constraint violation. While many safe multi-agent reinforcement
learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms
suffer from unstable training in this setting. To tackle this, we use the
epigraph form for constrained optimization to improve training stability and
prove that the centralized epigraph form problem can be solved in a distributed
fashion by each agent. This results in a novel centralized training distributed
execution MARL algorithm named Def-MARL. Simulation experiments on 8 different
tasks across 2 different simulators show that Def-MARL achieves the best
overall performance, satisfies safety constraints, and maintains stable
training. Real-world hardware experiments on Crazyflie quadcopters demonstrate
the ability of Def-MARL to safely coordinate agents to complete complex
collaborative tasks compared to other methods.

</details>

### [237] [Advancing Embodied Intelligence in Robotic-Assisted Endovascular Procedures: A Systematic Review of AI Solutions](https://arxiv.org/abs/2504.15327)
*Tianliang Yao,Bo Lu,Markus Kowarschik,Yixuan Yuan,Hubin Zhao,Sebastien Ourselin,Kaspar Althoefer,Junbo Ge,Peng Qi*

Main category: cs.RO

TLDR: 本文综述了机器人系统在血管内手术中的应用，重点介绍了具身智能（EI）和数据驱动方法如何提升手术精度和临床效果。


<details>
  <summary>Details</summary>
Motivation: 血管内手术需要高精度和灵活性，但传统方法存在操作者疲劳、辐射暴露和人类精度限制等问题。机器人系统和EI的引入为解决这些问题提供了新思路。

Method: 结合数据驱动方法、计算机视觉、医学图像分析和机器学习技术，实现实时血管分割、设备跟踪和标志物检测。强化学习和模仿学习用于优化导航策略。

Result: EI和数据驱动方法显著提升了机器人系统的智能感知和控制能力，为血管内手术带来更高的自主性和临床效果。

Conclusion: 本文为未来研究提供了框架，强调了更大自主性和临床改进的潜力，并探讨了联邦学习、可解释AI和人机协作等新兴趋势。

Abstract: Endovascular procedures have revolutionized the treatment of vascular
diseases thanks to minimally invasive solutions that significantly reduce
patient recovery time and enhance clinical outcomes. However, the precision and
dexterity required during these procedures poses considerable challenges for
interventionists. Robotic systems have emerged offering transformative
solutions, addressing issues such as operator fatigue, radiation exposure, and
the inherent limitations of human precision. The integration of Embodied
Intelligence (EI) into these systems signifies a paradigm shift, enabling
robots to navigate complex vascular networks and adapt to dynamic physiological
conditions. Data-driven approaches, advanced computer vision, medical image
analysis, and machine learning techniques, are at the forefront of this
evolution. These methods augment procedural intelligence by facilitating
real-time vessel segmentation, device tracking, and anatomical landmark
detection. Reinforcement learning and imitation learning further refine
navigation strategies and replicate experts' techniques. This review
systematically examines the integration of EI principles into robotic
technologies, in relation to endovascular procedures. We discuss recent
advancements in intelligent perception and data-driven control, and their
practical applications in robot-assisted endovascular procedures. By critically
evaluating current limitations and emerging opportunities, this review
establishes a framework for future developments, emphasizing the potential for
greater autonomy and improved clinical outcomes. Emerging trends and specific
areas of research, such as federated learning for medical data sharing,
explainable AI for clinical decision support, and advanced human-robot
collaboration paradigms, are also explored, offering insights into the future
direction of this rapidly evolving field.

</details>

### [238] [A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities](https://arxiv.org/abs/2504.15654)
*Md Abdul Baset Sarker,Art Nguyen,Sigmond Kukla,Kevin Fite,Masudul H. Imtiaz*

Main category: cs.RO

TLDR: 本文介绍了一种新型AI视觉儿童假肢手，专为10-12岁上肢残疾儿童设计，具有仿生外观、多关节功能和轻量化设计，结合3D打印技术和机器视觉，提供低成本、可定制的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有肌电假肢存在成本高、功能有限等问题，无法满足低收入家庭需求。本文旨在通过AI视觉技术开发一种低成本、高性能的儿童假肢手。

Method: 采用3D打印技术，集成机器视觉、传感和嵌入式计算，通过微摄像头与低功耗FPGA实现实时物体检测和精确抓握。

Result: 深度学习物体检测和抓握分类模型准确率分别为96%和100%，力预测的平均绝对误差为0.018。

Conclusion: 该假肢手通过AI视觉技术实现了低成本、高性能和低功耗，为儿童上肢残疾提供了创新解决方案。

Abstract: This paper introduces a novel AI vision-enabled pediatric prosthetic hand
designed to assist children aged 10-12 with upper limb disabilities. The
prosthesis features an anthropomorphic appearance, multi-articulating
functionality, and a lightweight design that mimics a natural hand, making it
both accessible and affordable for low-income families. Using 3D printing
technology and integrating advanced machine vision, sensing, and embedded
computing, the prosthetic hand offers a low-cost, customizable solution that
addresses the limitations of current myoelectric prostheses. A micro camera is
interfaced with a low-power FPGA for real-time object detection and assists
with precise grasping. The onboard DL-based object detection and grasp
classification models achieved accuracies of 96% and 100% respectively. In the
force prediction, the mean absolute error was found to be 0.018. The features
of the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted
micro camera for artificial sensing, enabling a wide range of hand-based tasks;
b) real-time object detection and distance estimation for precise grasping; and
c) ultra-low-power operation that delivers high performance within constrained
power and resource limits.

</details>

### [239] [Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to Cherry-Picking](https://arxiv.org/abs/2504.15414)
*Dylan Khor,Bowen Weng*

Main category: cs.RO

TLDR: 论文提出了一种基于最坏情况性能转移优化的方法，用于解决强化学习策略在模拟到现实转移中的后收敛问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注预收敛阶段，但无法消除奖励的噪声振荡，导致策略选择依赖启发式或人为挑选。

Method: 提出了一种凸二次约束线性规划问题，用于优化最坏情况性能转移。

Result: 实验证明该方法在将强化学习策略从模拟转移到现实实验室测试中有效。

Conclusion: 该方法为后收敛阶段的模拟到现实转移提供了一种有效的解决方案。

Abstract: Learning-based approaches, particularly reinforcement learning (RL), have
become widely used for developing control policies for autonomous agents, such
as locomotion policies for legged robots. RL training typically maximizes a
predefined reward (or minimizes a corresponding cost/loss) by iteratively
optimizing policies within a simulator. Starting from a randomly initialized
policy, the empirical expected reward follows a trajectory with an overall
increasing trend. While some policies become temporarily stuck in local optima,
a well-defined training process generally converges to a reward level with
noisy oscillations. However, selecting a policy for real-world deployment is
rarely an analytical decision (i.e., simply choosing the one with the highest
reward) and is instead often performed through trial and error. To improve
sim-to-real transfer, most research focuses on the pre-convergence stage,
employing techniques such as domain randomization, multi-fidelity training,
adversarial training, and architectural innovations. However, these methods do
not eliminate the inevitable convergence trajectory and noisy oscillations of
rewards, leading to heuristic policy selection or cherry-picking. This paper
addresses the post-convergence sim-to-real transfer problem by introducing a
worst-case performance transference optimization approach, formulated as a
convex quadratic-constrained linear programming problem. Extensive experiments
demonstrate its effectiveness in transferring RL-based locomotion policies from
simulation to real-world laboratory tests.

</details>

### [240] [Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction](https://arxiv.org/abs/2504.15766)
*Tobias Demmler,Lennart Hartung,Andreas Tamke,Thao Dang,Alexander Hegai,Karsten Haug,Lars Mikelsons*

Main category: cs.RO

TLDR: 论文提出了一种改进的MTR模型，通过引入动态意图点来提升轨迹预测的准确性，尤其是在长时间预测和复杂交通场景中。


<details>
  <summary>Details</summary>
Motivation: 静态意图点与地图数据在特定交通场景中不匹配，导致预测结果不准确或不现实。

Method: 在MTR模型中集成场景特定的动态意图点，并在Waymo Open Motion Dataset上进行训练和评估。

Result: 动态意图点的引入显著提高了轨迹预测的准确性，尤其是在长时间预测中。

Conclusion: 动态意图点的集成是解决静态意图点局限性的有效方法，提升了预测模型的性能。

Abstract: In autonomous driving, accurately predicting the movements of other traffic
participants is crucial, as it significantly influences a vehicle's planning
processes. Modern trajectory prediction models strive to interpret complex
patterns and dependencies from agent and map data. The Motion Transformer (MTR)
architecture and subsequent work define the most accurate methods in common
benchmarks such as the Waymo Open Motion Benchmark. The MTR model employs
pre-generated static intention points as initial goal points for trajectory
prediction. However, the static nature of these points frequently leads to
misalignment with map data in specific traffic scenarios, resulting in
unfeasible or unrealistic goal points. Our research addresses this limitation
by integrating scene-specific dynamic intention points into the MTR model. This
adaptation of the MTR model was trained and evaluated on the Waymo Open Motion
Dataset. Our findings demonstrate that incorporating dynamic intention points
has a significant positive impact on trajectory prediction accuracy, especially
for predictions over long time horizons. Furthermore, we analyze the impact on
ground truth trajectories which are not compliant with the map data or are
illegal maneuvers.

</details>

### [241] [SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems](https://arxiv.org/abs/2504.15305)
*Abhishek Tyagi,Charu Gaur*

Main category: cs.RO

TLDR: Veg是一个自主空中监视平台，集成了视觉SLAM、高级控制架构和嵌入式视觉模块，支持GPS独立导航、实时对象和人脸识别，并具备故障检测与恢复能力。


<details>
  <summary>Details</summary>
Motivation: 设计一个适用于受限环境的自主无人机平台，整合实时定位、故障恢复和嵌入式AI功能。

Method: 采用LQR内环和PD外环轨迹控制的级联控制设计，利用ORB-SLAM3进行6自由度定位，支持基于Dijkstra路径规划的导航，并嵌入轻量级CNN和PCA的视觉系统。

Result: 通过仿真和实际测试验证了平台的实时定位、故障恢复和嵌入式AI功能。

Conclusion: Veg平台成功整合了多项关键技术，适用于复杂环境中的自主监视任务。

Abstract: We present an autonomous aerial surveillance platform, Veg, designed as a
fault-tolerant quadcopter system that integrates visual SLAM for
GPS-independent navigation, advanced control architecture for dynamic
stability, and embedded vision modules for real-time object and face
recognition. The platform features a cascaded control design with an LQR
inner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for
6-DoF localization and loop closure, and supports waypoint-based navigation
through Dijkstra path planning over SLAM-derived maps. A real-time Failure
Detection and Identification (FDI) system detects rotor faults and executes
emergency landing through re-routing. The embedded vision system, based on a
lightweight CNN and PCA, enables onboard object detection and face recognition
with high precision. The drone operates fully onboard using a Raspberry Pi 4
and Arduino Nano, validated through simulations and real-world testing. This
work consolidates real-time localization, fault recovery, and embedded AI on a
single platform suitable for constrained environments.

</details>

### [242] [LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning](https://arxiv.org/abs/2504.15472)
*Pingcheng Jian,Xiao Wei,Yanbaihui Liu,Samuel A. Moore,Michael M. Zavlanos,Boyuan Chen*

Main category: cs.RO

TLDR: LAPP是一种利用大语言模型（LLM）自动生成偏好标签的机器人学习框架，减少人工干预，实现高效、可定制和复杂行为的学习。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工设计的奖励、演示或昂贵的偏好标签，LAPP旨在通过LLM自动生成偏好标签，降低学习成本并提升灵活性。

Method: LAPP整合LLM到强化学习反馈循环中，通过轨迹级偏好预测训练在线偏好预测器，指导策略优化。

Result: 在四足运动和灵巧操作任务中，LAPP表现出高效学习、更高性能、更快适应和精确控制能力，成功完成高难度任务如四足后空翻。

Conclusion: LAPP为偏好驱动的机器人学习提供了可扩展的新方向。

Abstract: We introduce Large Language Model-Assisted Preference Prediction (LAPP), a
novel framework for robot learning that enables efficient, customizable, and
expressive behavior acquisition with minimum human effort. Unlike prior
approaches that rely heavily on reward engineering, human demonstrations,
motion capture, or expensive pairwise preference labels, LAPP leverages large
language models (LLMs) to automatically generate preference labels from raw
state-action trajectories collected during reinforcement learning (RL). These
labels are used to train an online preference predictor, which in turn guides
the policy optimization process toward satisfying high-level behavioral
specifications provided by humans. Our key technical contribution is the
integration of LLMs into the RL feedback loop through trajectory-level
preference prediction, enabling robots to acquire complex skills including
subtle control over gait patterns and rhythmic timing. We evaluate LAPP on a
diverse set of quadruped locomotion and dexterous manipulation tasks and show
that it achieves efficient learning, higher final performance, faster
adaptation, and precise control of high-level behaviors. Notably, LAPP enables
robots to master highly dynamic and expressive tasks such as quadruped
backflips, which remain out of reach for standard LLM-generated or handcrafted
rewards. Our results highlight LAPP as a promising direction for scalable
preference-driven robot learning.

</details>

### [243] [Few-Shot Vision-Language Action-Incremental Policy Learning](https://arxiv.org/abs/2504.15517)
*Mingchen Song,Xiang Deng,Guoqiang Zhong,Qi Lv,Jia Wan,Yinchuan Li,Jianye Hao,Weili Guan*

Main category: cs.RO

TLDR: 论文提出了一种名为TOPIC的方法，用于解决机器人模仿学习中数据稀缺和持续学习的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的机器人操作方法需要大量数据，且难以实现对新任务的持续学习。

Method: TOPIC通过任务特定提示（TSP）和多模态信息交互解决数据稀缺问题，并通过连续进化策略（CES）构建任务关系图以增强持续学习能力。

Result: 实验表明，TOPIC在成功率上优于现有基线方法26%以上。

Conclusion: TOPIC在机器人操作任务中首次实现了少样本持续学习，显著提升了现有方法的性能。

Abstract: Recently, Transformer-based robotic manipulation methods utilize multi-view
spatial representations and language instructions to learn robot motion
trajectories by leveraging numerous robot demonstrations. However, the
collection of robot data is extremely challenging, and existing methods lack
the capability for continuous learning on new tasks with only a few
demonstrations. In this paper, we formulate these challenges as the Few-Shot
Action-Incremental Learning (FSAIL) task, and accordingly design a Task-prOmpt
graPh evolutIon poliCy (TOPIC) to address these issues. Specifically, to
address the data scarcity issue in robotic imitation learning, TOPIC learns
Task-Specific Prompts (TSP) through the deep interaction of multi-modal
information within few-shot demonstrations, thereby effectively extracting the
task-specific discriminative information. On the other hand, to enhance the
capability for continual learning on new tasks and mitigate the issue of
catastrophic forgetting, TOPIC adopts a Continuous Evolution Strategy (CES).
CES leverages the intrinsic relationships between tasks to construct a task
relation graph, which effectively facilitates the adaptation of new tasks by
reusing skills learned from previous tasks. TOPIC pioneers few-shot continual
learning in the robotic manipulation task, and extensive experimental results
demonstrate that TOPIC outperforms state-of-the-art baselines by over 26$\%$ in
success rate, significantly enhancing the continual learning capabilities of
existing Transformer-based policies.

</details>

### [244] [RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios](https://arxiv.org/abs/2504.15541)
*Qichao Liu,Heye Huang,Shiyue Zhao,Lei Shi,Soyoung Ahn,Xiaopeng Li*

Main category: cs.RO

TLDR: RiskNet是一个交互感知的风险预测框架，结合确定性风险建模和概率行为预测，用于自动驾驶车辆在复杂场景中的安全评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在长尾场景和高不确定性环境中的安全性是一个关键挑战，需要更全面的风险评估方法。

Method: RiskNet采用场论模型捕捉车辆、周围代理和基础设施的交互，并结合基于图神经网络的轨迹预测模块处理行为不确定性。

Result: 在多个数据集上的评估显示，RiskNet在准确性、响应性和方向敏感性上显著优于传统方法，并具有强泛化能力。

Conclusion: RiskNet为长尾场景中的安全关键决策提供了统一基础，支持实时、场景自适应的风险预测。

Abstract: Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios
remains a critical challenge, particularly under high uncertainty and complex
multi-agent interactions. To address this, we propose RiskNet, an
interaction-aware risk forecasting framework, which integrates deterministic
risk modeling with probabilistic behavior prediction for comprehensive risk
assessment. At its core, RiskNet employs a field-theoretic model that captures
interactions among ego vehicle, surrounding agents, and infrastructure via
interaction fields and force. This model supports multidimensional risk
evaluation across diverse scenarios (highways, intersections, and roundabouts),
and shows robustness under high-risk and long-tail settings. To capture the
behavioral uncertainty, we incorporate a graph neural network (GNN)-based
trajectory prediction module, which learns multi-modal future motion
distributions. Coupled with the deterministic risk field, it enables dynamic,
probabilistic risk inference across time, enabling proactive safety assessment
under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning
lane changes, turns, and complex merges, demonstrate that our method
significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC
Field) in terms of accuracy, responsiveness, and directional sensitivity, while
maintaining strong generalization across scenarios. This framework supports
real-time, scenario-adaptive risk forecasting and demonstrates strong
generalization across uncertain driving environments. It offers a unified
foundation for safety-critical decision-making in long-tail scenarios.

</details>

### [245] [SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation](https://arxiv.org/abs/2504.15561)
*Jingkai Xu,Xiangli Nie*

Main category: cs.RO

TLDR: SPECI是一种新型的层次化持续模仿学习框架，通过动态技能提取和任务级知识转移，显著提升机器人操作的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖静态训练，难以适应动态环境；现有持续模仿学习方法忽视了机器人操作的技能特性或依赖手动定义技能，导致跨任务知识转移不足。

Method: SPECI框架包括多模态感知融合模块、高层技能推理模块和低层动作执行模块，通过可扩展技能代码本和注意力驱动机制实现技能获取与重用。

Result: 实验表明，SPECI在所有评估指标上均优于现有方法，展现出卓越的双向知识转移和整体性能。

Conclusion: SPECI通过动态技能提取和任务级参数优化，为机器人操作的终身适应提供了高效解决方案。

Abstract: Real-world robot manipulation in dynamic unstructured environments requires
lifelong adaptability to evolving objects, scenes and tasks. Traditional
imitation learning relies on static training paradigms, which are ill-suited
for lifelong adaptation. Although Continual Imitation Learnin (CIL) enables
incremental task adaptation while preserving learned knowledge, current CIL
methods primarily overlook the intrinsic skill characteristics of robot
manipulation or depend on manually defined and rigid skills, leading to
suboptimal cross-task knowledge transfer. To address these issues, we propose
Skill Prompts-based HiErarchical Continual Imitation Learning (SPECI), a novel
end-to-end hierarchical CIL policy architecture for robot manipulation. The
SPECI framework consists of a multimodal perception and fusion module for
heterogeneous sensory information encoding, a high-level skill inference module
for dynamic skill extraction and selection, and a low-level action execution
module for precise action generation. To enable efficient knowledge transfer on
both skill and task levels, SPECI performs continual implicit skill acquisition
and reuse via an expandable skill codebook and an attention-driven skill
selection mechanism. Furthermore, we introduce mode approximation to augment
the last two modules with task-specific and task-sharing parameters, thereby
enhancing task-level knowledge transfer. Extensive experiments on diverse
manipulation task suites demonstrate that SPECI consistently outperforms
state-of-the-art CIL methods across all evaluated metrics, revealing
exceptional bidirectional knowledge transfer and superior overall performance.

</details>

### [246] [Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation](https://arxiv.org/abs/2504.15876)
*Qizhen Wu Lei Chen,Kexin Liu,Jinhu Lü*

Main category: cs.RO

TLDR: 提出了一种基于分层强化学习的双向方法，用于动态交互离散命令和连续动作，提升群机器人对抗场景中的决策效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统任务与运动规划方法将决策分为两层，但单向结构无法捕捉层间依赖关系，限制了动态环境中的适应性。

Method: 采用分层强化学习的双向方法，结合跨训练技术和轨迹预测模型，实现任务分配与路径规划的动态交互。

Result: 实验表明，该方法在对抗胜率上超过80%，决策时间低于0.01秒，优于现有方法。

Conclusion: 该方法在泛化能力和实际应用性上表现出色，适用于动态环境中的群机器人对抗场景。

Abstract: In swarm robotics, confrontation scenarios, including strategic
confrontations, require efficient decision-making that integrates discrete
commands and continuous actions. Traditional task and motion planning methods
separate decision-making into two layers, but their unidirectional structure
fails to capture the interdependence between these layers, limiting
adaptability in dynamic environments. Here, we propose a novel bidirectional
approach based on hierarchical reinforcement learning, enabling dynamic
interaction between the layers. This method effectively maps commands to task
allocation and actions to path planning, while leveraging cross-training
techniques to enhance learning across the hierarchical framework. Furthermore,
we introduce a trajectory prediction model that bridges abstract task
representations with actionable planning goals. In our experiments, it achieves
over 80\% in confrontation win rate and under 0.01 seconds in decision time,
outperforming existing approaches. Demonstrations through large-scale tests and
real-world robot experiments further emphasize the generalization capabilities
and practical applicability of our method.

</details>

### [247] [RaSCL: Radar to Satellite Crossview Localization](https://arxiv.org/abs/2504.15899)
*Blerim Abdullai,Tony Wang,Xinyuan Qiao,Florian Shkurti,Timothy D. Barfoot*

Main category: cs.RO

TLDR: 提出了一种不依赖GNSS的全局定位方法，通过地面雷达与高空RGB图像的配准，结合里程计和全局位姿优化，实现高效定位。


<details>
  <summary>Details</summary>
Motivation: GNSS在许多实时自主应用中不可靠、不准确且不足，需要一种替代方案。

Method: 通过地面雷达与高空RGB图像的配准，结合里程计和全局位姿优化，提取关键特征进行定位。

Result: 在多种地理条件和机器人平台上验证了方法的有效性，包括无人水面艇和城市/郊区驾驶数据集。

Conclusion: 该方法为GNSS不可靠的场景提供了一种有效的全局定位解决方案。

Abstract: GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous
field applications. In this work, we present a GNSS-free global localization
solution that contains a method of registering imaging radar on the ground with
overhead RGB imagery, with joint optimization of relative poses from odometry
and global poses from our overhead registration. Previous works have used
various combinations of ground sensors and overhead imagery, and different
feature extraction and matching methods. These include various handcrafted and
deep-learning-based methods for extracting features from overhead imagery. Our
work presents insights on extracting essential features from RGB overhead
images for effective global localization against overhead imagery using only
ground radar and a single georeferenced initial guess. We motivate our method
by evaluating it on datasets in diverse geographic conditions and robotic
platforms, including on an Unmanned Surface Vessel (USV) as well as urban and
suburban driving datasets.

</details>

### [248] [Visual Place Cell Encoding: A Computational Model for Spatial Representation and Cognitive Mapping](https://arxiv.org/abs/2504.15953)
*Chance J. Hamilton,Alfredo Weitzenfeld*

Main category: cs.RO

TLDR: VPCE模型通过视觉输入模拟位置细胞激活，验证了视觉地标在空间编码中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 探索视觉输入是否足以生成类似生物位置细胞的空间表征，支持认知映射。

Method: 使用机器人摄像头捕获的图像提取高维特征，通过聚类和径向基函数计算激活。

Result: VPCE能区分视觉相似但空间不同的位置，并适应环境变化。

Conclusion: 结构化视觉输入足以生成位置细胞样空间表征，支持生物启发的认知映射。

Abstract: This paper presents the Visual Place Cell Encoding (VPCE) model, a
biologically inspired computational framework for simulating place cell-like
activation using visual input. Drawing on evidence that visual landmarks play a
central role in spatial encoding, the proposed VPCE model activates visual
place cells by clustering high-dimensional appearance features extracted from
images captured by a robot-mounted camera. Each cluster center defines a
receptive field, and activation is computed based on visual similarity using a
radial basis function. We evaluate whether the resulting activation patterns
correlate with key properties of biological place cells, including spatial
proximity, orientation alignment, and boundary differentiation. Experiments
demonstrate that the VPCE can distinguish between visually similar yet
spatially distinct locations and adapt to environment changes such as the
insertion or removal of walls. These results suggest that structured visual
input, even in the absence of motion cues or reward-driven learning, is
sufficient to generate place-cell-like spatial representations and support
biologically inspired cognitive mapping.

</details>

### [249] [ForesightNav: Learning Scene Imagination for Efficient Exploration](https://arxiv.org/abs/2504.16062)
*Hardik Shah,Jiaxu Xing,Nico Messikommer,Boyang Sun,Marc Pollefeys,Davide Scaramuzza*

Main category: cs.RO

TLDR: ForesightNav是一种受人类想象和推理启发的探索策略，通过预测未探索区域的上下文信息（如占用和语义细节），显著提升机器人在未知环境中的探索效率。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何利用先验知识在未知环境中进行探索决策，以开发具备类似能力的自主机器人。

Method: 提出ForesightNav，赋予机器人预测未探索区域上下文信息的能力，从而选择有意义的长期导航目标。

Result: 在Structured3D数据集上验证，展示了准确的占用预测和优越的未探索场景几何预测性能，PointNav完成率100%，ObjectNav SPL达67%。

Conclusion: 想象驱动的推理能显著提升自主系统的通用性和探索效率。

Abstract: Understanding how humans leverage prior knowledge to navigate unseen
environments while making exploratory decisions is essential for developing
autonomous robots with similar abilities. In this work, we propose
ForesightNav, a novel exploration strategy inspired by human imagination and
reasoning. Our approach equips robotic agents with the capability to predict
contextual information, such as occupancy and semantic details, for unexplored
regions. These predictions enable the robot to efficiently select meaningful
long-term navigation goals, significantly enhancing exploration in unseen
environments. We validate our imagination-based approach using the Structured3D
dataset, demonstrating accurate occupancy prediction and superior performance
in anticipating unseen scene geometry. Our experiments show that the
imagination module improves exploration efficiency in unseen environments,
achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav
on the Structured3D Validation split. These contributions demonstrate the power
of imagination-driven reasoning for autonomous systems to enhance generalizable
and efficient exploration.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [250] [The Hardness of Learning Quantum Circuits and its Cryptographic Applications](https://arxiv.org/abs/2504.15343)
*Bill Fefferman,Soumik Ghosh,Makrand Sinha,Henry Yuen*

Main category: quant-ph

TLDR: 论文提出基于随机量子电路的硬度假设，构建了安全的量子密码学原语，包括单向状态生成器、数字签名方案等，并探讨了其在NISQ设备上的实现潜力。


<details>
  <summary>Details</summary>
Motivation: 探索量子密码学的新基础，摆脱对单向函数的依赖，并研究其在近量子设备上的可行性。

Method: 基于随机量子电路的硬度假设，构建多种量子密码学原语，分析量子学习算法的性能，并证明黑盒下界。

Result: 成功构造了安全的量子密码学方案，并展示了其在噪声环境下的实现可能性。

Conclusion: 量子学习理论与密码学的联系在量子领域同样成立，为近量子设备的密码学应用提供了新方向。

Abstract: We show that concrete hardness assumptions about learning or cloning the
output state of a random quantum circuit can be used as the foundation for
secure quantum cryptography. In particular, under these assumptions we
construct secure one-way state generators (OWSGs), digital signature schemes,
quantum bit commitments, and private key encryption schemes. We also discuss
evidence for these hardness assumptions by analyzing the best-known quantum
learning algorithms, as well as proving black-box lower bounds for cloning and
learning given state preparation oracles.
  Our random circuit-based constructions provide concrete instantiations of
quantum cryptographic primitives whose security do not depend on the existence
of one-way functions. The use of random circuits in our constructions also
opens the door to NISQ-friendly quantum cryptography. We discuss noise tolerant
versions of our OWSG and digital signature constructions which can potentially
be implementable on noisy quantum computers connected by a quantum network. On
the other hand, they are still secure against noiseless quantum adversaries,
raising the intriguing possibility of a useful implementation of an end-to-end
cryptographic protocol on near-term quantum computers. Finally, our
explorations suggest that the rich interconnections between learning theory and
cryptography in classical theoretical computer science also extend to the
quantum setting.

</details>

<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [251] [Transport f divergences](https://arxiv.org/abs/2504.15515)
*Wuchen Li*

Main category: math.ST

TLDR: 本文提出了一种基于凸函数和Jacobi算子的散度度量方法，用于衡量一维样本空间中概率密度函数的差异，称为“传输$f$-散度”。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过映射函数的Jacobi算子构造一种新的散度度量，以更好地衡量概率密度函数之间的差异。

Method: 基于凸函数和映射函数的Jacobi算子构造传输$f$-散度，并分析其性质，如不变性、凸性、变分公式和泰勒展开。

Result: 提出了传输$f$-散度的定义，并展示了其在生成模型中的应用示例。

Conclusion: 传输$f$-散度是一种有效的度量方法，具有多种数学性质，适用于生成模型等领域。

Abstract: We define a class of divergences to measure differences between probability
density functions in one-dimensional sample space. The construction is based on
the convex function with the Jacobi operator of mapping function that
pushforwards one density to the other. We call these information measures {\em
transport $f$-divergences}. We present several properties of transport
$f$-divergences, including invariances, convexities, variational formulations,
and Taylor expansions in terms of mapping functions. Examples of transport
$f$-divergences in generative models are provided.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [252] [Efficient Discovery of Motif Transition Process for Large-Scale Temporal Graphs](https://arxiv.org/abs/2504.15979)
*Zhiyuan Zheng,Jianpeng Qi,Jiantao Li,Guoqing Chao,Junyu Dong,Yanwei Yu*

Main category: cs.DB

TLDR: 提出了一种并行算法PTMT，用于发现大规模时序图中的模体动态转换过程，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于预定义模体，难以全面捕捉时序图中模体的动态转换和相互关系。

Method: PTMT结合树形框架与时序分区策略，分三阶段实现并行处理：生长区扩展、重叠感知结果聚合和模体转换确定性编码。

Result: 在10个真实数据集上，PTMT比现有最优方法快12.0到50.3倍。

Conclusion: PTMT能高效且准确地追踪时序图中模体的动态转换，适用于大规模分析。

Abstract: Understanding the dynamic transition of motifs in temporal graphs is
essential for revealing how graph structures evolve over time, identifying
critical patterns, and predicting future behaviors, yet existing methods often
focus on predefined motifs, limiting their ability to comprehensively capture
transitions and interrelationships. We propose a parallel motif transition
process discovery algorithm, PTMT, a novel parallel method for discovering
motif transition processes in large-scale temporal graphs. PTMT integrates a
tree-based framework with the temporal zone partitioning (TZP) strategy, which
partitions temporal graphs by time and structure while preserving lossless
motif transitions and enabling massive parallelism. PTMT comprises three
phases: growth zone parallel expansion, overlap-aware result aggregation, and
deterministic encoding of motif transitions, ensuring accurate tracking of
dynamic transitions and interactions. Results on 10 real-world datasets
demonstrate that PTMT achieves speedups ranging from 12.0$\times$ to
50.3$\times$ compared to the SOTA method.

</details>

<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [253] [Full waveform inversion with CNN-based velocity representation extension](https://arxiv.org/abs/2504.15826)
*Xinru Mu,Omar M. Saad,Tariq Alkhalifah*

Main category: physics.geo-ph

TLDR: 提出了一种结合卷积神经网络（CNN）的全波形反演方法（VRE-FWI），通过减少噪声提高速度模型精度，计算成本仅增加约1%。


<details>
  <summary>Details</summary>
Motivation: 数值模拟中的离散化误差和不完整地震数据采集会引入噪声，影响速度梯度和反演精度。

Method: 使用CNN在正演模拟前优化速度模型，采用自监督学习更新速度和网络参数，提出两种实现方案。

Result: 合成和实际数据测试表明，VRE-FWI比传统FWI具有更高的反演精度。

Conclusion: VRE-FWI在计算成本仅小幅增加的情况下，显著提升了速度反演的准确性。

Abstract: Full waveform inversion (FWI) updates the velocity model by minimizing the
discrepancy between observed and simulated data. However, discretization errors
in numerical modeling and incomplete seismic data acquisition can introduce
noise, which propagates through the adjoint operator and affects the accuracy
of the velocity gradient, thereby impacting the FWI inversion accuracy. To
mitigate the influence of noise on the gradient, we employ a convolutional
neural network (CNN) to refine the velocity model before performing the forward
simulation, aiming to reduce noise and provide a more accurate velocity update
direction. We use the same data misfit loss to update both the velocity and
network parameters, thereby forming a self-supervised learning procedure. We
propose two implementation schemes, which differ in whether the velocity update
passes through the CNN. In both methodologies, the velocity representation is
extended (VRE) by using a neural network in addition to the grid-based
velocities. Thus, we refer to this general approach as VRE-FWI. Synthetic and
real data tests demonstrate that the proposed VRE-FWI achieves higher velocity
inversion accuracy compared to traditional FWI, at a marginal additional
computational cost of approximately 1%.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [254] [VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation](https://arxiv.org/abs/2504.15659)
*Anjiang Wei,Huanmi Tan,Tarun Suresh,Daniel Mendoza,Thiago S. F. X. Teixeira,Ke Wang,Caroline Trippel,Alex Aiken*

Main category: cs.AR

TLDR: VERICODER是一个针对RTL代码生成的模型，通过功能验证的数据集微调，显著提升了功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有RTL数据集多关注语法有效性而非功能验证，导致生成的代码可能不符合预期行为。

Method: 采用结合单元测试生成和反馈导向优化的方法，构建功能验证的数据集，并基于此微调模型。

Result: VERICODER在VerilogEval和RTLLM上功能正确性指标达到最优，相对提升高达71.7%和27.4%。

Conclusion: 功能验证的高质量数据集对RTL代码生成至关重要。

Abstract: Recent advances in Large Language Models (LLMs) have sparked growing interest
in applying them to Electronic Design Automation (EDA) tasks, particularly
Register Transfer Level (RTL) code generation. While several RTL datasets have
been introduced, most focus on syntactic validity rather than functional
validation with tests, leading to training examples that compile but may not
implement the intended behavior. We present VERICODER, a model for RTL code
generation fine-tuned on a dataset validated for functional correctness. This
fine-tuning dataset is constructed using a novel methodology that combines unit
test generation with feedback-directed refinement. Given a natural language
specification and an initial RTL design, we prompt a teacher model
(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design
based on its simulation results using the generated tests. If necessary, the
teacher model also updates the tests to ensure they comply with the natural
language specification. As a result of this process, every example in our
dataset is functionally validated, consisting of a natural language
description, an RTL implementation, and passing tests. Fine-tuned on this
dataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics
in functional correctness on VerilogEval and RTLLM, with relative gains of up
to 71.7% and 27.4% respectively. An ablation study further shows that models
trained on our functionally validated dataset outperform those trained on
functionally non-validated datasets, underscoring the importance of
high-quality datasets in RTL code generation.

</details>

### [255] [Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback](https://arxiv.org/abs/2504.15804)
*Ning Wang,Bingkun Yao,Jie Zhou,Yuchen Hu,Xi Wang,Nan Guan,Zhe Jiang*

Main category: cs.AR

TLDR: 论文提出了一种将验证反馈集成到Verilog生成LLM训练中的方法，通过自动生成测试平台并利用强化学习优化生成代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在Verilog生成中功能正确性不足，缺乏足够的验证数据（如测试平台），需改进。

Method: 引入自动测试平台生成流程，利用VCS反馈减少幻觉；采用强化学习（DPO）优化生成代码的功能正确性。

Result: 在多个基准测试中，方法显著优于现有技术，生成功能正确的Verilog代码。

Conclusion: 通过验证反馈和强化学习，方法有效提升了Verilog生成的功能正确性，开源了相关资源。

Abstract: Large language models (LLMs) have shown strong performance in Verilog
generation from natural language description. However, ensuring the functional
correctness of the generated code remains a significant challenge. This paper
introduces a method that integrates verification insights from testbench into
the training of Verilog generation LLMs, aligning the training with the
fundamental goal of hardware design: functional correctness. The main obstacle
in using LLMs for Verilog code generation is the lack of sufficient functional
verification data, particularly testbenches paired with design specifications
and code. To address this problem, we introduce an automatic testbench
generation pipeline that decomposes the process and uses feedback from the
Verilog compiler simulator (VCS) to reduce hallucination and ensure
correctness. We then use the testbench to evaluate the generated codes and
collect them for further training, where verification insights are introduced.
Our method applies reinforcement learning (RL), specifically direct preference
optimization (DPO), to align Verilog code generation with functional
correctness by training preference pairs based on testbench outcomes. In
evaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,
and VerilogEval v2, our approach consistently outperforms state-of-the-art
baselines in generating functionally correct Verilog code. We open source all
training code, data, and models at
https://anonymous.4open.science/r/VeriPrefer-E88B.

</details>

<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [256] [Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE Problem using Soft Actor-Critic Reinforcement Learning](https://arxiv.org/abs/2504.15679)
*Brandon Panos,Ivan Milic*

Main category: astro-ph.SR

TLDR: 提出了一种基于强化学习的无Lambda算子方法，用于解决非局部热动平衡辐射传输问题，通过奖励机制优化策略，无需预计算数据集或复杂的梯度回传。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要构建近似Lambda算子（Λ*），且依赖预计算数据集或可微求解器，复杂场景下难以实现。

Method: 将问题建模为控制任务，强化学习代理通过奖励机制学习深度依赖的源函数S(τ)，无需显式知识或Λ*构造。

Result: 实验表明，传统贪婪训练的前馈神经网络无法解决问题，而提出的Λ*-Free方法在复杂场景中具有潜力。

Conclusion: 该方法为太阳物理中实现统计平衡提供了一种新的加速形式，首次直接应用强化学习解决基础物理约束。

Abstract: We present a novel reinforcement learning (RL) approach for solving the
classical 2-level atom non-LTE radiative transfer problem by framing it as a
control task in which an RL agent learns a depth-dependent source function
$S(\tau)$ that self-consistently satisfies the equation of statistical
equilibrium (SE). The agent's policy is optimized entirely via reward-based
interactions with a radiative transfer engine, without explicit knowledge of
the ground truth. This method bypasses the need for constructing approximate
lambda operators ($\Lambda^*$) common in accelerated iterative schemes.
Additionally, it requires no extensive precomputed labeled datasets to extract
a supervisory signal, and avoids backpropagating gradients through the complex
RT solver itself. Finally, we show through experiment that a simple feedforward
neural network trained greedily cannot solve for SE, possibly due to the moving
target nature of the problem. Our $\Lambda^*-\text{Free}$ method offers
potential advantages for complex scenarios (e.g., atmospheres with enhanced
velocity fields, multi-dimensional geometries, or complex microphysics) where
$\Lambda^*$ construction or solver differentiability is challenging.
Additionally, the agent can be incentivized to find more efficient policies by
manipulating the discount factor, leading to a reprioritization of immediate
rewards. If demonstrated to generalize past its training data, this RL
framework could serve as an alternative or accelerated formalism to achieve SE.
To the best of our knowledge, this study represents the first application of
reinforcement learning in solar physics that directly solves for a fundamental
physical constraint.

</details>

<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [257] [Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data](https://arxiv.org/abs/2504.15448)
*Yanampally Abhiram Reddy,Siddhi Agarwal,Vikram Parashar,Arshiya Arora*

Main category: econ.GN

TLDR: 本文提出了一种结合NLP和机器学习的实时企业声誉监测系统，通过混合情感检测框架分析社交媒体数据，揭示了不同企业的公众情感差异。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体时代，了解公众对企业的情感对投资者、政策制定者和研究者至关重要。

Method: 采用混合情感检测框架（VADER和DistilBERT），结合预处理和集成分类方法，分析多平台社交媒体数据。

Result: 结果显示企业间情感差异显著，如亚马逊（81.2）和三星（45.8）情感积极，微软（21.7）和沃尔玛（21.9）情感消极。

Conclusion: 该框架为利益相关者提供了基于全面情感分析的可操作见解，支持战略决策。

Abstract: In the age of social media, understanding public sentiment toward major
corporations is crucial for investors, policymakers, and researchers. This
paper presents a comprehensive sentiment analysis system tailored for corporate
reputation monitoring, combining Natural Language Processing (NLP) and machine
learning techniques to accurately interpret public opinion in real time. The
methodology integrates a hybrid sentiment detection framework leveraging both
rule-based models (VADER) and transformer-based deep learning models
(DistilBERT), applied to social media data from multiple platforms. The system
begins with robust preprocessing involving noise removal and text
normalization, followed by sentiment classification using an ensemble approach
to ensure both interpretability and contextual accuracy. Results are visualized
through sentiment distribution plots, comparative analyses, and temporal
sentiment trends for enhanced interpretability. Our analysis reveals
significant disparities in public sentiment across major corporations, with
companies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment
scores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment
profiles. These findings demonstrate the utility of our multi-source sentiment
framework in providing actionable insights regarding corporate public
perception, enabling stakeholders to make informed strategic decisions based on
comprehensive sentiment analysis.

</details>

<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [258] [LithOS: An Operating System for Efficient Machine Learning on GPUs](https://arxiv.org/abs/2504.15465)
*Patrick H. Coppock,Brian Zhang,Eliot H. Solomon,Vasilis Kypriotis,Leon Yang,Bikash Sharma,Dan Schatzberg,Todd C. Mowry,Dimitrios Skarlatos*

Main category: cs.OS

TLDR: LithOS是一个面向GPU的操作系统，通过新颖的调度和资源管理机制，显著提升了GPU利用率和能源效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习对GPU需求的激增，如何在满足多样化需求的同时优化资源利用成为挑战。LithOS旨在实现透明、细粒度的GPU管理。

Method: LithOS引入了TPC调度器、透明内核原子化、硬件资源动态调整和轻量级电源管理机制。

Result: 在推理和混合任务中，LithOS显著降低了尾延迟并提高了吞吐量，同时节省了GPU资源和能源。

Conclusion: LithOS为GPU操作系统研究奠定了基础，显著提升了GPU效率。

Abstract: The surging demand for GPUs in datacenters for machine learning (ML) has made
efficient GPU utilization crucial. However, meeting the diverse needs of ML
models while optimizing resource usage is challenging. To enable transparent,
fine-grained GPU management that maximizes utilization and energy efficiency
while maintaining strong isolation, an operating system (OS) approach is
needed. This paper introduces LithOS, a first step toward a GPU OS. LithOS
includes the following new abstractions and mechanisms for efficient GPU
resource management: (i) a novel TPC Scheduler that supports spatial scheduling
at the granularity of individual TPCs, unlocking efficient TPC stealing between
workloads; (ii) transparent kernel atomization to reduce head-of-line blocking
and enable dynamic resource reallocation mid-execution; (iii) a lightweight
hardware right-sizing mechanism that determines the minimal TPC resources
needed per atom; and (iv) a transparent power management mechanism that reduces
power consumption based on in-flight work behavior. We implement LithOS in Rust
and evaluate its performance across extensive ML environments, comparing it to
state-of-the-art solutions from NVIDIA and prior research. For inference
stacking, LithOS reduces tail latencies by 13x compared to MPS; compared to the
best SotA, it reduces tail latencies by 3x while improving aggregate throughput
by 1.6x. In hybrid inference-training stacking, LithOS reduces tail latencies
by 4.7x compared to MPS; compared to the best SotA, it reduces tail latencies
1.18x while improving aggregate throughput by 1.35x. Finally, for a modest
performance hit under 4%, LithOS's right-sizing provides a quarter of GPU
capacity savings on average, while for a 7% hit, its power management yields a
quarter of a GPU's energy savings. Overall, LithOS increases GPU efficiency,
establishing a foundation for future OS research on GPUs.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [259] [SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow](https://arxiv.org/abs/2504.09697)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.GR

TLDR: SPICE是一种无需训练的流程，结合扩散模型和Canny边缘ControlNet模型，支持高分辨率、多步编辑，并在复杂编辑任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的图像编辑模型在局部编辑、详细提示遵循和多步编辑质量上表现不足。

Method: 结合基础扩散模型和Canny边缘ControlNet模型，实现自由形式编辑。

Result: 在语义、风格和结构编辑任务中表现优于现有方法，用户评价最高。

Conclusion: SPICE为图像编辑提供了高效、灵活的解决方案，支持进一步研究和艺术探索。

Abstract: Recent prompt-based image editing models have demonstrated impressive
prompt-following capability at structural editing tasks. However, existing
models still fail to perform local edits, follow detailed editing prompts, or
maintain global image quality beyond a single editing step. To address these
challenges, we introduce SPICE, a training-free workflow that accepts arbitrary
resolutions and aspect ratios, accurately follows user requirements, and
improves image quality consistently during more than 100 editing steps. By
synergizing the strengths of a base diffusion model and a Canny edge ControlNet
model, SPICE robustly handles free-form editing instructions from the user.
SPICE outperforms state-of-the-art baselines on a challenging realistic
image-editing dataset consisting of semantic editing (object addition, removal,
replacement, and background change), stylistic editing (texture changes), and
structural editing (action change) tasks. Not only does SPICE achieve the
highest quantitative performance according to standard evaluation metrics, but
it is also consistently preferred by users over existing image-editing methods.
We release the workflow implementation for popular diffusion model Web UIs to
support further research and artistic exploration.

</details>

### [260] [Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation](https://arxiv.org/abs/2504.15329)
*Yike Zhang,Eduardo Davalos,Jack Noble*

Main category: cs.GR

TLDR: 本文提出了一种交互式3D到2D可视化与标注工具Vision6D，用于支持6D姿态估计研究，填补了2D场景投影与3D场景之间的技术空白。


<details>
  <summary>Details</summary>
Motivation: 6D姿态估计在机器人辅助任务中至关重要，但现有工具缺乏交互式3D到2D可视化与标注功能，Vision6D旨在解决这一问题。

Method: 开发了Vision6D工具，支持用户通过视觉提示和空间关系在2D场景中交互式标注3D对象的6D姿态，仅需相机内参矩阵即可完成未知变换矩阵的标注。

Result: 通过Linemod和HANDAL数据集验证，Vision6D生成的标注与真实姿态一致，用户研究表明其界面直观且标注准确。

Conclusion: Vision6D为6D姿态估计研究提供了高效工具，其开源特性将进一步推动相关领域的发展。

Abstract: Accurate 6D pose estimation has gained more attention over the years for
robotics-assisted tasks that require precise interaction with physical objects.
This paper presents an interactive 3D-to-2D visualization and annotation tool
to support the 6D pose estimation research community. To the best of our
knowledge, the proposed work is the first tool that allows users to visualize
and manipulate 3D objects interactively on a 2D real-world scene, along with a
comprehensive user study. This system supports robust 6D camera pose annotation
by providing both visual cues and spatial relationships to determine object
position and orientation in various environments. The annotation feature in
Vision6D is particularly helpful in scenarios where the transformation matrix
between the camera and world objects is unknown, as it enables accurate
annotation of these objects' poses using only the camera intrinsic matrix. This
capability serves as a foundational step in developing and training advanced
pose estimation models across various domains. We evaluate Vision6D's
effectiveness by utilizing widely-used open-source pose estimation datasets
Linemod and HANDAL through comparisons between the default ground-truth camera
poses with manual annotations. A user study was performed to show that Vision6D
generates accurate pose annotations via visual cues in an intuitive 3D user
interface. This approach aims to bridge the gap between 2D scene projections
and 3D scenes, offering an effective way for researchers and developers to
solve 6D pose annotation related problems. The software is open-source and
publicly available at https://github.com/InteractiveGL/vision6D.

</details>

### [261] [Neural Kinematic Bases for Fluids](https://arxiv.org/abs/2504.15657)
*Yibo Liu,Paul Kry,Kenny Erleben,Noam Aigerman,Sune Darkner,Teseo Schneider*

Main category: cs.GR

TLDR: 提出了一种基于MLP表示的无网格流体模拟方法，通过设计损失函数确保神经基满足正交性、无散度、边界对齐和平滑性等物理特性。


<details>
  <summary>Details</summary>
Motivation: 通过神经基拟合输入流场草图，并继承其物理特性，实现实时动画。

Method: 设计损失函数确保神经基满足正交性、无散度、边界对齐和平滑性，利用标准时间积分器实现实时动画。

Result: 神经基适用于不同域，并可自然扩展到三维。

Conclusion: 该方法通过神经基实现了高效且物理准确的流体模拟。

Abstract: We propose mesh-free fluid simulations that exploit a kinematic neural basis
for velocity fields represented by an MLP. We design a set of losses that
ensures that these neural bases satisfy fundamental physical properties such as
orthogonality, divergence-free, boundary alignment, and smoothness. Our neural
bases can then be used to fit an input sketch of a flow, which will inherit the
same fundamental properties from the bases. We then can animate such flow in
real-time using standard time integrators. Our neural bases can accommodate
different domains and naturally extend to three dimensions.

</details>

### [262] [Low-Rank Adaptation of Neural Fields](https://arxiv.org/abs/2504.15933)
*Anh Truong,Ahmed H. Mahmoud,Mina Konaković Luković,Justin Solomon*

Main category: cs.GR

TLDR: 提出了一种基于低秩适应（LoRA）的参数高效策略，用于更新神经场（NF），适用于低计算硬件。


<details>
  <summary>Details</summary>
Motivation: 神经场的小变化编码问题未得到充分研究，现有图形技术（如法线贴图和视频压缩）利用冗余高效编码小变化，但神经场的类似方法较少。

Method: 将LoRA方法从参数高效微调LLM社区引入，适应于实例特定的神经场，避免需要大型预训练模型。

Result: 在图像滤波、视频压缩和几何编辑实验中验证了方法的有效性和多功能性。

Conclusion: LoRA策略为神经场的小变化编码提供了一种高效且通用的解决方案。

Abstract: Processing visual data often involves small adjustments or sequences of
changes, such as in image filtering, surface smoothing, and video storage.
While established graphics techniques like normal mapping and video compression
exploit redundancy to encode such small changes efficiently, the problem of
encoding small changes to neural fields (NF) -- neural network
parameterizations of visual or physical functions -- has received less
attention.
  We propose a parameter-efficient strategy for updating neural fields using
low-rank adaptations (LoRA). LoRA, a method from the parameter-efficient
fine-tuning LLM community, encodes small updates to pre-trained models with
minimal computational overhead. We adapt LoRA to instance-specific neural
fields, avoiding the need for large pre-trained models yielding a pipeline
suitable for low-compute hardware.
  We validate our approach with experiments in image filtering, video
compression, and geometry editing, demonstrating its effectiveness and
versatility for representing neural field updates.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [263] [Real-Time Optimal Design of Experiment for Parameter Identification of Li-Ion Cell Electrochemical Model](https://arxiv.org/abs/2504.15578)
*Ian Mikesell,Samuel Filgueira da Silva,Mehmet Fatih Ozkan,Faissal El Idrissi,Prashanth Ramesh,Marcello Canova*

Main category: eess.SY

TLDR: 本文提出了一种基于强化学习（RL）的方法，用于动态优化锂离子电池（LiB）电化学模型的参数识别，通过硬件在环（HIL）验证，证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统参数识别方法需要大量数据收集且缺乏动态环境适应性，因此需要更高效、自适应的解决方案。

Method: 采用强化学习动态调整电流曲线，优化参数可识别性，并通过HIL实时实现。

Result: RL方法在减少建模误差和缩短实验时间方面优于传统测试协议。

Conclusion: RL框架为LiB参数识别提供了一种高效且适应性强的解决方案。

Abstract: Accurately identifying the parameters of electrochemical models of li-ion
battery (LiB) cells is a critical task for enhancing the fidelity and
predictive ability. Traditional parameter identification methods often require
extensive data collection experiments and lack adaptability in dynamic
environments. This paper describes a Reinforcement Learning (RL) based approach
that dynamically tailors the current profile applied to a LiB cell to optimize
the parameters identifiability of the electrochemical model. The proposed
framework is implemented in real-time using a Hardware-in-the-Loop (HIL) setup,
which serves as a reliable testbed for evaluating the RL-based design strategy.
The HIL validation confirms that the RL-based experimental design outperforms
conventional test protocols used for parameter identification in terms of both
reducing the modeling errors on a verification test and minimizing the duration
of the experiment used for parameter identification.

</details>

<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [264] [Fluorescence Reference Target Quantitative Analysis Library](https://arxiv.org/abs/2504.15496)
*Eammon A. Littler,Emmanuel A. Mannoh,Ethan P. M. LaRochelle*

Main category: physics.med-ph

TLDR: QUEL-QAL是一个开源的Python库，旨在标准化荧光成像系统的性能评估，支持关键指标分析并提升透明度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 荧光引导手术（FGS）领域缺乏标准化的性能评估工具，现有方法有限且不一致。

Method: 开发了QUEL-QAL库，提供模块化工作流，支持ROI检测、统计分析和可视化，兼容Python生态系统。

Result: QUEL-QAL支持响应线性、检测限、深度敏感性和空间分辨率等关键指标，符合监管和学术要求。

Conclusion: QUEL-QAL为荧光成像系统的标准化评估提供了基础工具，促进透明度和开发效率。

Abstract: Standardized performance evaluation of fluorescence imaging systems remains a
critical unmet need in the field of fluorescence-guided surgery (FGS). While
the American Association of Physicists in Medicine (AAPM) TG311 report and
recent FDA draft guidance provide recommended metrics for system
characterization, practical tools for extracting these metrics remain limited,
inconsistent, and often inaccessible. We present QUEL-QAL, an open-source
Python library designed to streamline and standardize the quantitative analysis
of fluorescence images using solid reference targets. The library provides a
modular, reproducible workflow that includes region of interest (ROI)
detection, statistical analysis, and visualization capabilities. QUEL-QAL
supports key metrics such as response linearity, limit of detection, depth
sensitivity, and spatial resolution, in alignment with regulatory and academic
guidance. Built on widely adopted Python packages, the library is designed to
be extensible, enabling users to adapt it to novel target designs and analysis
protocols. By promoting transparency, reproducibility, and regulatory
alignment, QUEL-QAL offers a foundational tool to support standardized
benchmarking and accelerate the development and evaluation of fluorescence
imaging systems.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [265] [A Graph Based Raman Spectral Processing Technique for Exosome Classification](https://arxiv.org/abs/2504.15324)
*Vuong M. Ngo,Edward Bolger,Stan Goodwin,John O'Sullivan,Dinh Viet Cuong,Mark Roantree*

Main category: q-bio.QM

TLDR: 该论文提出了一种基于图数据库和新型光谱过滤方法的技术，用于提高拉曼光谱对胞外囊泡的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 胞外囊泡的复杂性需要综合分析方法，而传统拉曼光谱在灵敏度和样本浓度方面存在局限性。

Method: 利用Neo4j图数据库组织拉曼光谱数据，结合PageRank过滤器和最优降维技术进行光谱分析。

Result: 使用Extra Trees模型在分类高血糖、低血糖和正常样本时，准确率分别达到0.76和0.857。

Conclusion: 图数据库结合光谱过滤和降维技术显著提高了分类准确性，为生物医学应用提供了新工具。

Abstract: Exosomes are small vesicles crucial for cell signaling and disease
biomarkers. Due to their complexity, an "omics" approach is preferable to
individual biomarkers. While Raman spectroscopy is effective for exosome
analysis, it requires high sample concentrations and has limited sensitivity to
lipids and proteins. Surface-enhanced Raman spectroscopy helps overcome these
challenges. In this study, we leverage Neo4j graph databases to organize 3,045
Raman spectra of exosomes, enhancing data generalization. To further refine
spectral analysis, we introduce a novel spectral filtering process that
integrates the PageRank Filter with optimal Dimensionality Reduction. This
method improves feature selection, resulting in superior classification
performance. Specifically, the Extra Trees model, using our spectral processing
approach, achieves 0.76 and 0.857 accuracy in classifying hyperglycemic,
hypoglycemic, and normal exosome samples based on Raman spectra and surface,
respectively, with group 10-fold cross-validation. Our results show that
graph-based spectral filtering combined with optimal dimensionality reduction
significantly improves classification accuracy by reducing noise while
preserving key biomarker signals. This novel framework enhances Raman-based
exosome analysis, expanding its potential for biomedical applications, disease
diagnostics, and biomarker discovery.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [266] [Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming](https://arxiv.org/abs/2504.15440)
*Andrey Fradkin*

Main category: cs.CY

TLDR: 论文总结了大型语言模型（LLM）需求的三个特点：快速初始采用、模型发布吸引用户类型不同、多模型同时使用普遍。


<details>
  <summary>Details</summary>
Motivation: 研究LLM市场的需求动态，揭示模型发布和用户行为的特点。

Method: 使用OpenRouter（一个LLM市场）的数据进行分析。

Result: 发现LLM市场存在显著的水平与垂直差异化，提供商有机会维持需求和定价权。

Conclusion: 尽管技术快速发展，LLM市场仍存在差异化机会。

Abstract: This paper documents three stylized facts about the demand for Large Language
Models (LLMs) using data from OpenRouter, a prominent LLM marketplace. First,
new models experience rapid initial adoption that stabilizes within weeks.
Second, model releases differ substantially in whether they primarily attract
new users or substitute demand from competing models. Third, multihoming, using
multiple models simultaneously, is common among apps. These findings suggest
significant horizontal and vertical differentiation in the LLM market, implying
opportunities for providers to maintain demand and pricing power despite rapid
technological advances.

</details>

### [267] [Trends in AI Supercomputers](https://arxiv.org/abs/2504.16026)
*Konstantin F. Pilz,James Sanders,Robi Rahman,Lennart Heim*

Main category: cs.CY

TLDR: 论文分析了2019至2025年间500台AI超级计算机的数据，发现计算性能每9个月翻倍，硬件成本和功耗每年翻倍。到2030年，领先的AI超级计算机性能将达2×10²² FLOP/s，硬件成本2000亿美元，功耗9 GW。


<details>
  <summary>Details</summary>
Motivation: 研究AI超级计算机的发展趋势，为政策制定者提供资源需求、所有权和国家竞争力的评估依据。

Method: 创建并分析2019至2025年间500台AI超级计算机的数据集，涵盖性能、功耗、硬件成本、所有权和全球分布。

Result: 计算性能每9个月翻倍，硬件成本和功耗每年翻倍；美国占全球性能的75%，中国占15%；2030年领先系统性能将达2×10²² FLOP/s。

Conclusion: AI超级计算机正从科研工具发展为工业机器，公司份额增加，政府和学术界份额减少；趋势持续将带来巨大资源需求。

Abstract: Frontier AI development relies on powerful AI supercomputers, yet analysis of
these systems is limited. We create a dataset of 500 AI supercomputers from
2019 to 2025 and analyze key trends in performance, power needs, hardware cost,
ownership, and global distribution. We find that the computational performance
of AI supercomputers has doubled every nine months, while hardware acquisition
cost and power needs both doubled every year. The leading system in March 2025,
xAI's Colossus, used 200,000 AI chips, had a hardware cost of \$7B, and
required 300 MW of power, as much as 250,000 households. As AI supercomputers
evolved from tools for science to industrial machines, companies rapidly
expanded their share of total AI supercomputer performance, while the share of
governments and academia diminished. Globally, the United States accounts for
about 75% of total performance in our dataset, with China in second place at
15%. If the observed trends continue, the leading AI supercomputer in 2030 will
achieve $2\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a
hardware cost of \$200 billion, and require 9 GW of power. Our analysis
provides visibility into the AI supercomputer landscape, allowing policymakers
to assess key AI trends like resource needs, ownership, and national
competitiveness.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [268] [Trustworthy Decentralized Autonomous Machines: A New Paradigm in Automation Economy](https://arxiv.org/abs/2504.15676)
*Fernando Castillo,Oscar Castillo,Eduardo Brito,Simon Espinola*

Main category: cs.MA

TLDR: DAMs结合AI、区块链和物联网技术，实现去中心化自主经济代理，管理数字和物理资产，推动从信任经济到无信任经济的转变。


<details>
  <summary>Details</summary>
Motivation: 探索DAMs的技术基础和挑战，解决中心化系统的低效问题，减少财富差距，推动后劳动经济。

Method: 整合AI驱动的决策、物联网操作自主性和区块链治理，实现资源优化和去中心化所有权。

Result: DAMs提供可扩展、透明和公平的资产管理方案，为经济机会民主化提供可能。

Conclusion: DAMs在推动无信任经济模型、优化资源分配和减少财富差距方面具有重要潜力。

Abstract: Decentralized Autonomous Machines (DAMs) represent a transformative paradigm
in automation economy, integrating artificial intelligence (AI), blockchain
technology, and Internet of Things (IoT) devices to create self-governing
economic agents participating in Decentralized Physical Infrastructure Networks
(DePIN). Capable of managing both digital and physical assets and unlike
traditional Decentralized Autonomous Organizations (DAOs), DAMs extend autonomy
into the physical world, enabling trustless systems for Real and Digital World
Assets (RDWAs). In this paper, we explore the technological foundations, and
challenges of DAMs and argue that DAMs are pivotal in transitioning from
trust-based to trustless economic models, offering scalable, transparent, and
equitable solutions for asset management. The integration of AI-driven
decision-making, IoT-enabled operational autonomy, and blockchain-based
governance allows DAMs to decentralize ownership, optimize resource allocation,
and democratize access to economic opportunities. Therefore, in this research,
we highlight the potential of DAMs to address inefficiencies in centralized
systems, reduce wealth disparities, and foster a post-labor economy.

</details>

### [269] [A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations](https://arxiv.org/abs/2504.15301)
*Zoi Lygizou,Dimitris Kalles*

Main category: cs.MA

TLDR: 本文提出了一种基于生物启发的信任模型，通过本地存储信任数据和自我评估能力，解决了代理移动性、行为变化和冷启动问题。新算法通过自我分类机制检测性能下降，优于原始模型和FIRE模型。


<details>
  <summary>Details</summary>
Motivation: 现有信任模型在代理移动性、行为变化和冷启动问题上存在不足，需要更适应动态环境的解决方案。

Method: 引入生物启发的信任模型，结合自我分类机制的新算法，评估其性能和适应性。

Result: 新算法在动态行为处理上优于原始模型和FIRE模型，适应性更强。

Conclusion: 该模型在动态环境中表现优异，但仍需进一步研究以应对极端环境变化和潜在攻击。

Abstract: Trust management provides an alternative solution for securing open, dynamic,
and distributed multi-agent systems, where conventional cryptographic methods
prove to be impractical. However, existing trust models face challenges related
to agent mobility, changing behaviors, and the cold start problem. To address
these issues we introduced a biologically inspired trust model in which
trustees assess their own capabilities and store trust data locally. This
design improves mobility support, reduces communication overhead, resists
disinformation, and preserves privacy. Despite these advantages, prior
evaluations revealed limitations of our model in adapting to provider
population changes and continuous performance fluctuations. This study proposes
a novel algorithm, incorporating a self-classification mechanism for providers
to detect performance drops potentially harmful for the service consumers.
Simulation results demonstrate that the new algorithm outperforms its original
version and FIRE, a well-known trust and reputation model, particularly in
handling dynamic trustee behavior. While FIRE remains competitive under extreme
environmental changes, the proposed algorithm demonstrates greater adaptability
across various conditions. In contrast to existing trust modeling research,
this study conducts a comprehensive evaluation of our model using widely
recognized trust model criteria, assessing its resilience against common
trust-related attacks while identifying strengths, weaknesses, and potential
countermeasures. Finally, several key directions for future research are
proposed.

</details>

### [270] [The Formation of Production Networks: How Supply Chains Arise from Simple Learning with Minimal Information](https://arxiv.org/abs/2504.16010)
*Tuong Manh Vu,Ernesto Carrella,Robert Axtell,Omar A. Guerrero*

Main category: cs.MA

TLDR: 论文提出了一种模型，企业通过强化学习在不确定环境中定价、生产和采购，内生形成稳态生产网络，并适应各种冲击。


<details>
  <summary>Details</summary>
Motivation: 研究企业如何在不确定环境中通过学习和适应形成生产网络，无需依赖均衡或完全信息假设。

Method: 采用强化学习方法，企业根据异质性技术调整价格、产量和采购，内生形成生产网络。

Result: 模型展示了企业能够适应需求、生产力和技术冲击，重塑生产网络，并分析了上下游影响。

Conclusion: 该模型为研究生产网络的动态适应性和冲击传导提供了新视角。

Abstract: We develop a model where firms determine the price at which they sell their
differentiable goods, the volume that they produce, and the inputs (types and
amounts) that they purchase from other firms. A steady-state production network
emerges endogenously without resorting to assumptions such as equilibrium or
perfect knowledge about production technologies. Through a simple version of
reinforcement learning, firms with heterogeneous technologies cope with
uncertainty and maximize profits. Due to this learning process, firms can adapt
to shocks such as demand shifts, suppliers/clients closure, productivity
changes, and production technology modifications; effectively reshaping the
production network. To demonstrate the potential of this model, we analyze the
upstream and downstream impact of demand and productivity shocks.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [271] [Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software](https://arxiv.org/abs/2504.15549)
*Anjali Khurana,Xiaotian Su,April Yi Wang,Parmit K Chilana*

Main category: cs.HC

TLDR: 研究比较了全自动（AutoCopilot）和半自动（GuidedCopilot）助手在用户体验上的差异，发现半自动助手在用户控制、软件实用性和学习性上表现更优，尤其在探索性和创造性任务中。


<details>
  <summary>Details</summary>
Motivation: 探讨基于大语言模型的助手在软件任务中的最佳自动化水平，以优化用户体验。

Method: 设计并实现全自动和半自动助手，通过用户研究（N=20）比较其表现，随后改进半自动助手并验证（N=10）。

Result: 半自动助手在用户控制和学习性上表现更好，全自动助手在简单任务中更省时。改进后的半自动助手进一步提升了体验。

Conclusion: 用户控制和定制化指导是设计下一代助手的关键，需平衡自动化与用户参与。

Abstract: Large Language Model (LLM)-based in-application assistants, or copilots, can
automate software tasks, but users often prefer learning by doing, raising
questions about the optimal level of automation for an effective user
experience. We investigated two automation paradigms by designing and
implementing a fully automated copilot (AutoCopilot) and a semi-automated
copilot (GuidedCopilot) that automates trivial steps while offering
step-by-step visual guidance. In a user study (N=20) across data analysis and
visual design tasks, GuidedCopilot outperformed AutoCopilot in user control,
software utility, and learnability, especially for exploratory and creative
tasks, while AutoCopilot saved time for simpler visual tasks. A follow-up
design exploration (N=10) enhanced GuidedCopilot with task-and state-aware
features, including in-context preview clips and adaptive instructions. Our
findings highlight the critical role of user control and tailored guidance in
designing the next generation of copilots that enhance productivity, support
diverse skill levels, and foster deeper software engagement.

</details>

### [272] [iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment](https://arxiv.org/abs/2504.15743)
*Seung Gyu Jeong,Sung Woo Nam,Seong Kwan Jung,Seong-Eun Kim*

Main category: cs.HC

TLDR: 智能手机系统结合深度学习算法，通过内置麦克风检测儿童肺炎风险，无需昂贵设备，用户研究显示高分类性能和接受度。


<details>
  <summary>Details</summary>
Motivation: 在医生资源有限的地区，早期检测儿童肺炎具有挑战性，需要低成本且高效的解决方案。

Method: 利用智能手机内置麦克风和深度学习算法，结合电子听诊器和手机数据集，开发端到端框架进行呼吸音分析。

Result: 系统分类性能强，用户接受度高，能够帮助早期干预，减少儿童肺炎死亡。

Conclusion: 智能手机结合深度学习为远程儿科护理提供了公平且全面的解决方案。

Abstract: Respiratory auscultation is crucial for early detection of pediatric
pneumonia, a condition that can quickly worsen without timely intervention. In
areas with limited physician access, effective auscultation is challenging. We
present a smartphone-based system that leverages built-in microphones and
advanced deep learning algorithms to detect abnormal respiratory sounds
indicative of pneumonia risk. Our end-to-end deep learning framework employs
domain generalization to integrate a large electronic stethoscope dataset with
a smaller smartphone-derived dataset, enabling robust feature learning for
accurate respiratory assessments without expensive equipment. The accompanying
mobile application guides caregivers in collecting high-quality lung sound
samples and provides immediate feedback on potential pneumonia risks. User
studies show strong classification performance and high acceptance,
demonstrating the system's ability to facilitate proactive interventions and
reduce preventable childhood pneumonia deaths. By seamlessly integrating into
ubiquitous smartphones, this approach offers a promising avenue for more
equitable and comprehensive remote pediatric care.

</details>

### [273] [Supporting Data-Frame Dynamics in AI-assisted Decision Making](https://arxiv.org/abs/2504.15894)
*Chengbo Zheng,Tim Miller,Alina Bialkowski,H Peter Soyer,Monika Janda*

Main category: cs.HC

TLDR: 提出了一种基于数据框架理论和评估AI范式的混合主动框架，支持人类与AI协作构建、验证和调整假设，并通过皮肤癌诊断原型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI决策支持系统未能很好地支持动态证据与假设的交互，需要一种更灵活的方法。

Method: 采用混合主动框架，结合数据框架理论和评估AI范式，利用概念瓶颈模型实现可解释的交互和动态假设更新。

Result: 通过皮肤癌诊断原型展示了框架的有效性，支持人类与AI协作决策。

Conclusion: 该框架为高风险决策提供了动态协作支持，具有实际应用潜力。

Abstract: High stakes decision-making often requires a continuous interplay between
evolving evidence and shifting hypotheses, a dynamic that is not well supported
by current AI decision support systems. In this paper, we introduce a
mixed-initiative framework for AI assisted decision making that is grounded in
the data-frame theory of sensemaking and the evaluative AI paradigm. Our
approach enables both humans and AI to collaboratively construct, validate, and
adapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer
diagnosis prototype that leverages a concept bottleneck model to facilitate
interpretable interactions and dynamic updates to diagnostic hypotheses.

</details>

### [274] [Recent Advances and Future Directions in Extended Reality (XR): Exploring AI-Powered Spatial Intelligence](https://arxiv.org/abs/2504.15970)
*Baichuan Zeng*

Main category: cs.HC

TLDR: 本文回顾了扩展现实（XR）技术的发展，包括硬件和软件的演进，分析了当前最先进的XR产品，并探讨了未来方向，如多模态AI和物联网驱动的数字孪生技术。


<details>
  <summary>Details</summary>
Motivation: XR技术作为连接物理和虚拟世界的桥梁，具有广泛的应用潜力，未来将无处不在。本文旨在探讨其发展现状和未来趋势。

Method: 通过分析XR的硬件（如显示器和传感器）和软件（如视觉任务和用户界面）框架，比较和评估当前最先进的XR产品性能。

Result: 研究发现，商业XR设备在空间智能方面支持高质量性能的需求，但仍需进一步优化。

Conclusion: 未来XR的发展应聚焦于多模态AI和数字孪生技术的集成，以实现适应性更强的XR系统，并通过空间智能创造更真实的数字空间。

Abstract: Extended Reality (XR), encompassing Augmented Reality (AR), Virtual Reality
(VR) and Mixed Reality (MR), is a transformative technology bridging the
physical and virtual world and it has diverse potential which will be
ubiquitous in the future. This review examines XR's evolution through
foundational framework - hardware ranging from monitors to sensors and software
ranging from visual tasks to user interface; highlights state of the art (SOTA)
XR products with the comparison and analysis of performance based on their
foundational framework; discusses how commercial XR devices can support the
demand of high-quality performance focusing on spatial intelligence. For future
directions, attention should be given to the integration of multi-modal AI and
IoT-driven digital twins to enable adaptive XR systems. With the concept of
spatial intelligence, future XR should establish a new digital space with
realistic experience that benefits humanity. This review underscores the
pivotal role of AI in unlocking XR as the next frontier in human-computer
interaction.

</details>

### [275] [Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support](https://arxiv.org/abs/2504.16021)
*Dinithi Dissanayake,Suranga Nanayakkara*

Main category: cs.HC

TLDR: 论文提出了一种上下文感知的认知增强框架，通过动态调整AI干预以维持或恢复认知流，提升复杂决策中的沉浸感。


<details>
  <summary>Details</summary>
Motivation: 在AI增强推理中，传统干预可能破坏认知流状态，从而阻碍决策效果。

Method: 利用多模态行为线索（如注视行为、输入犹豫、交互速度）动态调整干预，基于类型、时机和规模三个关键因素。

Result: 提出认知流概念，实现个性化、自适应且最小干扰的AI支持。

Conclusion: 上下文感知的增强方法优于静态干预，能更好地支持复杂决策中的深度沉浸。

Abstract: Flow theory describes an optimal cognitive state where individuals experience
deep focus and intrinsic motivation when a task's difficulty aligns with their
skill level. In AI-augmented reasoning, interventions that disrupt the state of
cognitive flow can hinder rather than enhance decision-making. This paper
proposes a context-aware cognitive augmentation framework that adapts
interventions based on three key contextual factors: type, timing, and scale.
By leveraging multimodal behavioral cues (e.g., gaze behavior, typing
hesitation, interaction speed), AI can dynamically adjust cognitive support to
maintain or restore flow. We introduce the concept of cognitive flow, an
extension of flow theory in AI-augmented reasoning, where interventions are
personalized, adaptive, and minimally intrusive. By shifting from static
interventions to context-aware augmentation, our approach ensures that AI
systems support deep engagement in complex decision-making and reasoning
without disrupting cognitive immersion.

</details>